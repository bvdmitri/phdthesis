@book{mcbook,
   author = {Art B. Owen},
   year = 2013,
   title = {Monte Carlo theory, methods and examples}
}

@incollection{helmholtz66,
  added-at = {2011-05-09T23:10:52.000+0200},
  address = {New York},
  author = {Helmholtz, H. {von}},
  biburl = {https://www.bibsonomy.org/bibtex/2c6867d7d01b73a493f7192c0105ea725/josephausterwei},
  booktitle = {Treatise on physiological optics},
  editor = {Southall, J. P. C.},
  interhash = {e76315511ac52ea84200ae966a39581f},
  intrahash = {c6867d7d01b73a493f7192c0105ea725},
  keywords = {imported},
  publisher = {Dover},
  timestamp = {2011-05-10T10:42:42.000+0200},
  title = {Concerning the perceptions in general},
  volume = 3,
  year = {1866/1962}
}

@incollection{meyering_helmholtzs_1989,
	address = {Dordrecht},
	title = {Helmholtz’s {Theory} of {Unconscious} {Inferences}},
	isbn = {978-94-010-7592-3 978-94-009-2423-9},
	url = {http://link.springer.com/10.1007/978-94-009-2423-9_10},
	language = {en},
	urldate = {2023-07-13},
	booktitle = {Historical {Roots} of {Cognitive} {Science}},
	publisher = {Springer Netherlands},
	author = {Meyering, Theo C.},
	collaborator = {Meyering, Theo C.},
	year = {1989},
	doi = {10.1007/978-94-009-2423-9_10},
	pages = {181--208},
}

@article{gregory_perceptions_1980,
	title = {Perceptions as hypotheses},
	volume = {290},
	issn = {0080-4622, 2054-0280},
        author = {Gregory, R. L.},
	url = {https://royalsocietypublishing.org/doi/10.1098/rstb.1980.0090},
	doi = {10.1098/rstb.1980.0090},
	abstract = {Perceptions may be compared with hypotheses in science. The methods of acquiring scientific knowledge provide a working paradigm for investigating processes of perception. Much as the information channels of instruments, such as radio telescopes, transmit signals which are processed according to various assumptions to give useful data, so neural signals are processed to give data for perception. To understand perception, the signal codes and the stored knowledge or assumptions used for deriving perceptual hypotheses must be discovered. Systematic perceptual errors are important clues for appreciating signal channel limitations, and for discovering hypothesis-generating procedures. Although this distinction between ‘physiological’ and ‘cognitive’ aspects of perception may be logically clear, it is in practice surprisingly difficult to establish which are responsible even for clearly established phenomena such as the classical distortion illusions. Experimental results are presented, aimed at distinguishing between and discovering what happens when there is mismatch with the neural signal channel, and when neural signals are processed inappropriately for the current situation. This leads us to make some distinctions between perceptual and scientific hypotheses, which raise in a new form the problem: What are ‘objects’?},
	language = {en},
	number = {1038},
	urldate = {2023-07-13},
	journal = {Philosophical Transactions of the Royal Society of London. B, Biological Sciences},
	month = jul,
	year = {1980},
	pages = {181--197},
}

@article{beck_if-problem_1994,
	title = {The if-problem in automatic differentiation},
	volume = {50},
	issn = {03770427},
	url = {https://linkinghub.elsevier.com/retrieve/pii/0377042794902941},
	doi = {10.1016/0377-0427(94)90294-1},
	language = {en},
	number = {1-3},
	urldate = {2023-07-17},
	journal = {Journal of Computational and Applied Mathematics},
	author = {Beck, Thomas and Fischer, Herbert},
	month = may,
	year = {1994},
	pages = {119--131},
}

@article{morita_determining_2008,
	title = {Determining the effective sample size of a parametric prior},
	volume = {64},
	issn = {1541-0420},
	doi = {10.1111/j.1541-0420.2007.00888.x},
	abstract = {We present a definition for the effective sample size of a parametric prior distribution in a Bayesian model, and propose methods for computing the effective sample size in a variety of settings. Our approach first constructs a prior chosen to be vague in a suitable sense, and updates this prior to obtain a sequence of posteriors corresponding to each of a range of sample sizes. We then compute a distance between each posterior and the parametric prior, defined in terms of the curvature of the logarithm of each distribution, and the posterior minimizing the distance defines the effective sample size of the prior. For cases where the distance cannot be computed analytically, we provide a numerical approximation based on Monte Carlo simulation. We provide general guidelines for application, illustrate the method in several standard cases where the answer seems obvious, and then apply it to some nonstandard settings.},
	language = {eng},
	number = {2},
	journal = {Biometrics},
	author = {Morita, Satoshi and Thall, Peter F. and Müller, Peter},
	month = jun,
	year = {2008},
	pmid = {17764481},
	pmcid = {PMC3081791},
	keywords = {Bayes Theorem, Biometry, Computer Simulation, Data Interpretation, Statistical, Decision Support Techniques, Models, Statistical, Sample Size},
	pages = {595--602},
}

@article{bagaev_reactive_2023,
	title = {Reactive {Message} {Passing} for {Scalable} {Bayesian} {Inference}},
	volume = {2023},
	issn = {1875-919X, 1058-9244},
	url = {https://www.hindawi.com/journals/sp/2023/6601690/},
	doi = {10.1155/2023/6601690},
	abstract = {We introduce reactive message passing (RMP) as a framework for executing schedule-free, scalable, and, potentially, more robust message passing-based inference in a factor graph representation of a probabilistic model. RMP is based on the reactive programming style, which only describes how nodes in a factor graph react to changes in connected nodes. We recognize reactive programming as the suitable programming abstraction for message passing-based methods that improve robustness, scalability, and execution time of the inference procedure and are useful for all future implementations of message passing methods. We also present our own implementation ReactiveMP.jl, which is a Julia package for realizing RMP through minimization of a constrained Bethe free energy. By user-defined specification of local form and factorization constraints on the variational posterior distribution, ReactiveMP.jl executes hybrid message passing algorithms including belief propagation, variational message passing, expectation propagation, and expectation maximization update rules. Experimental results demonstrate the great performance of our RMP implementation compared to other Julia packages for Bayesian inference across a range of probabilistic models. In particular, we show that the RMP framework is capable of performing Bayesian inference for large-scale probabilistic state-space models with hundreds of thousands of random variables on a standard laptop computer.},
	language = {en},
	urldate = {2023-07-21},
	journal = {Scientific Programming},
	author = {Bagaev, Dmitry and De Vries, Bert},
	editor = {Peña, Antonio J.},
	month = may,
	year = {2023},
	pages = {1--26},
}

@article{lunn_bugs_2009,
	title = {The {BUGS} project: {Evolution}, critique and future directions: {THE} {BUGS} {PROJECT}},
	volume = {28},
	issn = {02776715},
	shorttitle = {The {BUGS} project},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/sim.3680},
	doi = {10.1002/sim.3680},
	language = {en},
	number = {25},
	urldate = {2023-07-24},
	journal = {Statistics in Medicine},
	author = {Lunn, David and Spiegelhalter, David and Thomas, Andrew and Best, Nicky},
	month = nov,
	year = {2009},
	pages = {3049--3067},
}

@misc{beckers_principled_2022,
	title = {Principled {Pruning} of {Bayesian} {Neural} {Networks} through {Variational} {Free} {Energy} {Minimization}},
	url = {http://arxiv.org/abs/2210.09134},
	doi = {10.48550/arXiv.2210.09134},
	abstract = {Bayesian model reduction provides an efficient approach for comparing the performance of all nested sub-models of a model, without re-evaluating any of these sub-models. Until now, Bayesian model reduction has been applied mainly in the computational neuroscience community. In this paper, we formulate and apply Bayesian model reduction to perform principled pruning of Bayesian neural networks, based on variational free energy minimization. This novel parameter pruning scheme solves the shortcomings of many current state-of-the-art pruning methods that are used by the signal processing community. The proposed approach has a clear stopping criterion and minimizes the same objective that is used during training. Next to these theoretical benefits, our experiments indicate better model performance in comparison to state-of-the-art pruning schemes.},
	urldate = {2022-10-18},
	publisher = {arXiv},
	author = {Beckers, Jim and van Erp, Bart and Zhao, Ziyue and Kondrashov, Kirill and de Vries, Bert},
	month = oct,
	year = {2022},
	note = {arXiv:2210.09134 [cs, eess]},
	keywords = {Computer Science - Machine Learning, Electrical Engineering and Systems Science - Signal Processing},
}

@article{van_de_laar_realising_2023,
	title = {Realising {Synthetic} {Active} {Inference} {Agents}, {Part} {II}: {Variational} {Message} {Updates}},
	shorttitle = {Realising {Synthetic} {Active} {Inference} {Agents}, {Part} {II}},
	journal = {arXiv preprint arXiv:2306.02733},
	author = {van de Laar, Thijs and Koudahl, Magnus and De Vries, Bert},
	year = {2023},
	keywords = {/unread},
}

@article{koudahl_realising_2023,
	title = {Realising {Synthetic} {Active} {Inference} {Agents}, {Part} {I}: {Epistemic} {Objectives} and {Graphical} {Specification} {Language}},
	shorttitle = {Realising {Synthetic} {Active} {Inference} {Agents}, {Part} {I}},
	journal = {arXiv preprint arXiv:2306.08014},
	author = {Koudahl, Magnus and van de Laar, Thijs and De Vries, Bert},
	year = {2023},
	keywords = {/unread},
}

@article{wagenmakers_bayesian_2018,
	title = {Bayesian inference for psychology. {Part} {I}: {Theoretical} advantages and practical ramifications},
	volume = {25},
	issn = {1069-9384, 1531-5320},
	shorttitle = {Bayesian inference for psychology. {Part} {I}},
	url = {http://link.springer.com/10.3758/s13423-017-1343-3},
	doi = {10.3758/s13423-017-1343-3},
	language = {en},
	number = {1},
	urldate = {2023-07-28},
	journal = {Psychonomic Bulletin \& Review},
	author = {Wagenmakers, Eric-Jan and Marsman, Maarten and Jamil, Tahira and Ly, Alexander and Verhagen, Josine and Love, Jonathon and Selker, Ravi and Gronau, Quentin F. and Šmíra, Martin and Epskamp, Sacha and Matzke, Dora and Rouder, Jeffrey N. and Morey, Richard D.},
	month = feb,
	year = {2018},
	pages = {35--57},
}

@article{Revach_kalmannet_2022,
  author={Revach, Guy and Shlezinger, Nir and Ni, Xiaoyong and Escoriza, Adrià López and van Sloun, Ruud J. G. and Eldar, Yonina C.},
  journal={IEEE Transactions on Signal Processing}, 
  title={KalmanNet: Neural Network Aided Kalman Filtering for Partially Known Dynamics}, 
  year={2022},
  volume={70},
  number={},
  pages={1532-1547},
  doi={10.1109/TSP.2022.3158588}
}


@InProceedings{pmlr-v138-tehrani20a,
  title = 	 {Bean Machine: A Declarative Probabilistic Programming Language For Efficient Programmable Inference},
  author =       {Tehrani, Nazanin and Arora, Nimar S. and Li, Yucen Lily and Shah, Kinjal Divesh and Noursi, David and Tingley, Michael and Torabi, Narjes and Masouleh, Sepehr and Lippert, Eric and Meijer, Erik},
  booktitle = 	 {Proceedings of the 10th International Conference on Probabilistic Graphical Models},
  pages = 	 {485--496},
  year = 	 {2020},
  editor = 	 {Jaeger, Manfred and Nielsen, Thomas Dyhre},
  volume = 	 {138},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {23--25 Sep},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v138/tehrani20a/tehrani20a.pdf},
  url = 	 {https://proceedings.mlr.press/v138/tehrani20a.html},
  abstract = 	 {A number of imperative Probabilistic Programming Languages (PPLs) have been recently proposed, but the imperative style choice makes it very hard to deduce the dependence structure between the latent variables, which can also change from iteration to iteration.
 We propose a new declarative style PPL, Bean Machine, and demonstrate that in this new language, the dynamic dependence structure is readily available.
 Although we are not the first to propose a declarative PPL or to observe the advantages of knowing the dependence structure, we take the idea further by showing other inference techniques that become feasible or easier in this style.
 We show that it is very easy for users to program inference by composition (combining different inference techniques for different parts of the model), customization (providing a custom hand-written inference method for specific variables), and blocking (specifying blocks of random variables that should be sampled together) in a declarative language.
 A number of empirical results are provided where we backup these claims modulo the runtime inefficiencies of unvectorized Python.
 As a fringe benefit, we note that it is very easy to translate statistical models written in mathematical notation into our language.}
}

@article{FRISTON20231,
title = {The free energy principle made simpler but not too simple},
journal = {Physics Reports},
volume = {1024},
pages = {1-29},
year = {2023},
note = {The free energy principle made simpler but not too simple},
issn = {0370-1573},
doi = {https://doi.org/10.1016/j.physrep.2023.07.001},
url = {https://www.sciencedirect.com/science/article/pii/S037015732300203X},
author = {Karl Friston and Lancelot {Da Costa} and Noor Sajid and Conor Heins and Kai Ueltzhöffer and Grigorios A. Pavliotis and Thomas Parr},
keywords = {Self-organisation, Nonequilibrium, Variational inference, Bayesian, Markov blanket},
abstract = {This paper provides a concise description of the free energy principle, starting from a formulation of random dynamical systems in terms of a Langevin equation and ending with a Bayesian mechanics that can be read as a physics of sentience. It rehearses the key steps using standard results from statistical physics. These steps entail (i) establishing a particular partition of states based upon conditional independencies that inherit from sparsely coupled dynamics, (ii) unpacking the implications of this partition in terms of Bayesian inference and (iii) describing the paths of particular states with a variational principle of least action. Teleologically, the free energy principle offers a normative account of self-organisation in terms of optimal Bayesian design and decision-making, in the sense of maximising marginal likelihood or Bayesian model evidence. In summary, starting from a description of the world in terms of random dynamical systems, we end up with a description of self-organisation as sentient behaviour that can be interpreted as self-evidencing; namely, self-assembly, autopoiesis or active inference.}
}%

@MISC{JASP2023,
  AUTHOR = {{JASP Team}},
  TITLE  = {{JASP (Version 0.17.3)[Computer software]}},
  YEAR   = {2023},
  URL    = {https://jasp-stats.org/}
}

@article{schonbrodt_sequential_2017,
	title = {Sequential hypothesis testing with {Bayes} factors: {Efficiently} testing mean differences.},
	volume = {22},
	issn = {1939-1463, 1082-989X},
	shorttitle = {Sequential hypothesis testing with {Bayes} factors},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/met0000061},
	doi = {10.1037/met0000061},
	language = {en},
	number = {2},
	urldate = {2023-08-17},
	journal = {Psychological Methods},
	author = {Schönbrodt, Felix D. and Wagenmakers, Eric-Jan and Zehetleitner, Michael and Perugini, Marco},
	month = jun,
	year = {2017},
	pages = {322--339},
	file = {Submitted Version:/Users/bvdmitri/Zotero/storage/6T74SAXH/Schönbrodt et al. - 2017 - Sequential hypothesis testing with Bayes factors .pdf:application/pdf},
}

@article{schonbrodt_bayes_2018,
	title = {Bayes factor design analysis: {Planning} for compelling evidence},
	volume = {25},
	issn = {1069-9384, 1531-5320},
	shorttitle = {Bayes factor design analysis},
	url = {http://link.springer.com/10.3758/s13423-017-1230-y},
	doi = {10.3758/s13423-017-1230-y},
	language = {en},
	number = {1},
	urldate = {2023-08-17},
	journal = {Psychonomic Bulletin \& Review},
	author = {Schönbrodt, Felix D. and Wagenmakers, Eric-Jan},
	month = feb,
	year = {2018},
	pages = {128--142},
	file = {Full Text:/Users/bvdmitri/Zotero/storage/67LRB4TV/Schönbrodt and Wagenmakers - 2018 - Bayes factor design analysis Planning for compell.pdf:application/pdf},
}

@book{pindyck_econometric_1998,
	address = {Boston, Mass.},
	edition = {4. ed},
	series = {{McGraw}-{Hill} international editions economics series},
	title = {Econometric models and economic forecasts},
	isbn = {978-0-07-913292-5 978-0-07-115836-7 978-0-07-100866-2 978-0-07-848043-0 978-0-07-050208-6},
	language = {eng},
	publisher = {Irwin McGraw-Hill},
	author = {Pindyck, Robert S. and Rubinfeld, Daniel L.},
	year = {1998},
	annote = {Erg. bilden: Kawakatsu, Hiroyuki: A computer handbook using Eviews und White, Kenneth J.: Econometric models \& economic forecasts},
}

@inproceedings{hewitt_actor_model,
author = {Hewitt, Carl and Bishop, Peter and Steiger, Richard},
title = {A Universal Modular ACTOR Formalism for Artificial Intelligence},
year = {1973},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {This paper proposes a modular ACTOR architecture and definitional method for artificial intelligence that is conceptually based on a single kind of object: actors [or, if you will, virtual processors, activation frames, or streams]. The formalism makes no presuppositions about the representation of primitive data structures and control structures. Such structures can be programmed, micro-coded, or hard wired in a uniform modular fashion. In fact it is impossible to determine whether a given object is "really" represented as a list, a vector, a hash table, a function, or a process. The architecture will efficiently run the coming generation of PLANNER-like artificial intelligence languages including those requiring a high degree of parallelism. The efficiency is gained without loss of programming generality because it only makes certain actors more efficient; it does not change their behavioral characteristics. The architecture is general with respect to control structure and does not have or need goto, interrupt, or semaphore primitives. The formalism achieves the goals that the disallowed constructs are intended to achieve by other more structured methods.},
booktitle = {Proceedings of the 3rd International Joint Conference on Artificial Intelligence},
pages = {235–245},
numpages = {11},
location = {Stanford, USA},
series = {IJCAI'73}
}
