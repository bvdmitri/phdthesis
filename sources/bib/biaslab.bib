
@article{traunmuller_analytical_1990,
  title    = {Analytical expressions for the tonotopic sensory scale},
  volume   = {88},
  issn     = {0001-4966},
  url      = {http://scitation.aip.org/content/asa/journal/jasa/88/1/10.1121/1.399849
              },
  doi      = {10.1121/1.399849},
  abstract = {Accuracy and simplicity of analytical expressions for the
              relations between frequency and critical bandwidth as well as
              critical‐band rate (in Bark) are assessed for the purpose of
              applications in speech perception research and in speech
              technology. The equivalent rectangular bandwidth (ERB) is seen as a
              measure of frequency resolution, while the classical critical‐band
              rate is considered a measure of tonotopic position. For the
              conversion of frequency to critical‐band rate, and vice versa, the
              inversible formula z=[26.81/(1+1960/f )]−0.53 is proposed. Within
              the frequency range of the perceptually essential vowelformants
              (0.2–6.7 kHz), it agrees to within ±0.05 Bark with the Bark scale,
              originally published in the form of a table.},
  number   = {1},
  urldate  = {2015-04-13},
  journal  = {The Journal of the Acoustical Society of America},
  author   = {Traunmüller, Hartmut},
  month    = jul,
  year     = {1990},
  keywords = {Speech, Frequency measurement, Phonetic segments, Speech analysis,
              Speech Perception},
  pages    = {97--100},
  file     = {
              Snapshot:/Users/apodusenko/Zotero/storage/EGB7RSDS/1.html:text/html;Traunmüller
              - 1990 - Analytical expressions for the tonotopic sensory
              s.pdf:/Users/apodusenko/Zotero/storage/7SFJZNW3/Traunmüller - 1990 -
              Analytical expressions for the tonotopic sensory s.pdf:application/pdf}
}

@inproceedings{wang_auditory_1991,
  title     = {Auditory distortion measure for speech coding},
  doi       = {10.1109/ICASSP.1991.150384},
  abstract  = {A novel perceptually motivated objective measure for estimating
               the subjective quality of coded speech is presented. It takes into
               account auditory frequency warping (Bark transformation),
               critical-band integration, amplitude sensitivity variations with
               frequency, and conversion from loudness level to loudness. For each
               10 ms segment of an utterance, a weighted spectral vector is
               computed via 15 critical band filters. The overall distortion,
               called Bark spectral distortion (BSD), is the average squared
               Euclidean distance between spectral vectors of the original and
               coded utterance. In tests with speech distorted by a modulated
               noise reference unit or coded at rates of 2.4-64 kb/s, the measure
               predicted mean opinion score (MOS) ratings are notably better than
               segmental SNR. The standard error in estimating MOS scores with the
               new measure was 0.2-0.3},
  booktitle = {, 1991 {International} {Conference} on {Acoustics}, {Speech}, and
               {Signal} {Processing}, 1991. {ICASSP}-91},
  author    = {Wang, S. and Sekey, A. and Gersho, A.},
  month     = apr,
  year      = {1991},
  keywords  = {Speech enhancement, Noise measurement, Signal to noise ratio,
               speech intelligibility, 10 ms, 2.4 to 64 kbit/s, amplitude
               sensitivity variations, auditory distortion measure, auditory
               frequency warping, average squared Euclidean distance, Bark
               spectral distortion, Bark transformation, bearing, coded utterance,
               critical band filters, critical-band integration, Distortion
               measurement, encoding, Euclidean distance, Filters, Frequency
               conversion, hearing, loudness level, measure predicted mean opinion
               score, modulated noise reference unit, Modulation coding, speech
               analysis and processing, speech coding, standard error, subjective
               quality, Testing, weighted spectral vector},
  pages     = {493--496 vol.1},
  file      = {IEEE Xplore Abstract
               Record:/Users/apodusenko/Zotero/storage/BREWENCF/abs_all.html:text/html;Wang
               et al. - 1991 - Auditory distortion measure for speech
               coding.pdf:/Users/apodusenko/Zotero/storage/RDGAZE6P/Wang et al. - 1991
               - Auditory distortion measure for speech coding.pdf:application/pdf}
}

@article{honkela_variational_2004,
  title      = {Variational learning and bits-back coding: an information-theoretic
                view to {Bayesian} learning},
  shorttitle = {Variational learning and bits-back coding},
  abstract   = {Abstract—The bits-back coding first introduced by Wallace in 1990
                and later by Hinton and van Camp in 1993 provides an interesting
                link between Bayesian learning and information-theoretic
                minimum-description-length (MDL) learning approaches. The bits-back
                coding allows interpreting the cost function used in the
                variational Bayesian method called ensemble learning as a code
                length in addition to the Bayesian view of misfit of the posterior
                approximation and a lower bound of model evidence. Combining these
                two viewpoints provides interesting insights to the learning
                process and the functions of different parts of the model. In this
                paper, the problem of variational Bayesian learning of hierarchical
                latent variable models is used to demonstrate the benefits of the
                two views. The code-length interpretation provides new views to
                many parts of the problem such as model comparison and pruning and
                helps explain many phenomena occurring in learning. Index
                Terms—Bits-back coding, ensemble learning, hierarchical latent
                variable models, minimum description length, variational Bayesian
                learning. I.},
  journal    = {IEEE Transactions on Neural Networks},
  author     = {Honkela, Antti and Valpola, Harri},
  year       = {2004},
  file       = {Citeseer -
                Snapshot:/Users/apodusenko/Zotero/storage/E35J9PQQ/summary.html:text/html;Honkela
                and Valpola - 2004 - Variational learning and bits-back coding an
                info.pdf:/Users/apodusenko/Zotero/storage/7XPVQQ2B/Honkela and Valpola
                - 2004 - Variational learning and bits-back coding an
                info.pdf:application/pdf}
}

@article{todorov_efficient_2009,
  title    = {Efficient computation of optimal actions},
  volume   = {106},
  issn     = {0027-8424, 1091-6490},
  url      = {http://www.pnas.org/content/106/28/11478},
  doi      = {10.1073/pnas.0710743106},
  abstract = {Optimal choice of actions is a fundamental problem relevant to
              fields as diverse as neuroscience, psychology, economics, computer
              science, and control engineering. Despite this broad relevance the
              abstract setting is similar: we have an agent choosing actions over
              time, an uncertain dynamical system whose state is affected by
              those actions, and a performance criterion that the agent seeks to
              optimize. Solving problems of this kind remains hard, in part,
              because of overly generic formulations. Here, we propose a more
              structured formulation that greatly simplifies the construction of
              optimal control laws in both discrete and continuous domains. An
              exhaustive search over actions is avoided and the problem becomes
              linear. This yields algorithms that outperform Dynamic Programming
              and Reinforcement Learning, and thereby solve traditional problems
              more efficiently. Our framework also enables computations that were
              not possible before: composing optimal control laws by mixing
              primitives, applying deterministic methods to stochastic systems,
              quantifying the benefits of error tolerance, and inferring goals
              from behavioral data via convex optimization. Development of a
              general class of easily solvable problems tends to accelerate
              progress—as linear systems theory has done, for example. Our
              framework may have similar impact in fields where optimal choice of
              actions is relevant.},
  language = {en},
  number   = {28},
  urldate  = {2015-03-10},
  journal  = {Proceedings of the National Academy of Sciences},
  author   = {Todorov, Emanuel},
  month    = jul,
  year     = {2009},
  pmid     = {19574462},
  keywords = {action selection, cost function, linear Bellman equation,
              stochastic optimal control},
  pages    = {11478--11483},
  file     = {
              Snapshot:/Users/apodusenko/Zotero/storage/6I8IMFSN/11478.html:text/html;Todorov
              - 2009 - Efficient computation of optimal
              actions.pdf:/Users/apodusenko/Zotero/storage/XZDHT3W3/Todorov - 2009 -
              Efficient computation of optimal actions.pdf:application/pdf}
}

@inproceedings{buonanno_simulink_2014,
  address = {Salerno, Italy},
  title   = {Simulink {Implementation} of {Belief} {Propagation} in {Normal} {
             Factor} {Graphs}},
  author  = {Buonanno, Amedeo and Palmieri, Francesco},
  year    = {2014},
  file    = {Buonanno and Palmieri - 2014 - Simulink Implementation of Belief
             Propagation in N.pdf:/Users/apodusenko/Zotero/storage/9SEK8MHJ/Buonanno
             and Palmieri - 2014 - Simulink Implementation of Belief Propagation in
             N.pdf:application/pdf}
}

@misc{hu_general_2006,
  title    = {A general computation rule for lossy summaries/messages with examples
              from equalization},
  url      = {http://cds.cern.ch/record/973039},
  abstract = {Elaborating on prior work by Minka, we formulate a general
              computation rule for lossy messages. An important special case
              (with many applications in communications) is the conversion of \&
              quot;soft-bit\&quot; messages to Gaussian messages. By this method,
              the performance of a Kalman equalizer is improved, both for uncoded
              and coded transmission.},
  urldate  = {2015-03-04},
  journal  = {CERN Document Server},
  author   = {Hu, J. and Loeliger, H. A. and Kschischang, F. and Dauwels, J.},
  month    = jul,
  year     = {2006},
  file     = {Hu et al. - 2006 - A general computation rule for lossy
              summaries.pdf:/Users/apodusenko/Zotero/storage/9VTCXGBX/Hu et al. -
              2006 - A general computation rule for lossy
              summaries.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/FAD4R3JX/973039.html:text/html
              }
}

@article{fitzgerald_active_2015,
  title    = {Active inference, evidence accumulation, and the urn task},
  volume   = {27},
  issn     = {1530-888X},
  doi      = {10.1162/NECO_a_00699},
  abstract = {Deciding how much evidence to accumulate before making a decision
              is a problem we and other animals often face, but one that is not
              completely understood. This issue is particularly important because
              a tendency to sample less information (often known as reflection
              impulsivity) is a feature in several psychopathologies, such as
              psychosis. A formal understanding of information sampling may
              therefore clarify the computational anatomy of psychopathology. In
              this theoretical letter, we consider evidence accumulation in terms
              of active (Bayesian) inference using a generic model of Markov
              decision processes. Here, agents are equipped with beliefs about
              their own behavior-in this case, that they will make informed
              decisions. Normative decision making is then modeled using
              variational Bayes to minimize surprise about choice outcomes. Under
              this scheme, different facets of belief updating map naturally onto
              the functional anatomy of the brain (at least at a heuristic
              level). Of particular interest is the key role played by the
              expected precision of beliefs about control, which we have
              previously suggested may be encoded by dopaminergic neurons in the
              midbrain. We show that manipulating expected precision strongly
              affects how much information an agent characteristically samples,
              and thus provides a possible link between impulsivity and
              dopaminergic dysfunction. Our study therefore represents a step
              toward understanding evidence accumulation in terms of
              neurobiologically plausible Bayesian inference and may cast light
              on why this process is disordered in psychopathology.},
  language = {eng},
  number   = {2},
  journal  = {Neural Computation},
  author   = {FitzGerald, Thomas H. B. and Schwartenbeck, Philipp and Moutoussis,
              Michael and Dolan, Raymond J. and Friston, Karl},
  month    = feb,
  year     = {2015},
  pmid     = {25514108},
  pages    = {306--328},
  file     = {FitzGerald et al. - 2015 - Active inference, evidence accumulation,
              and the u.pdf:/Users/apodusenko/Zotero/storage/CIHE5ZDK/FitzGerald et
              al. - 2015 - Active inference, evidence accumulation, and the
              u.pdf:application/pdf}
}

@article{loftin_learning_2015,
  title      = {Learning behaviors via human-delivered discrete feedback: modeling
                implicit feedback strategies to speed up learning},
  issn       = {1387-2532, 1573-7454},
  shorttitle = {Learning behaviors via human-delivered discrete feedback},
  url        = {http://link.springer.com/article/10.1007/s10458-015-9283-7},
  doi        = {10.1007/s10458-015-9283-7},
  abstract   = {For real-world applications, virtual agents must be able to learn
                new behaviors from non-technical users. Positive and negative
                feedback are an intuitive way to train new behaviors, and existing
                work has presented algorithms for learning from such feedback. That
                work, however, treats feedback as numeric reward to be maximized,
                and assumes that all trainers provide feedback in the same way. In
                this work, we show that users can provide feedback in many
                different ways, which we describe as “training strategies.”
                Specifically, users may not always give explicit feedback in
                response to an action, and may be more likely to provide explicit
                reward than explicit punishment, or vice versa, such that the lack
                of feedback itself conveys information about the behavior. We
                present a probabilistic model of trainer feedback that describes
                how a trainer chooses to provide explicit reward and/or explicit
                punishment and, based on this model, develop two novel learning
                algorithms (SABL and I-SABL) which take trainer strategy into
                account, and can therefore learn from cases where no feedback is
                provided. Through online user studies we demonstrate that these
                algorithms can learn with less feedback than algorithms based on a
                numerical interpretation of feedback. Furthermore, we conduct an
                empirical analysis of the training strategies employed by users,
                and of factors that can affect their choice of strategy.},
  language   = {en},
  urldate    = {2015-02-24},
  journal    = {Autonomous Agents and Multi-Agent Systems},
  author     = {Loftin, Robert and Peng, Bei and MacGlashan, James and Littman,
                Michael L. and Taylor, Matthew E. and Huang, Jeff and Roberts, David
                L.},
  month      = feb,
  year       = {2015},
  keywords   = {Bayesian inference, machine learning, reinforcement learning,
                Artificial Intelligence (incl. Robotics), Computing Methodologies,
                Computer Systems Organization and Communication Networks,
                Human–computer interaction, Interactive learning, Learning from
                feedback, Software Engineering/Programming and Operating Systems,
                User Interfaces and Human Computer Interaction},
  pages      = {1--30},
  file       = {Loftin et al. - 2015 - Learning behaviors via human-delivered discrete
                fe.pdf:/Users/apodusenko/Zotero/storage/I42P4MC2/Loftin et al. - 2015 -
                Learning behaviors via human-delivered discrete
                fe.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/9FCI4IEJ/s10458-015-9283-7.html:text/html
                }
}

@misc{luttinen_bayespy_nodate,
  title  = {{BayesPy} documentation},
  url    = {http://www.bayespy.org/},
  author = {Luttinen, Jaakko}
}

@misc{noauthor_julia_nodate,
  title = {Julia 0.3 documentation},
  url   = {http://julia.readthedocs.org/en/release-0.3/}
}

@article{friston_generalised_2010,
  title   = {Generalised {Filtering}},
  url     = {ftp://ftp.math.ethz.ch/hg/emis/journals/HOA/MPE/Volume2010/621670.pdf},
  urldate = {2015-02-10},
  author  = {Friston, Karl and Stephan, Klaas and Li, Baojuan and Daunizeau, Jean
             },
  year    = {2010},
  file    = {Friston et al. - 2010 - Generalised
             Filtering.pdf:/Users/apodusenko/Zotero/storage/HDUFEVFX/Friston et al.
             - 2010 - Generalised Filtering.pdf:application/pdf}
}

@inproceedings{peterka_bayesian_1981,
  title     = {Bayesian {Approach} {To} {System} {Identification}},
  abstract  = {Contents 1 Introduction 1 2 Underlying Philosophy and Basic
               Relations 3 2.1 Two Basic Operations on Uncertainties . . . . . . .
               . . . . . . . . . . . . . . . . . . . . . 4 2.2 Independent
               Uncertain Quantities . . . . . . . . . . . . . . . . . . . . . . .
               . . . . . . . . 6 2.3 Derived Relations . . . . . . . . . . . . . .
               . . . . . . . . . . . . . . . . . . . . . . . . . . . 7 2.4
               Additional Remarks . . . . . . . . . . . . . . . . . . . . . . . .
               . . . . . . . . . . . . . . . 7 3 System Model, Reexamined from
               Bayesian Viewpoint 8 3.1 Discrete White Noise . . . . . . . . . . .
               . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12 3.2
               Measurable External Disturbances . . . . . . . . . . . . . . . . .
               . . . . . . . . . . . . . . 16 4 Parameter Estimation and Output
               Prediction 16 4.1 Estimation in Closed Control Loop: Natural
               Conditions of Control . . . . . . . . . . . . . 18 4.2 One-Shot
               Estimation . . . . . . . . . . . . . . . . . .},
  booktitle = {in {Trends} and {Progress} in {System} {Identification}, {P}. {
               Eykhoff}, {Ed}},
  publisher = {Pergamon Press},
  author    = {Peterka, V.},
  year      = {1981},
  pages     = {239--304},
  file      = {Citeseer -
               Snapshot:/Users/apodusenko/Zotero/storage/PHMN9G4J/summary.html:text/html;Peterka
               - 1981 - Bayesian Approach To System
               Identification.pdf:/Users/apodusenko/Zotero/storage/VGW58J44/Peterka -
               1981 - Bayesian Approach To System Identification.pdf:application/pdf}
}

@book{yost_fundamentals_1994,
  address    = {San Diego, CA, US},
  title      = {Fundamentals of hearing: {An} introduction (3rd ed.)},
  volume     = {xiii},
  copyright  = {(c) 2012 APA, all rights reserved},
  isbn       = {0-12-772690-X (Hardcover)},
  shorttitle = {Fundamentals of hearing},
  abstract   = {This book was written primarily as a textbook for students taking
                their first course in hearing, most likely in a speech and hearing
                program, psychology program, or neuroscience program. The book
                attempts to explain normal human hearing with a focus on the
                peripheral auditory system, coding of the basic attributes of sound
                , and the perception of simple sounds. . . . The way in which the
                nervous system codes for the basic attributes of sound and the
                perceptual consequences of that coding constitute the fundamentals
                of hearing.},
  publisher  = {Academic Press},
  author     = {Yost, William A.},
  year       = {1994},
  keywords   = {*Auditory Perception, Neurophysiology, Peripheral Nervous System},
  file       = {APA PsycNET
                Snapshot:/Users/apodusenko/Zotero/storage/QPVZP5TM/1994-97145-000.html:text/html
                }
}

@article{titsias_variational_nodate,
  title  = {Variational learning of inducing variables in sparse {Gaussian}
            processes},
  author = {Titsias, Michalis K.},
  file   = {Titsias - 2009 - Variational learning of inducing variables in
            spar.pdf:/Users/apodusenko/Zotero/storage/UUT98GXI/Titsias - 2009 -
            Variational learning of inducing variables in spar.pdf:application/pdf}
}

@article{sanchez_toward_2014,
  title      = {Toward a {New} {Application} of {Real}-{Time} {Electrophysiology}: {
                Online} {Optimization} of {Cognitive} {Neurosciences} {Hypothesis} {
                Testing}},
  volume     = {4},
  copyright  = {http://creativecommons.org/licenses/by/3.0/},
  shorttitle = {Toward a {New} {Application} of {Real}-{Time} {Electrophysiology
                }},
  url        = {http://www.mdpi.com/2076-3425/4/1/49},
  doi        = {10.3390/brainsci4010049},
  abstract   = {Brain-computer interfaces (BCIs) mostly rely on
                electrophysiological brain signals. Methodological and technical
                progress has largely solved the challenge of processing these
                signals online. The main issue that remains, however, is the
                identification of a reliable mapping between electrophysiological
                measures and relevant states of mind. This is why BCIs are highly
                dependent upon advances in cognitive neuroscience and neuroimaging
                research. Recently, psychological theories became more biologically
                plausible, leading to more realistic generative models of
                psychophysiological observations. Such complex interpretations of
                empirical data call for efficient and robust computational
                approaches that can deal with statistical model comparison, such as
                approximate Bayesian inference schemes. Importantly, the latter
                enable the optimization of a model selection error rate with
                respect to experimental control variables, yielding maximally
                powerful designs. In this paper, we use a Bayesian decision
                theoretic approach to cast model comparison in an online adaptive
                design optimization procedure. We show how to maximize design
                efficiency for individual healthy subjects or patients. Using
                simulated data, we demonstrate the face- and construct-validity of
                this approach and illustrate its extension to electrophysiology and
                multiple hypothesis testing based on recent psychophysiological
                models of perception. Finally, we discuss its implications for
                basic neuroscience and BCI itself.},
  language   = {en},
  number     = {1},
  urldate    = {2014-12-02},
  journal    = {Brain Sciences},
  author     = {Sanchez, Gaëtan and Daunizeau, Jean and Maby, Emmanuel and Bertrand,
                Olivier and Bompas, Aline and Mattout, Jérémie},
  month      = jan,
  year       = {2014},
  keywords   = {Bayesian model comparison, adaptive design optimization, Bayesian
                Decision Theory, brain-computer interfaces, cognitive neuroscience,
                generative models of brain functions, hypothesis testing, real-time
                electrophysiology},
  pages      = {49--72},
  file       = {Full Text PDF:/Users/apodusenko/Zotero/storage/7EIQFT29/Sanchez et al.
                - 2014 - Toward a New Application of Real-Time
                Electrophysi.pdf:application/pdf;Sanchez et al. - 2014 - Toward a New
                Application of Real-Time
                Electrophysi.pdf:/Users/apodusenko/Zotero/storage/7RZT5NIX/Sanchez et
                al. - 2014 - Toward a New Application of Real-Time
                Electrophysi.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/4A3XJ8PV/htm.html:text/html;Snapshot:/Users/apodusenko/Zotero/storage/SJEMG6MC/49.html:text/html
                }
}

@book{ries_lean_2011,
  address    = {New York},
  title      = {The lean startup: how today's entrepreneurs use continuous innovation
                to create radically successful businesses},
  isbn       = {978-0-307-88789-4 0-307-88789-8},
  shorttitle = {The lean startup},
  abstract   = {"Most startups are built to fail. But those failures, according to
                entrepreneur Eric Ries, are preventable. Startups don't fail
                because of bad execution, or missed deadlines, or blown budgets.
                They fail because they are building something nobody wants. Whether
                they arise from someone's garage or are created within a mature
                Fortune 500 organization, new ventures, by definition, are designed
                to create new products or services under conditions of extreme
                uncertainly. Their primary mission is to find out what customers
                ultimately will buy. One of the central premises of The Lean
                Startup movement is what Ries calls "validated learning" about the
                customer. It is a way of getting continuous feedback from customers
                so that the company can shift directions or alter its plans inch by
                inch, minute by minute. Rather than creating an elaborate business
                plan and a product-centric approach, Lean Startup prizes testing
                your vision continuously with your customers and making constant
                adjustments"--},
  language   = {English},
  publisher  = {Crown Business},
  author     = {Ries, Eric},
  year       = {2011},
  file       = {Ries - 2011 - The lean startup how today's entrepreneurs use
                co.epub:/Users/apodusenko/Zotero/storage/R5VQVAJ7/Ries - 2011 - The
                lean startup how today's entrepreneurs use
                co.epub:application/octet-stream;Ries - 2011 - The lean startup how
                today's entrepreneurs use
                co.pdf:/Users/apodusenko/Zotero/storage/UDRGIV27/Ries - 2011 - The lean
                startup how today's entrepreneurs use co.pdf:application/pdf}
}

@article{visser_seven_2011,
  title      = {Seven things to remember about hidden {Markov} models: {A} tutorial
                on {Markovian} models for time series},
  volume     = {55},
  issn       = {0022-2496},
  shorttitle = {Seven things to remember about hidden {Markov} models},
  url        = {http://www.sciencedirect.com/science/article/pii/S0022249611000691},
  doi        = {10.1016/j.jmp.2011.08.002},
  abstract   = {This paper provides a tutorial on key issues in hidden Markov
                modeling. Hidden Markov models have become very popular models for
                time series and longitudinal data in recent years due to a
                combination of (relative) simplicity and flexibility in adapting
                the model to novel situations. The tutorial covers the conceptual
                description of the model, estimation of parameters through maximum
                likelihood, and ends with an application to real data illustrating
                the possibilities.},
  number     = {6},
  urldate    = {2014-06-24},
  journal    = {Journal of Mathematical Psychology},
  author     = {Visser, Ingmar},
  month      = dec,
  year       = {2011},
  keywords   = {hidden Markov model, Dependent mixture model, Mixture model},
  pages      = {403--415},
  file       = {ScienceDirect
                Snapshot:/Users/apodusenko/Zotero/storage/I3EA2QU2/S0022249611000691.html:text/html;Visser
                - 2011 - Seven things to remember about hidden Markov
                model.pdf:/Users/apodusenko/Zotero/storage/9V547P7P/Visser - 2011 -
                Seven things to remember about hidden Markov model.pdf:application/pdf}
}

@article{friston_cognitive_2014,
  title      = {Cognitive {Dynamics}: {From} {Attractors} to {Active} {Inference}},
  volume     = {102},
  issn       = {0018-9219},
  shorttitle = {Cognitive {Dynamics}},
  doi        = {10.1109/JPROC.2014.2306251},
  abstract   = {This paper combines recent formulations of self-organization and
                neuronal processing to provide an account of cognitive dynamics
                from basic principles. We start by showing that inference (and
                autopoiesis) are emergent features of any (weakly mixing) ergodic
                random dynamical system. We then apply the emergent dynamics to
                action and perception in a way that casts action as the fulfillment
                of (Bayesian) beliefs about the causes of sensations. More formally
                , we formulate ergodic flows on global random attractors as a
                generalized descent on a free energy functional of the internal
                states of a system. This formulation rests on a partition of states
                based on a Markov blanket that separates internal states from
                hidden states in the external milieu. This separation means that
                the internal states effectively represent external states
                probabilistically. The generalized descent is then related to
                classical Bayesian (e.g., Kalman-Bucy) filtering and predictive
                coding-of the sort that might be implemented in the brain. Finally,
                we present two simulations. The first simulates a primordial soup
                to illustrate the emergence of a Markov blanket and (active)
                inference about hidden states. The second uses the same emergent
                dynamics to simulate action and action observation.},
  number     = {4},
  journal    = {Proceedings of the IEEE},
  author     = {Friston, K. and Sengupta, B. and Auletta, G.},
  month      = apr,
  year       = {2014},
  keywords   = {Bayesian filtering, belief networks, inference mechanisms, Bayes
                methods, Active inference, predictive coding, Free energy,
                Cognitive science, Dynamics, Active filters, attractors,
                autopoiesis, Bayesian networks, Biological systems, cognitive,
                cognitive dynamics, cognitive systems, ergodic random dynamical
                system, Markov blanket, Markov processes, Mathematical model,
                neuronal processing, random attractor, self-organization,
                Self-organizing networks},
  pages      = {427--445},
  file       = {Friston et al. - 2014 - Cognitive Dynamics From Attractors to Active
                Infe.pdf:/Users/apodusenko/Zotero/storage/NVIB2DWS/Friston et al. -
                2014 - Cognitive Dynamics From Attractors to Active
                Infe.pdf:application/pdf;IEEE Xplore Abstract
                Record:/Users/apodusenko/Zotero/storage/WS7TZFWH/abs_all.html:text/html
                }
}

@article{ortega_nonparametric_2012,
  title    = {A {Nonparametric} {Conjugate} {Prior} {Distribution} for the {
              Maximizing} {Argument} of a {Noisy} {Function}},
  url      = {http://arxiv.org/abs/1206.1898},
  abstract = {We propose a novel Bayesian approach to solve stochastic
              optimization problems that involve finding extrema of noisy,
              nonlinear functions. Previous work has focused on representing
              possible functions explicitly, which leads to a two-step procedure
              of first, doing inference over the function space and second,
              finding the extrema of these functions. Here we skip the
              representation step and directly model the distribution over
              extrema. To this end, we devise a non-parametric conjugate prior
              based on a kernel regressor. The resulting posterior distribution
              directly captures the uncertainty over the maximum of the unknown
              function. We illustrate the effectiveness of our model by
              optimizing a noisy, high-dimensional, non-convex objective
              function.},
  urldate  = {2014-10-29},
  journal  = {arXiv:1206.1898 [cs, math, stat]},
  author   = {Ortega, Pedro A. and Grau-Moya, Jordi and Genewein, Tim and Balduzzi
              , David and Braun, Daniel A.},
  month    = jun,
  year     = {2012},
  note     = {arXiv: 1206.1898},
  keywords = {Statistics - Machine Learning, Computer Science - Artificial
              Intelligence, Mathematics - Statistics Theory},
  file     = {Arg-Max Prior [Pedro A.
              Ortega]:/Users/apodusenko/Zotero/storage/PW7S7HHG/argmaxprior.html:text/html;arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/6FW429SJ/1206.html:text/html;Ortega
              et al. - 2012 - A Nonparametric Conjugate Prior Distribution for
              t.pdf:/Users/apodusenko/Zotero/storage/PAKND9PN/Ortega et al. - 2012 -
              A Nonparametric Conjugate Prior Distribution for t.pdf:application/pdf}
}

@book{murphy_machine_2012,
  address   = {Cambridge, Mass.},
  title     = {Machine learning a probabilistic perspective},
  isbn      = {978-0-262-30524-2 0-262-30524-0},
  url       = {
               http://search.ebscohost.com/login.aspx?direct=true&scope=site&db=nlebk&db=nlabk&AN=480968
               },
  abstract  = {"This textbook offers a comprehensive and self-contained
               introduction to the field of machine learning, based on a unified,
               probabilistic approach. The coverage combines breadth and depth,
               offering necessary background material on such topics as
               probability, optimization, and linear algebra as well as discussion
               of recent developments in the field, including conditional random
               fields, L1 regularization, and deep learning. The book is written
               in an informal, accessible style, complete with pseudo-code for the
               most important algorithms. All topics are copiously illustrated
               with color images and worked examples drawn from such application
               domains as biology, text processing, computer vision, and robotics.
               Rather than providing a cookbook of different heuristic methods,
               the book stresses a principled model-based approach, often using
               the language of graphical models to specify models in a concise and
               intuitive way. Almost all the models described have been
               implemented in a MATLAB software package--PMTK (probabilistic
               modeling toolkit)--that is freely available online"--Back cover.},
  language  = {English},
  urldate   = {2014-04-12},
  publisher = {MIT Press},
  author    = {Murphy, Kevin P},
  year      = {2012},
  file      = {Murphy - 2012 - Machine learning a probabilistic
               perspective.pdf:/Users/apodusenko/Zotero/storage/QH5GC78S/Murphy - 2012
               - Machine learning a probabilistic perspective.pdf:application/pdf}
}

@article{schwartenbeck_dopaminergic_2014,
  title    = {The {Dopaminergic} {Midbrain} {Encodes} the {Expected} {Certainty}
              about {Desired} {Outcomes}},
  issn     = {1047-3211, 1460-2199},
  url      = {http://cercor.oxfordjournals.org/content/early/2014/07/23/cercor.bhu159
              },
  doi      = {10.1093/cercor/bhu159},
  abstract = {Dopamine plays a key role in learning; however, its exact function
              in decision making and choice remains unclear. Recently, we
              proposed a generic model based on active (Bayesian) inference
              wherein dopamine encodes the precision of beliefs about optimal
              policies. Put simply, dopamine discharges reflect the confidence
              that a chosen policy will lead to desired outcomes. We designed a
              novel task to test this hypothesis, where subjects played a
              “limited offer” game in a functional magnetic resonance imaging
              experiment. Subjects had to decide how long to wait for a high
              offer before accepting a low offer, with the risk of losing
              everything if they waited too long. Bayesian model comparison
              showed that behavior strongly supported active inference, based on
              surprise minimization, over classical utility maximization schemes.
              Furthermore, midbrain activity, encompassing dopamine projection
              neurons, was accurately predicted by trial-by-trial variations in
              model-based estimates of precision. Our findings demonstrate that
              human subjects infer both optimal policies and the precision of
              those inferences, and thus support the notion that humans perform
              hierarchical probabilistic Bayesian inference. In other words,
              subjects have to infer both what they should do as well as how
              confident they are in their choices, where confidence may be
              encoded by dopaminergic firing.},
  language = {en},
  urldate  = {2014-07-29},
  journal  = {Cerebral Cortex},
  author   = {Schwartenbeck, Philipp and FitzGerald, Thomas H. B. and Mathys,
              Christoph and Dolan, Ray and Friston, Karl},
  month    = jul,
  year     = {2014},
  pmid     = {25056572},
  keywords = {Active inference, confidence, dopamine, neuroeconomics, precision},
  pages    = {bhu159},
  file     = {Schwartenbeck et al. - 2014 - The Dopaminergic Midbrain Encodes the
              Expected
              Cer.pdf:/Users/apodusenko/Zotero/storage/8DFAB2EV/Schwartenbeck et al.
              - 2014 - The Dopaminergic Midbrain Encodes the Expected
              Cer.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/8KVUKZCG/cercor.bhu159.html:text/html
              }
}

@incollection{pernkopf_introduction_2014,
  series    = {Academic {Press} {Library} in {Signal} {Processing}: {Volume} 1 {
               Signal} {Processing} {Theory} and {Machine} {Learning}},
  title     = {Introduction to {Probabilistic} {Graphical} {Models}},
  volume    = {Volume 1},
  isbn      = {2351-9819},
  url       = {http://www.sciencedirect.com/science/article/pii/B9780123965028000188},
  abstract  = {Over the last decades, probabilistic graphical models have become
               the method of choice for representing uncertainty. They are used in
               many research areas such as computer vision, speech processing,
               time-series and sequential data modeling, cognitive science,
               bioinformatics, probabilistic robotics, signal processing,
               communications and error-correcting coding theory, and in the area
               of artificial intelligence. This tutorial provides an introduction
               to probabilistic graphical models. We review three representations
               of probabilistic graphical models, namely, Markov networks or
               undirected graphical models, Bayesian networks or directed
               graphical models, and factor graphs. Then, we provide an overview
               about structure and parameter learning techniques. In particular,
               we discuss maximum likelihood and Bayesian learning, as well as
               generative and discriminative learning. Subsequently, we overview
               exact inference methods and briefly cover approximate inference
               techniques. Finally, we present typical applications for each of
               the three representations, namely, Bayesian networks for expert
               systems, dynamic Bayesian networks for speech processing, Markov
               random fields for image processing, and factor graphs for decoding
               error-correcting codes.},
  urldate   = {2014-06-24},
  booktitle = {Academic {Press} {Library} in {Signal} {Processing}},
  publisher = {Elsevier},
  author    = {Pernkopf, Franz and Peharz, Robert and Tschiatschek, Sebastian},
  editor    = {Paulo S.R. Diniz, Rama Chellappa {and} Sergios Theodoridis, Johan A.
               K. Suykens},
  year      = {2014},
  keywords  = {Factor graph, Bayesian network, Markov network, Parameter learning
               , Probabilistic graphical model, Probabilistic inference},
  pages     = {989--1064},
  file      = {Pernkopf et al. - 2014 - Introduction to Probabilistic Graphical
               Models.pdf:/Users/apodusenko/Zotero/storage/HBUQQMKG/Pernkopf et al. -
               2014 - Introduction to Probabilistic Graphical
               Models.pdf:application/pdf;ScienceDirect
               Snapshot:/Users/apodusenko/Zotero/storage/FIBRMQ89/B9780123965028000188.html:text/html
               }
}

@article{oreilly_making_2013,
  title    = {Making predictions in a changing world—inference, uncertainty, and
              learning},
  volume   = {7},
  url      = {http://journal.frontiersin.org/Journal/10.3389/fnins.2013.00105/full},
  doi      = {10.3389/fnins.2013.00105},
  abstract = {To function effectively, brains need to make predictions about
              their environment based on past experience, i.e., they need to
              learn about their environment. The algorithms by which learning
              occurs are of interest to neuroscientists, both in their own right
              (because they exist in the brain) and as a tool to model
              participants' incomplete knowledge of task parameters and hence, to
              better understand their behavior. This review focusses on a
              particular challenge for learning algorithms—how to match the rate
              at which they learn to the rate of change in the environment, so
              that they use as much observed data as possible whilst disregarding
              irrelevant, old observations. To do this algorithms must evaluate
              whether the environment is changing. We discuss the concepts of
              likelihood, priors and transition functions, and how these relate
              to change detection. We review expected and estimation uncertainty,
              and how these relate to change detection and learning rate. Finally
              , we consider the neural correlates of uncertainty and learning. We
              argue that the neural correlates of uncertainty bear a resemblance
              to neural systems that are active when agents actively explore
              their environments, suggesting that the mechanisms by which the
              rate of learning is set may be subject to top down control (in
              circumstances when agents actively seek new information) as well as
              bottom up control (by observations that imply change in the
              environment).},
  urldate  = {2014-08-25},
  journal  = {Decision Neuroscience},
  author   = {O'reilly, Jill X.},
  year     = {2013},
  keywords = {modeling, Learning, Bayes Theorem, Uncertainty, change detection,
              exploratory behavior},
  pages    = {105},
  file     = {O'reilly - 2013 - Making predictions in a changing world—inference,
              .pdf:/Users/apodusenko/Zotero/storage/I7RHZWRX/O'reilly - 2013 - Making
              predictions in a changing world—inference, .pdf:application/pdf}
}

@article{malik_variational_2013,
  title    = {A {Variational} {Bayesian} {Learning} {Approach} for {Nonlinear} {
              Acoustic} {Echo} {Control}},
  volume   = {61},
  issn     = {1053-587X},
  doi      = {10.1109/TSP.2013.2281021},
  abstract = {In this work, we present novel Bayesian algorithms for acoustic
              echo cancellation and residual echo suppression in the presence of
              a memoryless loudspeaker nonlinearity. The system nonlinearity is
              modeled using a basis-generic nonlinear expansion. This allows us
              to express the microphone observation in the DFT domain in terms of
              the nonlinear-expansion coefficients and the acoustic echo path. We
              augment the observation model with first-order Markov models for
              the echo-path vector and the nonlinear-expansion coefficients to
              arrive at a composite state-space model. The echo path vector and
              each nonlinear-expansion coefficient are designated as the unknown
              random variables in our Bayesian model. The posterior estimators
              for the random variables and the learning rules for the a priori
              unknown model parameters are then derived via the maximization of
              the variational lower bound on the log likelihood. We further show
              that a Bayesian post-filter for residual echo suppression can be
              derived by optimizing a minimum-mean-square error (MMSE) cost
              function subject to marginalization with respect to the posteriors
              estimated in the echo cancellation stage. The effectiveness of the
              approach is supported by simulation results and an analysis using
              instrumental performance measures.},
  number   = {23},
  journal  = {IEEE Transactions on Signal Processing},
  author   = {Malik, S. and Enzner, G.},
  month    = dec,
  year     = {2013},
  keywords = {Bayes methods, Random variables, variational techniques, least
              mean squares methods, Markov processes, Mathematical model,
              acoustic echo cancellation, acoustic echo path, acoustic signal
              processing, Acoustics, Adaptation models, basis-generic nonlinear
              expansion, Bayesian algorithms, Bayesian post-filter, Cascade
              modeling, composite state-space model, DFT domain, discrete Fourier
              transforms, echo path vector, echo suppression, filtering theory,
              first-order Markov models, instrumental performance measures,
              learning (artificial intelligence), learning rules, log likelihood,
              loudspeakers, memoryless loudspeaker nonlinearity, microphone
              observation, microphones, minimum-mean-square error cost function,
              nonlinear acoustic echo control, nonlinear echo cancellation,
              nonlinear-expansion coefficients, observation model, post-filtering
              , posterior estimators, recursive Bayesian estimator, residual echo
              suppression, state-space model, variational Bayesian learning,
              variational lower bound, Vectors},
  pages    = {5853--5867},
  file     = {IEEE Xplore Abstract
              Record:/Users/apodusenko/Zotero/storage/SP357V3E/abs_all.html:text/html;Malik
              and Enzner - 2013 - A Variational Bayesian Learning Approach for
              Nonli.pdf:/Users/apodusenko/Zotero/storage/4WHAQJ85/Malik and Enzner -
              2013 - A Variational Bayesian Learning Approach for
              Nonli.pdf:application/pdf}
}

@incollection{ortega_reinforcement_2011,
  series    = {Lecture {Notes} in {Computer} {Science}},
  title     = {Reinforcement {Learning} and the {Bayesian} {Control} {Rule}},
  copyright = {©2011 Springer-Verlag Berlin Heidelberg},
  isbn      = {978-3-642-22886-5 978-3-642-22887-2},
  url       = {http://link.springer.com/chapter/10.1007/978-3-642-22887-2_30},
  abstract  = {We present an actor-critic scheme for reinforcement learning in
               complex domains. The main contribution is to show that planning and
               I/O dynamics can be separated such that an intractable planning
               problem reduces to a simple multi-armed bandit problem, where each
               lever stands for a potentially arbitrarily complex policy.
               Furthermore, we use the Bayesian control rule to construct an
               adaptive bandit player that is universal with respect to a given
               class of optimal bandit players, thus indirectly constructing an
               adaptive agent that is universal with respect to a given class of
               policies.},
  language  = {en},
  number    = {6830},
  urldate   = {2014-10-26},
  booktitle = {Artificial {General} {Intelligence}},
  publisher = {Springer Berlin Heidelberg},
  author    = {Ortega, Pedro Alejandro and Braun, Daniel Alexander and Godsill,
               Simon},
  editor    = {Schmidhuber, Jürgen and Thórisson, Kristinn R. and Looks, Moshe},
  month     = jan,
  year      = {2011},
  keywords  = {reinforcement learning, Artificial Intelligence (incl. Robotics),
               Pattern Recognition, actor-critic, Algorithm Analysis and Problem
               Complexity, Bayesian control rule, Information Systems Applications
               (incl.Internet), Mathematical Logic and Formal Languages},
  pages     = {281--285},
  file      = {Ortega et al. - 2011 - Reinforcement Learning and the Bayesian Control
               Ru.pdf:/Users/apodusenko/Zotero/storage/NNDE5AM5/Ortega et al. - 2011 -
               Reinforcement Learning and the Bayesian Control
               Ru.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/MUG5FHNI/978-3-642-22887-2_30.html:text/html
               }
}

@misc{welling_van_2014,
  title   = {Van veel data, snelle computers en complexe modellen tot lerende
             machines (intreerede)},
  url     = {http://www.oratiereeks.nl/upload/pdf/PDF-8573weboratie_Welling_HR.pdf},
  urldate = {2014-04-17},
  author  = {Welling, Max},
  year    = {2014},
  file    = {Welling - 2014 - Van veel data, snelle computers en complexe
             modell.pdf:/Users/apodusenko/Zotero/storage/R48TWZZC/Welling - 2014 -
             Van veel data, snelle computers en complexe modell.pdf:application/pdf}
}

@book{simon_sciences_1996,
  address   = {Cambridge, Mass.},
  title     = {The sciences of the artificial},
  isbn      = {0-585-36010-3 978-0-585-36010-2 0-262-25700-9 978-0-262-25700-8},
  url       = {
               http://search.ebscohost.com/login.aspx?direct=true&scope=site&db=nlebk&db=nlabk&AN=49230
               },
  language  = {English},
  urldate   = {2014-04-12},
  publisher = {MIT Press},
  author    = {Simon, Herbert A},
  year      = {1996},
  file      = {Simon - 1996 - The sciences of the
               artificial.pdf:/Users/apodusenko/Zotero/storage/4A34WX98/Simon - 1996 -
               The sciences of the artificial.pdf:application/pdf}
}

@patent{pardo_systems_2014,
  title       = {Systems, methods, and apparatus for equalization preference learning},
  url         = {http://www.google.com/patents/US20140272883},
  abstract    = {Systems, methods, and apparatus for equalization preference
                 learning are provided. An example method includes receiving a first
                 label for a first audio concept for a media object and applying
                 active learning to select a first example not yet rated by a first
                 current user. The example method includes collecting a first user
                 rating, by the first current user, of the first example compared to
                 the first audio concept and applying transfer learning to combine
                 the first user rating with ratings from prior users of examples not
                 yet rated by the first current user to build a model of the first
                 audio concept. The example method includes creating a tool operable
                 by the first user to generate examples close to and far from the
                 first label to modify the media object.},
  nationality = {United States},
  assignee    = {Northwestern University},
  number      = {US20140272883 A1},
  urldate     = {2014-10-28},
  author      = {Pardo, Bryan and Madjar, Alexander M. and Little, David Frank and
                 Gergle, Darren},
  month       = sep,
  year        = {2014},
  note        = {U.S. Classification 434/319; International Classification G09B5/04;
                 Cooperative Classification G09B5/04},
  file        = {Pardo et al. - 2014 - Systems, methods, and apparatus for equalization
                 p.pdf:/Users/apodusenko/Zotero/storage/3BSPVGRC/Pardo et al. - 2014 -
                 Systems, methods, and apparatus for equalization p.pdf:application/pdf}
}

@patent{dillon_programmable_2013,
  title       = {Programmable auditory prosthesis with trainable automatic adaptation
                 to acoustic conditions},
  url         = {http://www.google.nl/patents/US8532317},
  abstract    = {An auditory prosthesis (30) comprising a microphone (27) for
                 receiving the sound and producing a microphone signal responding to
                 the received sound, an output device for providing audio signals in
                 a form receivable by a user of the prosthesis (30), a sound
                 processing unit (33) operable to receive the microphone signal and
                 carry out a processing operation on the microphone signal to
                 produce an output signal in a form suitable to operate the output
                 device, wherein the sound processing unit (33) is operable in a
                 first mode in which the processing operation comprises at least one
                 variable processing factor which is adjustable by a user to a
                 setting which causes the output signal of the sound processing unit
                 (33) to be adjusted according to the preference of the user for the
                 characteristics of the current acoustic environment.},
  nationality = {United States},
  assignee    = {Hearworks Pty Limited},
  number      = {US8532317 B2},
  urldate     = {2014-10-28},
  author      = {Dillon, Harvey and Zakis, Justin Andrew and McDermott, Hugh Joseph
                 and Keidser, Gitte},
  month       = sep,
  year        = {2013},
  note        = {U.S. Classification 381/314, 381/326, 381/315, 607/57; International
                 Classification A61N1/00, A61N1/36, H04R25/00; Cooperative
                 Classification H04R2225/41, H04R25/606, A61N1/36032},
  file        = {Dillon et al. - 2013 - Programmable auditory prosthesis with trainable
                 au.pdf:/Users/apodusenko/Zotero/storage/RFV6JW5T/Dillon et al. - 2013 -
                 Programmable auditory prosthesis with trainable au.pdf:application/pdf}
}

@book{frankl_mans_2006,
  address   = {Boston},
  title     = {Man's search for meaning},
  isbn      = {0-8070-1427-3 978-0-8070-1427-1 0-8070-1426-5 978-0-8070-1426-4},
  abstract  = {In this work, a Viennese psychiatrist tells his grim experiences
               in a German concentration camp which led him to logotherapy, an
               existential method of psychiatry. This work has riveted generations
               of readers with its descriptions of life in Nazi death camps and
               its lessons for spiritual survival. Between 1942 and 1945 the
               author, a psychiatrist labored in four different camps, including
               Auschwitz, while his parents, brother, and pregnant wife perished.
               Based on his own experience and the stories of his many patients,
               he argues that we cannot avoid suffering but we can choose how to
               cope with it, find meaning in it, and move forward with renewed
               purpose. His theory, known as logotherapy, from the Greek word
               logos (meaning), holds that our primary drive in life is not
               pleasure, as Freud maintained, but the discovery and pursuit of
               what we personally find meaningful.},
  language  = {English},
  publisher = {Beacon Press},
  author    = {Frankl, Viktor E},
  year      = {2006},
  file      = {Frankl - 2006 - Man's search for
               meaning.pdf:/Users/apodusenko/Zotero/storage/VPSS7RNF/Frankl - 2006 -
               Man's search for meaning.pdf:application/pdf}
}

@article{ziniel_binary_2014,
  title    = {Binary {Linear} {Classification} and {Feature} {Selection} via {
              Generalized} {Approximate} {Message} {Passing}},
  url      = {http://arxiv.org/abs/1401.0872},
  abstract = {For the problem of binary linear classification and feature
              selection, we propose algorithmic approaches to classifier design
              based on the generalized approximate message passing (GAMP)
              algorithm, recently proposed in the context of compressive sensing.
              Our work focuses on the regime where the number of features greatly
              exceeds the number of training examples, but where only a few
              features suffice for accurate classification. We show that
              sum-product GAMP can be used to (approximately) minimize the
              classification error rate and max-sum GAMP can be used to minimize
              a wide variety of regularized loss functions. Moreover, we show how
              a "turbo" extension to GAMP allows us to learn weight vectors that
              exhibit structured sparsity. Furthermore, we describe an
              expectation-maximization (EM)-based scheme to learn the associated
              model parameters online, as an alternative to cross-validation, and
              we show that GAMP's state evolution framework can be used to
              accurately predict the misclassification rate. Finally, we present
              a detailed numerical study to confirm the accuracy, speed, and
              flexibility afforded by our GAMP-based approaches to binary linear
              classification.},
  urldate  = {2014-04-19},
  journal  = {arXiv:1401.0872 [cs, math, stat]},
  author   = {Ziniel, Justin and Schniter, Philip},
  month    = jan,
  year     = {2014},
  keywords = {Statistics - Machine Learning, Computer Science - Information
              Theory},
  file     = {arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/D3FNK298/1401.html:text/html;Ziniel
              and Schniter - 2014 - Binary Linear Classification and Feature
              Selection.pdf:/Users/apodusenko/Zotero/storage/CDSH2NWE/Ziniel and
              Schniter - 2014 - Binary Linear Classification and Feature
              Selection.pdf:application/pdf}
}

@inproceedings{borji_bayesian_2013,
  title   = {Bayesian optimization explains human active search},
  url     = {http://machinelearning.wustl.edu/mlpapers/papers/NIPS2013_4952},
  urldate = {2014-10-16},
  author  = {Borji, Ali and Itti, Laurent},
  year    = {2013},
  pages   = {55--63},
  file    = {Borji and Itti - 2013 - Bayesian optimization explains human active
             search.pdf:/Users/apodusenko/Zotero/storage/FHNX7STC/Borji and Itti -
             2013 - Bayesian optimization explains human active
             search.pdf:application/pdf;Borji and Itti - 2013 - supplement -
             Bayesian optimization explains human active
             search.pdf:/Users/apodusenko/Zotero/storage/MRHQFZFQ/Borji and Itti -
             2013 - supplement - Bayesian optimization explains human active
             search.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/NDGSNC38/NIPS2013_4952.html:text/html
             }
}

@article{berger-tal_exploration-exploitation_2014,
  title      = {The {Exploration}-{Exploitation} {Dilemma}: {A} {Multidisciplinary} {
                Framework}},
  volume     = {9},
  shorttitle = {The {Exploration}-{Exploitation} {Dilemma}},
  url        = {http://dx.doi.org/10.1371/journal.pone.0095693},
  doi        = {10.1371/journal.pone.0095693},
  abstract   = {The trade-off between the need to obtain new knowledge and the
                need to use that knowledge to improve performance is one of the
                most basic trade-offs in nature, and optimal performance usually
                requires some balance between exploratory and exploitative
                behaviors. Researchers in many disciplines have been searching for
                the optimal solution to this dilemma. Here we present a novel model
                in which the exploration strategy itself is dynamic and varies with
                time in order to optimize a definite goal, such as the acquisition
                of energy, money, or prestige. Our model produced four very
                distinct phases: Knowledge establishment, Knowledge accumulation,
                Knowledge maintenance, and Knowledge exploitation, giving rise to a
                multidisciplinary framework that applies equally to humans, animals
                , and organizations. The framework can be used to explain a
                multitude of phenomena in various disciplines, such as the movement
                of animals in novel landscapes, the most efficient resource
                allocation for a start-up company, or the effects of old age on
                knowledge acquisition in humans.},
  number     = {4},
  urldate    = {2014-11-30},
  journal    = {PLoS ONE},
  author     = {Berger-Tal, Oded and Nathan, Jonathan and Meron, Ehud and Saltz,
                David},
  month      = apr,
  year       = {2014},
  pages      = {e95693},
  file       = {Berger-Tal et al. - 2014 - The Exploration-Exploitation Dilemma A
                Multidisci.pdf:/Users/apodusenko/Zotero/storage/PJU4EP36/Berger-Tal et
                al. - 2014 - The Exploration-Exploitation Dilemma A
                Multidisci.pdf:application/pdf;PLoS
                Snapshot:/Users/apodusenko/Zotero/storage/VWW9HXFR/infodoi10.1371journal.pone.html:text/html
                }
}

@article{moran_brain_2014,
  title      = {The {Brain} {Ages} {Optimally} to {Model} {Its} {Environment}: {
                Evidence} from {Sensory} {Learning} over the {Adult} {Lifespan}},
  volume     = {10},
  shorttitle = {The {Brain} {Ages} {Optimally} to {Model} {Its} {Environment}},
  url        = {http://dx.doi.org/10.1371/journal.pcbi.1003422},
  doi        = {10.1371/journal.pcbi.1003422},
  abstract   = {Author SummaryWhile studies of aging are widely framed in terms of
                their demarcation of degenerative processes, the brain provides a
                unique opportunity to uncover the adaptive effects of getting
                older. Though intuitively reasonable, that life-experience and
                wisdom should reside somewhere in human cortex, these features have
                eluded neuroscientific explanation. The present study utilizes a
                “Bayesian Brain” framework to motivate an analysis of cortical
                circuit processing. From a Bayesian perspective, the brain
                represents a model of its environment and offers predictions about
                the world, while responding, through changing synaptic strengths to
                novel interactions and experiences. We hypothesized that these
                predictive and updating processes are modified as we age,
                representing an optimization of neuronal architecture. Using novel
                sensory stimuli we demonstrate that synaptic connections of older
                brains resist trial by trial learning to provide a robust model of
                their sensory environment. These older brains are capable of
                processing a wider range of sensory inputs – representing
                experienced generalists. We thus explain how, contrary to a
                singularly degenerative point-of-view, aging neurobiological
                effects may be understood, in sanguine terms, as adaptive and
                useful.},
  number     = {1},
  urldate    = {2014-06-22},
  journal    = {PLoS Comput Biol},
  author     = {Moran, Rosalyn J. and Symmonds, Mkael and Dolan, Raymond J. and
                Friston, Karl J.},
  month      = jan,
  year       = {2014},
  pages      = {e1003422},
  file       = {Moran et al. - 2014 - The Brain Ages Optimally to Model Its
                Environment.pdf:/Users/apodusenko/Zotero/storage/EA9XZ965/Moran et al.
                - 2014 - The Brain Ages Optimally to Model Its
                Environment.pdf:application/pdf;PLoS
                Snapshot:/Users/apodusenko/Zotero/storage/CE3WGDEK/infodoi10.1371journal.pcbi.html:text/html
                }
}

@book{beck_test-driven_2003,
  address    = {Boston},
  title      = {Test-driven development: by example},
  isbn       = {0-321-14653-0 978-0-321-14653-3},
  shorttitle = {Test-driven development},
  abstract   = {"This book follows two TDD projects from start to finish,
                illustrating techniques programmers can use to easily and
                dramatically increase the quality of their work. The examples are
                followed by references to the featured TDD patterns and
                refactorings. With its emphasis on agile methods and fast
                development strategies, Test-Driven Development is sure to inspire
                readers to embrace these under-utilized but powerful techniques."
                --Jacket.},
  language   = {English},
  publisher  = {Addison-Wesley},
  author     = {Beck, Kent},
  year       = {2003},
  file       = {Beck - 2003 - Test-driven development by
                example.epub:/Users/apodusenko/Zotero/storage/S7JZBGUE/Beck - 2003 -
                Test-driven development by example.epub:application/octet-stream}
}

@article{weick_rethinking_nodate,
  title    = {Rethinking industrial research, development and innovation in the
              21st century},
  issn     = {0160-791X},
  url      = {http://www.sciencedirect.com/science/article/pii/S0160791X13000985},
  doi      = {10.1016/j.techsoc.2013.12.005},
  abstract = {Solving problems related to energy, water, food supplies, health
              and the environment requires breakthrough innovation and the
              fundamental research that underlies it. Enhanced governmental
              funding in research is critical, but industry also needs to step
              up. This article focuses on the ways Google, Edwards Lifesciences,
              Tesla Motors, and Space X are making investment in far sighted
              research a priority, and have developed processes that ensure that
              scientists and engineers remain central to the organization even as
              it grows. Their leaders demonstrate a passion for using science and
              technology to solve major problems, and view companies as vehicles
              for inventing the future.},
  urldate  = {2014-06-24},
  journal  = {Technology in Society},
  author   = {Weick, Cynthia Wagner and Jain, Ravi K.},
  keywords = {Basic research, Breakthrough innovation, Industrial research},
  file     = {ScienceDirect Full Text
              PDF:/Users/apodusenko/Zotero/storage/SBW352BQ/Weick and Jain -
              Rethinking industrial research, development and
              in.pdf:application/pdf;ScienceDirect
              Snapshot:/Users/apodusenko/Zotero/storage/DBXJF5QU/S0160791X13000985.html:text/html
              }
}

@article{manchon_receiver_2014,
  title   = {Receiver {Architectures} for {MIMO}-{OFDM} {Based} on a {Combined} {
             VMP}-{SP} {Algorithm}},
  issn    = {0018-9448},
  journal = {IEEE Transactions on Information Theory},
  author  = {Manchón, Carles Navarro},
  year    = {2014},
  file    = {Manchón - 2014 - Receiver Architectures for MIMO-OFDM Based on a
             Co.pdf:/Users/apodusenko/Zotero/storage/T3DFGR8N/Manchón - 2014 -
             Receiver Architectures for MIMO-OFDM Based on a
             Co.pdf:application/pdf;SFX by Ex Libris
             Inc.:/Users/apodusenko/Zotero/storage/P8WXKECR/sfxaub.html:text/html}
}

@misc{yedidia_generalized_2002,
  title  = {Generalized belief propagation and free energy minimization},
  author = {Yedidia, Jonathan S.},
  year   = {2002},
  file   = {Yedidia - 2002 - Generalized belief propagation and free energy
            min.pdf:/Users/apodusenko/Zotero/storage/VQQ4GF2I/Yedidia - 2002 -
            Generalized belief propagation and free energy min.pdf:application/pdf}
}

@article{kroll_signal_2014,
  title    = {A {Signal} {Processor} for {Gaussian} {Message} {Passing}},
  url      = {http://arxiv.org/abs/1404.3162},
  abstract = {In this paper, we present a novel signal processing unit built
              upon the theory of factor graphs, which is able to address a wide
              range of signal processing algorithms. More specifically, the
              demonstrated factor graph processor (FGP) is tailored to Gaussian
              message passing algorithms. We show how to use a highly
              configurable systolic array to solve the message update equations
              of nodes in a factor graph efficiently. A proper instruction set
              and compilation procedure is presented. In a recursive least
              squares channel estimation example we show that the FGP can compute
              a message update faster than a state-ofthe- art DSP. The results
              demonstrate the usabilty of the FGP architecture as a flexible HW
              accelerator for signal-processing and communication systems.},
  urldate  = {2014-04-19},
  journal  = {arXiv:1404.3162 [cs]},
  author   = {Kröll, Harald and Zwicky, Stefan and Odermatt, Reto and Bruderer,
              Lukas and Burg, Andreas and Huang, Qiuting},
  month    = apr,
  year     = {2014},
  keywords = {Computer Science - Hardware Architecture},
  file     = {arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/KDH5DBMV/1404.html:text/html;Kröll
              et al. - 2014 - A Signal Processor for Gaussian Message
              Passing.pdf:/Users/apodusenko/Zotero/storage/56USWTFW/Kröll et al. -
              2014 - A Signal Processor for Gaussian Message
              Passing.pdf:application/pdf}
}

@book{sarno_divided_2009,
  title     = {The divided mind},
  publisher = {HarperCollins},
  author    = {Sarno, John E.},
  year      = {2009},
  file      = {Sarno - 2009 - The divided
               mind.pdf:/Users/apodusenko/Zotero/storage/C8SENCRE/Sarno - 2009 - The
               divided mind.pdf:application/pdf}
}

@book{sarno_healing_2010,
  address    = {New York},
  title      = {Healing back pain: the mind-body connection},
  isbn       = {0-446-55768-4 978-0-446-55768-9},
  shorttitle = {Healing back pain},
  abstract   = {A guide to overcoming back pain without drugs or surgery
                identifies psychological factors that can cause back pain,
                discusses Tension Myositis Syndrome, and offers case histories and
                results from mind-body research.},
  language   = {English},
  publisher  = {Wellness Central},
  author     = {Sarno, John E},
  year       = {2010},
  file       = {Sarno - 2010 - Healing back pain the mind-body
                connection.pdf:/Users/apodusenko/Zotero/storage/JJ32Z9XE/Sarno - 2010 -
                Healing back pain the mind-body connection.pdf:application/pdf}
}

@book{rothman_behind_2005,
  address    = {Raleigh, N.C.},
  title      = {Behind closed doors: secrets of great management},
  isbn       = {0-9766940-2-6 978-0-9766940-2-1},
  shorttitle = {Behind closed doors},
  abstract   = {Great management is difficult to see as it occurs. It's possible
                to see the results of great management, but it's not easy to see
                how managers achieve those results. You can learn to be a better
                manager--even a great manager--with this guide. You'll follow along
                as Sam, a manager just brought on board, learns the ropes and deals
                with his new team over the course of his first eight weeks on the
                job. From scheduling and managing resources to helping team members
                grow and prosper, you'll be there as Sam makes it happen. You'll
                find powerful tips covering: Delegating effectively, Using feedback
                and goal-setting, Developing influence, Handling one-on-one
                meetings, Coaching and mentoring, Deciding what work to do--and
                what not to do. - Publisher.},
  language   = {English},
  publisher  = {Pragmatic Bookshelf},
  author     = {Rothman, Johanna and Derby, Esther},
  year       = {2005},
  file       = {Rothman and Derby - 2005 - Behind closed doors secrets of great
                management.pdf:/Users/apodusenko/Zotero/storage/9HCT24P6/Rothman and
                Derby - 2005 - Behind closed doors secrets of great
                management.pdf:application/pdf}
}

@misc{hanson_julia_2013,
  title   = {Julia {Helps}},
  url     = {http://blog.leahhanson.us/julia-helps.html},
  urldate = {2014-07-08},
  author  = {Hanson, Leah},
  year    = {2013},
  file    = {Hanson - 2013 - Julia
             Helps.html:/Users/apodusenko/Zotero/storage/6HQ2N2BJ/Hanson - 2013 -
             Julia Helps.html:text/html}
}

@misc{enthought_introduction_2014,
  title        = {Introduction to {Julia} - {Part} 2 {\textbar} {SciPy} 2014 {\textbar}
                  {David} {Sanders}},
  url          = {http://www.youtube.com/watch?v=I3JH5Bg46yU&feature=youtube_gdata_player
                  },
  urldate      = {2014-07-15},
  collaborator = {{Enthought}},
  month        = jul,
  year         = {2014}
}

@misc{enthought_introduction_2014-1,
  title        = {Introduction to {Julia} - {Part} 1 {\textbar} {SciPy} 2014 {\textbar}
                  {David} {Sanders}},
  url          = {http://www.youtube.com/watch?v=vWkgEddb4-A&feature=youtube_gdata_player
                  },
  urldate      = {2014-07-15},
  collaborator = {{Enthought}},
  month        = jul,
  year         = {2014}
}

@misc{bradleysetzler_kalman_nodate,
  title    = {Kalman {Filter} for {Panel} {Data} and {MLE} in {Julia}, {Part} 1},
  url      = {
              http://juliaeconomics.com/2014/06/27/kalman-filter-for-panel-data-and-mle-in-julia-part-1/
              },
  abstract = {* The script to reproduce the results of this tutorial in Julia is
              located here. The Kalman Filter is notoriously difficult to
              estimate. I think this can be attributed to the following issues:
              It n...},
  urldate  = {2014-07-15},
  journal  = {Julia/Economics},
  author   = {{bradleysetzler}},
  file     = {Kalman Filter for Panel Data and MLE in Julia, Part 1 |
              Julia/Economics:/Users/apodusenko/Zotero/storage/F6BSPUAF/kalman-filter-for-panel-data-and-mle-in-julia-part-1.html:text/html
              }
}

@misc{snell_julia_2014,
  title  = {Julia for {Machine} {Learning}},
  author = {Snell, Jake},
  month  = may,
  year   = {2014},
  file   = {Snell - 2014 - Julia for Machine
            Learning.pdf:/Users/apodusenko/Zotero/storage/XRF4QMBD/Snell - 2014 -
            Julia for Machine Learning.pdf:application/pdf}
}

@article{turner_time-frequency_2014,
  title  = {Time-frequency analysis as probabilistic infrence},
  author = {Turner, Ryan D. and Sahani, Maneesh},
  year   = {2014},
  file   = {
            gp-audio.pdf:/Users/apodusenko/Zotero/storage/NQAEWRP3/gp-audio.pdf:application/pdf;GTFtNMF.html:/Users/apodusenko/Zotero/storage/G6D3EMD6/GTFtNMF.html:text/html;probFB.html:/Users/apodusenko/Zotero/storage/GCHFRITJ/probFB.html:text/html;supplementary-material.pdf:/Users/apodusenko/Zotero/storage/3BEDN3MB/supplementary-material.pdf:application/pdf;Turner
            - 2012 - grant research note- Audio time-frequency analysis as
            probabilistic
            inference.pdf:/Users/apodusenko/Zotero/storage/KC4QFN9N/turner-first-grant-note.pdf:application/pdf;Turner
            and Sahani - 2014 - Time-frequency analysis as probabilistic
            infrence.pdf:/Users/apodusenko/Zotero/storage/T2ZK2W5H/Turner and
            Sahani - 2014 - Time-frequency analysis as probabilistic
            infrence.pdf:application/pdf}
}

@article{barto_novelty_2013,
  title    = {Novelty or {Surprise}?},
  volume   = {4},
  issn     = {1664-1078},
  url      = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3858647/},
  doi      = {10.3389/fpsyg.2013.00907},
  abstract = {Novelty and surprise play significant roles in animal behavior and
              in attempts to understand the neural mechanisms underlying it. They
              also play important roles in technology, where detecting
              observations that are novel or surprising is central to many
              applications, such as medical diagnosis, text processing,
              surveillance, and security. Theories of motivation, particularly of
              intrinsic motivation, place novelty and surprise among the primary
              factors that arouse interest, motivate exploratory or avoidance
              behavior, and drive learning. In many of these studies, novelty and
              surprise are not distinguished from one another: the words are used
              more-or-less interchangeably. However, while undeniably closely
              related, novelty and surprise are very different. The purpose of
              this article is first to highlight the differences between novelty
              and surprise and to discuss how they are related by presenting an
              extensive review of mathematical and computational proposals
              related to them, and then to explore the implications of this for
              understanding behavioral and neuroscience data. We argue that
              opportunities for improved understanding of behavior and its neural
              basis are likely being missed by failing to distinguish between
              novelty and surprise.},
  urldate  = {2014-06-24},
  journal  = {Frontiers in Psychology},
  author   = {Barto, Andrew and Mirolli, Marco and Baldassarre, Gianluca},
  month    = dec,
  year     = {2013},
  pmid     = {24376428},
  pmcid    = {PMC3858647},
  file     = {Barto et al. - 2013 - Novelty or
              Surprise.pdf:/Users/apodusenko/Zotero/storage/SRJNE6WQ/Barto et al. -
              2013 - Novelty or Surprise.pdf:application/pdf}
}

@misc{moldovan_conjugate_2010,
  title    = {The {Conjugate} {Prior} for the {Normal} {Distribution}},
  abstract = {Scribe from Stat260: Bayesian Modelling and Inference - Michael
              Jordan class},
  author   = {Moldovan, Teodor},
  year     = {2010},
  file     = {Moldovan - 2010 - The Conjugate Prior for the Normal
              Distribution.pdf:/Users/apodusenko/Zotero/storage/F5C9UNUU/Moldovan -
              2010 - The Conjugate Prior for the Normal
              Distribution.pdf:application/pdf}
}

@misc{motlik_efficiency_2014,
  title     = {Efficiency in {Development} {Workflows}},
  url       = {http://codeship.io},
  publisher = {ebook by codeship},
  author    = {Motlik, Florian},
  year      = {2014},
  file      = {Motlik - 2014 - Efficiency in Development
               Workflows.pdf:/Users/apodusenko/Zotero/storage/THSJBN8S/Motlik - 2014 -
               Efficiency in Development Workflows.pdf:application/pdf}
}

@inproceedings{lawrence_hierarchical_2007,
  title     = {Hierarchical {Gaussian} process latent variable models},
  url       = {http://dl.acm.org/citation.cfm?id=1273557},
  urldate   = {2015-01-28},
  booktitle = {Proceedings of the 24th international conference on {Machine}
               learning},
  publisher = {ACM},
  author    = {Lawrence, Neil D. and Moore, Andrew J.},
  year      = {2007},
  pages     = {481--488},
  file      = {Lawrence and Moore - 2007 - Hierarchical Gaussian process latent
               variable mode.pdf:/Users/apodusenko/Zotero/storage/PX52X9PG/Lawrence
               and Moore - 2007 - Hierarchical Gaussian process latent variable
               mode.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/RQPD2QJA/citation.html:text/html
               }
}

@phdthesis{csato_gaussian_2002,
  title  = {Gaussian {Processes} - {Iterative} {Sparse} {Approximations}},
  author = {Csató, Lehel},
  year   = {2002},
  file   = {Csató - 2002 - Gaussian Processes - Iterative Sparse
            Approximatio.pdf:/Users/apodusenko/Zotero/storage/T4DVRHUC/Csató - 2002
            - Gaussian Processes - Iterative Sparse
            Approximatio.pdf:application/pdf}
}

@incollection{quinonero-candela_analysis_2005,
  series    = {Lecture {Notes} in {Computer} {Science}},
  title     = {Analysis of {Some} {Methods} for {Reduced} {Rank} {Gaussian} {Process
               } {Regression}},
  copyright = {©2005 Springer-Verlag Berlin Heidelberg},
  isbn      = {978-3-540-24457-8 978-3-540-30560-6},
  url       = {http://link.springer.com/chapter/10.1007/978-3-540-30560-6_4},
  abstract  = {While there is strong motivation for using Gaussian Processes
               (GPs) due to their excellent performance in regression and
               classification problems, their computational complexity makes them
               impractical when the size of the training set exceeds a few
               thousand cases. This has motivated the recent proliferation of a
               number of cost-effective approximations to GPs, both for
               classification and for regression. In this paper we analyze one
               popular approximation to GPs for regression: the reduced rank
               approximation. While generally GPs are equivalent to infinite
               linear models, we show that Reduced Rank Gaussian Processes (RRGPs)
               are equivalent to finite sparse linear models. We also introduce
               the concept of degenerate GPs and show that they correspond to
               inappropriate priors. We show how to modify the RRGP to prevent it
               from being degenerate at test time. Training RRGPs consists both in
               learning the covariance function hyperparameters and the support
               set. We propose a method for learning hyperparameters for a given
               support set. We also review the Sparse Greedy GP (SGGP)
               approximation (Smola and Bartlett, 2001), which is a way of
               learning the support set for given hyperparameters based on
               approximating the posterior. We propose an alternative method to
               the SGGP that has better generalization capabilities. Finally we
               make experiments to compare the different ways of training a RRGP.
               We provide some Matlab code for learning RRGPs.},
  language  = {en},
  number    = {3355},
  urldate   = {2014-12-11},
  booktitle = {Switching and {Learning} in {Feedback} {Systems}},
  publisher = {Springer Berlin Heidelberg},
  author    = {Quiñonero-Candela, Joaquin and Rasmussen, Carl Edward},
  editor    = {Murray-Smith, Roderick and Shorten, Robert},
  month     = jan,
  year      = {2005},
  keywords  = {Artificial Intelligence (incl. Robotics), Simulation and Modeling,
               Computation by Abstract Devices, Dynamical Systems and Ergodic
               Theory, Probability and Statistics in Computer Science, Special
               Purpose and Application-Based Systems},
  pages     = {98--127},
  file      = {Quiñonero-Candela and Rasmussen - 2005 - Analysis of Some Methods for
               Reduced Rank
               Gaussian.pdf:/Users/apodusenko/Zotero/storage/XHR8A57R/Quiñonero-Candela
               and Rasmussen - 2005 - Analysis of Some Methods for Reduced Rank
               Gaussian.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/ZDEF5KBX/978-3-540-30560-6_4.html:text/html
               }
}

@article{quinonero-candela_unifying_2005,
  title    = {A {Unifying} {View} of {Sparse} {Approximate} {Gaussian} {Process} {
              Regression}},
  volume   = {6},
  issn     = {1532-4435},
  url      = {http://dl.acm.org/citation.cfm?id=1046920.1194909},
  abstract = {We provide a new unifying view, including all existing proper
              probabilistic sparse approximations for Gaussian process
              regression. Our approach relies on expressing the effective prior
              which the methods are using. This allows new insights to be gained,
              and highlights the relationship between existing methods. It also
              allows for a clear theoretically justified ranking of the closeness
              of the known approximations to the corresponding full GPs. Finally
              we point directly to designs of new better sparse approximations,
              combining the best of the existing strategies, within attractive
              computational constraints.},
  urldate  = {2014-12-11},
  journal  = {J. Mach. Learn. Res.},
  author   = {Quiñonero-Candela, Joaquin and Rasmussen, Carl Edward},
  month    = dec,
  year     = {2005},
  pages    = {1939--1959},
  file     = {Quiñonero-Candela and Rasmussen - 2005 - A Unifying View of Sparse
              Approximate Gaussian
              Pro.pdf:/Users/apodusenko/Zotero/storage/W8KD5RMW/Quiñonero-Candela and
              Rasmussen - 2005 - A Unifying View of Sparse Approximate Gaussian
              Pro.pdf:application/pdf}
}

@article{seeger_bayesian_2000,
  title   = {Bayesian {Model} {Selection} for {Support} {Vector} {Machines}, {
             Gaussian} {Processes} and {Other} {Kernel} {Classifiers}},
  url     = {http://infoscience.epfl.ch/record/161324},
  urldate = {2014-12-03},
  journal = {Proceedings of the 13th Annual Conference on Neural Information
             Processing Systems},
  author  = {Seeger, Matthias},
  year    = {2000},
  pages   = {603--609},
  file    = {Bayesian Model Selection for Support Vector Machines, Gaussian
             Processes and Other Kernel
             Classifiers.pdf:/Users/apodusenko/Zotero/storage/ITP4C8V3/Bayesian
             Model Selection for Support Vector Machines, Gaussian Processes and
             Other Kernel Classifiers.pdf:application/pdf}
}

@article{friston_post_2011,
  title    = {Post hoc {Bayesian} model selection},
  volume   = {56},
  issn     = {1053-8119},
  url      = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3112494/},
  doi      = {10.1016/j.neuroimage.2011.03.062},
  abstract = {This note describes a Bayesian model selection or optimization
              procedure for post hoc inferences about reduced versions of a full
              model. The scheme provides the evidence (marginal likelihood) for
              any reduced model as a function of the posterior density over the
              parameters of the full model. It rests upon specifying models
              through priors on their parameters, under the assumption that the
              likelihood remains the same for all models considered. This
              provides a quick and efficient scheme for scoring arbitrarily large
              numbers of models, after inverting a single (full) model. In turn,
              this enables the selection among discrete models that are
              distinguished by the presence or absence of free parameters, where
              free parameters are effectively removed from the model using very
              precise shrinkage priors. An alternative application of this post
              hoc model selection considers continuous model spaces, defined in
              terms of hyperparameters (sufficient statistics) of the prior
              density over model parameters. In this instance, the prior (model)
              can be optimized with respect to its evidence. The expressions for
              model evidence become remarkably simple under the Laplace
              (Gaussian) approximation to the posterior density. Special cases of
              this scheme include Savage–Dickey density ratio tests for reduced
              models and automatic relevance determination in model optimization.
              We illustrate the approach using general linear models and a more
              complicated nonlinear state-space model.},
  number   = {4-2},
  urldate  = {2014-12-02},
  journal  = {Neuroimage},
  author   = {Friston, Karl and Penny, Will},
  month    = jun,
  year     = {2011},
  pmid     = {21459150},
  pmcid    = {PMC3112494},
  pages    = {2089--2099},
  file     = {Friston and Penny - 2011 - Post hoc Bayesian model
              selection.pdf:/Users/apodusenko/Zotero/storage/6T6CKFUG/Friston and
              Penny - 2011 - Post hoc Bayesian model selection.pdf:application/pdf}
}

@book{deng_model-based_nodate,
  title    = {A {Model}-{Based} {Approach} for the {Development} of {Lms} {
              Algorithms}},
  abstract = {The LMS algorithm is one of the most popular adaptive filter
              algorithms. Many variants of the algorithm have been developed for
              different applications. In this paper, we propose a unified
              modelbased approach for developing LMS algorithms. We use a number
              of probability density functions to model the filtering error and
              the filter coefficients. The filter coefficients are determined by
              maximizing the posterior distribution function. We demonstrate that
              using this approach, we can not only develop existing LMS
              algorithms with further insights, we can also explore a number of
              new algorithms with certain desired properties such as robustness
              and sparseness. 1.},
  author   = {Deng, Guang and Ng, Wai-yin},
  file     = {Citeseer -
              Snapshot:/Users/apodusenko/Zotero/storage/8QV27GHP/summary.html:text/html;Deng
              and Ng - A Model-Based Approach for the Development of Lms
              .pdf:/Users/apodusenko/Zotero/storage/4QBKEUV7/Deng and Ng - A
              Model-Based Approach for the Development of Lms .pdf:application/pdf}
}

@article{mohammad-djafari_bayesian_2012,
  title     = {Bayesian approach with prior models which enforce sparsity in signal
               and image processing},
  volume    = {2012},
  copyright = {2012 Mohammad-Djafari; licensee Springer.},
  issn      = {1687-6180},
  url       = {http://asp.eurasipjournals.com/content/2012/1/52/abstract},
  doi       = {10.1186/1687-6180-2012-52},
  abstract  = {In this review article, we propose to use the Bayesian inference
               approach for inverse problems in signal and image processing, where
               we want to infer on sparse signals or images. The sparsity may be
               directly on the original space or in a transformed space. Here, we
               consider it directly on the original space (impulsive signals). To
               enforce the sparsity, we consider the probabilistic models and try
               to give an exhaustive list of such prior models and try to classify
               them. These models are either heavy tailed (generalized Gaussian,
               symmetric Weibull, Student-t or Cauchy, elastic net, generalized
               hyperbolic and Dirichlet) or mixture models (mixture of Gaussians,
               Bernoulli-Gaussian, Bernoulli-Gamma, mixture of translated
               Gaussians, mixture of multinomial, etc.). Depending on the prior
               model selected, the Bayesian computations (optimization for the
               joint maximum a posteriori (MAP) estimate or MCMC or variational
               Bayes approximations (VBA) for posterior means (PM) or complete
               density estimation) may become more complex. We propose these
               models, discuss on different possible Bayesian estimators, drive
               the corresponding appropriate algorithms, and discuss on their
               corresponding relative complexities and performances.},
  language  = {en},
  number    = {1},
  urldate   = {2014-11-02},
  journal   = {EURASIP Journal on Advances in Signal Processing},
  author    = {Mohammad-Djafari, Ali},
  month     = mar,
  year      = {2012},
  keywords  = {Bayesian approach, inverse problems, sparse priors, sparsity},
  pages     = {52},
  file      = {Mohammad-Djafari - 2012 - Bayesian approach with prior models which
               enforce .pdf:/Users/apodusenko/Zotero/storage/VERDUQ6C/Mohammad-Djafari
               - 2012 - Bayesian approach with prior models which enforce
               .pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/JM33WTFP/52.html:text/html
               }
}

@incollection{turner_two_2011,
  title     = {Two problems with variational expectation maximisation for
               time-series models},
  booktitle = {Bayesian {Time} series models},
  publisher = {Cambridge University Press},
  author    = {Turner, Richard and Sahani, Maneesh},
  year      = {2011},
  pages     = {109--130},
  file      = {slides - Two problems with variational expectation
               maximisa.pdf:/Users/apodusenko/Zotero/storage/NHQXXEPC/slides - Two
               problems with variational expectation
               maximisa.pdf:application/pdf;Turner and Sahani - 2011 - Two problems
               with variational expectation
               maximisa.pdf:/Users/apodusenko/Zotero/storage/362KS6I2/Turner and
               Sahani - 2011 - Two problems with variational expectation
               maximisa.pdf:application/pdf;video - Two problems with variational
               expectation maximisation for time-series models -
               ResearchGate:/Users/apodusenko/Zotero/storage/39542SDZ/video - Two
               problems with variational expectation maximisation for time-series
               models - ResearchGate.html:text/html}
}

@article{cunningham_unifying_2014,
  title    = {Unifying linear dimensionality reduction},
  url      = {http://arxiv.org/abs/1406.0873},
  abstract = {Linear dimensionality reduction methods are a cornerstone of
              analyzing high dimensional data, due to their simple geometric
              interpretations and typically attractive computational properties.
              These methods capture many data features of interest, such as
              covariance, dynamical structure, correlation between data sets,
              input-output relationships, and margin between data classes.
              Methods have been developed with a variety of names and motivations
              in many fields, and perhaps as a result the deeper connections
              between all these methods have not been understood. Here we unify
              methods from this disparate literature as optimization programs
              over matrix manifolds. We discuss principal component analysis,
              factor analysis, linear multidimensional scaling, Fisher's linear
              discriminant analysis, canonical correlations analysis, maximum
              autocorrelation factors, slow feature analysis, undercomplete
              independent component analysis, linear regression, and more. This
              optimization framework helps elucidate some rarely discussed
              shortcomings of well-known methods, such as the suboptimality of
              certain eigenvector solutions. Modern techniques for optimization
              over matrix manifolds enable a generic linear dimensionality
              reduction solver, which accepts as input data and an objective to
              be optimized, and returns, as output, an optimal low-dimensional
              projection of the data. This optimization framework further allows
              rapid development of novel variants of classical methods, which we
              demonstrate here by creating an orthogonal-projection canonical
              correlations analysis. More broadly, we suggest that our generic
              linear dimensionality reduction solver can move linear
              dimensionality reduction toward becoming a blackbox,
              objective-agnostic numerical technology.},
  urldate  = {2014-10-16},
  journal  = {arXiv:1406.0873 [stat]},
  author   = {Cunningham, John P. and Ghahramani, Zoubin},
  month    = jun,
  year     = {2014},
  note     = {arXiv: 1406.0873},
  keywords = {Statistics - Machine Learning},
  file     = {arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/7D2DTXVD/1406.html:text/html;Cunningham
              and Ghahramani - 2014 - Unifying linear dimensionality
              reduction.pdf:/Users/apodusenko/Zotero/storage/BNX29WMU/Cunningham and
              Ghahramani - 2014 - Unifying linear dimensionality
              reduction.pdf:application/pdf}
}

@article{csato_sparse_2002,
  title    = {Sparse {On}-{Line} {Gaussian} {Processes}},
  volume   = {14},
  issn     = {0899-7667},
  url      = {http://dx.doi.org/10.1162/089976602317250933},
  doi      = {10.1162/089976602317250933},
  abstract = {We develop an approach for sparse representations of gaussian
              process (GP) models (which are Bayesian types of kernel machines)
              in order to overcome their limitations for large data sets. The
              method is based on a combination of a Bayesian on-line algorithm,
              together with a sequential construction of a relevant subsample of
              the data that fully specifies the prediction of the GP model. By
              using an appealing parameterization and projection techniques in a
              reproducing kernel Hilbert space, recursions for the effective
              parameters and a sparse gaussian approximation of the posterior
              process are obtained. This allows for both a propagation of
              predictions and Bayesian error measures. The significance and
              robustness of our approach are demonstrated on a variety of
              experiments.},
  number   = {3},
  urldate  = {2014-07-25},
  journal  = {Neural Computation},
  author   = {Csató, Lehel and Opper, Manfred},
  month    = mar,
  year     = {2002},
  pages    = {641--668},
  file     = {Csató and Opper - 2002 - Sparse On-Line Gaussian
              Processes.pdf:/Users/apodusenko/Zotero/storage/MTX7RJFZ/Csató and Opper
              - 2002 - Sparse On-Line Gaussian Processes.pdf:application/pdf;Neural
              Computation
              Snapshot:/Users/apodusenko/Zotero/storage/VA8S5V9Z/089976602317250933.html:text/html
              }
}

@article{seeger_fast_2003,
  title    = {Fast {Forward} {Selection} to {Speed} {Up} {Sparse} {Gaussian} {
              Process} {Regression}},
  url      = {http://infoscience.epfl.ch/record/161318},
  urldate  = {2014-07-25},
  journal  = {Artificial Intelligence and Statistics 9},
  author   = {Seeger, Matthias and Williams, Christopher and Lawrence, Neil},
  year     = {2003},
  keywords = {Gaussian process, Greedy forward selection, Sparse approximation},
  file     = {Seeger et al. - 2003 - Fast Forward Selection to Speed Up Sparse
              Gaussian.pdf:/Users/apodusenko/Zotero/storage/74A8N8RB/Seeger et al. -
              2003 - Fast Forward Selection to Speed Up Sparse
              Gaussian.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/94NNJCMZ/161318.html:text/html
              }
}

@misc{snelson_sparse_2006,
  type     = {Conference or {Workshop} {Item}},
  title    = {Sparse {Gaussian} processes using pseudo-inputs},
  url      = {http://eprints.pascal-network.org/archive/00002916/},
  abstract = {We present a new Gaussian process (GP) regression model whose
              covariance is parameterized by the the locations of M pseudo-input
              points, which we learn by a gradient based optimization. We take M{
              \textless}N, where N is the number of real data points, and hence
              obtain a sparse regression method which has O(M{\textasciicircum}2
              N) training cost and O(M{\textasciicircum}2) prediction cost per
              test case. We also find hyperparameters of the covariance function
              in the same joint optimization. The method can be viewed as a
              Bayesian regression model with particular input dependent noise.
              The method turns out to be closely related to several other sparse
              GP approaches, and we discuss the relation in detail. We finally
              demonstrate its performance on some large data sets, and make a
              direct comparison to other sparse GP methods. We show that our
              method can match full GP performance with small M, i.e. very sparse
              solutions, and it significantly outperforms other approaches in
              this regime.},
  urldate  = {2014-07-25},
  author   = {Snelson, Ed and Ghahramani, Zoubin},
  year     = {2006},
  keywords = {Learning/Statistics \& Optimisation, Theory \& Algorithms},
  file     = {Snelson and Ghahramani - 2006 - Sparse Gaussian processes using
              pseudo-inputs.pdf:/Users/apodusenko/Zotero/storage/JDARWACM/Snelson and
              Ghahramani - 2006 - Sparse Gaussian processes using
              pseudo-inputs.pdf:application/pdf}
}

@misc{noauthor_julia_nodate-1,
  title   = {Julia {Studio}},
  url     = {http://forio.com/labs/julia-studio/tutorials/intermediate/1/index.html},
  urldate = {2014-07-22},
  file    = {Snapshot:/Users/apodusenko/Zotero/storage/GCGNM8UJ/1.html:text/html}
}

@inproceedings{huber_recursive_2013,
  title     = {Recursive {Gaussian} process regression},
  doi       = {10.1109/ICASSP.2013.6638281},
  abstract  = {For large data sets, performing Gaussian process regression is
               computationally demanding or even intractable. If data can be
               processed sequentially, the recursive regression method proposed in
               this paper allows incorporating new data with constant computation
               time. For this purpose two operations are performed alternating on
               a fixed set of so-called basis vectors used for estimating the
               latent function: First, inference of the latent function at the new
               inputs. Second, utilization of the new data for updating the
               estimate. Numerical simulations show that the proposed approach
               significantly reduces the computation time and at the same time
               provides more accurate estimates compared to existing on-line
               and/or sparse Gaussian process regression approaches.},
  booktitle = {2013 {IEEE} {International} {Conference} on {Acoustics}, {Speech}
               and {Signal} {Processing} ({ICASSP})},
  author    = {Huber, M.F.},
  month     = may,
  year      = {2013},
  keywords  = {Regression Analysis, Kalman filters, Bayesian filtering, Kernel,
               Bayes methods, Inference, Training, filtering theory, Vectors,
               basis vectors, constant computation time, data utilization,
               Gaussian processes, Joints, large data sets, latent function
               estimation, numerical simulations, on-line regression, recursive
               Gaussian process regression, recursive processing, Runtime,
               smoothing},
  pages     = {3362--3366},
  file      = {Huber - 2013 - Recursive Gaussian process
               regression.pdf:/Users/apodusenko/Zotero/storage/2REH4394/Huber - 2013 -
               Recursive Gaussian process regression.pdf:application/pdf;IEEE Xplore
               Abstract
               Record:/Users/apodusenko/Zotero/storage/B863ZH7N/abs_all.html:text/html
               }
}

@article{bezanson_array_2014,
  title      = {Array operators using multiple dispatch: a design methodology for
                array implementations in dynamic languages},
  shorttitle = {Array operators using multiple dispatch},
  url        = {http://arxiv.org/abs/1407.3845},
  doi        = {10.1145/2627373.2627383},
  abstract   = {Arrays are such a rich and fundamental data type that they tend to
                be built into a language, either in the compiler or in a large
                low-level library. Defining this functionality at the user level
                instead provides greater flexibility for application domains not
                envisioned by the language designer. Only a few languages, such as
                C++ and Haskell, provide the necessary power to define \$n\$
                -dimensional arrays, but these systems rely on compile-time
                abstraction, sacrificing some flexibility. In contrast, dynamic
                languages make it straightforward for the user to define any
                behavior they might want, but at the possible expense of
                performance. As part of the Julia language project, we have
                developed an approach that yields a novel trade-off between
                flexibility and compile-time analysis. The core abstraction we use
                is multiple dispatch. We have come to believe that while multiple
                dispatch has not been especially popular in most kinds of
                programming, technical computing is its killer application. By
                expressing key functions such as array indexing using multi-method
                signatures, a surprising range of behaviors can be obtained, in a
                way that is both relatively easy to write and amenable to compiler
                analysis. The compact factoring of concerns provided by these
                methods makes it easier for user-defined types to behave
                consistently with types in the standard library.},
  urldate    = {2014-07-17},
  journal    = {arXiv:1407.3845 [cs]},
  author     = {Bezanson, Jeff and Chen, Jiahao and Karpinski, Stefan and Shah,
                Viral and Edelman, Alan},
  month      = jul,
  year       = {2014},
  note       = {arXiv: 1407.3845},
  keywords   = {Computer Science - Programming Languages, D.3.3},
  file       = {arXiv.org
                Snapshot:/Users/apodusenko/Zotero/storage/QAAARVJC/1407.html:text/html;Bezanson
                et al. - 2014 - Array operators using multiple dispatch a design
                .pdf:/Users/apodusenko/Zotero/storage/3NAJ8B5A/Bezanson et al. - 2014 -
                Array operators using multiple dispatch a design .pdf:application/pdf}
}

@article{noauthor_srinivas_nodate,
  title = {Srinivas, {Krause}, {Seeger} - 2010 - {Gaussian} {Process} {
           Optimization} in the {Bandit} {Setting} {No} {Regret} and {
           Experimental} {Design}.pdf},
  file  = {Srinivas, Krause, Seeger - 2010 - Gaussian Process Optimization in the
           Bandit Setting No Regret and Experimental
           Design.pdf:/Users/apodusenko/Zotero/storage/Q6AS8ZWZ/Srinivas, Krause,
           Seeger - 2010 - Gaussian Process Optimization in the Bandit Setting No
           Regret and Experimental Design.pdf:application/pdf}
}

@article{langguth_tinnitus:_2013,
  title      = {Tinnitus: causes and clinical management},
  volume     = {12},
  issn       = {1474-4422},
  shorttitle = {Tinnitus},
  url        = {http://www.sciencedirect.com/science/article/pii/S1474442213701601},
  doi        = {10.1016/S1474-4422(13)70160-1},
  abstract   = {Summary Tinnitus is the perception of sound in the absence of a
                corresponding external acoustic stimulus. With prevalence ranging
                from 10\% to 15\%, tinnitus is a common disorder. Many people
                habituate to the phantom sound, but tinnitus severely impairs
                quality of life of about 1–2\% of all people. Tinnitus has
                traditionally been regarded as an otological disorder, but advances
                in neuroimaging methods and development of animal models have
                increasingly shifted the perspective towards its neuronal
                correlates. Increased neuronal firing rate, enhanced neuronal
                synchrony, and changes in the tonotopic organisation are recorded
                in central auditory pathways in reaction to deprived auditory input
                and represent—together with changes in non-auditory brain areas—the
                neuronal correlate of tinnitus. Assessment of patients includes a
                detailed case history, measurement of hearing function,
                quantification of tinnitus severity, and identification of causal
                factors, associated symptoms, and comorbidities. Most widely used
                treatments for tinnitus involve counselling, and best evidence is
                available for cognitive behavioural therapy. New pathophysiological
                insights have prompted the development of innovative brain-based
                treatment approaches to directly target the neuronal correlates of
                tinnitus.},
  number     = {9},
  urldate    = {2014-06-24},
  journal    = {The Lancet Neurology},
  author     = {Langguth, Berthold and Kreuzer, Peter M and Kleinjung, Tobias and De
                Ridder, Dirk},
  month      = sep,
  year       = {2013},
  pages      = {920--930},
  file       = {Langguth et al. - 2013 - Tinnitus causes and clinical
                management.pdf:/Users/apodusenko/Zotero/storage/84ZNHZE2/Langguth et
                al. - 2013 - Tinnitus causes and clinical
                management.pdf:application/pdf;ScienceDirect
                Snapshot:/Users/apodusenko/Zotero/storage/6RP7FGEB/S1474442213701601.html:text/html
                }
}

@inproceedings{smith_audio_2009,
  address = {Como, Italy},
  title   = {Audio {FFT} {Filter} {Banks}},
  author  = {Smith, Julius},
  year    = {2009},
  file    = {Smith - 2009 - Audio FFT Filter
             Banks.pdf:/Users/apodusenko/Zotero/storage/EP4Q3QPJ/Smith - 2009 -
             Audio FFT Filter Banks.pdf:application/pdf}
}

@article{ostwald_tutorial_2014,
  title    = {A tutorial on variational {Bayes} for latent linear stochastic
              time-series models},
  volume   = {60},
  issn     = {0022-2496},
  url      = {http://www.sciencedirect.com/science/article/pii/S0022249614000352},
  doi      = {10.1016/j.jmp.2014.04.003},
  abstract = {Variational Bayesian methods for the identification of latent
              stochastic time-series models comprising both observed and
              unobserved random variables have recently gained momentum in
              machine learning, theoretical neuroscience, and neuroimaging
              methods development. Despite their established use as a
              computationally efficient alternative to sampling-based methods,
              their practical application in mathematical psychology has so far
              been limited. In this tutorial we attempt to provide an
              introductory overview of the theoretical underpinnings that the
              variational Bayesian approach to latent stochastic time-series
              models rests on by discussing its application in the linear case.},
  urldate  = {2014-06-21},
  journal  = {Journal of Mathematical Psychology},
  author   = {Ostwald, Dirk and Kirilina, Evgeniya and Starke, Ludger and
              Blankenburg, Felix},
  month    = jun,
  year     = {2014},
  pages    = {1--19},
  file     = {Ostwald et al. - 2014 - A tutorial on variational Bayes for latent
              linear .pdf:/Users/apodusenko/Zotero/storage/8XSUP2SE/Ostwald et al. -
              2014 - A tutorial on variational Bayes for latent linear
              .pdf:application/pdf;Ostwald_et_al_R2_Supplement.pdf:/Users/apodusenko/Zotero/storage/T6XFZ769/Ostwald_et_al_R2_Supplement.pdf:application/pdf;ScienceDirect
              Snapshot:/Users/apodusenko/Zotero/storage/TXFNRTTJ/S0022249614000352.html:text/html
              }
}

@misc{noauthor_forio/julia-studio_nodate,
  title    = {forio/julia-studio},
  url      = {https://github.com/forio/julia-studio},
  abstract = {julia-studio - An IDE for the Julia Language},
  urldate  = {2014-06-13},
  journal  = {GitHub},
  file     = {
              Snapshot:/Users/apodusenko/Zotero/storage/MB8PMNX4/julia_studio_help.html:text/html
              }
}

@article{sarkka_recursive_2009,
  title    = {Recursive {Noise} {Adaptive} {Kalman} {Filtering} by {Variational} {
              Bayesian} {Approximations}},
  volume   = {54},
  abstract = {This article considers the application of variational Bayesian
              methods to joint recursive estimation of the dynamic state and the
              time-varying measurement noise parameters in linear state space
              models. The proposed adaptive Kalman filtering method is based on
              forming a separable variational approximation to the joint
              posterior distribution of states and noise parameters on each time
              step separately. The result is a recursive algorithm, where on each
              step the state is estimated with Kalman filter and the sufficient
              statistics of the noise variances are estimated with a fixed-point
              iteration. The performance of the algorithm is demonstrated with
              simulated data.},
  number   = {3},
  journal  = {IEEE Transactions on Automatic Control},
  author   = {Sarkka, S and Nummenmaa, A},
  year     = {2009},
  pages    = {596--600},
  file     = {Sarkka and Nummenmaa - 2009 - Recursive Noise Adaptive Kalman
              Filtering by Varia.pdf:/Users/apodusenko/Zotero/storage/INQQFNRE/Sarkka
              and Nummenmaa - 2009 - Recursive Noise Adaptive Kalman Filtering by
              Varia.pdf:application/pdf}
}

@techreport{de_freitas_hierarchical_1998,
  title       = {Hierarchical {Bayesian}-{Kalman} {Models} {For} {Regularisation} {And
                 } {ARD} {In} {Sequential} {Learning}},
  institution = {Department of Engineering, Cambridge Univercity},
  author      = {de Freitas, Nando and Niranjan, Mahesan and Gee, Andrew},
  year        = {1998},
  file        = {de Freitas et al. - 1998 - Hierarchical Bayesian-Kalman Models For
                 Regularisa.pdf:/Users/apodusenko/Zotero/storage/5VP3UCXD/de Freitas et
                 al. - 1998 - Hierarchical Bayesian-Kalman Models For
                 Regularisa.pdf:application/pdf}
}

@book{bezanson_julia_2014,
  title  = {Julia {Language} {Documentation} - {Release} 0.3.0-dev},
  author = {Bezanson, Jeff},
  year   = {2014},
  file   = {Bezanson - 2014 - Julia Language Documentation - Release
            0.3.0-dev.pdf:/Users/apodusenko/Zotero/storage/8NHVQ6VX/Bezanson - 2014
            - Julia Language Documentation - Release 0.3.0-dev.pdf:application/pdf}
}

@misc{github_github_2014,
  title   = {Github {Guides}},
  url     = {https://guides.github.com/},
  urldate = {2014-05-15},
  author  = {github},
  year    = {2014}
}

@phdthesis{bagautdinov_machine_2013,
  title  = {A {Machine} {Learning} {Framework} for {Bayesian} {Signal} {
            Processing}},
  school = {TU Eindhoven},
  author = {Bagautdinov, Timur},
  month  = aug,
  year   = {2013},
  file   = {Bagautdinov - 2013 - A Machine Learning Framework for Bayesian Signal
            P.pdf:/Users/apodusenko/Zotero/storage/IAPZ9HAR/Bagautdinov - 2013 - A
            Machine Learning Framework for Bayesian Signal P.pdf:application/pdf}
}

@incollection{rezek_ensemble_2005,
  series    = {Advanced {Information} and {Knowledge} {Processing}},
  title     = {Ensemble {Hidden} {Markov} {Models} with {Extended} {Observation} {
               Densities} for {Biosignal} {Analysis}},
  copyright = {©2005 Springer-Verlag London Limited},
  isbn      = {978-1-85233-778-0 978-1-84628-119-8},
  url       = {http://link.springer.com/chapter/10.1007/1-84628-119-9_14},
  abstract  = {Hidden Markov Models (HMM) have proven to be very useful in a
               variety of biomedical applications. The most established method for
               estimating HMM parameters is the maximum likelihood method which
               has shortcomings, such as repeated estimation and penalisation of
               the likelihood score, that are well known. This paper describes a
               variational learning approach to try and improve on the
               maximum-likelihood estimators. Emphasis lies on the fact that for
               HMMs with observation models that are from the exponential family
               of distributions, all HMM parameters and hidden state variables can
               be derived from a single loss function, namely the Kullback-Leibler
               divergence. Practical issues, such as model initialisation and
               choice of model order, are described. The paper concludes with
               application of three types of observation model HMMs to a variety
               of biomedical data, such as EEG and ECG, from different
               physiological experiments and conditions.},
  urldate   = {2014-05-15},
  booktitle = {Probabilistic {Modeling} in {Bioinformatics} and {Medical} {
               Informatics}},
  publisher = {Springer London},
  author    = {Rezek, Iead and Roberts, Stephen},
  editor    = {MSc, Dirk Husmeier DiplPhys and MSc, Richard Dybowski BSc and CPhys,
               DPhil, MIEEE, MIoP, Stephen Roberts MA},
  month     = jan,
  year      = {2005},
  keywords  = {Bioinformatics, Algorithm Analysis and Problem Complexity,
               Probability and Statistics in Computer Science, Health Informatics,
               Math Applications in Computer Science, Statistics for Life Sciences
               , Medicine, Health Sciences},
  pages     = {419--450},
  file      = {Rezek and Roberts - 2005 - Ensemble Hidden Markov Models with Extended
               Observ.pdf:/Users/apodusenko/Zotero/storage/BI5JM8I6/Rezek and Roberts
               - 2005 - Ensemble Hidden Markov Models with Extended
               Observ.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/Z3C5GSDG/10.html:text/html
               }
}

@misc{mackay_humble_2006,
  title  = {The humble {Gaussian} distribution},
  author = {MacKay, David},
  year   = {2006},
  file   = {MacKay - 2006 - The humble Gaussian
            distribution.pdf:/Users/apodusenko/Zotero/storage/MCKJFWF5/MacKay -
            2006 - The humble Gaussian distribution.pdf:application/pdf}
}

@misc{lin_julia_2013,
  title  = {A {Julia} {Framework} for {Bayesian} {Inference}},
  author = {Lin, Dahua},
  year   = {2013},
  file   = {Lin - 2013 - A Julia Framework for Bayesian
            Inference.pdf:/Users/apodusenko/Zotero/storage/Z4UNI6KI/Lin - 2013 - A
            Julia Framework for Bayesian Inference.pdf:application/pdf}
}

@misc{dauwels_factor_2006,
  title  = {Factor {Graph} {Based} {Inference}},
  author = {Dauwels, Justin},
  year   = {2006},
  file   = {Dauwels - 2006 - Factor Graph Based
            Inference.pdf:/Users/apodusenko/Zotero/storage/CCMW5KDS/Dauwels - 2006
            - Factor Graph Based Inference.pdf:application/pdf}
}

@misc{de_vries_data-driven_2014,
  title  = {Data-driven {Hearing} {Loss} {Compensation}},
  author = {de Vries, Bert},
  month  = may,
  year   = {2014},
  file   = {de Vries - 2014 - Data-driven Hearing Loss
            Compensation.pdf:/Users/apodusenko/Zotero/storage/CVVJNXAF/de Vries -
            2014 - Data-driven Hearing Loss Compensation.pdf:application/pdf}
}

@article{themelis_variational_2014,
  title    = {A variational {Bayes} framework for sparse adaptive estimation},
  url      = {http://arxiv.org/abs/1401.2771},
  abstract = {Recently, a number of mostly \${\textbackslash}ell\_1\$-norm
              regularized least squares type deterministic algorithms have been
              proposed to address the problem of {\textbackslash}emph\{sparse\}
              adaptive signal estimation and system identification. From a
              Bayesian perspective, this task is equivalent to maximum a
              posteriori probability estimation under a sparsity promoting
              heavy-tailed prior for the parameters of interest. Following a
              different approach, this paper develops a unifying framework of
              sparse {\textbackslash}emph\{variational Bayes\} algorithms that
              employ heavy-tailed priors in conjugate hierarchical form to
              facilitate posterior inference. The resulting fully automated
              variational schemes are first presented in a batch iterative form.
              Then it is shown that by properly exploiting the structure of the
              batch estimation task, new sparse adaptive variational Bayes
              algorithms can be derived, which have the ability to impose and
              track sparsity during real-time processing in a time-varying
              environment. The most important feature of the proposed algorithms
              is that they completely eliminate the need for computationally
              costly parameter fine-tuning, a necessary ingredient of sparse
              adaptive deterministic algorithms. Extensive simulation results are
              provided to demonstrate the effectiveness of the new sparse
              variational Bayes algorithms against state-of-the-art deterministic
              techniques for adaptive channel estimation. The results show that
              the proposed algorithms are numerically robust and exhibit in
              general superior estimation performance compared to their
              deterministic counterparts.},
  urldate  = {2014-04-28},
  journal  = {arXiv:1401.2771 [stat]},
  author   = {Themelis, Konstantinos E. and Rontogiannis, Athanasios A. and
              Koutroumbas, Konstantinos D.},
  month    = jan,
  year     = {2014},
  keywords = {Statistics - Machine Learning},
  file     = {arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/SJMBG89T/1401.html:text/html;Themelis
              et al. - 2014 - A variational Bayes framework for sparse adaptive
              .pdf:/Users/apodusenko/Zotero/storage/WD35S2KJ/Themelis et al. - 2014 -
              A variational Bayes framework for sparse adaptive .pdf:application/pdf}
}

@article{hershey_accelerating_2012,
  title      = {Accelerating {Inference}: towards a full {Language}, {Compiler} and {
                Hardware} stack},
  shorttitle = {Accelerating {Inference}},
  url        = {http://arxiv.org/abs/1212.2991},
  abstract   = {We introduce Dimple, a fully open-source API for probabilistic
                modeling. Dimple allows the user to specify probabilistic models in
                the form of graphical models, Bayesian networks, or factor graphs,
                and performs inference (by automatically deriving an inference
                engine from a variety of algorithms) on the model. Dimple also
                serves as a compiler for GP5, a hardware accelerator for inference.
                },
  urldate    = {2014-04-28},
  journal    = {arXiv:1212.2991 [cs, stat]},
  author     = {Hershey, Shawn and Bernstein, Jeff and Bradley, Bill and Schweitzer,
                Andrew and Stein, Noah and Weber, Theo and Vigoda, Ben},
  month      = dec,
  year       = {2012},
  keywords   = {Statistics - Machine Learning, Computer Science - Artificial
                Intelligence, Computer Science - Software Engineering},
  file       = {arXiv.org
                Snapshot:/Users/apodusenko/Zotero/storage/H54JM7IM/1212.html:text/html;Hershey
                et al. - 2012 - Accelerating Inference towards a full Language,
                C.pdf:/Users/apodusenko/Zotero/storage/WNNPHQV9/Hershey et al. - 2012 -
                Accelerating Inference towards a full Language, C.pdf:application/pdf}
}

@inproceedings{edwards_signal_2002,
  title     = {Signal processing, hearing aid design, and the psychoacoustic turing
               test},
  volume    = {4},
  doi       = {10.1109/ICASSP.2002.5745533},
  abstract  = {A psychoacoustic Turing Test is proposed as a means for assessing
               hearing aid signal processing: a hearing aid is considered
               successful if the aided impaired listener is indistinguishable from
               a normal hearing listener using psychoacoustic measures. Forward
               masking, loudness summation, and frequency resolution tests are
               examined within this construct. The performance of hearing impaired
               listeners using multiband compression is predicted and compared to
               the performance of normal hearing listeners. The results provide a
               measure of hearing aid performance and a method for selecting
               between different signal processing designs.},
  booktitle = {2002 {IEEE} {International} {Conference} on {Acoustics}, {Speech}
               , and {Signal} {Processing} ({ICASSP})},
  author    = {Edwards, Brent},
  month     = may,
  year      = {2002},
  keywords  = {Auditory system, Fires},
  pages     = {IV--3996--IV--3999},
  file      = {Edwards - 2002 - Signal processing, hearing aid design, and the
               psy.pdf:/Users/apodusenko/Zotero/storage/RTF8NXRK/Edwards - 2002 -
               Signal processing, hearing aid design, and the
               psy.pdf:application/pdf;IEEE Xplore Abstract
               Record:/Users/apodusenko/Zotero/storage/AFKAUSFH/login.html:text/html}
}

@article{levitt_transformed_1971,
  title    = {Transformed {Up}‐{Down} {Methods} in {Psychoacoustics}},
  volume   = {49},
  issn     = {0001-4966},
  url      = {
              http://scitation.aip.org/content/asa/journal/jasa/49/2B/10.1121/1.1912375
              },
  doi      = {10.1121/1.1912375},
  abstract = {During the past decade a number of variations in the simple
              up‐down procedure have been used in psychoacoustic testing. A broad
              class of these methods is described with due emphasis on the
              related problems of parameter estimation and the efficient placing
              of observations. The advantages of up‐down methods are many,
              including simplicity, high efficiency, robustness, small‐sample
              reliability, and relative freedom from restrictive assumptions.
              Several applications of these procedures in psychoacoustics are
              described, including examples where conventional techniques are
              inapplicable.},
  number   = {2B},
  urldate  = {2014-04-28},
  journal  = {The Journal of the Acoustical Society of America},
  author   = {Levitt, H.},
  month    = feb,
  year     = {1971},
  keywords = {Psychological acoustics, Testing procedures},
  pages    = {467--477},
  file     = {Levitt - 1971 - Transformed Up‐Down Methods in
              Psychoacoustics.pdf:/Users/apodusenko/Zotero/storage/X2E2PENI/Levitt -
              1971 - Transformed Up‐Down Methods in
              Psychoacoustics.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/U8C9BPA9/1.html:text/html
              }
}

@misc{de_vries_audiogram_2013,
  title  = {audiogram by {GPC}: scratch notes},
  author = {de Vries, Bert},
  year   = {2013},
  file   = {de Vries - 2013 - audiogram by GPC scratch
            notes.pdf:/Users/apodusenko/Zotero/storage/3RRB7FBF/de Vries - 2013 -
            audiogram by GPC scratch notes.pdf:application/pdf}
}

@article{grassi_mlp:_2009,
  title      = {{MLP}: {A} {MATLAB} toolbox for rapid and reliable auditory threshold
                estimation},
  volume     = {41},
  issn       = {1554-351X, 1554-3528},
  shorttitle = {{MLP}},
  url        = {http://www.psy.unipd.it/~grassi/mlp.html},
  doi        = {10.3758/BRM.41.1.20},
  abstract   = {In this article, we present MLP, a MATLAB toolbox enabling
                auditory thresholds estimation via the adaptive maximum likelihood
                procedure proposed by David Green (1990, 1993). This adaptive
                procedure is particularly appealing for those psychologists who
                need to estimate thresholds with a good degree of accuracy and in a
                short time. Together with a description of the toolbox, the present
                text provides an introduction to the threshold estimation theory
                and a theoretical explanation of the maximum likelihood adaptive
                procedure. MLP comes with a graphical interface, and it is provided
                with several built-in, classic psychoacoustics experiments ready to
                use at a mouse click.},
  language   = {en},
  number     = {1},
  urldate    = {2014-04-28},
  journal    = {Behavior Research Methods},
  author     = {Grassi, Massimo and Soranzo, Alessandro},
  month      = feb,
  year       = {2009},
  keywords   = {Cognitive Psychology},
  pages      = {20--28},
  file       = {Grassi and Soranzo - 2009 - MLP A MATLAB toolbox for rapid and
                reliable audit.pdf:/Users/apodusenko/Zotero/storage/FGTD9FNA/Grassi and
                Soranzo - 2009 - MLP A MATLAB toolbox for rapid and reliable
                audit.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/SNZV7JKT/10.3758BRM.41.1.html:text/html
                }
}

@misc{pigeon_online_2013,
  title   = {Online {Audiogram} and {Hearing} {Test}},
  url     = {http://myhearingtest.net/},
  urldate = {2014-04-25},
  author  = {Pigeon, Stephane},
  year    = {2013},
  file    = {Online Audiogram and Hearing Test | Unbiased &
             Free:/Users/apodusenko/Zotero/storage/HTR6QEI3/myhearingtest.net.html:text/html
             }
}

@article{stadler_probabilistic_2009,
  title      = {Probabilistic {Modelling} of {Hearing} : {Speech} {Recognition} and {
                Optimal} {Audiometry}},
  shorttitle = {Probabilistic {Modelling} of {Hearing}},
  url        = {http://www.diva-portal.org/smash/record.jsf?pid=diva2:216427},
  abstract   = {Hearing loss afflicts as many as 10{\textbackslash}\% of our
                population.Fortunately, technologies designed to alleviate the
                effects ofhearing loss are improving rapidly, including cochlear
                implantsand the increasi ...},
  language   = {eng},
  urldate    = {2014-04-25},
  author     = {Stadler, Svante},
  year       = {2009},
  note       = {Hearing loss afflicts as many as 10{\textbackslash}\% of our
                population.Fortunately, technologies designed to alleviate the effects
                ofhearing loss are improving rapidly, including cochlear implantsand
                the increasi ...},
  keywords   = {Audiometry, auditory models, Cochlear Implants, diagnostic methods
                , human speech recognition, optimal experiments, probabilistic
                modelling, psychoacoustics, speech modelling},
  file       = {
                Snapshot:/Users/apodusenko/Zotero/storage/A77DZ34F/record.html:text/html;Stadler
                - 2009 - Probabilistic Modelling of Hearing Speech
                Recogn.pdf:/Users/apodusenko/Zotero/storage/U85GAU34/Stadler - 2009 -
                Probabilistic Modelling of Hearing Speech Recogn.pdf:application/pdf}
}

@article{barthelme_flexible_2008,
  title    = {A flexible {Bayesian} method for adaptive measurement in
              psychophysics},
  url      = {http://arxiv.org/abs/0809.0387},
  abstract = {In psychophysical experiments time and the limited goodwill of
              participants is usually a major constraint. This has been the main
              motivation behind the early development of adaptive methods for the
              measurements of psychometric thresholds. More recently methods have
              been developed to measure whole psychometric functions in an
              adaptive way. Here we describe a Bayesian method to measure
              adaptively any aspect of a psychophysical function, taking
              inspiration from Kontsevich and Tyler's optimal Bayesian
              measurement method. Our method is implemented in a complete and
              easy-to-use MATLAB package.},
  urldate  = {2014-04-25},
  journal  = {arXiv:0809.0387 [stat]},
  author   = {Barthelmé, Simon and Mamassian, Pascal},
  month    = sep,
  year     = {2008},
  keywords = {Statistics - Applications},
  file     = {arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/BTV98MTN/0809.html:text/html;Barthelmé
              and Mamassian - 2008 - A flexible Bayesian method for adaptive
              measuremen.pdf:/Users/apodusenko/Zotero/storage/S3NPSNCG/Barthelmé and
              Mamassian - 2008 - A flexible Bayesian method for adaptive
              measuremen.pdf:application/pdf}
}

@article{g._noma_predicting_2013,
  title    = {Predicting hearing loss symptoms from {Audiometry} data using {FP}-{
              Growth} {Algorithm} and {Bayesian} {Classifier}},
  issn     = {1991-8178},
  url      = {http://eprints2.utem.edu.my/9888/},
  abstract = {This paper presents the results of applying machine learning
              algorithms to predict hearing loss symptoms given air and bone
              conduction audiometry thresholds. FP-Growth (frequent pattern
              growth) algorithm was employed as a feature extraction technique.
              The effect of extracting naïve Bayes classifier’s vocabulary from
              patterns generated by FP-Growth algorithm was explored. Both
              multivariate Bernoulli and multinomial naïve Bayes models were used
              with and without the feature extraction. The results were validated
              with repeated random sub-sampling validation performed using 5
              partitions with 10, 20, 30, 40 and 50 training examples
              respectively averaged over 10 iterations. The multivariate
              Bernoulli model with feature extraction is found to be more
              accurate in predicting hearing loss symptoms with average error
              rate of only 0, 0.5, 1, 1.75 and 5.4\% for the partitions with 10,
              20, 30, 40 and 50 training examples respectively compared to
              multinomial model with feature extraction. However, the two models
              with feature extraction produce better results than same models
              without feature extraction.},
  urldate  = {2014-04-25},
  journal  = {Australian Journal of Basic and Applied Sciences (AJBAS)},
  author   = {G. Noma, Nasir and Mohd Khanapi, Abd Ghani and Mohamad Khir,
              Abdullah and Noorizan, Yahya},
  month    = jul,
  year     = {2013},
  keywords = {QA76 Computer software},
  pages    = {35--43},
  file     = {Snapshot:/Users/apodusenko/Zotero/storage/C59V52Z5/9888.html:text/html
              }
}

@article{remus_comparison_2007,
  title      = {A comparison of adaptive psychometric procedures based on the theory
                of optimal experiments and bayesian techniques: implications for
                cochlear implant testing},
  volume     = {69},
  issn       = {0031-5117},
  shorttitle = {A comparison of adaptive psychometric procedures based on the
                theory of optimal experiments and bayesian techniques},
  abstract   = {Numerous previous studies have focused on the development of quick
                and efficient adaptive psychometric procedures. In psychophysics,
                there is often a model of the psychometric function supported by
                previous studies for the task of interest. The theory of optimal
                experiments provides a framework for utilizing a model of the
                process to develop quick and efficient sequential-testing
                strategies for estimating model parameters, making it appropriate
                for developing adaptive psychophysical-testing methods. In this
                study, we investigated the application of sequential parameter
                search strategies based on the theory of optimal experiments and
                Bayesian adaptive procedures for measuring psychophysical
                variables. The results presented in this article suggest that more
                sophisticated psychometric procedures can expedite the measurement
                of psychophysical variables. Such techniques for quickly collecting
                psychophysical data may be particularly useful in cochlear implant
                research, where a large set of psychophysical variables are useful
                for characterizing the performance of an implanted device. It is to
                be hoped that further development of these techniques will make
                psychophysical measurements available to clinicians for tuning and
                optimizing the speech processors of individual cochlear implant
                patients.},
  language   = {eng},
  number     = {3},
  journal    = {Perception \& psychophysics},
  author     = {Remus, Jeremiah J and Collins, Leslie M},
  month      = apr,
  year       = {2007},
  pmid       = {17672419},
  keywords   = {Humans, Bayes Theorem, Speech Perception, Cochlear Implants,
                Adaptation, Physiological, Models, Biological, Psychometrics,
                Psychophysics},
  pages      = {311--323},
  file       = {Remus and Collins - 2007 - A comparison of adaptive psychometric
                procedures b.pdf:/Users/apodusenko/Zotero/storage/Q8F6US6U/Remus and
                Collins - 2007 - A comparison of adaptive psychometric procedures
                b.pdf:application/pdf}
}

@article{bisgaard_standard_2010,
  title    = {Standard audiograms for the {IEC} 60118-15 measurement procedure},
  volume   = {14},
  issn     = {1940-5588},
  doi      = {10.1177/1084713810379609},
  abstract = {For the characterization of hearing aids, a new test method has
              been defined in the new International Electrotechnical Commission
              (IEC) standard 60118-15. For this characterization, the hearing aid
              will be set to actual user settings as programmed by standard
              fitting software from the hearing aid manufacturer. To limit the
              variation of programming outcomes, 10 standard audiograms, which
              cover the entire range of audiograms met in clinical practice, have
              been defined. This article describes how the set of standard
              audiograms has been developed. This set of standard audiogram has
              been derived by a vector quantization analysis method on a database
              of 28,244 audiograms. Using this analysis method, sets of typical
              audiograms have been obtained of sizes 12 and 60. It turned out
              that the smaller set could not be used for selecting audiograms as
              sloping audiograms were absent. Therefore, the larger set has been
              analyzed to provide seven standard audiograms for flat and
              moderately sloping hearing loss and three standard audiograms for
              steep hearing loss.},
  language = {eng},
  number   = {2},
  journal  = {Trends in amplification},
  author   = {Bisgaard, Nikolai and Vlaming, Marcel S M G and Dahlquist, Martin},
  month    = jun,
  year     = {2010},
  pmid     = {20724358},
  keywords = {Audiometry, Pure-Tone, Humans, Software, Acoustic Stimulation,
              Auditory Threshold, Equipment Design, Materials Testing, Models,
              Statistical, Signal Processing, Computer-Assisted, Sweden},
  pages    = {113--120},
  file     = {Bisgaard et al. - 2010 - Standard audiograms for the IEC 60118-15
              measureme.pdf:/Users/apodusenko/Zotero/storage/NNCI3G3Z/Bisgaard et al.
              - 2010 - Standard audiograms for the IEC 60118-15
              measureme.pdf:application/pdf}
}

@techreport{johnson_matlab_2002,
  title    = {{MATLAB} {Programming} {Style} {Guidelines} - {File} {Exchange} - {
              MATLAB} {Central}},
  url      = {
              http://www.mathworks.nl/matlabcentral/fileexchange/file_infos/2529-matlab-programming-style-guidelines
              },
  abstract = {13 pages of advice on writing clear code.},
  urldate  = {2014-04-22},
  author   = {Johnson, Richard},
  year     = {2002},
  keywords = {file exchange, link exchange, matlab and simulink community,
              matlab answers, matlab blog, matlab central, matlab community,
              newsgroup access, simulink blog},
  file     = {Johnson - 2002 - MATLAB Programming Style Guidelines - File
              Exchang.pdf:/Users/apodusenko/Zotero/storage/FZMPXM47/Johnson - 2002 -
              MATLAB Programming Style Guidelines - File
              Exchang.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/DDBSC6WN/2529-matlab-programming-style-guidelines.html:text/html
              }
}

@misc{peyton_jones_how_2004,
  title   = {How to {Write} a {Great} {Research} {Paper} - {Seven} {Simple} {
             Suggestions}},
  url     = {https://www.youtube.com/watch?v=g3dkRsTqdDA},
  urldate = {2014-04-21},
  author  = {Peyton Jones, Simon},
  year    = {2004},
  file    = {How to write a great research paper - Microsoft
             Research:/Users/apodusenko/Zotero/storage/FLH492JU/write-great-research-paper.html:text/html;Video
             & Audio\: How to write a great research paper - 2012 Lecture 6 -
             Research Skills -
             Metadata:/Users/apodusenko/Zotero/storage/I7G73A8W/1464870.html:text/html
             }
}

@misc{peyton_jones_how_1993,
  title  = {How to give a good research talk},
  author = {Peyton Jones, Simon},
  year   = {1993},
  file   = {How to give a good research talk" by Simon Peyton Jones -
            YouTube:/Users/apodusenko/Zotero/storage/ANJ92GUF/playlist.html:text/html;Peyton
            Jones - 1993 - How to give a good research
            talk.pdf:/Users/apodusenko/Zotero/storage/G6EEKFW6/Peyton Jones - 1993
            - How to give a good research talk.pdf:application/pdf}
}

@article{cassidy_bayesian_2002,
  title    = {Bayesian nonstationary autoregressive models for biomedical signal
              analysis},
  volume   = {49},
  issn     = {0018-9294},
  doi      = {10.1109/TBME.2002.803511},
  abstract = {We describe a variational Bayesian algorithm for the estimation of
              a multivariate autoregressive model with time-varying coefficients
              that adapt according to a linear dynamical system. The algorithm
              allows for time and frequency domain characterization of
              nonstationary multivariate signals and is especially suited to the
              analysis of event-related data. Results are presented on synthetic
              data and real electroencephalogram data recorded in event-related
              desynchronization and photic synchronization scenarios.},
  number   = {10},
  journal  = {IEEE Transactions on Biomedical Engineering},
  author   = {Cassidy, M.J. and Penny, W.D.},
  month    = oct,
  year     = {2002},
  keywords = {Humans, Regression Analysis, Algorithms, Signal processing
              algorithms, Bayes methods, medical signal processing, Bayesian
              methods, Bayes Theorem, Computer Simulation, autoregressive
              processes, Likelihood Functions, Models, Statistical, Signal
              Processing, Computer-Assisted, Algorithm design and analysis,
              Bayesian nonstationary autoregressive models, biomedical signal
              analysis, Brain modeling, EEG analysis, electroencephalography,
              event-related desynchronization, Evoked Potentials, Visual,
              frequency domain characterization, Frequency synchronization,
              frequency-domain analysis, Kalman smoother, linear dynamical system
              , Linear Models, Nervous system, photic synchronization scenarios,
              physiological models, Quality Control, Signal analysis, time domain
              characterization, Time Factors, time series, Time series analysis,
              Time varying systems, time-domain analysis, time-varying
              coefficients, variational Bayesian algorithm},
  pages    = {1142--1152},
  file     = {Cassidy and Penny - 2002 - Bayesian nonstationary autoregressive
              models for b.pdf:/Users/apodusenko/Zotero/storage/5X6JI4T4/Cassidy and
              Penny - 2002 - Bayesian nonstationary autoregressive models for
              b.pdf:application/pdf;IEEE Xplore Abstract
              Record:/Users/apodusenko/Zotero/storage/FCDK52XZ/login.html:text/html}
}

@phdthesis{dauwels_graphical_2006,
  title    = {On {Graphical} {Models} for {Communications} and {Machine} {Learning}
              : {Algorithms}, {Bounds}, and {Analog} {Implementation}},
  url      = {http://www.dauwels.com/Justin/thesis.pdf},
  school   = {ETH Zurich},
  author   = {Dauwels, J.},
  year     = {2006},
  keywords = {EM Algorithm},
  file     = {Dauwels - On Graphical Models For Communications and Machine
              Learning.pdf:/Users/apodusenko/Zotero/storage/QK6PT8H3/Dauwels - On
              Graphical Models For COmmunications and Machine
              Learning.pdf:application/pdf}
}

@article{wand_fully_2013,
  title   = {Fully {Simplified} {Multivariate} {Normal} {Updates} in {Non}-{
             Conjugate} {Variational} {Message} {Passing}},
  url     = {http://works.bepress.com/matt_wand/8},
  urldate = {2014-04-17},
  author  = {Wand, Matt},
  year    = {2013},
  file    = {
             Snapshot:/Users/apodusenko/Zotero/storage/DX46I656/8.html:text/html;Wand
             - 2013 - Fully Simplified Multivariate Normal Updates in
             No.pdf:/Users/apodusenko/Zotero/storage/M4EUHB6S/Wand - 2013 - Fully
             Simplified Multivariate Normal Updates in No.pdf:application/pdf}
}

@inproceedings{gray_characteristic_2002,
  title     = {Characteristic functions in radar and sonar},
  url       = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1026999},
  urldate   = {2014-04-15},
  booktitle = {System {Theory}, 2002. {Proceedings} of the {Thirty}-{Fourth} {
               Southeastern} {Symposium} on},
  publisher = {IEEE},
  author    = {Gray, John E. and Addison, S. R.},
  year      = {2002},
  pages     = {31--35},
  file      = {Gray and Addison - 2002 - Characteristic functions in radar and
               sonar.pdf:/Users/apodusenko/Zotero/storage/9CZNNHDG/Gray and Addison -
               2002 - Characteristic functions in radar and sonar.pdf:application/pdf}
}

@techreport{hennig_animating_2013,
  type        = {Technical},
  title       = {Animating {Samples} from {Gaussian} {Distributions}},
  abstract    = {The animations displayed below are animated samples from
                 correlated Gaussian beliefs, following closed trajectories along
                 equipotential lines of the probability distribution. They o⬚er a
                 more expressive view of the structure of samples from Gaussian
                 processes than static samples. This document explains how to
                 generate them, using Matlab, tikz, and LATEX, in that order. If you
                 do not see two animations with wobbly lines on the ⬚rst proper page
                 of this document, try opening it with Adobe Reader.},
  number      = {8},
  institution = {MAX PLANCK INSTITUTE FOR INTELLIGENT SYSTEMS},
  author      = {Hennig, Philipp},
  year        = {2013},
  file        = {Hennig - 2013 - Animating Samples from Gaussian
                 Distributions.pdf:/Users/apodusenko/Zotero/storage/QNMPMRAU/Hennig -
                 2013 - Animating Samples from Gaussian
                 Distributions.pdf:application/pdf}
}

@article{deng_tutorial_2014,
  title    = {A tutorial survey of architectures, algorithms, and applications for
              deep learning},
  volume   = {3},
  doi      = {10.1017/atsip.2013.9},
  abstract = {In this invited paper, my overview material on the same topic as
              presented in the plenary overview session of APSIPA-2011 and the
              tutorial material presented in the same conference [1] are expanded
              and updated to include more recent developments in deep learning.
              The previous and the updated materials cover both theory and
              applications, and analyze its future directions. The goal of this
              tutorial survey is to introduce the emerging area of deep learning
              or hierarchical learning to the APSIPA community. Deep learning
              refers to a class of machine learning techniques, developed largely
              since 2006, where many stages of non-linear information processing
              in hierarchical architectures are exploited for pattern
              classification and for feature learning. In the more recent
              literature, it is also connected to representation learning, which
              involves a hierarchy of features or concepts where higher-level
              concepts are defined from lower-level ones and where the same
              lower-level concepts help to define higher-level ones. In this
              tutorial survey, a brief history of deep learning research is
              discussed first. Then, a classificatory scheme is developed to
              analyze and summarize major work reported in the recent deep
              learning literature. Using this scheme, I provide a
              taxonomy-oriented survey on the existing deep architectures and
              algorithms in the literature, and categorize them into three
              classes: generative, discriminative, and hybrid. Three
              representative deep architectures – deep autoencoders, deep
              stacking networks with their generalization to the temporal domain
              (recurrent networks), and deep neural networks (pretrained with
              deep belief networks) – one in each of the three classes, are
              presented in more detail. Next, selected applications of deep
              learning are reviewed in broad areas of signal and information
              processing including audio/speech, image/vision, multimodality,
              language modeling, natural language processing, and information
              retrieval. Finally, future directions of deep learning are
              discussed and analyzed.},
  journal  = {APSIPA Transactions on Signal and Information Processing},
  author   = {Deng, Li},
  year     = {2014},
  keywords = {Algorithms, Deep learning, Information processing},
  pages    = {null--null},
  file     = {Cambridge Journals
              Snapshot:/Users/apodusenko/Zotero/storage/EE4IHMSG/displayAbstract.html:text/html;Deng
              - 2014 - A tutorial survey of architectures, algorithms,
              an.pdf:/Users/apodusenko/Zotero/storage/PSCDX4K9/Deng - 2014 - A
              tutorial survey of architectures, algorithms, an.pdf:application/pdf}
}

@article{roberts_gaussian_2012,
  title    = {Gaussian processes for time-series modelling},
  volume   = {371},
  issn     = {1364-503X, 1471-2962},
  url      = {
              http://rsta.royalsocietypublishing.org/content/371/1984/20110550.full.pdf+html
              },
  doi      = {10.1098/rsta.2011.0550},
  language = {en},
  number   = {1984},
  urldate  = {2014-04-12},
  journal  = {Philosophical Transactions of the Royal Society A: Mathematical,
              Physical and Engineering Sciences},
  author   = {Roberts, S. and Osborne, M. and Ebden, M. and Reece, S. and Gibson,
              N. and Aigrain, S.},
  month    = dec,
  year     = {2012},
  pages    = {20110550--20110550},
  file     = {Roberts et al. - 2012 - Gaussian processes for time-series
              modelling.pdf:/Users/apodusenko/Zotero/storage/H4Q8RF3Z/Roberts et al.
              - 2012 - Gaussian processes for time-series
              modelling.pdf:application/pdf;Sign
              In:/Users/apodusenko/Zotero/storage/4U57VFEV/20110550.full.html:text/html
              }
}

@article{ghahramani_bayesian_2013,
  title    = {Bayesian non-parametrics and the probabilistic approach to modelling},
  volume   = {371},
  issn     = {1364-503X, 1471-2962},
  url      = {http://rsta.royalsocietypublishing.org/content/371/1984/20110553},
  doi      = {10.1098/rsta.2011.0553},
  abstract = {Modelling is fundamental to many fields of science and
              engineering. A model can be thought of as a representation of
              possible data one could predict from a system. The probabilistic
              approach to modelling uses probability theory to express all
              aspects of uncertainty in the model. The probabilistic approach is
              synonymous with Bayesian modelling, which simply uses the rules of
              probability theory in order to make predictions, compare
              alternative models, and learn model parameters and structure from
              data. This simple and elegant framework is most powerful when
              coupled with flexible probabilistic models. Flexibility is achieved
              through the use of Bayesian non-parametrics. This article provides
              an overview of probabilistic modelling and an accessible survey of
              some of the main tools in Bayesian non-parametrics. The survey
              covers the use of Bayesian non-parametrics for modelling unknown
              functions, density estimation, clustering, time-series modelling,
              and representing sparsity, hierarchies, and covariance structure.
              More specifically, it gives brief non-technical overviews of
              Gaussian processes, Dirichlet processes, infinite hidden Markov
              models, Indian buffet processes, Kingman’s coalescent, Dirichlet
              diffusion trees and Wishart processes.},
  language = {en},
  number   = {1984},
  urldate  = {2014-04-12},
  journal  = {Philosophical Transactions of the Royal Society A: Mathematical,
              Physical and Engineering Sciences},
  author   = {Ghahramani, Zoubin},
  month    = feb,
  year     = {2013},
  pmid     = {23277609},
  keywords = {machine learning, probabilistic modelling, Bayesian statistics,
              non-parametrics},
  pages    = {20110553},
  file     = {Ghahramani - 2013 - Bayesian non-parametrics and the probabilistic
              app.pdf:/Users/apodusenko/Zotero/storage/MEXFFZTJ/Ghahramani - 2013 -
              Bayesian non-parametrics and the probabilistic
              app.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/UXETBUFE/20110553.html:text/html
              }
}

@article{bishop_model-based_2013,
  title    = {Model-based machine learning},
  volume   = {371},
  issn     = {1364-503X, 1471-2962},
  url      = {http://rsta.royalsocietypublishing.org/content/371/1984/20120222},
  doi      = {10.1098/rsta.2012.0222},
  abstract = {Several decades of research in the field of machine learning have
              resulted in a multitude of different algorithms for solving a broad
              range of problems. To tackle a new application, a researcher
              typically tries to map their problem onto one of these existing
              methods, often influenced by their familiarity with specific
              algorithms and by the availability of corresponding software
              implementations. In this study, we describe an alternative
              methodology for applying machine learning, in which a bespoke
              solution is formulated for each new application. The solution is
              expressed through a compact modelling language, and the
              corresponding custom machine learning code is then generated
              automatically. This model-based approach offers several major
              advantages, including the opportunity to create highly tailored
              models for specific scenarios, as well as rapid prototyping and
              comparison of a range of alternative models. Furthermore, newcomers
              to the field of machine learning do not have to learn about the
              huge range of traditional methods, but instead can focus their
              attention on understanding a single modelling environment. In this
              study, we show how probabilistic graphical models, coupled with
              efficient inference algorithms, provide a very flexible foundation
              for model-based machine learning, and we outline a large-scale
              commercial application of this framework involving tens of millions
              of users. We also describe the concept of probabilistic programming
              as a powerful software environment for model-based machine learning
              , and we discuss a specific probabilistic programming language
              called Infer.NET, which has been widely used in practical
              applications.},
  language = {en},
  number   = {1984},
  urldate  = {2014-04-12},
  journal  = {Philosophical Transactions of the Royal Society A: Mathematical,
              Physical and Engineering Sciences},
  author   = {Bishop, Christopher M.},
  month    = feb,
  year     = {2013},
  pmid     = {23277612},
  keywords = {Bayesian inference, graphical probabilistic programming, Infer.
              NET},
  pages    = {20120222},
  file     = {Bishop - 2013 - Model-based machine
              learning.pdf:/Users/apodusenko/Zotero/storage/5Z3G4KRN/Bishop - 2013 -
              Model-based machine
              learning.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/DZN8HQVQ/20120222.html:text/html
              }
}

@article{blei_build_2014,
  title      = {Build, {Compute}, {Critique}, {Repeat}: {Data} {Analysis} with {
                Latent} {Variable} {Models}},
  volume     = {1},
  shorttitle = {Build, {Compute}, {Critique}, {Repeat}},
  url        = {
                http://www.annualreviews.org/doi/abs/10.1146/annurev-statistics-022513-115657
                },
  doi        = {10.1146/annurev-statistics-022513-115657},
  abstract   = {We survey latent variable models for solving data-analysis
                problems. A latent variable model is a probabilistic model that
                encodes hidden patterns in the data. We uncover these patterns from
                their conditional distribution and use them to summarize data and
                form predictions. Latent variable models are important in many
                fields, including computational biology, natural language
                processing, and social network analysis. Our perspective is that
                models are developed iteratively: We build a model, use it to
                analyze data, assess how it succeeds and fails, revise it, and
                repeat. We describe how new research has transformed these
                essential activities. First, we describe probabilistic graphical
                models, a language for formulating latent variable models. Second,
                we describe mean field variational inference, a generic algorithm
                for approximating conditional distributions. Third, we describe how
                to use our analyses to solve problems: exploring the data, forming
                predictions, and pointing us in the direction of improved models.},
  number     = {1},
  urldate    = {2014-04-12},
  journal    = {Annual Review of Statistics and Its Application},
  author     = {Blei, David M.},
  year       = {2014},
  pages      = {203--232},
  file       = {Blei - 2014 - Build, Compute, Critique, Repeat Data Analysis
                wi.pdf:/Users/apodusenko/Zotero/storage/SU5UBUMJ/Blei - 2014 - Build,
                Compute, Critique, Repeat Data Analysis wi.pdf:application/pdf}
}

@article{beal_variational_2006,
  title    = {Variational {Bayesian} learning of directed graphical models with
              hidden variables},
  volume   = {1},
  issn     = {1936-0975, 1931-6690},
  url      = {http://projecteuclid.org/euclid.ba/1340370943},
  doi      = {10.1214/06-BA126},
  abstract = {A key problem in statistics and machine learning is inferring
              suitable structure of a model given some observed data. A Bayesian
              approach to model comparison makes use of the marginal likelihood
              of each candidate model to form a posterior distribution over
              models; unfortunately for most models of interest, notably those
              containing hidden or latent variables, the marginal likelihood is
              intractable to compute. We present the variational Bayesian (VB)
              algorithm for directed graphical models, which optimises a lower
              bound approximation to the marginal likelihood in a procedure
              similar to the standard EM algorithm. We show that for a large
              class of models, which we call conjugate exponential, the VB
              algorithm is a straightforward generalisation of the EM algorithm
              that incorporates uncertainty over model parameters. In a thorough
              case study using a small class of bipartite DAGs containing hidden
              variables, we compare the accuracy of the VB approximation to
              existing asymptotic-data approximations such as the Bayesian
              Information Criterion (BIC) and the Cheeseman-Stutz (CS) criterion,
              and also to a sampling based gold standard, Annealed Importance
              Sampling (AIS). We find that the VB algorithm is empirically
              superior to CS and BIC, and much faster than AIS. Moreover, we
              prove that a VB approximation can always be constructed in such a
              way that guarantees it to be more accurate than the CS
              approximation.},
  language = {EN},
  number   = {4},
  urldate  = {2014-04-12},
  journal  = {Bayesian Analysis},
  author   = {Beal, Matthew J. and Ghahramani, Zoubin},
  month    = dec,
  year     = {2006},
  note     = {Mathematical Reviews number (MathSciNet) MR2282207},
  keywords = {Graphical models, Variational Bayes, Model selection, Markov chain
              Monte Carlo, EM Algorithm, approximate Bayesian inference, Bayes
              Factors, Directed Acyclic Graphs},
  pages    = {793--831},
  file     = {Beal and Ghahramani - 2006 - Variational Bayesian learning of directed
              graphica.pdf:/Users/apodusenko/Zotero/storage/R7ZWS6NX/Beal and
              Ghahramani - 2006 - Variational Bayesian learning of directed
              graphica.pdf:application/pdf}
}

@article{friston_free-energy_2009,
  title      = {The free-energy principle: a rough guide to the brain?},
  volume     = {13},
  issn       = {13646613},
  shorttitle = {The free-energy principle},
  url        = {http://linkinghub.elsevier.com/retrieve/pii/S136466130900117X},
  doi        = {10.1016/j.tics.2009.04.005},
  language   = {en},
  number     = {7},
  urldate    = {2014-04-10},
  journal    = {Trends in Cognitive Sciences},
  author     = {Friston, Karl},
  month      = jul,
  year       = {2009},
  keywords   = {neuroscience},
  pages      = {293--301},
  file       = {Friston - 2009 - The free-energy principle a rough guide to the
                brain.pdf:/Users/apodusenko/Zotero/storage/RPTWNSIP/Friston - 2009 -
                The free-energy principle a rough guide to the
                brain.pdf:application/pdf}
}

@phdthesis{beal_variational_2003,
  title    = {Variational algorithms for approximate {Bayesian} inference},
  url      = {http://www.cse.buffalo.edu/faculty/mbeal/papers/beal03.pdf},
  urldate  = {2014-04-10},
  school   = {University of London},
  author   = {Beal, Matthew James},
  year     = {2003},
  keywords = {variational Bayes},
  file     = {Beal - 2003 - Variational algorithms for approximate Bayesian
              in.pdf:/Users/apodusenko/Zotero/storage/J3MCP3VX/Beal - 2003 -
              Variational algorithms for approximate Bayesian in.pdf:application/pdf}
}

@article{wingate_automated_2013,
  title    = {Automated variational inference in probabilistic programming},
  url      = {http://arxiv.org/abs/1301.1299},
  urldate  = {2014-04-10},
  journal  = {arXiv preprint arXiv:1301.1299},
  author   = {Wingate, David and Weber, Theophane},
  year     = {2013},
  keywords = {variational Bayes},
  file     = {Wingate and Weber - 2013 - Automated variational inference in
              probabilistic p.pdf:/Users/apodusenko/Zotero/storage/BBU4KG64/Wingate
              and Weber - 2013 - Automated variational inference in probabilistic
              p.pdf:application/pdf}
}

@inproceedings{deisenroth_general_2011,
  title      = {A general perspective on {Gaussian} filtering and smoothing: {
                Explaining} current and deriving new algorithms},
  shorttitle = {A general perspective on {Gaussian} filtering and smoothing},
  abstract   = {We present a general probabilistic perspective on Gaussian
                filtering and smoothing. This allows us to show that common
                approaches to Gaussian filtering/smoothing can be distinguished
                solely by their methods of computing/approximating the means and
                covariances of joint probabilities. This implies that novel filters
                and smoothers can be derived straight forwardly by providing
                methods for computing these moments. Based on this insight, we
                derive the cubature Kalman smoother and propose a novel robust
                filtering and smoothing algorithm based on Gibbs sampling.},
  booktitle  = {American {Control} {Conference} ({ACC}), 2011},
  author     = {Deisenroth, M.P. and Ohlsson, H.},
  month      = jun,
  year       = {2011},
  keywords   = {Kalman filters, smoothing methods, Gibbs sampling, Approximation
                algorithms, Gaussian approximation, Gaussian processes, Joints,
                Kalman smoother, Covariance matrix, Gaussian filtering, Gaussian
                smoothing, joint probabilities covariances, new algorithm
                derivation, probabilistic perspective, robust filtering, Time
                measurement},
  pages      = {1807--1812},
  file       = {Deisenroth, Ohlsson - Unknown - A General Perspective on Gaussian
                Filtering and Smoothing Explaining Current and Deriving New
                Algorithm.pdf:/Users/apodusenko/Zotero/storage/ZCX5NV6Z/Deisenroth,
                Ohlsson - Unknown - A General Perspective on Gaussian Filtering and
                Smoothing Explaining Current and Deriving New
                Algorithm.pdf:application/pdf;IEEE Xplore Abstract
                Record:/Users/apodusenko/Zotero/storage/B4CHERTV/login.html:text/html}
}

@techreport{minka_discriminative_2005,
  title       = {Discriminative models, not discriminative training},
  url         = {
                 http://research.microsoft.com/pubs/70229/tr-2005-144.pdf?origin=publication_detail
                 },
  urldate     = {2014-04-10},
  institution = {Technical Report MSR-TR-2005-144, Microsoft Research},
  author      = {Minka, Tom},
  year        = {2005},
  file        = {Minka - 2005 - Discriminative models , not discriminative
                 training.pdf:/Users/apodusenko/Zotero/storage/ER8JZSRU/Minka - 2005 -
                 Discriminative models , not discriminative training.pdf:application/pdf
                 }
}

@article{magnusson_how_1996,
  title    = {How to {Write} {Backwards}},
  volume   = {77},
  issn     = {0012-9623},
  url      = {http://www.esajournals.org/doi/abs/10.2307/20168029},
  doi      = {10.2307/20168029},
  abstract = {See full-text article at JSTOR},
  number   = {2},
  urldate  = {2014-04-10},
  journal  = {Bulletin of the Ecological Society of America},
  author   = {Magnusson, William E.},
  month    = apr,
  year     = {1996},
  pages    = {88--88},
  file     = {ESA
              Snapshot:/Users/apodusenko/Zotero/storage/7Q7UEXGD/showCitFormats.html:text/html;Magnusson
              - 1996 - How to Write
              Backwards.pdf:/Users/apodusenko/Zotero/storage/2UC4PWQJ/Magnusson -
              1996 - How to Write Backwards.pdf:application/pdf}
}

@article{mubeen_evidence-based_2011,
  title      = {Evidence-{Based} {Filters} for {Signal} {Detection}: {Application} to
                {Evoked} {Brain} {Responses}},
  shorttitle = {Evidence-{Based} {Filters} for {Signal} {Detection}},
  url        = {http://arxiv.org/abs/1107.1257},
  urldate    = {2014-04-10},
  journal    = {arXiv preprint arXiv:1107.1257},
  author     = {Mubeen, M. Asim and Knuth, Kevin H.},
  year       = {2011},
  file       = {Mubeen, Knuth - 2011 - Evidence-Based Filters for Signal
                Detection.pdf:/Users/apodusenko/Zotero/storage/T7EV7UCP/Mubeen, Knuth -
                2011 - Evidence-Based Filters for Signal Detection.pdf:application/pdf}
}

@inproceedings{enzner_bayesian_2010,
  title     = {Bayesian inference model for applications of time-varying acoustic
               system identification},
  url       = {
               http://www.eurasip.org/Proceedings/Eusipco/Eusipco2010/Contents/papers/1569291461.pdf
               },
  urldate   = {2014-04-10},
  booktitle = {Proc. {EUSIPCO}},
  author    = {Enzner, Gerald},
  year      = {2010},
  pages     = {2126--2130},
  file      = {Enzer - 2010 - Bayesian Inference Model for Applications of
               Time-Varying Acoustic System Identification
               .pdf:/Users/apodusenko/Zotero/storage/QW42SQXS/Enzer - 2010 - Bayesian
               Inference Model for Applications of Time-Varying Acoustic System
               Identification .pdf:application/pdf}
}

@article{lewicki_signal_2010,
  title    = {A signal take on speech},
  volume   = {466},
  url      = {
              http://www.researchgate.net/publication/45630186_Information_theory_A_signal_take_on_speech/file/9c9605229c4f0572f8.pdf
              },
  urldate  = {2014-04-10},
  journal  = {Nature},
  author   = {Lewicki, Michael S.},
  year     = {2010},
  keywords = {speech},
  pages    = {821--822},
  file     = {Lewicki - 2010 - A signal take on
              speech.pdf:/Users/apodusenko/Zotero/storage/XI2XEHK6/Lewicki - 2010 - A
              signal take on speech.pdf:application/pdf}
}

@article{zurek_hearing_2007,
  title   = {Hearing loss and prosthesis simulation in audiology},
  volume  = {60},
  url     = {
             http://journals.lww.com/thehearingjournal/Abstract/2007/07000/Hearing_loss_and_prosthesis_simulation_in.8.aspx
             },
  number  = {7},
  urldate = {2014-04-10},
  journal = {The Hearing Journal},
  author  = {Zurek, Patrick M. and Desloge, Joseph G.},
  year    = {2007},
  pages   = {32--33},
  file    = {Zurek and Desloge - 2007 - Hearing loss and prosthesis simulation in
             audiolog.pdf:/Users/apodusenko/Zotero/storage/UFK3M3DH/Zurek and
             Desloge - 2007 - Hearing loss and prosthesis simulation in
             audiolog.pdf:application/pdf}
}

@article{perez-cruz_gaussian_2013,
  title    = {Gaussian {Processes} for {Nonlinear} {Signal} {Processing}},
  volume   = {30},
  issn     = {1053-5888},
  url      = {http://arxiv.org/abs/1303.2823},
  doi      = {10.1109/MSP.2013.2250352},
  abstract = {Gaussian processes (GPs) are versatile tools that have been
              successfully employed to solve nonlinear estimation problems in
              machine learning, but that are rarely used in signal processing. In
              this tutorial, we present GPs for regression as a natural nonlinear
              extension to optimal Wiener filtering. After establishing their
              basic formulation, we discuss several important aspects and
              extensions, including recursive and adaptive algorithms for dealing
              with non-stationarity, low-complexity solutions, non-Gaussian noise
              models and classification scenarios. Furthermore, we provide a
              selection of relevant applications to wireless digital
              communications.},
  number   = {4},
  urldate  = {2014-04-10},
  journal  = {IEEE Signal Processing Magazine},
  author   = {Pérez-Cruz, Fernando and Van Vaerenbergh, Steven and Murillo-Fuentes
              , Juan José and Lázaro-Gredilla, Miguel and Santamaria, Ignacio},
  month    = jul,
  year     = {2013},
  note     = {arXiv:1303.2823 [cs, math, stat]},
  keywords = {Computer Science - Learning, Statistics - Machine Learning,
              Computer Science - Information Theory},
  pages    = {40--50},
  file     = {arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/DRBF6CUD/1303.html:text/html;Pérez-Cruz
              et al. - 2013 - Gaussian Processes for Nonlinear Signal
              Processing.pdf:/Users/apodusenko/Zotero/storage/BWJ6BBDC/Pérez-Cruz et
              al. - 2013 - Gaussian Processes for Nonlinear Signal
              Processing.pdf:application/pdf}
}

@article{tenenbaum_how_2011,
  title      = {How to {Grow} a {Mind}: {Statistics}, {Structure}, and {Abstraction}},
  volume     = {331},
  issn       = {0036-8075, 1095-9203},
  shorttitle = {How to {Grow} a {Mind}},
  url        = {http://www.sciencemag.org/cgi/doi/10.1126/science.1192788},
  doi        = {10.1126/science.1192788},
  language   = {en},
  number     = {6022},
  urldate    = {2014-04-10},
  journal    = {Science},
  author     = {Tenenbaum, J. B. and Kemp, C. and Griffiths, T. L. and Goodman, N.
                D.},
  month      = mar,
  year       = {2011},
  pages      = {1279--1285},
  file       = {Tenenbaum et al. - 2011 - How to Grow a Mind Statistics, Structure,
                and Abs.pdf:/Users/apodusenko/Zotero/storage/74CDF2VN/Tenenbaum et al.
                - 2011 - How to Grow a Mind Statistics, Structure, and
                Abs.pdf:application/pdf}
}

@article{raiko_building_2007,
  title   = {Building blocks for variational {Bayesian} learning of latent
             variable models},
  volume  = {8},
  url     = {http://dl.acm.org/citation.cfm?id=1248665},
  urldate = {2014-04-10},
  journal = {The Journal of Machine Learning Research},
  author  = {Raiko, Tapani and Valpola, Harri and Harva, Markus and Karhunen,
             Juha},
  year    = {2007},
  pages   = {155--201},
  file    = {Valpola et al. - 2007 - Building blocks for variational Bayesian
             learning of latent variable
             models.pdf:/Users/apodusenko/Zotero/storage/RF5JPZJ3/Valpola et al. -
             2007 - Building blocks for variational Bayesian learning of latent
             variable models.pdf:application/pdf}
}

@article{oliver_layered_2004,
  title    = {Layered representations for learning and inferring office activity
              from multiple sensory channels},
  volume   = {96},
  issn     = {10773142},
  url      = {http://linkinghub.elsevier.com/retrieve/pii/S1077314204000724},
  doi      = {10.1016/j.cviu.2004.02.004},
  language = {en},
  number   = {2},
  urldate  = {2014-04-10},
  journal  = {Computer Vision and Image Understanding},
  author   = {Oliver, Nuria and Garg, Ashutosh and Horvitz, Eric},
  month    = nov,
  year     = {2004},
  pages    = {163--180},
  file     = {Oliver et al. - 2004 - Layered representations for learning and
              inferring.pdf:/Users/apodusenko/Zotero/storage/WR2BRIA9/Oliver et al. -
              2004 - Layered representations for learning and
              inferring.pdf:application/pdf}
}

@misc{numenta_hierarchical_2011,
  title     = {Hierarchical temporal {Memory} - including {HTM} {Cortical} {Learning
               } {Algorithms}},
  publisher = {Numenta white paper},
  author    = {Numenta},
  month     = sep,
  year      = {2011},
  file      = {Hawkins (Numenta) - 2011 - HTM Cortical Learning
               Algorithms.pdf:/Users/apodusenko/Zotero/storage/S36ZJIR6/Hawkins
               (Numenta) - 2011 - HTM Cortical Learning Algorithms.pdf:application/pdf
               }
}

@inproceedings{nielsen_efficient_2013,
  title     = {Efficient individualization of hearing aid processed sound},
  url       = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6637677},
  urldate   = {2014-04-10},
  booktitle = {Acoustics, {Speech} and {Signal} {Processing} ({ICASSP}), 2013 {
               IEEE} {International} {Conference} on},
  publisher = {IEEE},
  author    = {Nielsen, Jens Brehm and Nielsen, Jakob},
  year      = {2013},
  keywords  = {Gaussian process},
  pages     = {398--402},
  file      = {Nielsen and Nielsen - 2013 - Efficient individualization of hearing
               aid process.pdf:/Users/apodusenko/Zotero/storage/QCHX46U8/Nielsen and
               Nielsen - 2013 - Efficient individualization of hearing aid
               process.pdf:application/pdf}
}

@misc{mansinghka_beyond_nodate,
  title      = {Beyond calculation: {Probabilistic} {Computing} {Machines} and {
                Universal} {Stochastic} {Inference}},
  shorttitle = {Beyond calculation},
  url        = {
                http://www.academia.edu/1799309/Beyond_calculation_Probabilistic_Computing_Machines_and_Universal_Stochastic_Inference
                },
  abstract   = {Beyond calculation: Probabilistic Computing Machines and Universal
                Stochastic Inference},
  urldate    = {2014-04-10},
  author     = {Mansinghka, Vikash},
  keywords   = {History, academia, academics, Biology, Chemistry, Computer Science
                , Earth Sciences, Economics, English, Geography, Law, Math,
                Medicine, Philosophy, Physics, Political Science, Psychology,
                Religion, research, universities},
  file       = {Mansinghka - 2010 - Beyond calculation probabilistic computing
                machines and universal stochastic
                inference.pdf:/Users/apodusenko/Zotero/storage/XJQXZAJZ/Mansinghka -
                2010 - Beyond calculation probabilistic computing machines and
                universal stochastic
                inference.pdf:application/pdf;Mansinghka_S.pdf:/Users/apodusenko/Zotero/storage/QGZH3B4P/Mansinghka_S.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/8MPUZMQ6/Beyond_calculation_Probabilistic_Computing_Machines_and_Universal_Stochastic_Inference.html:text/html
                }
}

@article{loredo_bayesian_2004,
  title   = {Bayesian adaptive exploration},
  url     = {http://arxiv.org/abs/astro-ph/0409386},
  urldate = {2014-04-10},
  journal = {arXiv preprint astro-ph/0409386},
  author  = {Loredo, Thomas J.},
  year    = {2004},
  file    = {Loredo - 2004 - Bayesian adaptive
             exploration.pdf:/Users/apodusenko/Zotero/storage/DVB45XBA/Loredo - 2004
             - Bayesian adaptive exploration.pdf:application/pdf}
}

@phdthesis{koch_factor_2007,
  title  = {A {Factor} {Graph} {Approach} to {Model}-{Based} {Signal} {Separation
            }},
  url    = {http://e-collection.library.ethz.ch/view/eth:29419},
  school = {ETH Zurich},
  author = {Koch, Volker Maximillian},
  year   = {2007},
  file   = {Koch - 2007 - A Factor Graph Approach to Model-Based Signal
            Separation.pdf:/Users/apodusenko/Zotero/storage/EE2XXXP2/Koch - 2007 -
            A Factor Graph Approach to Model-Based Signal
            Separation.pdf:application/pdf}
}

@article{jordan_modular_1995,
  title   = {Modular and hierarchical learning systems},
  url     = {
             http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.45.3224&rep=rep1&type=pdf
             },
  urldate = {2014-04-10},
  journal = {The handbook of brain theory and neural networks},
  author  = {Jordan, Michael I. and Jacobs, Robert A.},
  year    = {1995},
  pages   = {579--582},
  file    = {Jordan, Jacobs - Unknown - Modular and hierarchical learning
             systems.pdf:/Users/apodusenko/Zotero/storage/TAM8XEFH/Jordan, Jacobs -
             Unknown - Modular and hierarchical learning systems.pdf:application/pdf
             }
}

@article{jakubov_distributed_2013,
  title   = {Distributed {Extended} {Kalman} {Filter} for {Position}, {Velocity},
             {Time} {Estimation} in {Satellite} {Navigation} {Receivers}.},
  volume  = {22},
  url     = {
             http://search.ebscohost.com/login.aspx?direct=true&profile=ehost&scope=site&authtype=crawler&jrnl=12102512&AN=90458159&h=JgukY%2BlJhky2wXeAWvLTsdi4C%2FEB4OCWsFMjKB7hfSnxGBSoaUzib1m0C863f50zrQrwN7qmSJJdjt4T3qksAA%3D%3D&crl=c
             },
  number  = {3},
  urldate = {2014-04-10},
  journal = {Radioengineering},
  author  = {Jakubov, Ondrej and Kovar, Pavel and Kacmarik, Petr and Vejrazka,
             Frantisek},
  year    = {2013},
  file    = {Jakubov - 2013 - Distributed Extended Kalman Filter for Position,
             Velocity, Time Estimation in Satellite Navigation
             Receivers.pdf:/Users/apodusenko/Zotero/storage/4V36HGHV/Jakubov - 2013
             - Distributed Extended Kalman Filter for Position, Velocity, Time
             Estimation in Satellite Navigation Receivers.pdf:application/pdf}
}

@article{huang_predictive_2011,
  title    = {Predictive coding},
  volume   = {2},
  issn     = {19395078},
  url      = {http://doi.wiley.com/10.1002/wcs.142},
  doi      = {10.1002/wcs.142},
  language = {en},
  number   = {5},
  urldate  = {2014-04-10},
  journal  = {Wiley Interdisciplinary Reviews: Cognitive Science},
  author   = {Huang, Yanping and Rao, Rajesh P. N.},
  month    = sep,
  year     = {2011},
  pages    = {580--593},
  file     = {Huang and Rao - 2011 - Predictive
              coding.pdf:/Users/apodusenko/Zotero/storage/G5S5G5ZP/Huang and Rao -
              2011 - Predictive coding.pdf:application/pdf}
}

@misc{hinton_adaptation_nodate,
  title  = {Adaptation at multiple time-scales - {An} overview of how biology
            solves search problems},
  author = {Hinton, Geoffrey},
  note   = {lecture Notes CSC2535: Advanced Machine Learning},
  file   = {Hinton - Adaptation at multiple time-scales - An overview
            o.pdf:/Users/apodusenko/Zotero/storage/TB68IDQH/Hinton - Adaptation at
            multiple time-scales - An overview o.pdf:application/pdf}
}

@article{hensman_gaussian_2013,
  title    = {Gaussian processes for big data},
  url      = {http://arxiv.org/abs/1309.6835},
  urldate  = {2014-04-10},
  journal  = {arXiv preprint arXiv:1309.6835},
  author   = {Hensman, James and Fusi, Nicolo and Lawrence, Neil D.},
  year     = {2013},
  keywords = {Gaussian process, big data, on-line learning},
  file     = {Hensman et al. - 2013 - Gaussian processes for big
              data.pdf:/Users/apodusenko/Zotero/storage/XZITKP6T/Hensman et al. -
              2013 - Gaussian processes for big data.pdf:application/pdf}
}

@article{friston_free-energy_2007,
  title    = {Free-energy and the brain},
  volume   = {159},
  issn     = {0039-7857, 1573-0964},
  url      = {http://link.springer.com/10.1007/s11229-007-9237-y},
  doi      = {10.1007/s11229-007-9237-y},
  language = {en},
  number   = {3},
  urldate  = {2014-04-10},
  journal  = {Synthese},
  author   = {Friston, Karl J. and Stephan, Klaas E.},
  month    = nov,
  year     = {2007},
  pages    = {417--458},
  file     = {Friston, Stephan - 2007 - Free-energy and the
              brain.pdf:/Users/apodusenko/Zotero/storage/XUF4UJGM/Friston, Stephan -
              2007 - Free-energy and the brain.pdf:application/pdf}
}

@inproceedings{bonilla_gaussian_2010,
  title     = {Gaussian {Process} {Preference} {Elicitation}.},
  url       = {
               https://papers.nips.cc/paper/4141-gaussian-process-preference-elicitation.pdf
               },
  urldate   = {2014-04-10},
  booktitle = {{NIPS}},
  author    = {Bonilla, Edwin V. and Guo, Shengbo and Sanner, Scott},
  year      = {2010},
  keywords  = {Gaussian process},
  pages     = {262--270},
  file      = {Bonilla et al. - 2010 - Gaussian Process Preference
               Elicitation..pdf:/Users/apodusenko/Zotero/storage/H8J62Z36/Bonilla et
               al. - 2010 - Gaussian Process Preference
               Elicitation..pdf:application/pdf}
}

@article{van_waterschoot_adaptive_2009,
  title    = {Adaptive feedback cancellation for audio applications},
  volume   = {89},
  issn     = {01651684},
  url      = {http://linkinghub.elsevier.com/retrieve/pii/S0165168409002084},
  doi      = {10.1016/j.sigpro.2009.04.036},
  language = {en},
  number   = {11},
  urldate  = {2014-04-10},
  journal  = {Signal Processing},
  author   = {van Waterschoot, Toon and Moonen, Marc},
  month    = nov,
  year     = {2009},
  pages    = {2185--2201},
  file     = {van Waterschoot and Moonen - 2009 - Adaptive feedback cancellation for
              audio applicati.pdf:/Users/apodusenko/Zotero/storage/8MFMU8W5/van
              Waterschoot and Moonen - 2009 - Adaptive feedback cancellation for
              audio applicati.pdf:application/pdf}
}

@article{stilp_cochlea-scaled_2010,
  title    = {Cochlea-scaled entropy, not consonants, vowels, or time, best
              predicts speech intelligibility},
  volume   = {107},
  issn     = {0027-8424, 1091-6490},
  url      = {http://www.pnas.org/cgi/doi/10.1073/pnas.0913625107},
  doi      = {10.1073/pnas.0913625107},
  language = {en},
  number   = {27},
  urldate  = {2014-04-10},
  journal  = {Proceedings of the National Academy of Sciences},
  author   = {Stilp, C. E. and Kluender, K. R.},
  month    = jul,
  year     = {2010},
  pages    = {12387--12392},
  file     = {Stilp, Kluender - 2010 - Cochlea-scaled entropy, not consonants,
              vowels, or time, best predicts speech
              intelligibility.pdf:/Users/apodusenko/Zotero/storage/DW3TJAVP/Stilp,
              Kluender - 2010 - Cochlea-scaled entropy, not consonants, vowels, or
              time, best predicts speech intelligibility.pdf:application/pdf}
}

@article{staines_variational_2012,
  title   = {Variational {Optimization}},
  url     = {http://arxiv.org/abs/1212.4507},
  urldate = {2014-04-10},
  journal = {arXiv preprint arXiv:1212.4507},
  author  = {Staines, Joe and Barber, David},
  year    = {2012},
  file    = {Staines and Barber - 2012 - Variational
             Optimization.pdf:/Users/apodusenko/Zotero/storage/UCSUEZNU/Staines and
             Barber - 2012 - Variational Optimization.pdf:application/pdf}
}

@unpublished{sarkka_bayesian_2012,
  title   = {Bayesian estimation of time-varying systems: discrete-time systems},
  urldate = {2014-04-10},
  author  = {Sarkka, Simo},
  year    = {2012},
  note    = {course lecture notes},
  file    = {Sarkka - 2012 - Bayesian estimation of time-varying systems
             discr.pdf:/Users/apodusenko/Zotero/storage/79QXKHAG/Sarkka - 2012 -
             Bayesian estimation of time-varying systems discr.pdf:application/pdf}
}

@article{poon_sum-product_2012,
  title      = {Sum-{Product} {Networks}: {A} {New} {Deep} {Architecture}},
  shorttitle = {Sum-{Product} {Networks}},
  url        = {http://arxiv.org/abs/1202.3732},
  abstract   = {The key limiting factor in graphical model inference and learning
                is the complexity of the partition function. We thus ask the
                question: what are general conditions under which the partition
                function is tractable? The answer leads to a new kind of deep
                architecture, which we call sum-product networks (SPNs). SPNs are
                directed acyclic graphs with variables as leaves, sums and products
                as internal nodes, and weighted edges. We show that if an SPN is
                complete and consistent it represents the partition function and
                all marginals of some graphical model, and give semantics to its
                nodes. Essentially all tractable graphical models can be cast as
                SPNs, but SPNs are also strictly more general. We then propose
                learning algorithms for SPNs, based on backpropagation and EM.
                Experiments show that inference and learning with SPNs can be both
                faster and more accurate than with standard deep networks. For
                example, SPNs perform image completion better than state-of-the-art
                deep networks for this task. SPNs also have intriguing potential
                connections to the architecture of the cortex.},
  urldate    = {2014-04-10},
  journal    = {arXiv:1202.3732 [cs, stat]},
  author     = {Poon, Hoifung and Domingos, Pedro},
  month      = feb,
  year       = {2012},
  keywords   = {Computer Science - Learning, Statistics - Machine Learning,
                Computer Science - Artificial Intelligence},
  file       = {arXiv.org
                Snapshot:/Users/apodusenko/Zotero/storage/JTPAHW2G/1202.html:text/html;Poon
                and Domingos - 2012 - Sum-Product Networks A New Deep
                Architecture.pdf:/Users/apodusenko/Zotero/storage/5DQQZJ9D/Poon and
                Domingos - 2012 - Sum-Product Networks A New Deep
                Architecture.pdf:application/pdf}
}

@inproceedings{osborne_gaussian_2009,
  title     = {Gaussian processes for global optimization},
  url       = {
               http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.352.9682&rep=rep1&type=pdf
               },
  urldate   = {2014-04-10},
  booktitle = {3rd international conference on learning and intelligent
               optimization ({LION3})},
  publisher = {Citeseer},
  author    = {Osborne, Michael A. and Garnett, Roman and Roberts, Stephen J.},
  year      = {2009},
  keywords  = {Gaussian process},
  pages     = {1--15},
  file      = {Osborne et al. - 2009 - Gaussian processes for global
               optimization.pdf:/Users/apodusenko/Zotero/storage/VRC5AV8W/Osborne et
               al. - 2009 - Gaussian processes for global
               optimization.pdf:application/pdf}
}

@article{ormerod_explaining_2010,
  title    = {Explaining {Variational} {Approximations}},
  volume   = {64},
  issn     = {0003-1305, 1537-2731},
  url      = {http://www.tandfonline.com/doi/abs/10.1198/tast.2010.09058},
  doi      = {10.1198/tast.2010.09058},
  language = {en},
  number   = {2},
  urldate  = {2014-04-10},
  journal  = {The American Statistician},
  author   = {Ormerod, J. T. and Wand, M. P.},
  month    = may,
  year     = {2010},
  pages    = {140--153},
  file     = {Ormerod and Wand - 2010 - Explaining Variational
              Approximations.pdf:/Users/apodusenko/Zotero/storage/88QU86WG/Ormerod
              and Wand - 2010 - Explaining Variational
              Approximations.pdf:application/pdf}
}

@phdthesis{malik_bayesian_2012,
  title  = {Bayesian learning of linear and nonlinear acoustic models in
            hands-free communication},
  url    = {
            http://www-brs.ub.ruhr-uni-bochum.de/netahtml/HSS/Diss/MalikSarmad/diss.pdf
            },
  school = {Ruhr-Universitaet Bochum},
  author = {Malik, Sarmad},
  year   = {2012},
  file   = {Malik - 2012 - Dissertation - Bayesian learning of linear and
            nonlinear acoustic models in hands-free communication
            .pdf:/Users/apodusenko/Zotero/storage/PJMDXIMM/Malik - 2012 -
            Dissertation - Bayesian learning of linear and nonlinear acoustic
            models in hands-free communication .pdf:application/pdf}
}

@inproceedings{jensen_efficient_2011,
  title     = {Efficient preference learning with pairwise continuous observations
               and {Gaussian} {Processes}},
  url       = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6064616},
  urldate   = {2014-04-10},
  booktitle = {Machine {Learning} for {Signal} {Processing} ({MLSP}), 2011 {IEEE
               } {International} {Workshop} on},
  publisher = {IEEE},
  author    = {Jensen, Bjørn Sand and Nielsen, Jens Brehm and Larsen, Jan},
  year      = {2011},
  keywords  = {Gaussian process},
  pages     = {1--6},
  file      = {Jensen - 2011 - Efficient Preference Learning with Pairwise Continuous
               Observations and Gaussian
               Processes.pdf:/Users/apodusenko/Zotero/storage/PWBAHURI/Jensen - 2011 -
               Efficient Preference Learning with Pairwise Continuous Observations and
               Gaussian Processes.pdf:application/pdf}
}

@inproceedings{henter_gaussian_2012,
  title     = {Gaussian process dynamical models for nonparametric speech
               representation and synthesis},
  url       = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6288919},
  urldate   = {2014-04-10},
  booktitle = {Acoustics, {Speech} and {Signal} {Processing} ({ICASSP}), 2012 {
               IEEE} {International} {Conference} on},
  publisher = {IEEE},
  author    = {Henter, Gustav Eje and Frean, Marcus R. and Kleijn, W. Bastiaan},
  year      = {2012},
  pages     = {4505--4508},
  file      = {Henter et al. - 2012 - Gaussian process dynamical models for
               nonparametri.pdf:/Users/apodusenko/Zotero/storage/4U7UUSGQ/Henter et
               al. - 2012 - Gaussian process dynamical models for
               nonparametri.pdf:application/pdf}
}

@article{groot_nonlinear_2008,
  title   = {Nonlinear perception of hearing-impaired people using preference
             learning with {Gaussian} {Processes}},
  url     = {
             http://www.researchgate.net/publication/228337617_Nonlinear_perception_of_hearing-impaired_people_using_preference_learning_with_Gaussian_Processes/file/72e7e519f684837c93.pdf
             },
  urldate = {2014-04-10},
  journal = {Radboud University Nijmegen, Tech. Rep. ICIS-R08018},
  author  = {Groot, P. C. and Heskes, Tom and Dijkstra, Tjeerd MH},
  year    = {2008},
  file    = {Groot - 2008- Tech Report - Nonlinear perception of hearing impaired
             people using preference learning with
             GPs.pdf:/Users/apodusenko/Zotero/storage/D3VE27CM/Groot - 2008- Tech
             Report - Nonlinear perception of hearing impaired people using
             preference learning with GPs.pdf:application/pdf}
}

@article{friston_impact_2014,
  title    = {The {Impact} of {Active} {Inference} on {Engineering} [{Further} {
              Thoughts}]},
  volume   = {102},
  issn     = {0018-9219, 1558-2256},
  url      = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6777976},
  doi      = {10.1109/JPROC.2014.2307017},
  number   = {4},
  urldate  = {2014-04-10},
  journal  = {Proceedings of the IEEE},
  author   = {Friston, Karl},
  month    = apr,
  year     = {2014},
  keywords = {neuroscience},
  pages    = {446--446},
  file     = {Friston - 2014 - The impact of active inference on
              engineering.pdf:/Users/apodusenko/Zotero/storage/BZWPQBW4/Friston -
              2014 - The impact of active inference on
              engineering.pdf:application/pdf}
}

@misc{center_for_history_and_new_media_zotero_nodate,
  title  = {Zotero {Quick} {Start} {Gids}},
  url    = {http://zotero.org/support/quick_start_guide},
  author = {{Center for History and New Media}}
}

@article{behrens_learning_2007,
  title   = {Learning the value of information in an uncertain world},
  volume  = {10},
  issn    = {1097-6256},
  url     = {http://www.nature.com/doifinder/10.1038/nn1954},
  doi     = {10.1038/nn1954},
  number  = {9},
  urldate = {2014-04-10},
  journal = {Nature Neuroscience},
  author  = {Behrens, Timothy E J and Woolrich, Mark W and Walton, Mark E and
             Rushworth, Matthew F S},
  month   = sep,
  year    = {2007},
  pages   = {1214--1221},
  file    = {Behrens et al. - 2007 - Learning the value of information in an
             uncertain .pdf:/Users/apodusenko/Zotero/storage/G2IHVX7W/Behrens et al.
             - 2007 - Learning the value of information in an uncertain
             .pdf:application/pdf}
}

@misc{banerjee_compression_2011,
  title     = {The compression handbook},
  urldate   = {2014-04-10},
  publisher = {Starkey Hearing Research},
  author    = {Banerjee, Shilpi},
  year      = {2011},
  file      = {Banerjee - 2011 - The compression
               handbook.pdf:/Users/apodusenko/Zotero/storage/PSWJ27CN/Banerjee - 2011
               - The compression handbook.pdf:application/pdf}
}

@article{roweis_unifying_1999,
  title    = {A {Unifying} {Review} of {Linear} {Gaussian} {Models}},
  volume   = {11},
  issn     = {0899-7667},
  url      = {http://dx.doi.org/10.1162/089976699300016674},
  doi      = {10.1162/089976699300016674},
  abstract = {Factor analysis, principal component analysis, mixtures of
              gaussian clusters, vector quantization, Kalman filter models, and
              hidden Markov models can all be unified as variations of
              unsupervised learning under a single basic generative model. This
              is achieved by collecting together disparate observations and
              derivations made by many previous authors and introducing a new way
              of linking discrete and continuous state models using a simple
              nonlinearity. Through the use of other nonlinearities, we show how
              independent component analysis is also a variation of the same
              basic generative model. We show that factor analysis and mixtures
              of gaussians can be implemented in autoencoder neural networks and
              learned using squared error plus the same regularization term. We
              introduce a new model for static data, known as sensible principal
              component analysis, as well as a novel concept of spatially
              adaptive observation noise. We also review some of the literature
              involving global and local mixtures of the basic models and provide
              pseudocode for inference and learning for all the basic models.},
  number   = {2},
  urldate  = {2014-04-10},
  journal  = {Neural Computation},
  author   = {Roweis, Sam and Ghahramani, Zoubin},
  month    = feb,
  year     = {1999},
  pages    = {305--345},
  file     = {Neural Computation
              Snapshot:/Users/apodusenko/Zotero/storage/2X5CNT3G/089976699300016674.html:text/html;Roweis
              and Ghahramani - 1999 - A Unifying Review of Linear Gaussian
              Models.pdf:/Users/apodusenko/Zotero/storage/HZRE6FCD/Roweis and
              Ghahramani - 1999 - A Unifying Review of Linear Gaussian
              Models.pdf:application/pdf}
}

@book{hunt_pragmatic_1999,
  address   = {Boston},
  title     = {The {Pragmatic} {Programmer} {From} {Journeyman} to {Master}.},
  isbn      = {0-201-61622-X 978-0-201-61622-4},
  abstract  = {Annotation.},
  language  = {English},
  publisher = {Addison Wesley Professional},
  author    = {Hunt, Andrew and Thomas, David and Cunningham, Ward and {ProQuest}},
  year      = {1999}
}

@book{martin_clean_2009,
  address    = {Upper Saddle River, NJ},
  title      = {Clean code: a handbook of agile software craftsmanship},
  isbn       = {0-13-235088-2 978-0-13-235088-4},
  shorttitle = {Clean code},
  language   = {English},
  publisher  = {Prentice Hall},
  author     = {Martin, Robert C},
  year       = {2009}
}

@book{martin_clean_2011,
  address    = {Upper Saddle River, NJ},
  title      = {The clean coder: a code of conduct for professional programmers},
  isbn       = {0-13-708107-3 978-0-13-708107-3},
  shorttitle = {The clean coder},
  abstract   = {Presents practical advice on the disciplines, techniques, tools,
                and practices of computer programming and how to approach software
                development with a sense of pride, honor, and self-respect.},
  language   = {English},
  publisher  = {Prentice Hall},
  author     = {Martin, Robert C},
  year       = {2011}
}

@article{vandewalle_reproducible_2009,
  title   = {Reproducible research in signal processing},
  volume  = {26},
  issn    = {1053-5888},
  url     = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4815541},
  doi     = {10.1109/MSP.2009.932122},
  number  = {3},
  urldate = {2014-04-10},
  journal = {IEEE Signal Processing Magazine},
  author  = {Vandewalle, Patrick and Kovacevic, Jelena and Vetterli, Martin},
  month   = may,
  year    = {2009},
  pages   = {37--47},
  file    = {Vandewalle et al. - 2009 - Reproducible research in signal
             processing.pdf:/Users/apodusenko/Zotero/storage/6DQ7T6A7/Vandewalle et
             al. - 2009 - Reproducible research in signal
             processing.pdf:application/pdf}
}

@article{wilson_best_2014,
  title    = {Best {Practices} for {Scientific} {Computing}},
  volume   = {12},
  url      = {http://dx.doi.org/10.1371/journal.pbio.1001745},
  doi      = {10.1371/journal.pbio.1001745},
  abstract = {{\textless}p{\textgreater}We describe a set of best practices for
              scientific software development, based on research and experience,
              that will improve scientists' productivity and the reliability of
              their software.{\textless}/p{\textgreater}{\textless}/sec{
              \textgreater}},
  number   = {1},
  urldate  = {2014-04-11},
  journal  = {PLoS Biol},
  author   = {Wilson, Greg and Aruliah, D. A. and Brown, C. Titus and Chue Hong,
              Neil P. and Davis, Matt and Guy, Richard T. and Haddock, Steven H. D.
              and Huff, Kathryn D. and Mitchell, Ian M. and Plumbley, Mark D. and
              Waugh, Ben and White, Ethan P. and Wilson, Paul},
  month    = jan,
  year     = {2014},
  pages    = {e1001745},
  file     = {PLoS
              Snapshot:/Users/apodusenko/Zotero/storage/2EUB2THD/infodoi10.1371journal.pbio.html:text/html;Wilson
              et al. - 2014 - Best Practices for Scientific
              Computing.pdf:/Users/apodusenko/Zotero/storage/NMS5U2NV/Wilson et al. -
              2014 - Best Practices for Scientific Computing.pdf:application/pdf}
}

@article{trotta_bayes_2008,
  title      = {Bayes in the sky: {Bayesian} inference and model selection in
                cosmology},
  volume     = {49},
  shorttitle = {Bayes in the sky},
  url        = {http://www.tandfonline.com/doi/abs/10.1080/00107510802066753},
  number     = {2},
  urldate    = {2014-04-10},
  journal    = {Contemporary Physics},
  author     = {Trotta, Roberto},
  year       = {2008},
  pages      = {71--104},
  file       = {Trotta - 2008 - Bayes in the sky Bayesian inference and model
                sel.pdf:/Users/apodusenko/Zotero/storage/36CQZ2I6/Trotta - 2008 - Bayes
                in the sky Bayesian inference and model sel.pdf:application/pdf}
}

@article{yedidia_message-passing_2011,
  title   = {Message-passing algorithms for inference and optimization},
  volume  = {145},
  url     = {http://link.springer.com/article/10.1007/s10955-011-0384-7},
  number  = {4},
  urldate = {2014-04-10},
  journal = {Journal of Statistical Physics},
  author  = {Yedidia, Jonathan S.},
  year    = {2011},
  pages   = {860--890},
  file    = {Yedidia - 2011 - Message-passing algorithms for inference and
             optim.pdf:/Users/apodusenko/Zotero/storage/BK4PHB44/Yedidia - 2011 -
             Message-passing algorithms for inference and optim.pdf:application/pdf}
}

@article{frigola_integrated_2013,
  title    = {Integrated {Pre}-{Processing} for {Bayesian} {Nonlinear} {System} {
              Identification} with {Gaussian} {Processes}},
  url      = {http://arxiv.org/abs/1303.2912},
  abstract = {We introduce GP-FNARX: a new model for nonlinear system
              identification based on a nonlinear autoregressive exogenous model
              (NARX) with filtered regressors (F) where the nonlinear regression
              problem is tackled using sparse Gaussian processes (GP). We
              integrate data pre-processing with system identification into a
              fully automated procedure that goes from raw data to an identified
              model. Both pre-processing parameters and GP hyper-parameters are
              tuned by maximizing the marginal likelihood of the probabilistic
              model. We obtain a Bayesian model of the system's dynamics which is
              able to report its uncertainty in regions where the data is scarce.
              The automated approach, the modeling of uncertainty and its
              relatively low computational cost make of GP-FNARX a good candidate
              for applications in robotics and adaptive control.},
  urldate  = {2014-04-10},
  journal  = {arXiv:1303.2912 [cs, stat]},
  author   = {Frigola, Roger and Rasmussen, Carl Edward},
  month    = mar,
  year     = {2013},
  file     = {arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/4QP9KE4Q/1303.html:text/html;Frigola
              and Rasmussen - 2013 - Integrated Pre-Processing for Bayesian Nonlinear
              S.pdf:/Users/apodusenko/Zotero/storage/VXDXJA6E/Frigola and Rasmussen -
              2013 - Integrated Pre-Processing for Bayesian Nonlinear
              S.pdf:application/pdf}
}

@article{damianou_deep_2012,
  title    = {Deep {Gaussian} {Processes}},
  url      = {http://arxiv.org/abs/1211.0358},
  abstract = {In this paper we introduce deep Gaussian process (GP) models. Deep
              GPs are a deep belief network based on Gaussian process mappings.
              The data is modeled as the output of a multivariate GP. The inputs
              to that Gaussian process are then governed by another GP. A single
              layer model is equivalent to a standard GP or the GP latent
              variable model (GP-LVM). We perform inference in the model by
              approximate variational marginalization. This results in a strict
              lower bound on the marginal likelihood of the model which we use
              for model selection (number of layers and nodes per layer). Deep
              belief networks are typically applied to relatively large data sets
              using stochastic gradient descent for optimization. Our fully
              Bayesian treatment allows for the application of deep models even
              when data is scarce. Model selection by our variational bound shows
              that a five layer hierarchy is justified even when modelling a
              digit data set containing only 150 examples.},
  urldate  = {2014-04-10},
  journal  = {arXiv:1211.0358 [cs, math, stat]},
  author   = {Damianou, Andreas C. and Lawrence, Neil D.},
  month    = nov,
  year     = {2012},
  keywords = {Computer Science - Learning, Statistics - Machine Learning, 60G15,
              58E30, G.1.2, G.3, I.2.6, Mathematics - Probability},
  file     = {arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/RWAH5FHN/1211.html:text/html;Damianou
              and Lawrence - 2012 - Deep Gaussian
              Processes.pdf:/Users/apodusenko/Zotero/storage/2S7N584F/Damianou and
              Lawrence - 2012 - Deep Gaussian Processes.pdf:application/pdf}
}

@article{houlsby_bayesian_2011,
  title    = {Bayesian {Active} {Learning} for {Classification} and {Preference} {
              Learning}},
  url      = {http://arxiv.org/abs/1112.5745},
  abstract = {Information theoretic active learning has been widely studied for
              probabilistic models. For simple regression an optimal myopic
              policy is easily tractable. However, for other tasks and with more
              complex models, such as classification with nonparametric models,
              the optimal solution is harder to compute. Current approaches make
              approximations to achieve tractability. We propose an approach that
              expresses information gain in terms of predictive entropies, and
              apply this method to the Gaussian Process Classifier (GPC). Our
              approach makes minimal approximations to the full information
              theoretic objective. Our experimental performance compares
              favourably to many popular active learning algorithms, and has
              equal or lower computational complexity. We compare well to
              decision theoretic approaches also, which are privy to more
              information and require much more computational time. Secondly, by
              developing further a reformulation of binary preference learning to
              a classification problem, we extend our algorithm to Gaussian
              Process preference learning.},
  urldate  = {2014-04-10},
  journal  = {arXiv:1112.5745 [cs, stat]},
  author   = {Houlsby, Neil and Huszár, Ferenc and Ghahramani, Zoubin and Lengyel,
              Máté},
  month    = dec,
  year     = {2011},
  keywords = {Computer Science - Learning, Statistics - Machine Learning},
  file     = {arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/E4VGQA5T/1112.html:text/html;Houlsby
              et al. - 2011 - Bayesian Active Learning for Classification and
              Pr.pdf:/Users/apodusenko/Zotero/storage/BRWTRTUE/Houlsby et al. - 2011
              - Bayesian Active Learning for Classification and
              Pr.pdf:application/pdf}
}

@article{giordano_covariance_2014,
  title    = {Covariance {Matrices} for {Mean} {Field} {Variational} {Bayes}},
  url      = {http://arxiv.org/abs/1410.6853},
  abstract = {Mean Field Variational Bayes (MFVB) is a popular posterior
              approximation method due to its fast runtime on large-scale data
              sets. However, it is well known that a major failing of MFVB is its
              (sometimes severe) underestimates of the uncertainty of model
              variables and lack of information about model variable covariance.
              We develop a fast, general methodology for exponential families
              that augments MFVB to deliver accurate uncertainty estimates for
              model variables -- both for individual variables and coherently
              across variables. MFVB for exponential families defines a
              fixed-point equation in the means of the approximating posterior,
              and our approach yields a covariance estimate by perturbing this
              fixed point. Inspired by linear response theory, we call our method
              linear response variational Bayes (LRVB). We demonstrate the
              accuracy of our method on simulated data sets.},
  urldate  = {2015-11-27},
  journal  = {arXiv:1410.6853 [cs, stat]},
  author   = {Giordano, Ryan and Broderick, Tamara},
  month    = oct,
  year     = {2014},
  note     = {arXiv: 1410.6853},
  keywords = {Computer Science - Learning, Statistics - Machine Learning,
              Statistics - Methodology},
  file     = {arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/8H8F6XRN/1410.html:text/html;Giordano
              and Broderick - 2014 - Covariance Matrices for Mean Field Variational
              Bay.pdf:/Users/apodusenko/Zotero/storage/8DUXD9AS/Giordano and
              Broderick - 2014 - Covariance Matrices for Mean Field Variational
              Bay.pdf:application/pdf}
}

@article{petzschner_bayesian_2015,
  title    = {A {Bayesian} perspective on magnitude estimation},
  volume   = {19},
  issn     = {1364-6613},
  url      = {http://www.cell.com/article/S1364661315000509/abstract},
  doi      = {10.1016/j.tics.2015.03.002},
  abstract = {Our representation of the physical world requires judgments of
              magnitudes, such as loudness, distance, or time. Interestingly,
              magnitude estimates are often not veridical but subject to
              characteristic biases. These biases are strikingly similar across
              different sensory modalities, suggesting common processing
              mechanisms that are shared by different sensory systems. However,
              the search for universal neurobiological principles of magnitude
              judgments requires guidance by formal theories. Here, we discuss a
              unifying Bayesian framework for understanding biases in magnitude
              estimation. This Bayesian perspective enables a re-interpretation
              of a range of established psychophysical findings, reconciles
              seemingly incompatible classical views on magnitude estimation, and
              can guide future investigations of magnitude estimation and its
              neurobiological mechanisms in health and in psychiatric diseases,
              such as schizophrenia.},
  language = {English},
  number   = {5},
  urldate  = {2015-09-19},
  journal  = {Trends in Cognitive Sciences},
  author   = {Petzschner, Frederike H. and Glasauer, Stefan and Stephan, Klaas E.},
  month    = may,
  year     = {2015},
  pmid     = {25843543},
  keywords = {perceptual inference, Psychophysics, generative model,
              schizophrenia, Stevens’ power law, Weber-Fechner law},
  pages    = {285--293},
  file     = {Petzschner et al. - 2015 - A Bayesian perspective on magnitude
              estimation.pdf:/Users/apodusenko/Zotero/storage/VSAQ9PCX/Petzschner et
              al. - 2015 - A Bayesian perspective on magnitude
              estimation.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/ETD2PX9W/S1364-6613(15)00050-9.html:text/html
              }
}

@article{schwartenbeck_evidence_2015,
  title   = {Evidence for surprise minimization over value maximization in choice
             behavior},
  volume  = {5},
  issn    = {2045-2322},
  url     = {http://www.nature.com/articles/srep16575},
  doi     = {10.1038/srep16575},
  urldate = {2015-11-22},
  journal = {Scientific Reports},
  author  = {Schwartenbeck, Philipp and FitzGerald, Thomas H. B. and Mathys,
             Christoph and Dolan, Ray and Kronbichler, Martin and Friston, Karl},
  month   = nov,
  year    = {2015},
  pages   = {16575},
  file    = {Schwartenbeck et al. - 2015 - Evidence for surprise minimization over
             value maxi.pdf:/Users/apodusenko/Zotero/storage/R4ADCZTV/Schwartenbeck
             et al. - 2015 - Evidence for surprise minimization over value
             maxi.pdf:application/pdf}
}

@article{russo_learning_2014,
  title    = {Learning to {Optimize} via {Posterior} {Sampling}},
  volume   = {39},
  issn     = {0364-765X},
  url      = {http://pubsonline.informs.org/doi/abs/10.1287/moor.2014.0650},
  doi      = {10.1287/moor.2014.0650},
  abstract = {This paper considers the use of a simple posterior sampling
              algorithm to balance between exploration and exploitation when
              learning to optimize actions such as in multiarmed bandit problems.
              The algorithm, also known as Thompson Sampling and as probability
              matching, offers significant advantages over the popular upper
              confidence bound (UCB) approach, and can be applied to problems
              with finite or infinite action spaces and complicated relationships
              among action rewards. We make two theoretical contributions. The
              first establishes a connection between posterior sampling and UCB
              algorithms. This result lets us convert regret bounds developed for
              UCB algorithms into Bayesian regret bounds for posterior sampling.
              Our second theoretical contribution is a Bayesian regret bound for
              posterior sampling that applies broadly and can be specialized to
              many model classes. This bound depends on a new notion we refer to
              as the eluder dimension, which measures the degree of dependence
              among action rewards. Compared to UCB algorithm Bayesian regret
              bounds for specific model classes, our general bound matches the
              best available for linear models and is stronger than the best
              available for generalized linear models. Further, our analysis
              provides insight into performance advantages of posterior sampling,
              which are highlighted through simulation results that demonstrate
              performance surpassing recently proposed UCB algorithms.},
  number   = {4},
  urldate  = {2015-11-09},
  journal  = {Mathematics of Operations Research},
  author   = {Russo, Daniel and Van Roy, Benjamin},
  month    = apr,
  year     = {2014},
  pages    = {1221--1243},
  file     = {
              Snapshot:/Users/apodusenko/Zotero/storage/ZDW65TT4/moor.2014.html:text/html
              }
}

@article{scott_modern_2010,
  title    = {A {Modern} {Bayesian} {Look} at the {Multi}-armed {Bandit}},
  volume   = {26},
  issn     = {1524-1904},
  url      = {http://dx.doi.org/10.1002/asmb.874},
  doi      = {10.1002/asmb.874},
  abstract = {A multi-armed bandit is an experiment with the goal of
              accumulating rewards from a payoff distribution with unknown
              parameters that are to be learned sequentially. This article
              describes a heuristic for managing multi-armed bandits called
              randomized probability matching, which randomly allocates
              observations to arms according the Bayesian posterior probability
              that each arm is optimal. Advances in Bayesian computation have
              made randomized probability matching easy to apply to virtually any
              payoff distribution. This flexibility frees the experimenter to
              work with payoff distributions that correspond to certain classical
              experimental designs that have the potential to outperform methods
              that are ‘optimal’ in simpler contexts. I summarize the
              relationships between randomized probability matching and several
              related heuristics that have been used in the reinforcement
              learning literature. Copyright © 2010 John Wiley \& Sons, Ltd.},
  number   = {6},
  urldate  = {2015-11-09},
  journal  = {Appl. Stoch. Model. Bus. Ind.},
  author   = {Scott, Steven L.},
  month    = nov,
  year     = {2010},
  keywords = {Bayesian adaptive design, exploration vs exploitation, probability
              matching, sequential design},
  pages    = {639--658},
  file     = {Scott - 2010 - A Modern Bayesian Look at the Multi-armed
              Bandit.pdf:/Users/apodusenko/Zotero/storage/Q3W929H8/Scott - 2010 - A
              Modern Bayesian Look at the Multi-armed Bandit.pdf:application/pdf}
}

@article{fenton_sum_1960,
  title    = {The {Sum} of {Log}-{Normal} {Probability} {Distributions} in {Scatter
              } {Transmission} {Systems}},
  volume   = {8},
  issn     = {0096-2244},
  doi      = {10.1109/TCOM.1960.1097606},
  abstract = {The long-term fluctuation of transmission loss in scatter
              propagation systems has been found to have a logarithmicnormal
              distribution. In other words, the scatter loss in decibels has
              Gaussian statistical distribution. Therefore, in many important
              communication systems (e.g., FM), the noise power of a radio jump,
              or hop, has log-normal statistical distribution. In a multihop
              system, the noise power of each hop contributes to the total noise.
              The resulting noise of the system is therefore the statistical sum
              of the individual noise distributions. In multihop scatter systems
              and others, such as multichannel speech-transmission systems, the
              sum of several log-normal distributions is needed. No exact
              solution to this problem is known. The following discussion
              presents an approximate solution which is satisfactory in most
              practical cases. For tactical multihop scatter systems, a further
              approximation is proposed, which reduces significantly the
              necessary computation. An example of the computation is given.},
  number   = {1},
  journal  = {IRE Transactions on Communications Systems},
  author   = {Fenton, L.},
  month    = mar,
  year     = {1960},
  keywords = {Equations, Gaussian distribution, Random variables, Statistical
              distributions, Convolution, Fluctuations, Log-normal distribution,
              lognormal, Probability distribution, Propagation losses, Scattering
              },
  pages    = {57--67},
  file     = {Fenton - 1960 - The Sum of Log-Normal Probability Distributions
              in.pdf:/Users/apodusenko/Zotero/storage/K6X9QMHQ/Fenton - 1960 - The
              Sum of Log-Normal Probability Distributions in.pdf:application/pdf;IEEE
              Xplore Abstract
              Record:/Users/apodusenko/Zotero/storage/FDP2WCUF/login.html:text/html}
}

@article{wu_subband_1998,
  title    = {Subband {Kalman} filtering for speech enhancement},
  volume   = {45},
  url      = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=718814},
  number   = {8},
  urldate  = {2015-11-03},
  journal  = {Circuits and Systems II: Analog and Digital Signal Processing, IEEE
              Transactions on},
  author   = {Wu, Wen-Rong and Chen, Po-Cheng},
  year     = {1998},
  keywords = {SE Kalman},
  pages    = {1072--1083},
  file     = {
              Snapshot:/Users/apodusenko/Zotero/storage/DDJZCE6Q/login.html:text/html;Wu
              and Chen - 1998 - Subband Kalman filtering for speech
              enhancement.pdf:/Users/apodusenko/Zotero/storage/E54SUBXI/Wu and Chen -
              1998 - Subband Kalman filtering for speech
              enhancement.pdf:application/pdf}
}

@article{schwartz_distribution_1982,
  title    = {On the distribution function and moments of power sums with
              log-normal components},
  volume   = {61},
  issn     = {0005-8580},
  doi      = {10.1002/j.1538-7305.1982.tb04353.x},
  abstract = {An approximate technique is presented for the evaluation of the
              mean and variance of the power sums with log-normal components.
              Exact expressions for the moments with two components are developed
              and then used in a nested fashion to obtain the moments of the
              desired sum. The results indicate more accurate estimates of these
              quantities over a wider range of individual component variances
              than any previously reported procedure. Coupling our estimates with
              the Gaussian assumption for the power sum provides a
              characterization of the cumulative distribution function which
              agrees remarkably well with a Monte Carlo simulation in the 1 to 99
              percent range of the variate. Simple polynomial expressions
              obtained for the moments lead to an effective analytical tool for
              various system performance studies. They allow quick and accurate
              calculation of quantities such as cochannel interference caused by
              shadowing in mobile telephony.},
  number   = {7},
  journal  = {Bell System Technical Journal, The},
  author   = {Schwartz, S.C. and Yeh, Y.S.},
  month    = sep,
  year     = {1982},
  keywords = {lognormal},
  pages    = {1441--1462},
  file     = {IEEE Xplore Abstract
              Record:/Users/apodusenko/Zotero/storage/9NNJSW3U/abs_all.html:text/html;Schwartz
              and Yeh - 1982 - On the distribution function and moments of power
              .pdf:/Users/apodusenko/Zotero/storage/TMNXD7HN/Schwartz and Yeh - 1982
              - On the distribution function and moments of power
              .pdf:application/pdf}
}

@article{li_overview_2014,
  title    = {An overview of noise-robust automatic speech recognition},
  volume   = {22},
  url      = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6732927},
  number   = {4},
  urldate  = {2015-11-03},
  journal  = {Audio, Speech, and Language Processing, IEEE/ACM Transactions on},
  author   = {Li, Jinyu and Deng, Li and Gong, Yifan and Haeb-Umbach, Reinhold},
  year     = {2014},
  keywords = {SE},
  pages    = {745--777},
  file     = {
              double_column.pdf:/Users/apodusenko/Zotero/storage/R638WC82/double_column.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/89DUDEMZ/login.html:text/html
              }
}

@inproceedings{acero_hmm_2000,
  title     = {{HMM} adaptation using vector taylor series for noisy speech
               recognition.},
  url       = {http://research.microsoft.com:8082/pubs/76536/2000-alexac-icslpb.pdf},
  urldate   = {2015-11-02},
  booktitle = {{INTERSPEECH}},
  author    = {Acero, Alex and Deng, Li and Kristjansson, Trausti T. and Zhang,
               Jerry},
  year      = {2000},
  keywords  = {SE},
  pages     = {869--872},
  file      = {Acero et al. - 2000 - HMM adaptation using vector taylor series for
               nois.pdf:/Users/apodusenko/Zotero/storage/7HEEJW7J/Acero et al. - 2000
               - HMM adaptation using vector taylor series for
               nois.pdf:application/pdf}
}

@inproceedings{deng_recursive_2001,
  title     = {Recursive noise estimation using iterative stochastic approximation
               for stereo-based robust speech recognition},
  doi       = {10.1109/ASRU.2001.1034594},
  abstract  = {We present an algorithm for recursive estimation of parameters in
               a mildly nonlinear model involving incomplete data. In particular,
               we focus on the time-varying deterministic parameters of additive
               noise in the nonlinear model. For the nonstationary noise that we
               encounter in robust speech recognition, different observation data
               segments correspond to different noise parameter values. Hence,
               recursive estimation algorithms are more desirable than batch
               algorithms, since they can be designed to adaptively track the
               changing noise parameters. One such design based on the iterative
               stochastic approximation algorithm in the recursive-EM framework is
               described. This new algorithm jointly adapts time-varying noise
               parameters and the auxiliary parameters introduced to give a linear
               approximation of the nonlinear model. We present stereo-based
               robust speech recognition results for the AURORA task, which
               demonstrate the effectiveness of the new algorithm compared with a
               more traditional, MMSE noise estimation technique under otherwise
               identical experimental conditions.},
  booktitle = {{IEEE} {Workshop} on {Automatic} {Speech} {Recognition} and {
               Understanding}, 2001. {ASRU} '01},
  author    = {Deng, Li and Droppo, J. and Acero, A.},
  year      = {2001},
  keywords  = {Stochastic processes, iterative algorithm, Iterative algorithms,
               iterative methods, Acoustic noise, Recursive estimation, Speech
               enhancement, speech recognition, Stochastic resonance, Working
               environment noise, Testing, learning (artificial intelligence), SE,
               Additive noise, approximation algorithm, approximation theory,
               AURORA task, cepstral analysis, deterministic parameters, MMSE
               estimation, noise parameter estimation, Noise robustness, Piecewise
               linear approximation, robust speech recognition, stereo training
               data, stochastic algorithm, time-varying parameters},
  pages     = {81--84},
  file      = {Deng et al. - 2001 - Recursive noise estimation using iterative
               stochas.pdf:/Users/apodusenko/Zotero/storage/F6X3PZS7/Deng et al. -
               2001 - Recursive noise estimation using iterative
               stochas.pdf:application/pdf;IEEE Xplore Abstract
               Record:/Users/apodusenko/Zotero/storage/AZMGN8PZ/login.html:text/html}
}

@inproceedings{logan_factorial_1998,
  title     = {Factorial {HMMs} for acoustic modeling},
  volume    = {2},
  doi       = {10.1109/ICASSP.1998.675389},
  abstract  = {In the machine learning research field several extensions of
               hidden Markov models (HMMs) have been proposed. In this paper we
               study their possibilities and potential benefits for the field of
               acoustic modeling. We describe preliminary experiments using an
               alternative modeling approach known as factorial hidden Markov
               models (FHMMs). We present these models as extensions of HMMs and
               detail a modification to the original formulation which seems to
               allow a more natural fit to speech. We present experimental results
               on the phonetically balanced TIMIT database comparing the
               performance of FHMMs with HMMs. We also study alternative feature
               representations that might be more suited to FHMMs},
  booktitle = {Proceedings of the 1998 {IEEE} {International} {Conference} on {
               Acoustics}, {Speech} and {Signal} {Processing}, 1998},
  author    = {Logan, Beth and Moreno, P.},
  month     = may,
  year      = {1998},
  keywords  = {machine learning, Stochastic processes, speech recognition,
               acoustic signal processing, learning (artificial intelligence), SE,
               acoustic modeling, dynamic belief network, experimental results,
               factorial HMM, feature extraction, feature representations, FHMM,
               Floors, hidden Markov models, HMM, Laboratories, machine learning
               research, phonetically balanced TIMIT database, signal
               representation, Solids, Spatial databases, Switches, Yttrium},
  pages     = {813--816 vol.2},
  file      = {IEEE Xplore Abstract
               Record:/Users/apodusenko/Zotero/storage/UQQ8G6IH/login.html:text/html;Logan
               and Moreno - 1998 - Factorial HMMs for acoustic
               modeling.pdf:/Users/apodusenko/Zotero/storage/USRR9Z69/Logan and Moreno
               - 1998 - Factorial HMMs for acoustic modeling.pdf:application/pdf}
}

@article{ephraim_application_1989,
  title    = {On the application of hidden {Markov} models for enhancing noisy
              speech},
  volume   = {37},
  issn     = {0096-3518},
  doi      = {10.1109/29.45532},
  abstract = {A maximum-a-posteriori approach for enhancing speech signals which
              have been degraded by statistically independent additive noise is
              proposed. The approach is based on statistical modeling of the
              clean speech signal and the noise process using long training
              sequences from the two processes. Hidden Markov models (HMMs) with
              mixtures of Gaussian autoregressive (AR) output probability
              distributions (PDs) are used to model the clean speech signal. The
              model for the noise process depends on its nature. The parameter
              set of the HMM model is estimated using the Baum or the EM
              (estimation-maximization) algorithm. The noisy speech is enhanced
              by reestimating the clean speech waveform using the EM algorithm.
              Efficient approximations of the training and enhancement procedures
              are examined. This results in the segmental k-means approach for
              hidden Markov modeling, in which the state sequence and the
              parameter set of the model are alternately estimated. Similarly,
              the enhancement is done by alternate estimation of the state and
              observation sequences. An approximate improvement of 4.0-6.0 dB in
              signal-to-noise ratio (SNR) is achieved at 10-dB input SNR},
  number   = {12},
  journal  = {IEEE Transactions on Acoustics, Speech and Signal Processing},
  author   = {Ephraim, Y. and Malah, D. and Juang, B.-H.},
  month    = dec,
  year     = {1989},
  keywords = {Speech enhancement, Signal to noise ratio, Speech analysis,
              Distortion measurement, speech analysis and processing, Markov
              processes, SE, Additive noise, hidden Markov models, Degradation,
              estimation-maximization, Gaussian autoregressive,
              maximum-a-posteriori approach, probability distributions, segmental
              k-means approach, Speech processing, speech signals, State
              estimation, statistical modeling, training sequences},
  pages    = {1846--1856},
  file     = {Ephraim et al. - 1989 - On the application of hidden Markov models for
              enh.pdf:/Users/apodusenko/Zotero/storage/TBPU3FJF/Ephraim et al. - 1989
              - On the application of hidden Markov models for
              enh.pdf:application/pdf;IEEE Xplore Abstract
              Record:/Users/apodusenko/Zotero/storage/TBITFKA7/login.html:text/html}
}

@article{huber_pemo-q_2006,
  title    = {{PEMO}-{Q} -- {A} {New} {Method} for {Objective} {Audio} {Quality} {
              Assessment} {Using} a {Model} of {Auditory} {Perception}},
  volume   = {14},
  issn     = {1558-7916},
  doi      = {10.1109/TASL.2006.883259},
  abstract = {A new method for the objective assessment and prediction of
              perceived audio quality is introduced. It represents an expansion
              of the speech quality measure qC, introduced by Hansen and
              Kollmeier, and is based on a psychoacoustically validated,
              quantitative model of the "effective" peripheral auditory
              processing by Dau et al. To evaluate the audio quality of a given
              distorted signal relative to a corresponding high-quality reference
              signal, the auditory model is employed to compute "internal
              representations" of the signals, which are partly assimilated in
              order to account for assumed cognitive aspects. The linear cross
              correlation coefficient of the assimilated internal representations
              represents the perceptual similarity measure (PSM). PSM shows good
              correlations with subjective quality ratings if different types of
              audio signals are considered separately, whereas a better accuracy
              of signal-independent quality prediction is achieved by a second
              quality measure PSMt represented by the fifth percentile of the
              sequence of instantaneous audio quality PSM(t). The new measures
              were evaluated using a large database of subjective listening tests
              that were originally carried out on behalf of the International
              Telecommunication Union (ITU) and Moving Pictures Experts Group
              (MPEG) for the evaluation of various low bit-rate audio codecs.
              Additional tests with data unknown in the development phase of the
              model were carried out. Except for linear distortions, the new
              method shows a higher prediction accuracy than the ITU-R
              recommendation BS.1387 ("PEAQ") for the tested data},
  number   = {6},
  journal  = {IEEE Transactions on Audio, Speech, and Language Processing},
  author   = {Huber, R. and Kollmeier, B.},
  month    = nov,
  year     = {2006},
  keywords = {speech quality, Distortion measurement, hearing, Testing,
              Psychology, signal representation, Speech processing, audio coding,
              Audio databases, Audio quality, audio quality assessment, auditory
              model, Auditory Perception, bit-rate audio codecs, Codecs,
              distorted signal, high-quality reference signal, ITU-R
              recommendation BS.1387, linear cross correlation coefficient, MPEG
              standards, objective quality assessment, PEMO-Q, perceptual
              similarity measure, Psychoacoustic models, Q measurement, Quality
              assessment, signal internal representations},
  pages    = {1902--1911},
  file     = {IEEE Xplore Abstract
              Record:/Users/apodusenko/Zotero/storage/NS42N5XM/login.html:text/html}
}

@misc{beerends_speech_2008,
  title    = {Speech quality measurement for the hearing impaired on the basis of {
              PESQ}},
  url      = {http://resolver.tudelft.nl/uuid:eb7782d4-5563-4c57-9268-6bb07eba020b},
  abstract = {One of the research topics within the HearCom project, a European
              project that studies the impact of hearing loss on communication,
              is to find methods with which the speech quality as perceived by
              the hearing impaired can be measured objectively. ITU-T
              Recommendation P.862 PESQ and its wideband extension P.862.2, are
              obvious candidates for this despite the fact that they were
              developed for normally hearing subjects. This paper investigates
              the extent to which PESQ and possible simple extensions can be used
              to measure the quality of speech signals as perceived by hearing
              impaired subjects.},
  language = {en},
  urldate  = {2015-10-29},
  author   = {Beerends, J. G. and Krebber, J. and Huber, R. and Eneman, K. and
              Luts, H. and {TNO Informatie- en Communicatietechnologie}},
  month    = jan,
  year     = {2008}
}

@article{houtgast_evaluation_1971,
  title    = {Evaluation of {Speech} {Transmission} {Channels} by {Using} {
              Artificial} {Signals}},
  volume   = {25},
  abstract = {A method is presented by which the effect of some current types of
              interferences on speech intelligibility can be quantified on the
              basis of simple physical measurements. The approach is based on the
              relationship of perceptual differences and physical differences
              among speech sounds. This relationship suggests that the effect of
              a transmission channel on intelligibility is strongly related to
              the degree to which the spectral differences, originated at the
              talker side, are preserved at the listener side. This leads to the
              definition of the Speech Transmission Index (STI) based upon: (1) a
              simple artificial test signal by which a “standard” spectral
              difference is introduced at the talker side of the channel, and (2)
              an analyzing procedure to be applied to the test signal received at
              the listener side in order to quantify the degree of preservation
              of the spectral difference introduced. The test signal and the
              analyzing procedure are optimized on the basis of the rank-order
              correlation between the STI-values and PB-word scores obtained for
              50 different transmission channels, subjected to peak clipping,
              band-pass limiting, interfering noise or reverberation, in various
              degrees and a number of combinations. The relation between the STI
              approach and other methods is discussed.},
  number   = {6},
  journal  = {Acta Acustica united with Acustica},
  author   = {Houtgast, T. and Steeneken, H. J. M.},
  month    = dec,
  year     = {1971},
  pages    = {355--367}
}

@article{jepsen_characterizing_2011,
  title    = {Characterizing auditory processing and perception in individual
              listeners with sensorineural hearing loss},
  volume   = {129},
  issn     = {1520-8524},
  doi      = {10.1121/1.3518768},
  abstract = {This study considered consequences of sensorineural hearing loss
              in ten listeners. The characterization of individual hearing loss
              was based on psychoacoustic data addressing audiometric pure-tone
              sensitivity, cochlear compression, frequency selectivity, temporal
              resolution, and intensity discrimination. In the experiments it was
              found that listeners with comparable audiograms can show very
              different results in the supra-threshold measures. In an attempt to
              account for the observed individual data, a model of auditory
              signal processing and perception [Jepsen et al., J. Acoust. Soc.
              Am. 124, 422-438 (2008)] was used as a framework. The parameters of
              the cochlear processing stage of the model were adjusted to account
              for behaviorally estimated individual basilar-membrane input-output
              functions and the audiogram, from which the amounts of inner
              hair-cell and outer hair-cell losses were estimated as a function
              of frequency. All other model parameters were left unchanged. The
              predictions showed a reasonably good agreement with the measured
              individual data in the frequency selectivity and forward masking
              conditions while the variation of intensity discrimination
              thresholds across listeners was underestimated by the model. The
              model and the associated parameters for individual hearing-impaired
              listeners might be useful for investigating effects of individual
              hearing impairment in more complex conditions, such as speech
              intelligibility in noise.},
  language = {eng},
  number   = {1},
  journal  = {The Journal of the Acoustical Society of America},
  author   = {Jepsen, Morten L. and Dau, Torsten},
  month    = jan,
  year     = {2011},
  pmid     = {21303008},
  keywords = {Cochlea, Aged, Audiometry, Pure-Tone, Female, Humans, Male,
              Algorithms, Computer Simulation, psychoacoustics, Acoustic
              Stimulation, Auditory Threshold, Time Factors, Auditory Perception,
              Adult, Case-Control Studies, Hearing Loss, Sensorineural, Middle
              Aged, Models, Psychological, Persons With Hearing Impairments,
              Signal Detection, Psychological, Young Adult},
  pages    = {262--281}
}

@book{von_der_linden_bayesian_2014,
  title     = {Bayesian {Probability} {Theory}: {Applications} in the {Physical} {
               Sciences}},
  isbn      = {1-107-03590-2},
  publisher = {Cambridge University Press},
  author    = {von der Linden, Wolfgang and Dose, Volker and Von Toussaint, Udo},
  year      = {2014}
}

@article{blackwell_summary_2014,
  title      = {Summary health statistics for {U}.{S}. adults: national health
                interview survey, 2012},
  issn       = {0083-1972},
  shorttitle = {Summary health statistics for {U}.{S}. adults},
  abstract   = {OBJECTIVES: This report presents detailed tables from the 2012
                National Health Interview Survey (NHIS) for the civilian
                noninstitutionalized adult population, classified by sex, age, race
                and Hispanic origin, education, current employment status, family
                income, poverty status, health insurance coverage, marital status,
                and place and region of residence. Estimates (frequencies and
                percentages) are presented for selected chronic conditions and
                mental health characteristics, functional limitations, health
                status, health behaviors, health care access and utilization, and
                human immunodeficiency virus testing. Percentages and percent
                distributions are presented in both age-adjusted and unadjusted
                versions. DATA SOURCE: NHIS is a household, multistage probability
                sample survey conducted annually by interviewers of the U.S. Census
                Bureau for the Centers for Disease Control and Prevention's
                National Center for Health Statistics. In 2012, data were collected
                on 34,525 adults in the Sample Adult questionnaire. The conditional
                response rate was 79.7\%, and the final response rate was 61.2\%.
                The health information for adults in this report was obtained from
                one randomly selected adult per family. HIGHLIGHTS: In 2012, 61\%
                of adults aged 18 and over had excellent or very good health.
                Eleven percent of adults had been told by a doctor or other health
                professional that they had heart disease, 24\% had been told on two
                or more visits that they had hypertension, 9\% had been told that
                they had diabetes, and 21\% had been told that they had some for of
                arthritis, rheumatoid arthritis, gout, lupus, or fibromyalgia.
                Eighteen percent of adults were current smokers and 21\% were
                former smokers. Based on estimates of body mass index, 35\% of
                adults were overweight and 28\% were obese.},
  language   = {eng},
  number     = {260},
  journal    = {Vital and Health Statistics. Series 10, Data from the National
                Health Survey},
  author     = {Blackwell, Debra L. and Lucas, Jacqueline W. and Clarke, Tainya C.},
  month      = feb,
  year       = {2014},
  pmid       = {24819891},
  keywords   = {Aged, Female, Health Status, Humans, Male, Socioeconomic Factors,
                Adult, Middle Aged, Young Adult, Adolescent, Age Distribution, Body
                Mass Index, Continental Population Groups, Diet, Exercise, Health
                Behavior, Health Services, Health Services Accessibility, Health
                Surveys, Mental Health, National Center for Health Statistics
                (U.S.), Residence Characteristics, Sex Distribution, United States},
  pages      = {1--161}
}

@techreport{minka_hidden_1999,
  title       = {From hidden markov models to linear dynamical systems},
  number      = {531},
  institution = {VIsion and Modeling group, Media Lab, MIT},
  author      = {Minka, Tom},
  year        = {1999},
  file        = {Minka - 1999 - From hidden markov models to linear dynamical
                 syst.pdf:/Users/apodusenko/Zotero/storage/6G22W56G/Minka - 1999 - From
                 hidden markov models to linear dynamical syst.pdf:application/pdf}
}

@article{lange_robust_1989,
  title   = {Robust statistical modeling using the t distribution},
  volume  = {84},
  number  = {408},
  journal = {Journal of the American Statistical Association},
  author  = {Lange, Kenneth L. and Little, Roderick JA and Taylor, Jeremy MG},
  year    = {1989},
  pages   = {881--896}
}

@article{fonseca_objective_2008,
  title   = {Objective {Bayesian} analysis for the {Student}-t regression model},
  journal = {Biometrika},
  author  = {Fonseca, Thaís CO and Ferreira, Marco AR and Migon, Helio S.},
  year    = {2008},
  file    = {Fonseca - 2008 - Objective Bayesian analysis for the Student-t
             regression model.pdf:/Users/apodusenko/Zotero/storage/CVASRIJ3/Fonseca
             - 2008 - Objective Bayesian analysis for the Student-t regression
             model.pdf:application/pdf}
}

@article{kaptein_use_2014,
  title    = {The use of {Thompson} sampling to increase estimation precision},
  volume   = {47},
  issn     = {1554-3528},
  url      = {http://link.springer.com/article/10.3758/s13428-014-0480-0},
  doi      = {10.3758/s13428-014-0480-0},
  abstract = {In this article, we consider a sequential sampling scheme for
              efficient estimation of the difference between the means of two
              independent treatments when the population variances are unequal
              across groups. The sampling scheme proposed is based on a solution
              to bandit problems called Thompson sampling. While this approach is
              most often used to maximize the cumulative payoff over competing
              treatments, we show that the same method can also be used to
              balance exploration and exploitation when the aim of the
              experimenter is to efficiently increase estimation precision. We
              introduce this novel design optimization method and, by simulation,
              show its effectiveness.},
  language = {en},
  number   = {2},
  urldate  = {2015-10-15},
  journal  = {Behavior Research Methods},
  author   = {Kaptein, Maurits},
  month    = jul,
  year     = {2014},
  keywords = {Cognitive Psychology, Bandit problems, Design optimization,
              Thompson sampling},
  pages    = {409--423},
  file     = {Kaptein - 2014 - The use of Thompson sampling to increase
              estimatio.pdf:/Users/apodusenko/Zotero/storage/RNUDKNUC/Kaptein - 2014
              - The use of Thompson sampling to increase
              estimatio.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/RMRSBDAJ/10.html:text/html
              }
}

@inproceedings{smidl_bayesian_2012,
  title     = {Bayesian estimation of forgetting factor in adaptive filtering and
               change detection},
  doi       = {10.1109/SSP.2012.6319658},
  abstract  = {An adaptive filter is derived in a Bayesian framework from the
               assumption that the difference in the parameter distribution from
               one time to another is bounded in terms of the Kullback-Leibler
               divergence. We show an explicit link to the general concepts of
               exponential forgetting, and outline the details for a linear
               Gaussian model with unknown parameter and covariance. We extend the
               problem to an unknown forgetting factor, where we provide a
               particular prior that allows for abrupt changes in forgetting,
               which is useful in change detection problems. The Rao-Blackwellized
               particle filter is used for the implementation, and its performance
               is assessed in a simulation of system with abrupt changes of
               parameters.},
  booktitle = {2012 {IEEE} {Statistical} {Signal} {Processing} {Workshop} ({SSP}
               )},
  author    = {Smidl, V. and Gustafsson, F.},
  month     = aug,
  year      = {2012},
  keywords  = {Estimation, Abstracts, Bayesian methods, Entropy, exponential
               forgetting, change detection, Gaussian processes, adaptive
               filtering, adaptive filters, Bayesian estimation, Conferences,
               Covariance, forgetting factor, Kullback-Leibler divergence, Lead,
               linear Gaussian model, maximum entropy, parameter distribution,
               particle filtering (numerical methods), Rao-Blackwellized particle
               filter, Rao-Blackwellized particle filtering},
  pages     = {197--200},
  file      = {IEEE Xplore Abstract
               Record:/Users/apodusenko/Zotero/storage/TPDG2T6E/login.html:text/html;Smidl
               and Gustafsson - 2012 - Bayesian estimation of forgetting factor in
               adapti.pdf:/Users/apodusenko/Zotero/storage/AQ9RZNA8/Smidl and
               Gustafsson - 2012 - Bayesian estimation of forgetting factor in
               adapti.pdf:application/pdf}
}

@article{gershman_statistical_2014,
  title    = {Statistical {Computations} {Underlying} the {Dynamics} of {Memory} {
              Updating}},
  volume   = {10},
  url      = {http://dx.doi.org/10.1371/journal.pcbi.1003939},
  doi      = {10.1371/journal.pcbi.1003939},
  abstract = {Author Summary When do we modify old memories, and when do we
              create new ones? We suggest that this question can be answered
              statistically: The parsing of experience into distinct memory
              traces corresponds to inferences about the underlying structure of
              the environment. When sensory data change gradually over time, the
              brain infers that the environment has slowly been evolving, and the
              current representation of the environment (an existing memory
              trace) is updated. In contrast, abrupt changes indicate transitions
              between different structures, leading to the formation of new
              memories. While these ideas fall naturally out of statistical
              models of learning, they have not yet been directly tested in the
              domain of human memory. In this paper, we describe a model of
              statistical inference that instantiates these ideas, and test the
              model by asking human participants to reconstruct previously seen
              visual objects that have since changed gradually or abruptly. The
              results of this experiment support our theory of how the
              statistical structure of sensory experiences shapes memory
              formation.},
  number   = {11},
  urldate  = {2015-10-13},
  journal  = {PLoS Comput Biol},
  author   = {Gershman, Samuel J. and Radulescu, Angela and Norman, Kenneth A. and
              Niv, Yael},
  month    = nov,
  year     = {2014},
  pages    = {e1003939},
  file     = {Gershman et al. - 2014 - Statistical Computations Underlying the
              Dynamics o.pdf:/Users/apodusenko/Zotero/storage/8C3ZQ29N/Gershman et
              al. - 2014 - Statistical Computations Underlying the Dynamics
              o.pdf:application/pdf}
}

@article{pavelkova_state_2014,
  title     = {State and parameter estimation of state-space model with entry-wise
               correlated uniform noise},
  volume    = {28},
  copyright = {Copyright © 2013 John Wiley \& Sons, Ltd.},
  issn      = {1099-1115},
  url       = {http://onlinelibrary.wiley.com/doi/10.1002/acs.2438/abstract},
  doi       = {10.1002/acs.2438},
  abstract  = {Joint parameter and state estimation is proposed for linear
               state-space model with uniform, entry-wise correlated, state and
               output noises (LSU model for short). The adopted Bayesian modelling
               and approximate estimation produce an estimator that (a) provides
               the maximum a posteriori estimate enriched by information on its
               precision, (b) respects correlated noise entries without demanding
               the user to tune noise covariances, and (c) respects bounded nature
               of real-life variables. Copyright © 2013 John Wiley \& Sons, Ltd.},
  language  = {en},
  number    = {11},
  urldate   = {2015-10-12},
  journal   = {International Journal of Adaptive Control and Signal Processing},
  author    = {Pavelková, Lenka and Kárný, Miroslav},
  month     = nov,
  year      = {2014},
  keywords  = {bounded noise, estimation algorithms, filtering problems,
               state-space models, uncertain dynamic systems},
  pages     = {1189--1205},
  file      = {Pavelková and Kárný - 2014 - State and parameter estimation of
               state-space
               mode.pdf:/Users/apodusenko/Zotero/storage/B9NDR4CZ/Pavelková and Kárný
               - 2014 - State and parameter estimation of state-space
               mode.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/RZQZJIXD/abstract.html:text/html
               }
}

@article{lee_hierarchical_2003,
  title    = {Hierarchical {Bayesian} {Inference} in the {Visual} {Cortex}},
  volume   = {20},
  abstract = {this paper, we propose a Bayesian theory of hierarchical cortical
              computation based both on (a) the mathematical and computational
              ideas of computer vision and pattern the- ory and on (b) recent
              neurophysiological experimental evidence. We ,2 have proposed that
              Grenander's pattern theory 3 could potentially model the brain as a
              generafive model in such a way that feedback serves to disambiguate
              and 'explain away' the earlier representa- tion. The Helmholtz
              machine 4, 5 was an excellent step towards approximating this
              proposal, with feedback implementing priors. Its development,
              however, was rather limited, dealing only with binary images.
              Moreover, its feedback mechanisms were engaged only during the
              learning of the feedforward connections but not during perceptual
              inference, though the Gibbs sampling process for inference can
              potentially be interpreted as top-down feedback disambiguating low
              level representations? Rao and Ballard's predictive coding/Kalman
              filter model 6 did integrate generafive feedback in the perceptual
              inference process, but it was primarily a linear model and thus
              severely limited in practical utility. The data-driven Markov Chain
              Monte Carlo approach of Zhu and colleagues 7, 8 might be the most
              successful recent application of this proposal in solving real and
              difficult computer vision problems using generafive models, though
              its connection to the visual cortex has not been explored. Here, we
              bring in a powerful and widely applicable paradigm from artificial
              intelligence and computer vision to propose some new ideas about
              the algorithms of visual cortical process- ing and the nature of
              representations in the visual cortex. We will review some of our
              and others' neurophysiological experimental data to lend support to
              these ideas},
  number   = {7},
  journal  = {Journal Optical Society of America},
  author   = {Lee, Tai Sing and Mumford, David},
  year     = {2003},
  file     = {Citeseer -
              Snapshot:/Users/apodusenko/Zotero/storage/S6J6KCX6/summary.html:text/html;Lee
              and Mumford - 2003 - Hierarchical Bayesian Inference in the Visual
              Cort.pdf:/Users/apodusenko/Zotero/storage/SKH4MTZZ/Lee and Mumford -
              2003 - Hierarchical Bayesian Inference in the Visual
              Cort.pdf:application/pdf}
}

@incollection{tishby_information_2011,
  series    = {Springer {Series} in {Cognitive} and {Neural} {Systems}},
  title     = {Information {Theory} of {Decisions} and {Actions}},
  copyright = {©2011 Springer Science+Business Media, LLC},
  isbn      = {978-1-4419-1451-4 978-1-4419-1452-1},
  url       = {http://link.springer.com/chapter/10.1007/978-1-4419-1452-1_19},
  language  = {en},
  urldate   = {2015-10-09},
  booktitle = {Perception-{Action} {Cycle}},
  publisher = {Springer New York},
  author    = {Tishby, Naftali and Polani, Daniel},
  editor    = {Cutsuridis, Vassilis and Hussain, Amir and Taylor, John G.},
  year      = {2011},
  keywords  = {Neurobiology, Neurosciences, Computation by Abstract Devices,
               Signal, Image and Speech Processing},
  pages     = {601--636},
  file      = {
               Snapshot:/Users/apodusenko/Zotero/storage/ZA9DWR7B/10.html:text/html;Tishby
               and Polani - 2011 - Information Theory of Decisions and
               Actions.pdf:/Users/apodusenko/Zotero/storage/P7IS6SW7/Tishby and Polani
               - 2011 - Information Theory of Decisions and
               Actions.pdf:application/pdf}
}

@article{sallans_reinforcement_2004,
  title    = {Reinforcement {Learning} with {Factored} {States} and {Actions}},
  volume   = {5},
  issn     = {1532-4435},
  url      = {http://dl.acm.org/citation.cfm?id=1005332.1016794},
  abstract = {A novel approximation method is presented for approximating the
              value function and selecting good actions for Markov decision
              processes with large state and action spaces. The method
              approximates state-action values as negative free energies in an
              undirected graphical model called a product of experts. The model
              parameters can be learned efficiently because values and
              derivatives can be efficiently computed for a product of experts.
              Actions can be found even in large factored action spaces by the
              use of Markov chain Monte Carlo sampling. Simulation results show
              that the product of experts approximation can be used to solve
              large problems. In one simulation it is used to find actions in
              action spaces of size 240.},
  urldate  = {2015-10-09},
  journal  = {J. Mach. Learn. Res.},
  author   = {Sallans, Brian and Hinton, Geoffrey E.},
  month    = dec,
  year     = {2004},
  pages    = {1063--1088},
  file     = {Sallans and Hinton - 2004 - Reinforcement Learning with Factored
              States and Ac.pdf:/Users/apodusenko/Zotero/storage/3ZPGW9UR/Sallans and
              Hinton - 2004 - Reinforcement Learning with Factored States and
              Ac.pdf:application/pdf}
}

@article{frey_learning_2001,
  title    = {Learning dynamic noise models from noisy speech for robust noise
              recognition},
  journal  = {NIPS14},
  author   = {Frey, B. J. and Kristjansson, T. and Deng, L. and Acero, A.},
  year     = {2001},
  keywords = {SE},
  file     = {Frey - 2002 - ALGONQUIN learning dynamic noise models from noisy
              speech for robust speech
              recognition.pdf:/Users/apodusenko/Zotero/storage/KFE4WM32/Frey - 2002 -
              ALGONQUIN learning dynamic noise models from noisy speech for robust
              speech recognition.pdf:application/pdf}
}

@article{vandekerckhove_model_2014,
  title    = {Model {Comparison} and the {Principle} of {Parsimony}},
  url      = {http://escholarship.org/uc/item/9j47k5q9},
  abstract = {Model Comparison and the Principle of Parsimony Joachim
              Vandekerckhove Department of Cognitive Sciences, University of
              California, Irvine Dora Matzke Department of Psychology, University
              of Amsterdam Eric-Jan Wagenmakers Department of Psychology,
              University of Amsterdam Introduction At its core, the study of
              psychology is concerned with the discovery of plausible
              explanations for human behavior. For instance, one may observe that
              “practice makes perfect”: as people become more familiar with a
              task, they tend to execute it more quickly and with fewer errors.
              More interesting is the observation that practice tends to improve
              performance such that most of the beneﬁt is accrued early on, a
              pattern of diminishing returns that is well described by a power
              law (Logan, 1988; but see Heathcote, Brown, \& Mewhort, 2000). This
              pattern occurs across so many diﬀerent tasks (e.g., cigar rolling,
              maze solving, fact retrieval, and a variety of standard
              psychological tasks) that it is known as the “power law of
              practice”. Consider, for instance, the lexical decision task, a
              task in which participants have to decide quickly whether a letter
              string is an existing word (e.g., sunscreen) or not (e.g.,
              tolphin). When repeatedly presented with the same stimuli,
              participants show a power law decrease in their mean response
              latencies; in fact, they show a power law decrease in the entire
              response time distribution, that is, both the fast responses and
              the slow responses speed up with practice according to a power law
              (Logan, 1992). The observation that practice makes perfect is
              trivial, but the ﬁnding that practice- induced improvement follows
              a general law is not. Nevertheless, the power law of practice only
              provides a descriptive summary of the data and does not explain the
              reasons why practice should result in a power law improvement in
              performance. In order to go beyond direct observation and
              statistical summary, it is necessary to bridge the divide between
              observed performance on the one hand and the pertinent
              psychological processes on the other. Such bridges are built from a
              coherent set of assumptions about the underlying cognitive
              processes—a theory. Ideally, substantive psychological theories are
              formalized as quantitative models (Busemeyer \& Diederich, 2010;
              Lewandowsky \& Farrell, 2010). For example, the power law of
              practice has been explained by instance theory (Logan, 1992, This
              work was partially supported by the starting grant “Bayes or Bust”
              awarded by the European Research Council to EJW, and NSF grant \#
              1230118 from the Methods, Measurements, and Statistics panel to JV.
              },
  urldate  = {2015-10-09},
  journal  = {eScholarship},
  author   = {Vandekerckhove, J. and Matzke, D. and Wagenmakers, E.-J.},
  month    = jan,
  year     = {2014},
  file     = {
              Snapshot:/Users/apodusenko/Zotero/storage/MQNS29HT/9j47k5q9.html:text/html
              }
}

@article{stone_gaussian_2010,
  title   = {Gaussian processes for sample efficient reinforcement learning with {
             RMAX}-like exploration},
  url     = {http://www.cs.utexas.edu/users/ai-lab/?ECML10-jung},
  urldate = {2015-10-08},
  author  = {Stone, Tobias Jung {and} Peter},
  year    = {2010},
  file    = {
             Snapshot:/Users/apodusenko/Zotero/storage/2KC9HSFB/ai-lab.html:text/html;Stone
             - 2010 - Gaussian processes for sample efficient
             reinforcem.pdf:/Users/apodusenko/Zotero/storage/N2DB6W2N/Stone - 2010 -
             Gaussian processes for sample efficient reinforcem.pdf:application/pdf}
}

@article{shahriari_taking_nodate,
  title      = {Taking the {Human} {Out} of the {Loop}: {A} {Review} of {Bayesian} {
                Optimization}},
  shorttitle = {Taking the {Human} {Out} of the {Loop}},
  url        = {
                http://www.cs.ox.ac.uk/people/nando.defreitas/publications/BayesOptLoop.pdf
                },
  urldate    = {2015-10-08},
  author     = {Shahriari, Bobak and Swersky, Kevin and Wang, Ziyu and Adams, Ryan
                P. and de Freitas, Nando},
  file       = {Shahriari et al. - Taking the Human Out of the Loop A Review of
                Baye.pdf:/Users/apodusenko/Zotero/storage/CNUZRJSN/Shahriari et al. -
                Taking the Human Out of the Loop A Review of Baye.pdf:application/pdf}
}

@inproceedings{murphy_loopy_1999,
  title     = {Loopy belief propagation for approximate inference: {An} empirical
               study},
  isbn      = {1-55860-614-9},
  booktitle = {Proceedings of the {Fifteenth} conference on {Uncertainty} in
               artificial intelligence},
  publisher = {Morgan Kaufmann Publishers Inc.},
  author    = {Murphy, Kevin P. and Weiss, Yair and Jordan, Michael I.},
  year      = {1999},
  pages     = {467--475}
}

@inproceedings{lingyun_gu_single-channel_2008,
  title     = {Single-channel speech separation based on modulation frequency},
  isbn      = {978-1-4244-1483-3 978-1-4244-1484-0},
  url       = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4517537},
  doi       = {10.1109/ICASSP.2008.4517537},
  urldate   = {2015-10-08},
  publisher = {IEEE},
  author    = {{Lingyun Gu} and Stern, Richard M.},
  month     = mar,
  year      = {2008},
  keywords  = {SE},
  pages     = {25--28},
  file      = {Lingyun Gu and Stern - 2008 - Single-channel speech separation based
               on modulati.pdf:/Users/apodusenko/Zotero/storage/MD7FQZFF/Lingyun Gu
               and Stern - 2008 - Single-channel speech separation based on
               modulati.pdf:application/pdf}
}

@book{goodwin_adaptive_1984,
  title     = {Adaptive prediction, filtering and control},
  publisher = {Prentice hall},
  author    = {Goodwin, G. C. and Sin, Kwai Sang},
  year      = {1984}
}

@article{boll_suppression_1979,
  title    = {Suppression of acoustic noise in speech using spectral subtraction},
  volume   = {27},
  issn     = {0096-3518},
  doi      = {10.1109/TASSP.1979.1163209},
  abstract = {A stand-alone noise suppression algorithm is presented for
              reducing the spectral effects of acoustically added noise in
              speech. Effective performance of digital speech processors
              operating in practical environments may require suppression of
              noise from the digital wave-form. Spectral subtraction offers a
              computationally efficient, processor-independent approach to
              effective digital speech analysis. The method, requiring about the
              same computation as high-speed convolution, suppresses stationary
              noise from speech by subtracting the spectral noise bias calculated
              during nonspeech activity. Secondary procedures are then applied to
              attenuate the residual noise left after subtraction. Since the
              algorithm resynthesizes a speech waveform, it can be used as a
              pre-processor to narrow-band voice communications systems, speech
              recognition systems, or speaker authentication systems.},
  number   = {2},
  journal  = {IEEE Transactions on Acoustics, Speech and Signal Processing},
  author   = {Boll, S.},
  month    = apr,
  year     = {1979},
  keywords = {Acoustic noise, Speech enhancement, speech recognition, Working
              environment noise, Speech analysis, Convolution, Speech processing,
              Authentication, Narrowband, noise reduction},
  pages    = {113--120},
  file     = {Boll - 1979 - Suppression of acoustic noise in speech using
              spec.pdf:/Users/apodusenko/Zotero/storage/3HJEHFM2/Boll - 1979 -
              Suppression of acoustic noise in speech using
              spec.pdf:application/pdf;IEEE Xplore Abstract
              Record:/Users/apodusenko/Zotero/storage/MCKJSMJ9/abs_all.html:text/html
              }
}

@misc{ramirez_optimizing_2013,
  title   = {Optimizing {Noise} {Reduction} {Using} {Directional} {Speech} {
             Enhancement} - {Hearing} {Review}},
  url     = {
             http://www.hearingreview.com/2013/02/optimizing-noise-reduction-using-directional-speech-enhancement/
             },
  urldate = {2015-10-07},
  author  = {Ramirez, Patricia and Jons, Catherine and Powers, Thomas A.},
  month   = feb,
  year    = {2013},
  file    = {Optimizing Noise Reduction Using Directional Speech Enhancement -
             Hearing
             Review:/Users/apodusenko/Zotero/storage/ZVXCUMA8/optimizing-noise-reduction-using-directional-speech-enhancement.html:text/html
             }
}

@article{hamacher_signal_2005,
  title      = {Signal {Processing} in {High}-{End} {Hearing} {Aids}: {State} of the
                {Art}, {Challenges}, and {Future} {Trends}},
  volume     = {2005},
  copyright  = {2005 Hamacher et al.},
  issn       = {1687-6180},
  shorttitle = {Signal {Processing} in {High}-{End} {Hearing} {Aids}},
  url        = {http://asp.eurasipjournals.com/content/2005/18/152674/abstract},
  doi        = {10.1155/ASP.2005.2915},
  language   = {en},
  number     = {18},
  urldate    = {2015-10-07},
  journal    = {EURASIP Journal on Advances in Signal Processing},
  author     = {Hamacher, V. and Chalupper, J. and Eggers, J. and Fischer, E. and
                Kornagel, U. and Puder, H. and Rass, U.},
  month      = nov,
  year       = {2005},
  keywords   = {noise reduction, acoustic feedback, classification, compression,
                digital hearing aid, directional microphone},
  pages      = {152674},
  file       = {Hamacher et al. - 2005 - Signal Processing in High-End Hearing Aids
                State .pdf:/Users/apodusenko/Zotero/storage/8577726S/Hamacher et al. -
                2005 - Signal Processing in High-End Hearing Aids State
                .pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/6QH5GHTA/152674.html:text/html
                }
}

@article{oravecz_sequential_2016,
  title   = {Sequential {Bayesian} updating for big data},
  url     = {http://www.cidlab.com/prints/oravecz2016sequential.pdf},
  urldate = {2015-10-05},
  author  = {Oravecz, Zita and Huentelman, Matt and Vandekerckhove, Joachim},
  year    = {2016},
  file    = {Oravecz et al. - Sequential Bayesian updating for big
             data.pdf:/Users/apodusenko/Zotero/storage/SS8N4IH2/Oravecz et al. -
             Sequential Bayesian updating for big data.pdf:application/pdf}
}

@article{marin_resolving_2010,
  title   = {On resolving the {Savage}–{Dickey} paradox},
  volume  = {4},
  journal = {Electronic Journal of Statistics},
  author  = {Marin, Jean-Michel and Robert, Christian P.},
  year    = {2010},
  pages   = {643--654}
}

@misc{hoffman_why_2014,
  title  = {Why {Variational} {Inference} {Gives} {Bad} {Parameter} {Estimates}},
  url    = {https://sites.google.com/site/variationalworkshop/},
  author = {Hoffman, Matt},
  year   = {2014},
  file   = {Hoffman - 2014 - Why Variational Inference Gives Bad Parameter
            Esti.pdf:/Users/apodusenko/Zotero/storage/JZ8SSA8A/Hoffman - 2014 - Why
            Variational Inference Gives Bad Parameter Esti.pdf:application/pdf}
}

@article{fan_fast_2015,
  title    = {Fast {Second}-{Order} {Stochastic} {Backpropagation} for {Variational
              } {Inference}},
  url      = {http://arxiv.org/abs/1509.02866},
  abstract = {We propose a second-order (Hessian or Hessian-free) based
              optimization method for variational inference inspired by Gaussian
              backpropagation, and argue that quasi-Newton optimization can be
              developed as well. This is accomplished by generalizing the
              gradient computation in stochastic backpropagation via a
              reparametrization trick with lower complexity. As an illustrative
              example, we apply this approach to the problems of Bayesian
              logistic regression and variational auto-encoder (VAE).
              Additionally, we compute bounds on the estimator variance of
              intractable expectations for the family of Lipschitz continuous
              function. Our method is practical, scalable and model free. We
              demonstrate our method on several real-world datasets and provide
              comparisons with other stochastic gradient methods to show
              substantial enhancement in convergence rates.},
  urldate  = {2015-09-22},
  journal  = {arXiv:1509.02866 [stat]},
  author   = {Fan, Kai and Wang, Ziteng and Beck, Jeff and Kwok, James and Heller,
              Katherine},
  month    = sep,
  year     = {2015},
  note     = {arXiv: 1509.02866},
  keywords = {Statistics - Machine Learning},
  file     = {arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/EBINXJ3C/1509.html:text/html;Fan
              et al. - 2015 - Fast Second-Order Stochastic Backpropagation for
              V.pdf:/Users/apodusenko/Zotero/storage/PBGU5W47/Fan et al. - 2015 -
              Fast Second-Order Stochastic Backpropagation for V.pdf:application/pdf}
}

@article{giordano_linear_2015,
  title    = {Linear {Response} {Methods} for {Accurate} {Covariance} {Estimates}
              from {Mean} {Field} {Variational} {Bayes}},
  url      = {http://arxiv.org/abs/1506.04088},
  abstract = {Mean field variational Bayes (MFVB) is a popular posterior
              approximation method due to its fast runtime on large-scale data
              sets. However, it is well known that a major failing of MFVB is
              that it underestimates the uncertainty of model variables
              (sometimes severely) and provides no information about model
              variable covariance. We generalize linear response methods from
              statistical physics to deliver accurate uncertainty estimates for
              model variables---both for individual variables and coherently
              across variables. We call our method linear response variational
              Bayes (LRVB). When the MFVB posterior approximation is in the
              exponential family, LRVB has a simple, analytic form, even for
              non-conjugate models. Indeed, we make no assumptions about the form
              of the true posterior. We demonstrate the accuracy and scalability
              of our method on a range of models for both simulated and real
              data.},
  urldate  = {2015-09-22},
  journal  = {arXiv:1506.04088 [stat]},
  author   = {Giordano, Ryan and Broderick, Tamara and Jordan, Michael},
  month    = jun,
  year     = {2015},
  note     = {arXiv: 1506.04088},
  keywords = {Statistics - Machine Learning},
  file     = {arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/F7V877ID/1506.html:text/html;Giordano
              et al. - 2015 - Linear Response Methods for Accurate Covariance
              Es.pdf:/Users/apodusenko/Zotero/storage/76R6NIZ6/Giordano et al. - 2015
              - Linear Response Methods for Accurate Covariance
              Es.pdf:application/pdf}
}

@article{rabiner_tutorial_1989,
  title   = {A tutorial on hidden {Markov} models and selected applications in
             speech recognition},
  volume  = {77},
  number  = {2},
  journal = {Proceedings of the IEEE},
  author  = {Rabiner, Lawrence R.},
  year    = {1989},
  pages   = {257--286}
}

@inproceedings{loeliger_localizing_2009,
  title     = {Localizing, forgetting, and likelihood filtering in state-space
               models},
  doi       = {10.1109/ITA.2009.5044943},
  abstract  = {The context of this paper are cycle-free factor graphs such as
               hidden Markov models or linear state space models. The paper offers
               some observations and suggestions on ldquolocalizatingrdquo such
               models and their likelihoods. First, it is suggested that a
               localized version of the model likelihood, which is easily computed
               by forward sum-product message passing, may be useful for feature
               extraction and detection. Second, the notion of a ldquolocalrdquo
               model (local factor graph) is introduced. A first class of local
               models arises from exponential message damping and scale factors as
               in recursive least squares. A second class of local models arises
               from the problem of estimating the moment of a model switch from
               some known model A to some known model B. This problem can be
               solved by forward sum-product message passing in model A and
               backward sum-product message passing in model B. It is pointed out
               that this method is applicable to pulse position estimation for any
               pulse with a (deterministic or stochastic) state space model.},
  booktitle = {Information {Theory} and {Applications} {Workshop}, 2009},
  author    = {Loeliger, H.-A. and Bolliger, L. and Reller, Christoph and Korl, S.},
  month     = feb,
  year      = {2009},
  keywords  = {Message passing, graph theory, filtering theory, feature
               extraction, hidden Markov models, Switches, State estimation,
               backward sum-product message passing, Context modeling, cycle-free
               factor graphs, Damping, exponential message damping, filtering,
               forward sum-product message passing, Least squares methods,
               likelihood filtering, linear state space models, local factor graph
               , local model, model likelihood, model switch, moment estimation,
               Recursive least squares},
  pages     = {184--186},
  file      = {IEEE Xplore Abstract
               Record:/Users/apodusenko/Zotero/storage/JCFK398G/abs_all.html:text/html;Loeliger
               et al. - 2009 - Localizing, forgetting, and likelihood filtering
               i.pdf:/Users/apodusenko/Zotero/storage/M7CHWJJ6/Loeliger et al. - 2009
               - Localizing, forgetting, and likelihood filtering
               i.pdf:application/pdf}
}

@article{rombouts_how_2015,
  title    = {How attention can create synaptic tags for the learning of working
              memories in sequential tasks},
  volume   = {11},
  issn     = {1553-7358},
  doi      = {10.1371/journal.pcbi.1004060},
  abstract = {Intelligence is our ability to learn appropriate responses to new
              stimuli and situations. Neurons in association cortex are thought
              to be essential for this ability. During learning these neurons
              become tuned to relevant features and start to represent them with
              persistent activity during memory delays. This learning process is
              not well understood. Here we develop a biologically plausible
              learning scheme that explains how trial-and-error learning induces
              neuronal selectivity and working memory representations for
              task-relevant information. We propose that the response selection
              stage sends attentional feedback signals to earlier processing
              levels, forming synaptic tags at those connections responsible for
              the stimulus-response mapping. Globally released neuromodulators
              then interact with tagged synapses to determine their plasticity.
              The resulting learning rule endows neural networks with the
              capacity to create new working memory representations of task
              relevant information as persistent activity. It is remarkably
              generic: it explains how association neurons learn to store
              task-relevant information for linear as well as non-linear
              stimulus-response mappings, how they become tuned to category
              boundaries or analog variables, depending on the task demands, and
              how they learn to integrate probabilistic evidence for perceptual
              decisions.},
  language = {eng},
  number   = {3},
  journal  = {PLoS computational biology},
  author   = {Rombouts, Jaldert O. and Bohte, Sander M. and Roelfsema, Pieter R.},
  month    = mar,
  year     = {2015},
  pmid     = {25742003},
  pmcid    = {PMC4351255},
  pages    = {e1004060},
  file     = {Rombouts et al. - 2015 - How attention can create synaptic tags for
              the lea.pdf:/Users/apodusenko/Zotero/storage/7D9ES96G/Rombouts et al. -
              2015 - How attention can create synaptic tags for the
              lea.pdf:application/pdf}
}

@article{botvinick_model-based_2014,
  title     = {Model-based hierarchical reinforcement learning and human action
               control},
  volume    = {369},
  copyright = {. © 2014 The Authors. Published by the Royal Society under the
               terms of the Creative Commons Attribution License
               http://creativecommons.org/licenses/by/4.0/, which permits
               unrestricted use, provided the original author and source are
               credited.},
  issn      = {0962-8436, 1471-2970},
  url       = {http://rstb.royalsocietypublishing.org/content/369/1655/20130480},
  doi       = {10.1098/rstb.2013.0480},
  abstract  = {Recent work has reawakened interest in goal-directed or
               ‘model-based’ choice, where decisions are based on prospective
               evaluation of potential action outcomes. Concurrently, there has
               been growing attention to the role of hierarchy in decision-making
               and action control. We focus here on the intersection between these
               two areas of interest, considering the topic of hierarchical
               model-based control. To characterize this form of action control,
               we draw on the computational framework of hierarchical
               reinforcement learning, using this to interpret recent empirical
               findings. The resulting picture reveals how hierarchical
               model-based mechanisms might play a special and pivotal role in
               human decision-making, dramatically extending the scope and
               complexity of human behaviour.},
  language  = {en},
  number    = {1655},
  urldate   = {2015-09-17},
  journal   = {Philosophical Transactions of the Royal Society of London B:
               Biological Sciences},
  author    = {Botvinick, Matthew and Weinstein, Ari},
  month     = nov,
  year      = {2014},
  pmid      = {25267822},
  pages     = {20130480},
  file      = {Botvinick and Weinstein - 2014 - Model-based hierarchical
               reinforcement learning
               an.pdf:/Users/apodusenko/Zotero/storage/XP2QCGGC/Botvinick and
               Weinstein - 2014 - Model-based hierarchical reinforcement learning
               an.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/KFFSTCSM/20130480.html:text/html
               }
}

@article{gallager_low-density_1962,
  title   = {Low-density parity-check codes},
  volume  = {8},
  number  = {1},
  journal = {Information Theory, IRE Transactions on},
  author  = {Gallager, Robert G.},
  year    = {1962},
  pages   = {21--28}
}

@phdthesis{frigola-alcalde_bayesian_2015,
  address = {Cambridge, UK},
  title   = {Bayesian {Time} {Series} {Learning} with {Gaussian} {Processes}},
  school  = {Univercity of Cambridge},
  author  = {Frigola-Alcalde, Roger},
  year    = {2015},
  file    = {Frigola-Alcalde - 2015 - Bayesian Time Series Learning with Gaussian
             Proces.pdf:/Users/apodusenko/Zotero/storage/B3QGQGZE/Frigola-Alcalde -
             2015 - Bayesian Time Series Learning with Gaussian
             Proces.pdf:application/pdf}
}

@article{ortega_thermodynamics_2013,
  title     = {Thermodynamics as a theory of decision-making with
               information-processing costs},
  volume    = {469},
  copyright = {© 2013 The Author(s) Published by the Royal Society. All rights
               reserved.},
  issn      = {1364-5021, 1471-2946},
  url       = {http://rspa.royalsocietypublishing.org/content/469/2153/20120683},
  doi       = {10.1098/rspa.2012.0683},
  abstract  = {Perfectly rational decision-makers maximize expected utility, but
               crucially ignore the resource costs incurred when determining
               optimal actions. Here, we propose a thermodynamically inspired
               formalization of bounded rational decision-making where information
               processing is modelled as state changes in thermodynamic systems
               that can be quantified by differences in free energy. By optimizing
               a free energy, bounded rational decision-makers trade off expected
               utility gains and information-processing costs measured by the
               relative entropy. As a result, the bounded rational decision-making
               problem can be rephrased in terms of well-known variational
               principles from statistical physics. In the limit when
               computational costs are ignored, the maximum expected utility
               principle is recovered. We discuss links to existing
               decision-making frameworks and applications to human
               decision-making experiments that are at odds with expected utility
               theory. Since most of the mathematical machinery can be borrowed
               from statistical physics, the main contribution is to re-interpret
               the formalism of thermodynamic free-energy differences in terms of
               bounded rational decision-making and to discuss its relationship to
               human decision-making experiments.},
  language  = {en},
  number    = {2153},
  urldate   = {2015-03-06},
  journal   = {Proceedings of the Royal Society of London A: Mathematical,
               Physical and Engineering Sciences},
  author    = {Ortega, Pedro A. and Braun, Daniel A.},
  month     = may,
  year      = {2013},
  pages     = {20120683},
  file      = {Ortega and Braun - 2013 - Thermodynamics as a theory of
               decision-making
               with.pdf:/Users/apodusenko/Zotero/storage/HFQPNJJ7/Ortega and Braun -
               2013 - Thermodynamics as a theory of decision-making
               with.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/BUSA96SK/20120683.html:text/html
               }
}

@inproceedings{bratieres_scalable_2014,
  title     = {Scalable {Gaussian} {Process} {Structured} {Prediction} for {Grid} {
               Factor} {Graph} {Applications}},
  booktitle = {31st {International} {Conference} on {Machine} {Learning} ({ICML}
               -14)},
  author    = {Bratieres, Sebastien},
  year      = {2014},
  pages     = {334--342},
  file      = {Bratieres - 2014 - Scalable Gaussian Process Structured Prediction
               fo.pdf:/Users/apodusenko/Zotero/storage/6WIG5R9V/Bratieres - 2014 -
               Scalable Gaussian Process Structured Prediction fo.pdf:application/pdf}
}

@article{huemmer_nlms_2014,
  title    = {The {NLMS} algorithm with time-variant optimum stepsize derived from
              a {Bayesian} network perspective},
  url      = {http://arxiv.org/abs/1411.4834},
  abstract = {In this article, we derive a new stepsize adaptation for the
              normalized least mean square algorithm (NLMS) by describing the
              task of linear acoustic echo cancellation from a Bayesian network
              perspective. Similar to the well-known Kalman filter equations, we
              model the acoustic wave propagation from the loudspeaker to the
              microphone by a latent state vector and define a linear observation
              equation (to model the relation between the state vector and the
              observation) as well as a linear process equation (to model the
              temporal progress of the state vector). Based on additional
              assumptions on the statistics of the random variables in
              observation and process equation, we apply the
              expectation-maximization (EM) algorithm to derive an NLMS-like
              filter adaptation. By exploiting the conditional independence rules
              for Bayesian networks, we reveal that the resulting EM-NLMS
              algorithm has a stepsize update equivalent to the optimal-stepsize
              calculation proposed by Yamamoto and Kitayama in 1982, which has
              been adopted in many textbooks. As main difference, the
              instantaneous stepsize value is estimated in the M step of the EM
              algorithm (instead of being approximated by artificially extending
              the acoustic echo path). The EM-NLMS algorithm is experimentally
              verified for synthesized scenarios with both, white noise and male
              speech as input signal.},
  urldate  = {2015-09-08},
  journal  = {arXiv:1411.4834 [stat]},
  author   = {Huemmer, Christian and Maas, Roland and Kellermann, Walter},
  month    = nov,
  year     = {2014},
  note     = {arXiv: 1411.4834},
  keywords = {Statistics - Machine Learning},
  file     = {arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/2ZBMAEIS/1411.html:text/html;Huemmer
              et al. - 2014 - The NLMS algorithm with time-variant optimum
              steps.pdf:/Users/apodusenko/Zotero/storage/UUN5QNWQ/Huemmer et al. -
              2014 - The NLMS algorithm with time-variant optimum
              steps.pdf:application/pdf}
}

@article{hoffman_structured_2014,
  title    = {Structured {Stochastic} {Variational} {Inference}},
  url      = {http://arxiv.org/abs/1404.4114},
  abstract = {Stochastic variational inference makes it possible to approximate
              posterior distributions induced by large datasets quickly using
              stochastic optimization. The algorithm relies on the use of fully
              factorized variational distributions. However, this "mean-field"
              independence approximation limits the fidelity of the posterior
              approximation, and introduces local optima. We show how to relax
              the mean-field approximation to allow arbitrary dependencies
              between global parameters and local hidden variables, producing
              better parameter estimates by reducing bias, sensitivity to local
              optima, and sensitivity to hyperparameters.},
  urldate  = {2015-09-08},
  journal  = {arXiv:1404.4114 [cs]},
  author   = {Hoffman, Matthew D. and Blei, David M.},
  month    = apr,
  year     = {2014},
  note     = {arXiv: 1404.4114},
  keywords = {Computer Science - Learning},
  file     = {arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/DUDX533R/1404.html:text/html;Hoffman
              and Blei - 2014 - Structured Stochastic Variational
              Inference.pdf:/Users/apodusenko/Zotero/storage/XASKABQC/Hoffman and
              Blei - 2014 - Structured Stochastic Variational
              Inference.pdf:application/pdf}
}

@article{rohde_semiparametric_2015,
  title      = {Semiparametric {Mean} {Field} {Variational} {Bayes}: {General} {
                Principles} and {Numerical} {Issues}},
  shorttitle = {Semiparametric {Mean} {Field} {Variational} {Bayes}},
  url        = {http://works.bepress.com/matt_wand/15},
  urldate    = {2015-09-08},
  author     = {Rohde, David and Wand, Matt},
  year       = {2015},
  file       = {Rohde and Wand - 2015 - Semiparametric Mean Field Variational Bayes
                Gener.pdf:/Users/apodusenko/Zotero/storage/2RHGE4UU/Rohde and Wand -
                2015 - Semiparametric Mean Field Variational Bayes
                Gener.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/45B6PSEJ/15.html:text/html
                }
}

@techreport{de_vries_notes_2015,
  title  = {Notes on {Friston}'s {Anatomy} of {Choice}},
  author = {de Vries, Bert},
  year   = {2015},
  file   = {de Vries - 2015 - Notes on Friston's Anatomy of
            Choice.pdf:/Users/apodusenko/Zotero/storage/9TR4F3Z8/de Vries - 2015 -
            Notes on Friston's Anatomy of Choice.pdf:application/pdf}
}

@article{iso_226:_2003,
  title   = {226: 2003: {Acoustics}–{Normal} equal-loudness-level contours},
  journal = {International Organization for Standardization},
  author  = {ISO, ISO},
  year    = {2003}
}

@article{george_towards_2009,
  title    = {Towards a {Mathematical} {Theory} of {Cortical} {Micro}-circuits},
  volume   = {5},
  url      = {http://dx.doi.org/10.1371/journal.pcbi.1000532},
  doi      = {10.1371/journal.pcbi.1000532},
  abstract = {Author Summary Understanding the computational and information
              processing roles of cortical circuitry is one of the outstanding
              problems in neuroscience. In this paper, we work from a theory of
              neocortex that models it as a spatio-temporal hierarchical system
              to derive a biological cortical circuit. This is achieved by
              combining the computational constraints provided by the inference
              equations for this spatio-temporal hierarchy with anatomical data.
              The result is a mathematically consistent biological circuit that
              can be mapped to the cortical laminae and matches many prominent
              features of the mammalian neocortex. The mathematical model can
              serve as a starting point for the construction of machines that
              work like the brain. The resultant biological circuit can be used
              for modeling physiological phenomena and for deriving testable
              predictions about the brain.},
  number   = {10},
  urldate  = {2015-09-03},
  journal  = {PLoS Comput Biol},
  author   = {George, Dileep and Hawkins, Jeff},
  month    = oct,
  year     = {2009},
  pages    = {e1000532},
  file     = {George and Hawkins - 2009 - Towards a Mathematical Theory of Cortical
              Micro-ci.pdf:/Users/apodusenko/Zotero/storage/FKI3JEP5/George and
              Hawkins - 2009 - Towards a Mathematical Theory of Cortical
              Micro-ci.pdf:application/pdf}
}

@inproceedings{malkomes_active_nodate,
  title  = {Active {Structure} {Discovery} for {Gaussian} {Processes}},
  author = {Malkomes, Gustavo},
  file   = {
            ASD.pdf:/Users/apodusenko/Zotero/storage/PM9MP479/ASD.pdf:application/pdf
            }
}

@phdthesis{duvenaud_automatic_nodate,
  title  = {Automatic {Model} {Construction} with {Gaussian} {Processes}},
  author = {Duvenaud, David},
  file   = {
            thesis.pdf:/Users/apodusenko/Zotero/storage/U4H4BXE4/thesis.pdf:application/pdf
            }
}

@article{song_fast_2015,
  title    = {Fast, {Continuous} {Audiogram} {Estimation} {Using} {Machine} {
              Learning}},
  issn     = {1538-4667},
  doi      = {10.1097/AUD.0000000000000186},
  abstract = {OBJECTIVES: Pure-tone audiometry has been a staple of hearing
              assessments for decades. Many different procedures have been
              proposed for measuring thresholds with pure tones by systematically
              manipulating intensity one frequency at a time until a discrete
              threshold function is determined. The authors have developed a
              novel nonparametric approach for estimating a continuous threshold
              audiogram using Bayesian estimation and machine learning
              classification. The objective of this study was to assess the
              accuracy and reliability of this new method relative to a commonly
              used threshold measurement technique. DESIGN: The authors performed
              air conduction pure-tone audiometry on 21 participants between the
              ages of 18 and 90 years with varying degrees of hearing ability.
              Two repetitions of automated machine learning audiogram estimation
              and one repetition of conventional modified Hughson-Westlake
              ascending-descending audiogram estimation were acquired by an
              audiologist. The estimated hearing thresholds of these two
              techniques were compared at standard audiogram frequencies (i.e.,
              0.25, 0.5, 1, 2, 4, 8 kHz). RESULTS: The two threshold estimate
              methods delivered very similar estimates at standard audiogram
              frequencies. Specifically, the mean absolute difference between
              estimates was 4.16 ± 3.76 dB HL. The mean absolute difference
              between repeated measurements of the new machine learning procedure
              was 4.51 ± 4.45 dB HL. These values compare favorably with those of
              other threshold audiogram estimation procedures. Furthermore, the
              machine learning method generated threshold estimates from
              significantly fewer samples than the modified Hughson-Westlake
              procedure while returning a continuous threshold estimate as a
              function of frequency. CONCLUSIONS: The new machine learning
              audiogram estimation technique produces continuous threshold
              audiogram estimates accurately, reliably, and efficiently, making
              it a strong candidate for widespread application in clinical and
              research audiometry.},
  language = {ENG},
  journal  = {Ear and Hearing},
  author   = {Song, Xinyu D. and Wallace, Brittany M. and Gardner, Jacob R. and
              Ledbetter, Noah M. and Weinberger, Kilian Q. and Barbour, Dennis L.},
  month    = jun,
  year     = {2015},
  pmid     = {26258575},
  file     = {Song et al. - 2015 - Fast, Continuous Audiogram Estimation Using
              Machin.pdf:/Users/apodusenko/Zotero/storage/NIQHJGTE/Song et al. - 2015
              - Fast, Continuous Audiogram Estimation Using
              Machin.pdf:application/pdf}
}

@inproceedings{gardner_psychophysical_2015,
  address = {Amsterdam},
  title   = {Psychophysical {Detection} {Testing} with {Bayesian} {Active} {
             Learning}},
  author  = {Gardner, Jacob and Song, Xinyu and Weinberger, KIllian Q. and
             Barbour, Dennis and Cunningham, John},
  year    = {2015},
  file    = {Gardner et al. - 2015 - Psychophysical Detection Testing with Bayesian
             Act.pdf:/Users/apodusenko/Zotero/storage/MRJBXMZS/Gardner et al. - 2015
             - Psychophysical Detection Testing with Bayesian
             Act.pdf:application/pdf}
}

@article{friston_free-energy_2010,
  title   = {The free-energy principle: a unified brain theory?},
  volume  = {11},
  number  = {2},
  journal = {Nature Reviews Neuroscience},
  author  = {Friston, Karl},
  year    = {2010},
  pages   = {127--138},
  file    = {Friston - 2010 - The free energy principle a unified brain
             theory.pdf:/Users/apodusenko/Zotero/storage/TN4CPSHK/Friston - 2010 -
             The free energy principle a unified brain
             theory.pdf:application/pdf;Friston - 2010 - Unified brain - Supplement
             1.pdf:/Users/apodusenko/Zotero/storage/TRIJQ3IV/Friston - 2010 -
             Unified brain - Supplement 1.pdf:application/pdf;Friston - 2010 -
             Unified brain - Supplement
             2.pdf:/Users/apodusenko/Zotero/storage/UI5A6ESR/Friston - 2010 -
             Unified brain - Supplement 2.pdf:application/pdf;Friston - 2010 -
             Unified brain - Supplement
             3.pdf:/Users/apodusenko/Zotero/storage/SHI43RVC/Friston - 2010 -
             Unified brain - Supplement 3.pdf:application/pdf;Friston - 2010 -
             Unified brain - Supplement
             4.pdf:/Users/apodusenko/Zotero/storage/AEZ9N9VS/Friston - 2010 -
             Unified brain - Supplement 4.pdf:application/pdf;Friston - 2010 -
             Unified brain - Supplement
             5.pdf:/Users/apodusenko/Zotero/storage/53QUF6DK/Friston - 2010 -
             Unified brain - Supplement 5.pdf:application/pdf}
}

@article{adel_learning_nodate,
  title   = {Learning the {Structure} of {Sum}-{Product} {Networks} via an {SVD}
             -based {Algorithm}},
  url     = {http://auai.org/uai2015/proceedings/papers/83.pdf},
  urldate = {2015-08-27},
  author  = {Adel, Tameem and Balduzzi, David and Ghodsi, Ali},
  file    = {Adel et al. - Learning the Structure of Sum-Product Networks
             via.pdf:/Users/apodusenko/Zotero/storage/3D8I9BNC/Adel et al. -
             Learning the Structure of Sum-Product Networks via.pdf:application/pdf}
}

@article{botvinick_planning_2012,
  title    = {Planning as inference},
  volume   = {16},
  issn     = {1364-6613},
  url      = {http://www.sciencedirect.com/science/article/pii/S1364661312001957},
  doi      = {10.1016/j.tics.2012.08.006},
  abstract = {Recent developments in decision-making research are bringing the
              topic of planning back to center stage in cognitive science. This
              renewed interest reopens an old, but still unanswered question: how
              exactly does planning happen? What are the underlying information
              processing operations and how are they implemented in the brain?
              Although a range of interesting possibilities exists, recent work
              has introduced a potentially transformative new idea, according to
              which planning is accomplished through probabilistic inference.},
  number   = {10},
  urldate  = {2015-03-29},
  journal  = {Trends in Cognitive Sciences},
  author   = {Botvinick, Matthew and Toussaint, Marc},
  month    = oct,
  year     = {2012},
  pages    = {485--488},
  file     = {Botvinick and Toussaint - 2012 - Planning as
              inference.pdf:/Users/apodusenko/Zotero/storage/TSXBRW57/Botvinick and
              Toussaint - 2012 - Planning as
              inference.pdf:application/pdf;ScienceDirect
              Snapshot:/Users/apodusenko/Zotero/storage/NE74F4FU/S1364661312001957.html:text/html
              }
}

@article{friston_anatomy_2013,
  title      = {The anatomy of choice: active inference and agency},
  volume     = {7},
  shorttitle = {The anatomy of choice},
  url        = {
                http://journal.frontiersin.org/article/10.3389/fnhum.2013.00598/abstract
                },
  doi        = {10.3389/fnhum.2013.00598},
  abstract   = {This paper considers agency in the setting of embodied or active
                inference. In brief, we associate a sense of agency with prior
                beliefs about action and ask what sorts of beliefs underlie optimal
                behavior. In particular, we consider prior beliefs that action
                minimizes the Kullback–Leibler (KL) divergence between desired
                states and attainable states in the future. This allows one to
                formulate bounded rationality as approximate Bayesian inference
                that optimizes a free energy bound on model evidence. We show that
                constructs like expected utility, exploration bonuses, softmax
                choice rules and optimism bias emerge as natural consequences of
                this formulation. Previous accounts of active inference have
                focused on predictive coding and Bayesian filtering schemes for
                minimizing free energy. Here, we consider variational Bayes as an
                alternative scheme that provides formal constraints on the
                computational anatomy of inference and action—constraints that are
                remarkably consistent with neuroanatomy. Furthermore, this scheme
                contextualizes optimal decision theory and economic (utilitarian)
                formulations as pure inference problems. For example, expected
                utility theory emerges as a special case of free energy
                minimization, where the sensitivity or inverse temperature (of
                softmax functions and quantal response equilibria) has a unique and
                Bayes-optimal solution—that minimizes free energy. This sensitivity
                corresponds to the precision of beliefs about behavior, such that
                attainable goals are afforded a higher precision or confidence. In
                turn, this means that optimal behavior entails a representation of
                confidence about outcomes that are under an agent's control.},
  urldate    = {2015-08-24},
  journal    = {Frontiers in Human Neuroscience},
  author     = {Friston, Karl and Schwartenbeck, Philipp and Fitzgerald, Thomas and
                Moutoussis, Michael and Behrens, Tim and Dolan, Raymond J.},
  year       = {2013},
  keywords   = {agency, Bayesian, bounded rationality, inference, Active Inference
                , embodied cognition, free energy, utility theory},
  pages      = {598},
  file       = {Friston et al. - 2013 - The anatomy of choice active inference and
                agency.pdf:/Users/apodusenko/Zotero/storage/7BV2SBAB/Friston et al. -
                2013 - The anatomy of choice active inference and
                agency.pdf:application/pdf}
}

@book{hohwy_neural_2014,
  title     = {The {Neural} {Organ} {Explains} the {Mind}},
  isbn      = {978-3-95857-001-6},
  url       = {
               http://open-mind.net/papers/the-neural-organ-explains-the-mind/getAbstract
               },
  abstract  = {The free energy principle says that organisms act to maintain
               themselves in their expected states and that they achieve this by
               minimizing their free energy. This corresponds to the brain’s job
               of minimizing prediction error, selective sampling of sensory data,
               optimizing expected precisions, and minimizing complexity of
               internal models. These in turn map on to perception, action,
               attention, and model selection, respectively. This means that the
               free energy principle is extremely ambitious: it aims to explain
               everything about the mind. The principle is bound to be
               controversial, and hostage to empirical fortune. It may also be
               thought preposterous: the theory may seem either too ambitious or
               too trivial to be taken seriously. This chapter introduces the
               ideas behind the free energy principle and then proceeds to discuss
               the charge of preposterousness from the perspective of philosophy
               of science. It is shown that whereas it is ambitious, controversial
               and needs further evidence in its favour, it is not preposterous.
               The argument proceeds by appeal to: (i) the notion of inference to
               the best explanation, (ii) a comparison with the theory of
               evolution, (iii) the notion of explaining-away, and (iv) a
               “biofunctionalist” account of Bayesian processing. The heuristic
               starting point is the simple idea that the brain is just one among
               our bodily organs, each of which has an overall function. The
               outcome is not just a defence of the free energy principle against
               various challenges but also a deeper anchoring of this theory in
               philosophy of science, yielding an appreciation of the kind of
               explanation of the mind it offers.},
  language  = {en},
  urldate   = {2015-08-24},
  publisher = {Open MIND. Frankfurt am Main: MIND Group},
  author    = {Hohwy, Jakob},
  month     = jan,
  year      = {2014},
  file      = {Hohwy - 2014 - The Neural Organ Explains the
               Mind.pdf:/Users/apodusenko/Zotero/storage/PD7N9H4W/Hohwy - 2014 - The
               Neural Organ Explains the
               Mind.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/WR9ZU59C/the-neural-organ-explains-the-mind.html:text/html
               }
}

@incollection{luttinen_fast_2013,
  title     = {Fast variational {Bayesian} linear state-space model},
  url       = {http://link.springer.com/chapter/10.1007/978-3-642-40988-2_20},
  urldate   = {2015-08-19},
  booktitle = {Machine {Learning} and {Knowledge} {Discovery} in {Databases}},
  publisher = {Springer},
  author    = {Luttinen, Jaakko},
  year      = {2013},
  pages     = {305--320},
  file      = {Luttinen - 2013 - Fast variational Bayesian linear state-space
               model.pdf:/Users/apodusenko/Zotero/storage/T648G597/Luttinen - 2013 -
               Fast variational Bayesian linear state-space
               model.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/8P38JRUC/978-3-642-40988-2_20.html:text/html
               }
}

@article{palmieri_learning_2013,
  title    = {Learning {Non}-{Linear} {Functions} {With} {Factor} {Graphs}},
  volume   = {61},
  issn     = {1053-587X},
  doi      = {10.1109/TSP.2013.2270463},
  abstract = {We show how to use a discrete-variable factor graph for learning
              non-linear continuous functions from examples. The paper proposes a
              scheme for embedding soft quantization in a probabilistic Bayesian
              graph. The quantized input variables are grouped into a compound
              variable that is mapped through a stochastic matrix into the
              discrete output distribution. Specific output values are then
              obtained through a process of de-quantization. The information flow
              carried by message propagation is bi-directional and an algorithm
              for learning the factor graph parameters is explicitly derived. The
              model, that can easily merge discrete and continuous variables, is
              demonstrated with examples and simulations.},
  number   = {17},
  journal  = {IEEE Transactions on Signal Processing},
  author   = {Palmieri, F.A.N.},
  month    = sep,
  year     = {2013},
  keywords = {machine learning, factor graphs, graph theory, Bayes methods,
              de-quantization, discrete-variable factor graph, factor graph
              parameters, message propagation, non linear function approximation,
              nonlinear continuous functions, nonlinear functions, probabilistic
              Bayesian graph, quantisation (signal), soft quantization,
              stochastic matrix},
  pages    = {4360--4371},
  file     = {IEEE Xplore Abstract
              Record:/Users/apodusenko/Zotero/storage/JTTSV8IA/abs_all.html:text/html;Palmieri
              - 2013 - Learning Non-Linear Functions With Factor
              Graphs.pdf:/Users/apodusenko/Zotero/storage/MCDJSBGA/Palmieri - 2013 -
              Learning Non-Linear Functions With Factor Graphs.pdf:application/pdf}
}

@article{ortega_information_2011,
  title    = {Information, {Utility} \& {Bounded} {Rationality}},
  url      = {http://arxiv.org/abs/1107.5766},
  abstract = {Perfectly rational decision-makers maximize expected utility, but
              crucially ignore the resource costs incurred when determining
              optimal actions. Here we employ an axiomatic framework for bounded
              rational decision-making based on a thermodynamic interpretation of
              resource costs as information costs. This leads to a variational "
              free utility" principle akin to thermodynamical free energy that
              trades off utility and information costs. We show that bounded
              optimal control solutions can be derived from this variational
              principle, which leads in general to stochastic policies.
              Furthermore, we show that risk-sensitive and robust (minimax)
              control schemes fall out naturally from this framework if the
              environment is considered as a bounded rational and perfectly
              rational opponent, respectively. When resource costs are ignored,
              the maximum expected utility principle is recovered.},
  urldate  = {2015-08-06},
  journal  = {arXiv:1107.5766 [cs]},
  author   = {Ortega, Pedro A. and Braun, Daniel A.},
  month    = jul,
  year     = {2011},
  note     = {arXiv: 1107.5766},
  keywords = {Computer Science - Artificial Intelligence},
  file     = {arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/B7NIBA2R/1107.html:text/html;Ortega
              and Braun - 2011 - Information, Utility & Bounded
              Rationality.pdf:/Users/apodusenko/Zotero/storage/APTITGD3/Ortega and
              Braun - 2011 - Information, Utility & Bounded
              Rationality.pdf:application/pdf}
}

@article{greenwood_critical_1961,
  title    = {Critical {Bandwidth} and the {Frequency} {Coordinates} of the {
              Basilar} {Membrane}},
  volume   = {33},
  issn     = {0001-4966},
  url      = {
              http://scitation.aip.org/content/asa/journal/jasa/33/10/10.1121/1.1908437
              },
  doi      = {10.1121/1.1908437},
  abstract = {Masked audiograms were used to measure critical bandwidth. On the
              assumption that critical bands represent equal distances on the
              basilar membrane and that critical bandwidth increases
              exponentially with distance from the helicotrema, functions were
              derived which (1) relate critical bandwidth to frequency and to
              position on the basilar membrane and (2) relate position of maximum
              amplitude to frequency. The functions are consistent with Békésy\&
              apos;s optical observations and Mayer\&apos;s psychophysical data.
              The frequency‐position function is f = A (10 ax − 1) . The
              coefficient a is numerically identical with the coefficient in the
              exponential function fitting Békésy\&apos;s elasticity data.
              Functions of this form fit data from seven other species and the
              values of the coefficient a seem related to their respective
              elasticity, functions The interpretation of critical bandwidth as
              the frequency interval over which the cochlea sums power is
              supported by data of Mayer, and the hypothesis that critical bands
              represent equal distances on the basilar membrane is strengthened,
              one critical band corresponding to one millimeter. Problems for
              cochlear theory are posed (1) by the apparent equivalence of
              critical bandwidth, the derivative of the frequency‐position
              function, and the frequency interval over which spatial integration
              takes place, and (2) by the proportionality of these three at a
              given point to the compliance at that point.},
  number   = {10},
  urldate  = {2015-08-06},
  journal  = {The Journal of the Acoustical Society of America},
  author   = {Greenwood, Donald D.},
  month    = oct,
  year     = {1961},
  keywords = {Psychophysics, Elasticity, Elasticity theory},
  pages    = {1344--1356},
  file     = {Greenwood - 1961 - Critical Bandwidth and the Frequency Coordinates
              o.pdf:/Users/apodusenko/Zotero/storage/AXXPM2K7/Greenwood - 1961 -
              Critical Bandwidth and the Frequency Coordinates
              o.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/NKTVRQJ2/1.html:text/html
              }
}

@book{sherrington_mastering_nodate,
  title     = {Mastering {Julia}},
  publisher = {Packt Publishing},
  author    = {Sherrington, Malcolm},
  file      = {Code for Mastering
               Julia.zip:/Users/apodusenko/Zotero/storage/4PU5HMWG/Code for Mastering
               Julia.zip:application/x-zip-compressed;Sherrington - Mastering
               Julia.pdf:/Users/apodusenko/Zotero/storage/3CATMN6S/Sherrington -
               Mastering Julia.pdf:application/pdf}
}

@article{kass_bayes_1995,
  title   = {Bayes factors},
  volume  = {90},
  number  = {430},
  journal = {Journal of the american statistical association},
  author  = {Kass, Robert E. and Raftery, Adrian E.},
  year    = {1995},
  pages   = {773--795}
}

@article{raftery_bayesian_1995,
  title   = {Bayesian model selection in social research},
  volume  = {25},
  journal = {Sociological methodology},
  author  = {Raftery, Adrian E.},
  year    = {1995},
  pages   = {111--164}
}

@book{venema_compression_1998,
  title     = {Compression for {Clinicians}},
  isbn      = {978-1-56593-403-0},
  url       = {https://books.google.nl/books?id=mLJsAAAAMAAJ},
  publisher = {Singular Publishing Group},
  author    = {Venema, T.},
  year      = {1998}
}

@book{johnson_univariate_2005,
  title     = {Univariate discrete distributions},
  volume    = {444},
  isbn      = {0-471-71580-8},
  publisher = {John Wiley \& Sons},
  author    = {Johnson, Norman L. and Kemp, Adrienne W. and Kotz, Samuel},
  year      = {2005}
}

@misc{roweis_matrix_1999,
  title  = {Matrix identities},
  author = {Roweis, Sam},
  year   = {1999},
  file   = {Roweis - 1999 - Matrix
            identities.pdf:/Users/apodusenko/Zotero/storage/FGDPBWNG/Roweis - 1999
            - Matrix identities.pdf:application/pdf}
}

@misc{roweis_gaussian_1999,
  title  = {Gaussian identities},
  author = {Roweis, Sam},
  year   = {1999},
  file   = {Roweis - 1999 - Gaussian
            identities.pdf:/Users/apodusenko/Zotero/storage/JMTAW7TF/Roweis - 1999
            - Gaussian identities.pdf:application/pdf}
}

@article{ghahramani_probabilistic_2015,
  title     = {Probabilistic machine learning and artificial intelligence},
  volume    = {521},
  copyright = {© 2015 Nature Publishing Group, a division of Macmillan
               Publishers Limited. All Rights Reserved.},
  issn      = {0028-0836},
  url       = {
               http://www.nature.com/nature/journal/v521/n7553/abs/nature14541.html?utm_content=buffer579e7&utm_medium=social&utm_source=twitter.com&utm_campaign=buffer
               },
  doi       = {10.1038/nature14541},
  abstract  = {How can a machine learn from experience? Probabilistic modelling
               provides a framework for understanding what learning is, and has
               therefore emerged as one of the principal theoretical and practical
               approaches for designing machines that learn from data acquired
               through experience. The probabilistic framework, which describes
               how to represent and manipulate uncertainty about models and
               predictions, has a central role in scientific data analysis,
               machine learning, robotics, cognitive science and artificial
               intelligence. This Review provides an introduction to this
               framework, and discusses some of the state-of-the-art advances in
               the field, namely, probabilistic programming, Bayesian optimization
               , data compression and automatic model discovery. View full text},
  language  = {en},
  number    = {7553},
  urldate   = {2015-07-15},
  journal   = {Nature},
  author    = {Ghahramani, Zoubin},
  month     = may,
  year      = {2015},
  keywords  = {Computer Science, Mathematics and computing, Neuroscience},
  pages     = {452--459},
  file      = {Ghahramani - 2015 - Probabilistic machine learning and artificial
               inte.pdf:/Users/apodusenko/Zotero/storage/WWHUKFEU/Ghahramani - 2015 -
               Probabilistic machine learning and artificial
               inte.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/4AZ9JIBV/nature14541.html:text/html
               }
}

@article{kates_hearing-aid_2010,
  title   = {The hearing-aid speech quality index ({HASQI})},
  volume  = {58},
  number  = {5},
  journal = {Journal of the Audio Engineering Society},
  author  = {Kates, James M. and Arehart, Kathryn H.},
  year    = {2010},
  pages   = {363--381}
}

@article{ching_maximizing_2001,
  title   = {Maximizing effective audibility in hearing aid fitting},
  volume  = {22},
  number  = {3},
  journal = {Ear and Hearing},
  author  = {Ching, Teresa YC and Dillon, Harvey and Katsch, Richard and Byrne,
             Denis},
  year    = {2001},
  pages   = {212--224}
}

@article{pavlovic_articulation_1986,
  title   = {An articulation index based procedure for predicting the speech
             recognition performance of hearing‐impaired individuals},
  volume  = {80},
  number  = {1},
  journal = {The Journal of the Acoustical Society of America},
  author  = {Pavlovic, Chaslav V. and Studebaker, Gerald A. and Sherbecoe, Robert
             L.},
  year    = {1986},
  pages   = {50--57}
}

@incollection{aitzaz_factor_2012,
  title  = {Factor {Graphs} and {Message} {Passing} {Algorithms}},
  author = {Aitzaz, Ahmad and Serpedin, Erchin and Qaraqe, Khalid A},
  year   = {2012},
  file   = {Aitzaz et al. - 2012 - Factor Graphs and Message Passing
            Algorithms.pdf:/Users/apodusenko/Zotero/storage/QKAQI6PD/Aitzaz et al.
            - 2012 - Factor Graphs and Message Passing
            Algorithms.pdf:application/pdf}
}

@article{gelman_prior_2006,
  title   = {Prior distributions for variance parameters in hierarchical models
             (comment on article by {Browne} and {Draper})},
  volume  = {1},
  number  = {3},
  journal = {Bayesian analysis},
  author  = {Gelman, Andrew},
  year    = {2006},
  pages   = {515--534}
}

@article{jampani_consensus_2014,
  title    = {Consensus {Message} {Passing} for {Layered} {Graphical} {Models}},
  url      = {http://arxiv.org/abs/1410.7452},
  abstract = {Generative models provide a powerful framework for probabilistic
              reasoning. However, in many domains their use has been hampered by
              the practical difficulties of inference. This is particularly the
              case in computer vision, where models of the imaging process tend
              to be large, loopy and layered. For this reason bottom-up
              conditional models have traditionally dominated in such domains. We
              find that widely-used, general-purpose message passing inference
              algorithms such as Expectation Propagation (EP) and Variational
              Message Passing (VMP) fail on the simplest of vision models. With
              these models in mind, we introduce a modification to message
              passing that learns to exploit their layered structure by passing
              'consensus' messages that guide inference towards good solutions.
              Experiments on a variety of problems show that the proposed
              technique leads to significantly more accurate inference results,
              not only when compared to standard EP and VMP, but also when
              compared to competitive bottom-up conditional models.},
  urldate  = {2015-03-04},
  journal  = {arXiv:1410.7452 [cs]},
  author   = {Jampani, Varun and Eslami, S. M. Ali and Tarlow, Daniel and Kohli,
              Pushmeet and Winn, John},
  month    = oct,
  year     = {2014},
  note     = {arXiv: 1410.7452},
  keywords = {Computer Science - Learning, Computer Science - Artificial
              Intelligence, Computer Science - Computer Vision and Pattern
              Recognition},
  file     = {arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/CHPEMK55/1410.html:text/html;Jampani
              et al. - 2014 - Consensus Message Passing for Layered Graphical
              Mo.pdf:/Users/apodusenko/Zotero/storage/8ICBP443/Jampani et al. - 2014
              - Consensus Message Passing for Layered Graphical
              Mo.pdf:application/pdf}
}

@article{abrol_deterministic_2014,
  title    = {Deterministic {Annealing} for {Stochastic} {Variational} {Inference}},
  url      = {http://arxiv.org/abs/1411.1810},
  abstract = {Stochastic variational inference (SVI) maps posterior inference in
              latent variable models to non-convex stochastic optimization. While
              they enable approximate posterior inference for many otherwise
              intractable models, variational inference methods suffer from local
              optima. We introduce deterministic annealing for SVI to overcome
              this issue. We introduce a temperature parameter that
              deterministically deforms the objective, and then reduce this
              parameter over the course of the optimization. Initially it
              encourages high entropy variational distributions, which we find
              eases convergence to better optima. We test our method with Latent
              Dirichlet Allocation on three large corpora. Compared to SVI, we
              show improved predictive likelihoods on held-out data.},
  urldate  = {2015-05-07},
  journal  = {arXiv:1411.1810 [cs, stat]},
  author   = {Abrol, Farhan and Mandt, Stephan and Ranganath, Rajesh and Blei,
              David},
  month    = nov,
  year     = {2014},
  note     = {arXiv: 1411.1810},
  keywords = {Computer Science - Learning, Statistics - Machine Learning},
  file     = {Abrol et al. - 2014 - Deterministic Annealing for Stochastic
              Variational.pdf:/Users/apodusenko/Zotero/storage/KE2BK2DE/Abrol et al.
              - 2014 - Deterministic Annealing for Stochastic
              Variational.pdf:application/pdf;arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/XS392PWB/1411.html:text/html}
}

@article{wetzels_encompassing_2010,
  title   = {An encompassing prior generalization of the {Savage}–{Dickey} density
             ratio},
  volume  = {54},
  number  = {9},
  journal = {Computational Statistics \& Data Analysis},
  author  = {Wetzels, Ruud and Grasman, Raoul PPP and Wagenmakers, Eric-Jan},
  year    = {2010},
  pages   = {2094--2102}
}

@article{klugkist_bayes_2007,
  title   = {The {Bayes} factor for inequality and about equality constrained
             models},
  volume  = {51},
  number  = {12},
  journal = {Computational Statistics \& Data Analysis},
  author  = {Klugkist, Irene and Hoijtink, Herbert},
  year    = {2007},
  pages   = {6367--6379}
}

@article{kricos_influence_2000,
  title   = {The influence of nonaudiological variables on audiological
             rehabilitation outcomes},
  volume  = {21},
  number  = {4},
  journal = {Ear and Hearing},
  author  = {Kricos, Patricia B.},
  year    = {2000},
  pages   = {7S--14S}
}

@article{agrawal_prevalence_2008,
  title   = {Prevalence of hearing loss and differences by demographic
             characteristics among {US} adults: data from the {National} {Health}
             and {Nutrition} {Examination} {Survey}, 1999-2004},
  volume  = {168},
  number  = {14},
  journal = {Archives of Internal Medicine},
  author  = {Agrawal, Yuri and Platz, Elizabeth A. and Niparko, John K.},
  year    = {2008},
  pages   = {1522--1530}
}

@article{palmieri_comparison_2013,
  title    = {A {Comparison} of {Algorithms} for {Learning} {Hidden} {Variables} in
              {Normal} {Graphs}},
  url      = {http://arxiv.org/abs/1308.5576},
  abstract = {A Bayesian factor graph reduced to normal form consists in the
              interconnection of diverter units (or equal constraint units) and
              Single-Input/Single-Output (SISO) blocks. In this framework
              localized adaptation rules are explicitly derived from a
              constrained maximum likelihood (ML) formulation and from a minimum
              KL-divergence criterion using KKT conditions. The learning
              algorithms are compared with two other updating equations based on
              a Viterbi-like and on a variational approximation respectively. The
              performance of the various algorithm is verified on synthetic data
              sets for various architectures. The objective of this paper is to
              provide the programmer with explicit algorithms for rapid
              deployment of Bayesian graphs in the applications.},
  urldate  = {2015-06-10},
  journal  = {arXiv:1308.5576 [cs, math, stat]},
  author   = {Palmieri, Francesco A. N.},
  month    = aug,
  year     = {2013},
  note     = {arXiv: 1308.5576},
  keywords = {Statistics - Machine Learning, Computer Science - Information
              Theory, Computer Science - Systems and Control},
  file     = {arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/XFRQ36H7/1308.html:text/html;Palmieri
              - 2013 - A Comparison of Algorithms for Learning Hidden
              Var.pdf:/Users/apodusenko/Zotero/storage/NRI4WKTS/Palmieri - 2013 - A
              Comparison of Algorithms for Learning Hidden Var.pdf:application/pdf}
}

@book{balbaert_getting_2015,
  title     = {Getting started with {Julia} {Programming} {Language}},
  isbn      = {978-1-78328-479-5},
  abstract  = {Enter the exciting world of Julia, a high-performance language for
               technical computing About This BookWork with Julia in a multi-core,
               distributed, and networked environmentLearn the techniques to
               create blazingly fast programs with JuliaThe book walks you through
               various practical examples to get to grips with JuliaWho This Book
               Is ForThis book is for you if you are a data scientist or working
               on any technical or scientific computation projects. The book
               assumes you have a basic working knowledge of high-level dynamic
               languages such as MATLAB, R, Python, or Ruby. In Detail Julia is a
               new open source programming language that is used in the field of
               data science computing. It was created to solve the dilemma between
               high-level slow code and fast but low-level code, and the necessity
               to use both to achieve high performance. This book will give you a
               head start to tackle your numerical and data problems with Julia.
               Your journey will begin by learning how to set up a running Julia
               platform before exploring its various built-in types. You will then
               move on to cover the different functions and constructs in Julia.
               The book will then walk you through the two important collection
               types―arrays and matrices. Over the course of the book, you will
               also be introduced to homoiconicity, the meta-programming concept
               in Julia.Towards the concluding part of the book, you will also
               learn how to run external programs. This book will cover all you
               need to know about Julia to leverage its high speed and efficiency.
               },
  language  = {English},
  publisher = {Packt Publishing - ebooks Account},
  author    = {Balbaert, Ivo},
  month     = feb,
  year      = {2015},
  file      = {Balbaert - 2015 - Getting started with Julia Programming
               Language.epub:/Users/apodusenko/Zotero/storage/DTJBBSVU/Balbaert - 2015
               - Getting started with Julia Programming
               Language.epub:application/octet-stream;Balbaert - 2015 - Getting
               started with Julia Programming
               Language.pdf:/Users/apodusenko/Zotero/storage/7SFASNIH/Balbaert - 2015
               - Getting started with Julia Programming Language.pdf:application/pdf}
}

@article{cornelisse_input/output_1995,
  title      = {The input/output formula: a theoretical approach to the fitting of
                personal amplification devices},
  volume     = {97},
  issn       = {0001-4966},
  shorttitle = {The input/output formula},
  abstract   = {There is a growing trend for hearing aids to incorporate wide
                dynamic range compression. The input/output (I/O) hearing aid
                formula, presented in this report, is a general frequency-specific
                mathematical approach which describes the relationship between the
                input level of a signal delivered to a hearing aid and the output
                level produced by the hearing aid. The I/O formula relates basic
                psychoacoustic parameters, including hearing threshold level and
                uncomfortable listening level, to the electroacoustic
                characteristics of hearing aids. The main design goal of the I/O
                formula was to fit the acoustic region corresponding to the "
                extended" normal auditory dynamic range into the hearing-impaired
                individual's residual auditory dynamic range. The I/O approach can
                be used to fit hearing aids utilizing linear gain, linear
                compression or curvilinear compression to a hearing-impaired
                individual's residual auditory area.},
  language   = {eng},
  number     = {3},
  journal    = {The Journal of the Acoustical Society of America},
  author     = {Cornelisse, L. E. and Seewald, R. C. and Jamieson, D. G.},
  month      = mar,
  year       = {1995},
  pmid       = {7699167},
  keywords   = {Humans, Speech Perception, Auditory Threshold, Correction of
                Hearing Impairment, Models, Theoretical},
  pages      = {1854--1864}
}

@article{byrne_nal-nl1_2001,
  title      = {{NAL}-{NL1} procedure for fitting nonlinear hearing aids:
                characteristics and comparisons with other procedures},
  volume     = {12},
  issn       = {1050-0545},
  shorttitle = {{NAL}-{NL1} procedure for fitting nonlinear hearing aids},
  abstract   = {A new procedure for fitting nonlinear hearing aids (National
                Acoustic Laboratories' nonlinear fitting procedure, version 1
                [NAL-NL1]) is described. The rationale is to maximize speech
                intelligibility while constraining loudness to be normal or less.
                Speech intelligibility is predicted by the Speech Intelligibility
                Index (SII), which has been modified to account for the reduction
                in performance associated with increasing degrees of hearing loss,
                especially at high frequencies. Prescriptions are compared for the
                NAL-NL1, desired sensation level [input/output], FIG6, and a
                threshold version of the Independent Hearing Aid Fitting Forum
                procedures. For an average speech input level, the NAL-NL1
                prescriptions are very similar to those of the well-established
                NAL-Revised, Profound procedure. Compared with the other procedures
                , NAL-NL1 prescribes less low-frequency gain for flat and upward
                sloping audiograms. It prescribes less high-frequency gain for
                steeply sloping high-frequency hearing losses. NAL-NL1 tends to
                prescribe less compression than the other procedures. All
                procedures differ considerably from one another for some
                audiograms.},
  language   = {eng},
  number     = {1},
  journal    = {Journal of the American Academy of Audiology},
  author     = {Byrne, D. and Dillon, H. and Ching, T. and Katsch, R. and Keidser,
                G.},
  month      = jan,
  year       = {2001},
  pmid       = {11214977},
  keywords   = {Humans, Speech Perception, Acoustic Stimulation, Auditory
                Threshold, Equipment Design, Hearing Disorders, Prosthesis Fitting},
  pages      = {37--51}
}

@article{drugowitsch_variational_2013,
  title    = {Variational {Bayesian} inference for linear and logistic regression},
  url      = {http://arxiv.org/abs/1310.5438},
  abstract = {The article describe the model, derivation, and implementation of
              variational Bayesian inference for linear and logistic regression,
              both with and without automatic relevance determination. It has the
              dual function of acting as a tutorial for the derivation of
              variational Bayesian inference for simple models, as well as
              documenting, and providing brief examples for the MATLAB functions
              that implement this inference. These functions are freely available
              online.},
  urldate  = {2015-04-16},
  journal  = {arXiv:1310.5438 [stat]},
  author   = {Drugowitsch, Jan},
  month    = oct,
  year     = {2013},
  note     = {arXiv: 1310.5438},
  keywords = {Statistics - Machine Learning},
  file     = {arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/V48QC4Z3/1310.html:text/html;Drugowitsch
              - 2013 - Variational Bayesian inference for linear and
              logi.pdf:/Users/apodusenko/Zotero/storage/NGD9XGJA/Drugowitsch - 2013 -
              Variational Bayesian inference for linear and logi.pdf:application/pdf}
}

@inproceedings{barbour_optimizing_2015,
  title   = {Optimizing {Pure}-{Tone} {Audiometry} {Using} {Gaussian} {Processes}},
  url     = {
             http://aro2015mwm.conferencespot.org/59239-aro-1.1915996/t-002-1.1927068/f-023-1.1927187/ps-171-1.1927203
             },
  urldate = {2015-04-15},
  author  = {Barbour, Dennis},
  month   = feb,
  year    = {2015},
  file    = {Barbour - 2015 - Optimizing Pure-Tone Audiometry Using Gaussian
             Pro.pdf:/Users/apodusenko/Zotero/storage/RZXRFH54/Barbour - 2015 -
             Optimizing Pure-Tone Audiometry Using Gaussian
             Pro.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/9X4NBWZX/ps-171-1.html:text/html
             }
}

@inproceedings{barbour_auditory_2015,
  title   = {Auditory {Training} as {Portable} {Games}},
  url     = {
             http://aro2015mwm.conferencespot.org/59239-aro-1.1915996/t-004-1.1922727/f-107-1.1922768/sym-97-1.1922778/sym-97-1.1922779
             },
  urldate = {2015-04-15},
  author  = {Barbour, Dennis},
  month   = feb,
  year    = {2015},
  file    = {Barbour - 2015 - Auditory Training as Portable
             Games.pdf:/Users/apodusenko/Zotero/storage/JB39WEMC/Barbour - 2015 -
             Auditory Training as Portable
             Games.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/I4XKVNMV/sym-97-1.html:text/html
             }
}

@article{ozdamar_classification_1990,
  title    = {Classification of audiograms by sequential testing using a dynamic {
              Bayesian} procedure},
  volume   = {88},
  issn     = {0001-4966},
  url      = {http://scitation.aip.org/content/asa/journal/jasa/88/5/10.1121/1.400114
              },
  doi      = {10.1121/1.400114},
  abstract = {A new method for estimating audiograms using behavioral responses
              is presented. The method is based upon a modification of the
              Bayesian probability formula in which an outcome is predicted from
              a static set of events. In the new method, classification of
              audiograms by sequential testing (CAST), the probabilities of
              occurrence of audiogram patterns are dynamically updated according
              to the outcome of each test trial. Computer simulation using an
              infant response model suggests that the procedure is efficient,
              sensitive, and specific.},
  number   = {5},
  urldate  = {2015-04-15},
  journal  = {The Journal of the Acoustical Society of America},
  author   = {Özdamar, Özcan and Eilers, Rebecca E. and Miskiel, Edward and Widen,
              Judith},
  month    = nov,
  year     = {1990},
  keywords = {Computer Simulation, Testing procedures, Acoustic pattern
              recognition, Computer modeling, Probability theory},
  pages    = {2171--2179},
  file     = {Özdamar et al. - 1990 - Classification of audiograms by sequential
              testing.pdf:/Users/apodusenko/Zotero/storage/7J7REMNG/Özdamar et al. -
              1990 - Classification of audiograms by sequential
              testing.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/AIRJ4KSW/1.html:text/html
              }
}

@article{stadler_bayesian_2010,
  title    = {Bayesian {Optimal} {Pure} {Tone} {Audiometry} with {Prior} {Knowledge
              }},
  url      = {
              http://www.diva-portal.org/smash/record.jsf?pid=diva2%3A359751&dswid=-2443
              },
  abstract = {Bayesian Optimal Pure Tone Audiometry with Prior Knowledge},
  language = {eng},
  urldate  = {2015-04-15},
  author   = {Stadler, Svante and Leijon, Arne and Dijkstra, T. and de Vries, B.},
  year     = {2010},
  file     = {
              Snapshot:/Users/apodusenko/Zotero/storage/RDVAN6F2/record.html:text/html;Stadler
              et al. - 2010 - Bayesian Optimal Pure Tone Audiometry with Prior
              K.pdf:/Users/apodusenko/Zotero/storage/49K385X7/Stadler et al. - 2010 -
              Bayesian Optimal Pure Tone Audiometry with Prior K.pdf:application/pdf}
}

@article{watson_quest:_1983,
  title      = {Quest: {A} {Bayesian} adaptive psychometric method},
  volume     = {33},
  issn       = {0031-5117, 1532-5962},
  shorttitle = {Quest},
  url        = {http://link.springer.com/article/10.3758/BF03202828},
  doi        = {10.3758/BF03202828},
  abstract   = {An adaptive psychometric procedure that places each trial at the
                current most probable Bayesian estimate of threshold is described.
                The procedure takes advantage of the common finding that the human
                psychometric function is invariant in form when expressed as a
                function of log intensity. The procedure is simple, fast, and
                efficient, and may be easily implemented on any computer.},
  language   = {en},
  number     = {2},
  urldate    = {2015-04-15},
  journal    = {Perception \& Psychophysics},
  author     = {Watson, Andrew B. and Pelli, Denis G.},
  month      = mar,
  year       = {1983},
  keywords   = {Cognitive Psychology},
  pages      = {113--120},
  file       = {
                Snapshot:/Users/apodusenko/Zotero/storage/IRBTEKQ2/BF03202828.html:text/html;Watson
                and Pelli - 1983 - Quest A Bayesian adaptive psychometric
                method.pdf:/Users/apodusenko/Zotero/storage/CJ2ZP5BU/Watson and Pelli -
                1983 - Quest A Bayesian adaptive psychometric
                method.pdf:application/pdf}
}

@article{pio-lopez_active_2016,
  title      = {Active inference and robot control: a case study},
  volume     = {13},
  copyright  = {© 2016 The Authors.. Published by the Royal Society under the
                terms of the Creative Commons Attribution License
                http://creativecommons.org/licenses/by/4.0/, which permits
                unrestricted use, provided the original author and source are
                credited.},
  issn       = {1742-5689, 1742-5662},
  shorttitle = {Active inference and robot control},
  url        = {http://rsif.royalsocietypublishing.org/content/13/122/20160616},
  doi        = {10.1098/rsif.2016.0616},
  abstract   = {Active inference is a general framework for perception and action
                that is gaining prominence in computational and systems
                neuroscience but is less known outside these fields. Here, we
                discuss a proof-of-principle implementation of the active inference
                scheme for the control or the 7-DoF arm of a (simulated) PR2 robot.
                By manipulating visual and proprioceptive noise levels, we show
                under which conditions robot control under the active inference
                scheme is accurate. Besides accurate control, our analysis of the
                internal system dynamics (e.g. the dynamics of the hidden states
                that are inferred during the inference) sheds light on key aspects
                of the framework such as the quintessentially multimodal nature of
                control and the differential roles of proprioception and vision. In
                the discussion, we consider the potential importance of being able
                to implement active inference in robots. In particular, we briefly
                review the opportunities for modelling psychophysiological
                phenomena such as sensory attenuation and related failures of gain
                control, of the sort seen in Parkinson's disease. We also consider
                the fundamental difference between active inference and optimal
                control formulations, showing that in the former the heavy lifting
                shifts from solving a dynamical inverse problem to creating deep
                forward or generative models with dynamics, whose attracting sets
                prescribe desired behaviours.},
  language   = {en},
  number     = {122},
  urldate    = {2016-10-03},
  journal    = {Journal of The Royal Society Interface},
  author     = {Pio-Lopez, Léo and Nizard, Ange and Friston, Karl and Pezzulo,
                Giovanni},
  month      = sep,
  year       = {2016},
  pmid       = {27683002},
  pages      = {20160616},
  file       = {Pio-Lopez et al. - 2016 - Active inference and robot control a case
                study.pdf:/Users/apodusenko/Zotero/storage/24NPKC8U/Pio-Lopez et al. -
                2016 - Active inference and robot control a case
                study.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/7ED9INNE/20160616.html:text/html
                }
}

@inproceedings{honkela_-line_2003,
  title     = {On-line variational {Bayesian} learning},
  booktitle = {4th {International} {Symposium} on {Independent} {Component} {
               Analysis} and {Blind} {Signal} {Separation}},
  author    = {Honkela, Antti and Valpola, Harri},
  year      = {2003},
  pages     = {803--808}
}

@article{hu_bandit_2016,
  title    = {({Bandit}) {Convex} {Optimization} with {Biased} {Noisy} {Gradient} {
              Oracles}},
  url      = {http://arxiv.org/abs/1609.07087},
  abstract = {Algorithms for bandit convex optimization and online learning
              often rely on constructing noisy gradient estimates, which are then
              used in appropriately adjusted first-order algorithms, replacing
              actual gradients. Depending on the properties of the function to be
              optimized and the nature of "noise" in the bandit feedback, the
              bias and variance of gradient estimates exhibit various tradeoffs.
              In this paper we propose a novel framework that replaces the
              specific gradient estimation methods with an abstract oracle. With
              the help of the new framework we unify previous works, reproducing
              their results in a clean and concise fashion, while, perhaps more
              importantly, the framework also allows us to formally show that to
              achieve the optimal root-\$n\$ rate either the algorithms that use
              existing gradient estimators, or the proof techniques used to
              analyze them have to go beyond what exists today.},
  urldate  = {2016-09-23},
  journal  = {arXiv:1609.07087 [cs, stat]},
  author   = {Hu, Xiaowei and A., Prashanth L. and György, András and Szepesvári,
              Csaba},
  month    = sep,
  year     = {2016},
  note     = {arXiv: 1609.07087},
  keywords = {Computer Science - Learning, Statistics - Machine Learning},
  file     = {arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/PQ373JXH/1609.html:text/html;Hu
              et al. - 2016 - (Bandit) Convex Optimization with Biased Noisy
              Gra.pdf:/Users/apodusenko/Zotero/storage/ZDNMS5IV/Hu et al. - 2016 -
              (Bandit) Convex Optimization with Biased Noisy Gra.pdf:application/pdf}
}

@unpublished{marmin_efficient_2016,
  title    = {Efficient batch-sequential {Bayesian} optimization with moments of
              truncated {Gaussian} vectors},
  url      = {https://hal.archives-ouvertes.fr/hal-01361894},
  abstract = {We deal with the efficient parallelization of Bayesian global
              optimization algorithms, and more specifically of those based on
              the expected improvement criterion and its variants. A closed form
              formula relying on multivariate Gaussian cumulative distribution
              functions is established for a generalized version of the
              multipoint expected improvement criterion. In turn, the latter
              relies on intermediate results that could be of independent
              interest concerning moments of truncated Gaussian vectors. The
              obtained expansion of the criterion enables studying its
              differentiability with respect to point batches and calculating the
              corresponding gradient in closed form. Furthermore , we derive fast
              numerical approximations of this gradient and propose efficient
              batch optimization strategies. Numerical experiments illustrate
              that the proposed approaches enable computational savings of
              between one and two order of magnitudes, hence enabling
              derivative-based batch-sequential acquisition function maximization
              to become a practically implementable and efficient standard.},
  urldate  = {2016-09-22},
  author   = {Marmin, Sébastien and Chevalier, Clément and Ginsbourger, David},
  month    = sep,
  year     = {2016},
  note     = {working paper or preprint},
  keywords = {Expected Improvement, Kriging, Parallel Optimization},
  file     = {Marmin et al. - 2016 - Efficient batch-sequential Bayesian
              optimization w.pdf:/Users/apodusenko/Zotero/storage/DXZCVSCE/Marmin et
              al. - 2016 - Efficient batch-sequential Bayesian optimization
              w.pdf:application/pdf}
}

@inproceedings{kucukelbir_population_2015,
  title    = {Population {Empirical} {Bayes}},
  url      = {http://auai.org/uai2015/proceedings/papers/47.pdf},
  abstract = {Bayesian predictive inference analyzes a dataset to make
              predictions about new observations. When a model does not match the
              data, predictive accuracy suffers. We develop population empirical
              Bayes (pop-eb), a hierarchical framework that explicitly models the
              empirical population distribution as part of Bayesian analysis. We
              introduce a new concept, the latent dataset, as a hierarchical
              variable and set the empirical population as its prior. This leads
              to a new predictive density that mitigates model mismatch. We
              efficiently apply this method to complex models by proposing a
              stochastic variational inference algorithm, called bumping
              variational inference (bump-vi). We demonstrate improved predictive
              accuracy over classical Bayesian inference in three models: a
              linear regression model of health data, a Bayesian mixture model of
              natural images, and a latent Dirichlet allocation topic model of
              scientific documents.},
  author   = {Kucukelbir, A and Blei, D},
  year     = {2015},
  file     = {Kucukelbir and Blei - 2015 - Population Empirical
              Bayes.pdf:/Users/apodusenko/Zotero/storage/QM77GWXK/Kucukelbir and Blei
              - 2015 - Population Empirical Bayes.pdf:application/pdf}
}

@techreport{xenaki_spatial_2016,
  type        = {internal report},
  title       = {Spatial {Hearing} in {High} {Directivity} {Index} ({DI}) systems},
  institution = {GN ReSound},
  author      = {Xenaki, Angeliki},
  month       = jul,
  year        = {2016},
  file        = {Xenaki - 2016 - Spatial Hearing in High Directivity Index (DI)
                 sys.pdf:/Users/apodusenko/Zotero/storage/6WPP7VPP/Xenaki - 2016 -
                 Spatial Hearing in High Directivity Index (DI) sys.pdf:application/pdf}
}

@article{wipf_sparse_2004,
  title    = {Sparse {Bayesian} {Learning} for {Basis} {Selection}},
  volume   = {52},
  abstract = {Sparse Bayesian learning (SBL) and specifically relevance vector
              machines have received much attention in the machine learning
              literature as a means of achieving parsimonious representations in
              the context of regression and classification. The methodology
              relies on a parameterized prior that encourages models with few
              nonzero weights. In this paper, we adapt SBL to the signal
              processing problem of basis selection from overcomplete
              dictionaries, proving several results about the SBL cost function
              that elucidate its general behavior and provide solid theoretical
              justification for this application. Specifically, we have shown
              that SBL retains a desirable property of the ℓ0-norm diversity
              measure (i.e., the global minimum is achieved at the maximally
              sparse solution) while often possessing a more limited
              constellation of local minima. We have also demonstrated that the
              local minima that do exist are achieved at sparse solutions. Later,
              we provide a novel interpretation of SBL that gives us valuable
              insight into why it is successful in producing sparse
              representations. Finally, we include simulation studies comparing
              sparse Bayesian learning with basis pursuit and the more recent
              FOCal Underdetermined System Solver (FOCUSS) class of basis
              selection algorithms. These results indicate that our theoretical
              insights translate directly into improved performance.},
  number   = {8},
  journal  = {IEEE Transactions on Signal Processing},
  author   = {Wipf, David and Rao, Bhaskar},
  month    = aug,
  year     = {2004},
  pages    = {2153--2164},
  file     = {Wipf and Rao - 2004 - Sparse Bayesian Learning for Basis
              Selection.pdf:/Users/apodusenko/Zotero/storage/E3KFXRKT/Wipf and Rao -
              2004 - Sparse Bayesian Learning for Basis Selection.pdf:application/pdf
              }
}

@inproceedings{wipf_beamforming_2007,
  title     = {Beamforming using the relevance vector machine},
  abstract  = {Beamformers are spatial filters that pass source signals in
               particular focused locations while suppressing interference from
               elsewhere. The widely-used minimum variance adaptive beamformer
               (MVAB) creates such filters using a sample covariance estimate;
               however, the quality of this estimate deteriorates when the sources
               are correlated or the number of samples n is small. Herein, a
               modified beamformer is derived that replaces this problematic
               sample covariance with a robust maximum likelihood estimate
               obtained using the relevance vector machine (RVM), a Bayesian
               method for learning sparse models from possibly overcomplete
               feature sets. We prove that this substitution has the natural
               ability to remove the undesirable effects of correlations or
               limited data. When n becomes large and assuming uncorrelated
               sources, this method reduces to the exact MVAB. Simulations using
               direction-of-arrival data support these conclusions. Additionally,
               RVMs can potentially enhance a variety of traditional signal
               processing methods that rely on robust sample covariance estimates.
               },
  booktitle = {Proceedings of the 24th international conference on {Machine}
               learning},
  publisher = {ACM},
  author    = {Wipf, David and Nagarajan, Srikantan},
  year      = {2007},
  pages     = {1023--1030},
  file      = {Wipf and Nagarajan - 2007 - Beamforming using the relevance vector
               machine.pdf:/Users/apodusenko/Zotero/storage/DMWWTMHD/Wipf and
               Nagarajan - 2007 - Beamforming using the relevance vector
               machine.pdf:application/pdf}
}

@inproceedings{mladenov_lifted_2014,
  title     = {Lifted message passing as reparametrization of graphical models},
  url       = {http://www.auai.org/uai2014/proceedings/individuals/215.pdf},
  urldate   = {2016-09-14},
  booktitle = {Proc. of {UAI}},
  author    = {Mladenov, Martin and Globerson, Amir and Kersting, Kristian},
  year      = {2014},
  pages     = {603--612},
  file      = {Mladenov et al. - 2014 - Lifted message passing as reparametrization
               of gra.pdf:/Users/apodusenko/Zotero/storage/9T56XFJS/Mladenov et al. -
               2014 - Lifted message passing as reparametrization of
               gra.pdf:application/pdf}
}

@article{jaderberg_decoupled_2016,
  title    = {Decoupled {Neural} {Interfaces} using {Synthetic} {Gradients}},
  url      = {http://arxiv.org/abs/1608.05343},
  abstract = {Training directed neural networks typically requires
              forward-propagating data through a computation graph, followed by
              backpropagating error signal, to produce weight updates. All layers
              , or more generally, modules, of the network are therefore locked,
              in the sense that they must wait for the remainder of the network
              to execute forwards and propagate error backwards before they can
              be updated. In this work we break this constraint by decoupling
              modules by introducing a model of the future computation of the
              network graph. These models predict what the result of the modelled
              subgraph will produce using only local information. In particular
              we focus on modelling error gradients: by using the modelled
              synthetic gradient in place of true backpropagated error gradients
              we decouple subgraphs, and can update them independently and
              asynchronously i.e. we realise decoupled neural interfaces. We show
              results for feed-forward models, where every layer is trained
              asynchronously, recurrent neural networks (RNNs) where predicting
              one's future gradient extends the time over which the RNN can
              effectively model, and also a hierarchical RNN system with ticking
              at different timescales. Finally, we demonstrate that in addition
              to predicting gradients, the same framework can be used to predict
              inputs, resulting in models which are decoupled in both the forward
              and backwards pass -- amounting to independent networks which
              co-learn such that they can be composed into a single functioning
              corporation.},
  urldate  = {2016-09-12},
  journal  = {arXiv:1608.05343 [cs]},
  author   = {Jaderberg, Max and Czarnecki, Wojciech Marian and Osindero, Simon
              and Vinyals, Oriol and Graves, Alex and Kavukcuoglu, Koray},
  month    = aug,
  year     = {2016},
  note     = {arXiv: 1608.05343},
  keywords = {Computer Science - Learning},
  file     = {arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/9N535KHU/1608.html:text/html;Jaderberg
              et al. - 2016 - Decoupled Neural Interfaces using Synthetic
              Gradie.pdf:/Users/apodusenko/Zotero/storage/QKVS35BS/Jaderberg et al. -
              2016 - Decoupled Neural Interfaces using Synthetic
              Gradie.pdf:application/pdf}
}

@article{nasios_variational_2006,
  title   = {Variational learning for {Gaussian} mixture models},
  volume  = {36},
  number  = {4},
  journal = {IEEE Transactions on Systems, Man, and Cybernetics, Part B
             (Cybernetics)},
  author  = {Nasios, Nikolaos and Bors, Adrian G.},
  year    = {2006},
  pages   = {849--862}
}

@inproceedings{ghahramani_variational_1999,
  title     = {Variational {Inference} for {Bayesian} {Mixtures} of {Factor} {
               Analysers}.},
  volume    = {12},
  booktitle = {{NIPS}},
  author    = {Ghahramani, Zoubin and Beal, Matthew J.},
  year      = {1999},
  pages     = {449--455}
}

@article{ghahramani_variational_2000,
  title   = {Variational learning for switching state-space models},
  volume  = {12},
  number  = {4},
  journal = {Neural computation},
  author  = {Ghahramani, Zoubin and Hinton, Geoffrey E.},
  year    = {2000},
  pages   = {831--864}
}

@article{al-athari_estimation_2008,
  title   = {Estimation of the mean of truncated exponential distribution},
  volume  = {4},
  number  = {4},
  journal = {Journal of Mathematics and Statistics},
  author  = {Al-Athari, M. F. M.},
  year    = {2008},
  pages   = {284}
}

@article{henry_psychoacoustic_2000,
  title   = {Psychoacoustic measures of tinnitus},
  volume  = {11},
  number  = {3},
  journal = {JOURNAL-AMERICAN ACADEMY OF AUDIOLOGY},
  author  = {Henry, James A. and Meikle, Mary B.},
  year    = {2000},
  pages   = {138--155}
}

@inproceedings{toussaint_probabilistic_nodate,
  title     = {Probabilistic inference for solving discrete and continuous state {
               Markov} {Decision} {Processes}},
  url       = {
               http://delivery.acm.org/10.1145/1150000/1143963/p945-toussaint.pdf?ip=131.155.239.145&id=1143963&acc=ACTIVE%20SERVICE&key=0C390721DC3021FF%2EECCBF8AC29DF345E%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&CFID=823140770&CFTOKEN=88119656&__acm__=1470664356_dd187eaad542d4377bc6323d9640eb7c
               },
  abstract  = {Inference in Markov Decision Processes has recently received
               interest as a means to infer goals of an observed action, policy
               recognition, and also as a tool to compute policies. A particularly
               interesting aspect of the approach is that any existing inference
               technique in DBNs now becomes available for answering behavioral
               question--including those on continuous, factorial, or hierarchical
               state representations. Here we present an Expectation Maximization
               algorithm for computing optimal policies. Unlike previous
               approaches we can show that this actually optimizes the discounted
               expected future return for arbitrary reward functions and without
               assuming an ad hoc finite total time. The algorithm is generic in
               that any inference technique can be utilized in the E-step. We
               demonstrate this for exact inference on a discrete maze and
               Gaussian belief state propagation in continuous stochastic optimal
               control problems.},
  booktitle = {Proceedings of the 23rd international conference on {Machine}
               learning},
  author    = {Toussaint, Marc and Storkey, Amos},
  pages     = {945--952},
  file      = {
               p945-toussaint.pdf:/Users/apodusenko/Zotero/storage/EQ8RW9KJ/p945-toussaint.pdf:application/pdf
               }
}

@inproceedings{attias_planning_nodate,
  title    = {Planning by {Probabilistic} {Inference}},
  url      = {http://research.goldenmetallic.com/aistats03.pdf},
  abstract = {This paper presents and demonstrates a new approach to the problem
              of planning under uncertainty. Actions are treated as hidden
              variables, with their own prior distributions, in a probabilistic
              generative model involving actions and states. Planning is done by
              computing the posterior distribution over actions, conditioned on
              reaching the goal state within a specified number of steps. Under
              the new formulation, the toolbox of inference techniques be brought
              to bear on the planning problem. This paper focuses on problems
              with discrete actions and states, and discusses some extensions.},
  author   = {Attias, Hagai},
  file     = {
              aistats03.pdf:/Users/apodusenko/Zotero/storage/MZGP6AF6/aistats03.pdf:application/pdf
              }
}

@book{nielsen_systems_2015,
  title        = {Systems for {Personalization} of {Hearing} {Instruments}},
  abstract     = {Today, modern digital devices can be customized significantly to
                  the individual user by adjusting or optimizing multiple parameters
                  affecting the output of the devices. Such personal optimization of
                  devices is referred to as personalization. In the case of hearing
                  aids, personalization is not only a possibility offered to the user
                  , but a requirement that must be performed carefully and precisely
                  in order for the user to utilize the full potential of modern
                  multi-parameter hearing aids. Today though, personalization is
                  still based on a manual timeconsuming trial-and-error approach
                  performed by the user himself or, in case of hearing aids, by a
                  hearing-care professional based on typically ambiguous oral
                  feedback from the user. This often results in sub-optimal or even
                  inappropriate settings of multi-parameter devices. This
                  dissertation presents research on a machine-learning based
                  interactive personalization system to improve the personalization
                  of devices and, in particular, of hearing-aid devices. The proposed
                  personalization system iteratively learns a non-parametric
                  probabilistic model of a user’s assumed internal response function
                  over all possible settings of a multi-parameter device based
                  directly on sequential perceptual feedback from the user. A
                  sequential design based on active learning is used to obtain the
                  maximum of the user’s unknown internal response function in as few
                  iterations as possible. Experiments were conducted where the
                  proposed personalization system obtained a significantly preferred
                  setting for individual users within ten to twenty iterations in
                  scenarios with up to four parameters. Following a short
                  introduction that includes a summary of results and contributions,
                  the first main chapter focuses on the probabilistic modeling
                  framework in which a Gaussian process is used to model the user’s
                  unobserved internal response function. The first main challenge
                  addressed in this context is to account for inconsistent and thus
                  noisy user feedback. The second main challenge addressed is to
                  support feedback which closely reflects the user’s perception while
                  providing maximal information about it without imposing a high
                  cognitive load. In the second main chapter, active learning and
                  sequential design are discussed in relation to the challenge of
                  obtaining the setting that maximizes the user’s unobserved internal
                  response function in as few iterations as possible. For the
                  Gaussian process framework, an active learning criterion is
                  proposed specifically suitable for this type of optimization. The
                  final chapter contains an overall discussion and conclusion of the
                  present work and research based in part on the results from eight
                  scientific paper contributions contained in the appendices.},
  publisher    = {DTU Compute},
  author       = {Nielsen, Jens Brehm},
  collaborator = {Larsen, Jan and Nielsen, Jakob},
  year         = {2015},
  file         = {Nielsen - 2015 - Systems for Personalization of Hearing
                  Instruments.pdf:/Users/apodusenko/Zotero/storage/BMD2KVS7/Nielsen -
                  2015 - Systems for Personalization of Hearing
                  Instruments.pdf:application/pdf}
}

@mastersthesis{buechel_gesture_2015,
  title  = {Gesture {Recognition} using {Accelerometer} and {Gyroscope} {
            Measurements}},
  school = {ETH Zurich},
  author = {Buechel, Andreas},
  year   = {2015},
  file   = {Buechel - 2015 - Gesture Recognition using Accelerometer and
            Gyrosc.pdf:/Users/apodusenko/Zotero/storage/2SMQQQ5Q/Buechel - 2015 -
            Gesture Recognition using Accelerometer and Gyrosc.pdf:application/pdf}
}

@inproceedings{zalmai_gesture_2015,
  title     = {Gesture recognition from magnetic field measurements using a bank of
               linear state space models and local likelihood filtering},
  doi       = {10.1109/ICASSP.2015.7178435},
  abstract  = {Detecting and inferring the trajectory of a moving magnet from
               magnetic field measurements is a challenge due to a wide range of
               time scales and amplitudes of the recorded signals and limited
               computational power of devices embedding a magnetometer. In this
               paper, we model the magnetic field measurements using a bank of
               autonomous linear state space models and provide an efficient
               algorithm based on local likelihood filtering for reliably
               detecting and inferring the gesture causing the magnetic field
               variations.},
  booktitle = {2015 {IEEE} {International} {Conference} on {Acoustics}, {Speech}
               and {Signal} {Processing} ({ICASSP})},
  author    = {Zalmai, N. and Kaeslin, C. and Bruderer, L. and Neff, S. and
               Loeliger, H. A.},
  month     = apr,
  year      = {2015},
  keywords  = {Mathematical model, filtering theory, linear state space models,
               computational modeling, gesture recognition, local likelihood
               filtering, Magnetic devices, magnetic field measurement, magnetic
               field measurements, Magnetoacoustic effects, magnetometer,
               magnetometers, moving magnet, Real-time systems, Target tracking},
  pages     = {2569--2573},
  file      = {Zalmai et al. - 2015 - Gesture recognition from magnetic field
               measuremen.html:/Users/apodusenko/Zotero/storage/WA5I44NT/Zalmai et al.
               - 2015 - Gesture recognition from magnetic field
               measuremen.html:text/html;Zalmai et al. - 2015 - Gesture recognition
               from magnetic field
               measuremen.pdf:/Users/apodusenko/Zotero/storage/ATTS496P/Zalmai et al.
               - 2015 - Gesture recognition from magnetic field
               measuremen.pdf:application/pdf}
}

@inproceedings{penny_simultaneous_2014,
  address = {Copenhagen},
  title   = {Simultaneous {Localisation} and {Planning}},
  url     = {
             https://scholar.google.nl/scholar?cluster=10902420719852306613&hl=en&as_sdt=0
             ,5&sciodt=0,5},
  author  = {Penny, Will},
  year    = {2014},
  note    = {(Accessed on 07/20/2016)},
  file    = {Penny - 2014 - Simultaneous Localisation and
             Planning.pdf:/Users/apodusenko/Zotero/storage/DAHQFPE2/Penny - 2014 -
             Simultaneous Localisation and Planning.pdf:application/pdf}
}

@article{hu_latent_2015,
  title    = {Latent {Hierarchical} {Model} for {Activity} {Recognition}},
  url      = {http://arxiv.org/abs/1503.01820},
  abstract = {We present a novel hierarchical model for human activity
              recognition. In contrast to approaches that successively recognize
              actions and activities, our approach jointly models actions and
              activities in a unified framework, and their labels are
              simultaneously predicted. The model is embedded with a latent layer
              that is able to capture a richer class of contextual information in
              both state-state and observation-state pairs. Although loops are
              present in the model, the model has an overall linear-chain
              structure, where the exact inference is tractable. Therefore, the
              model is very efficient in both inference and learning. The
              parameters of the graphical model are learned with a Structured
              Support Vector Machine (Structured-SVM). A data-driven approach is
              used to initialize the latent variables; therefore, no manual
              labeling for the latent states is required. The experimental
              results from using two benchmark datasets show that our model
              outperforms the state-of-the-art approach, and our model is
              computationally more efficient.},
  urldate  = {2016-07-06},
  journal  = {arXiv:1503.01820 [cs]},
  author   = {Hu, Ninghang and Englebienne, Gwenn and Lou, Zhongyu and Kröse, Ben},
  month    = mar,
  year     = {2015},
  note     = {arXiv: 1503.01820},
  keywords = {Computer Science - Learning, Computer Science - Artificial
              Intelligence, Computer Science - Computer Vision and Pattern
              Recognition, Computer Science - Robotics},
  file     = {arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/AGJ8SXBZ/1503.html:text/html;Hu
              et al. - 2015 - Latent Hierarchical Model for Activity
              Recognition.pdf:/Users/apodusenko/Zotero/storage/ED6RB8VU/Hu et al. -
              2015 - Latent Hierarchical Model for Activity
              Recognition.pdf:application/pdf}
}

@article{motoi_bayesian_2011,
  title    = {Bayesian event detection for sport games with hidden {Markov} model},
  volume   = {15},
  issn     = {1433-7541, 1433-755X},
  url      = {http://link.springer.com/article/10.1007/s10044-011-0238-6},
  doi      = {10.1007/s10044-011-0238-6},
  abstract = {Event detection can be defined as the problem of detecting when a
              target event has occurred, from a given data sequence. Such an
              event detection problem can be found in many fields in science and
              engineering, such as signal processing, pattern recognition, and
              image processing. In recent years, many data sequences used in
              these fields, especially in video data analysis, tend to be high
              dimensional. In this paper, we propose a novel event detection
              method for high-dimensional data sequences in soccer video
              analysis. The proposed method assumes a Bayesian hidden Markov
              model with hyperparameter learning in addition to the parameter
              leaning. This is in an attempt to reduce undesired influences from
              ineffective components within the high-dimensional data.
              Implemention is performed by Markov Chain Monte Carlo. The proposed
              method was tested against an event detection problem with sequences
              of 40-dimensional feature values extracted from real professional
              soccer games. The algorithm appears functional.},
  language = {en},
  number   = {1},
  urldate  = {2016-06-30},
  journal  = {Pattern Analysis and Applications},
  author   = {Motoi, Shigeru and Misu, Toshie and Nakada, Yohei and Yazaki,
              Tomohiro and Kobayashi, Go and Matsumoto, Takashi and Yagi, Nobuyuki},
  month    = sep,
  year     = {2011},
  pages    = {59--72},
  file     = {Motoi et al. - 2011 - Bayesian event detection for sport games with
              hidd.pdf:/Users/apodusenko/Zotero/storage/ZS8RBUG8/Motoi et al. - 2011
              - Bayesian event detection for sport games with
              hidd.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/JRSP8K82/s10044-011-0238-6.html:text/html
              }
}

@article{hofman_bayesian_2008,
  title    = {Bayesian {Approach} to {Network} {Modularity}},
  volume   = {100},
  issn     = {0031-9007},
  url      = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2724184/},
  doi      = {10.1103/PhysRevLett.100.258701},
  abstract = {We present an efficient, principled, and interpretable technique
              for inferring module assignments and for identifying the optimal
              number of modules in a given network. We show how several existing
              methods for finding modules can be described as variant, special,
              or limiting cases of our work, and how the method overcomes the
              resolution limit problem, accurately recovering the true number of
              modules. Our approach is based on Bayesian methods for model
              selection which have been used with success for almost a century,
              implemented using a variational technique developed only in the
              past decade. We apply the technique to synthetic and real networks
              and outline how the method naturally allows selection among
              competing models.},
  number   = {25},
  urldate  = {2016-06-29},
  journal  = {Physical review letters},
  author   = {Hofman, Jake M. and Wiggins, Chris H.},
  month    = jun,
  year     = {2008},
  pmid     = {18643711},
  pmcid    = {PMC2724184},
  pages    = {258701},
  file     = {Hofman and Wiggins - 2008 - Bayesian Approach to Network
              Modularity.pdf:/Users/apodusenko/Zotero/storage/W6QR5F48/Hofman and
              Wiggins - 2008 - Bayesian Approach to Network
              Modularity.pdf:application/pdf}
}

@article{kochkin_marketrak_2010,
  title   = {{MarkeTrak} {VIII}: {Customer} satisfaction with hearing aids is
             slowly increasing},
  volume  = {63},
  url     = {
             http://www.betterhearing.org/hearingpedia/marketrak-publications/marketrak-viii-customer-satisfaction-hearing-aids-slowly
             },
  number  = {1},
  urldate = {2016-06-24},
  journal = {The Hearing Journal},
  author  = {Kochkin, Sergey},
  year    = {2010},
  pages   = {11--19},
  file    = {MarkeTrak VIII\: Customer satisfaction with hearing aids is slowly
             increasing:/Users/apodusenko/Zotero/storage/CCSKV6VJ/marketrak-viii-customer-satisfaction-hearing-aids-slowly.html:text/html
             }
}

@article{ardeshiri_bayesian_2015,
  title    = {Bayesian {Inference} via {Approximation} of {Log}-likelihood for {
              Priors} in {Exponential} {Family}},
  url      = {http://arxiv.org/abs/1510.01225},
  abstract = {In this paper, a Bayesian inference technique based on Taylor
              series approximation of the logarithm of the likelihood function is
              presented. The proposed approximation is devised for the case,
              where the prior distribution belongs to the exponential family of
              distributions. The logarithm of the likelihood function is
              linearized with respect to the sufficient statistic of the prior
              distribution in exponential family such that the posterior obtains
              the same exponential family form as the prior. Similarities between
              the proposed method and the extended Kalman filter for nonlinear
              filtering are illustrated. Furthermore, an extended target
              measurement update for target models where the target extent is
              represented by a random matrix having an inverse Wishart
              distribution is derived. The approximate update covers the
              important case where the spread of measurement is due to the target
              extent as well as the measurement noise in the sensor.},
  urldate  = {2016-06-24},
  journal  = {arXiv:1510.01225 [cs, stat]},
  author   = {Ardeshiri, Tohid and Orguner, Umut and Gustafsson, Fredrik},
  month    = oct,
  year     = {2015},
  note     = {arXiv: 1510.01225},
  keywords = {Computer Science - Learning, Statistics - Machine Learning},
  file     = {Ardeshiri et al. - 2015 - Bayesian Inference via Approximation of
              Log-likeli.pdf:/Users/apodusenko/Zotero/storage/4VC9JWBS/Ardeshiri et
              al. - 2015 - Bayesian Inference via Approximation of
              Log-likeli.pdf:application/pdf;arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/GFC39TW5/1510.html:text/html}
}

@article{friston_functional_2016,
  title      = {The {Functional} {Anatomy} of {Time}: {What} and {When} in the {Brain
                }},
  issn       = {1364-6613},
  shorttitle = {The {Functional} {Anatomy} of {Time}},
  url        = {http://www.sciencedirect.com/science/article/pii/S1364661316300407},
  doi        = {10.1016/j.tics.2016.05.001},
  abstract   = {This Opinion article considers the implications for functional
                anatomy of how we represent temporal structure in our exchanges
                with the world. It offers a theoretical treatment that tries to
                make sense of the architectural principles seen in mammalian
                brains. Specifically, it considers a factorisation between
                representations of temporal succession and representations of
                content or, heuristically, a segregation into when and what. This
                segregation may explain the central role of the hippocampus in
                neuronal hierarchies while providing a tentative explanation for
                recent observations of how ordinal sequences are encoded. The
                implications for neuroanatomy and physiology may have something
                important to say about how self-organised cell assembly sequences
                enable the brain to exhibit purposeful behaviour that transcends
                the here and now.},
  urldate    = {2016-06-12},
  journal    = {Trends in Cognitive Sciences},
  author     = {Friston, Karl and Buzsáki, Gyorgy},
  year       = {2016},
  keywords   = {Bayesian, Inference, Hippocampus, ordinal, sequences,
                spatiotemporal},
  file       = {Friston and Buzsáki - 2016 - The Functional Anatomy of Time What and
                When in t.pdf:/Users/apodusenko/Zotero/storage/XT9BBHHK/Friston and
                Buzsáki - 2016 - The Functional Anatomy of Time What and When in
                t.pdf:application/pdf;ScienceDirect
                Snapshot:/Users/apodusenko/Zotero/storage/2JH2M2BQ/S1364661316300407.html:text/html
                }
}

@article{gelfand_bayesian_1992,
  title   = {Bayesian analysis of constrained parameter and truncated data
             problems using {Gibbs} sampling},
  volume  = {87},
  number  = {418},
  journal = {Journal of the American Statistical Association},
  author  = {Gelfand, Alan E. and Smith, Adrian FM and Lee, Tai-Ming},
  year    = {1992},
  pages   = {523--532}
}

@article{klugkist_bayesian_2005,
  title   = {Bayesian model selection using encompassing priors},
  volume  = {59},
  number  = {1},
  journal = {Statistica Neerlandica},
  author  = {Klugkist, Irene and Kato, Bernet and Hoijtink, Herbert},
  year    = {2005},
  pages   = {57--69}
}

@phdthesis{kristjansson_speech_2002,
  title      = {Speech recognition in adverse environments: a probabilistic approach},
  shorttitle = {Speech recognition in adverse environments},
  urldate    = {2016-04-04},
  school     = {University of Waterloo},
  author     = {Kristjansson, Trausti Thor},
  year       = {2002},
  file       = {Kristjansson - 2002 - Speech recognition in adverse environments a
                prob.pdf:/Users/apodusenko/Zotero/storage/HVIBDD2I/Kristjansson - 2002
                - Speech recognition in adverse environments a prob.pdf:application/pdf
                }
}

@article{buonanno_towards_2015,
  title    = {Towards {Building} {Deep} {Networks} with {Bayesian} {Factor} {Graphs
              }},
  abstract = {We propose a Multi-Layer Network based on the Bayesian framework
              of the Factor Graphs in Reduced Normal Form (FGrn) applied to a
              two-dimensional lattice. The Latent Variable Model (LVM) is the
              basic building block of a quadtree hierarchy built on top of a
              bottom layer of random variables that represent pixels of an image,
              a feature map, or more generally a collection of spatially
              distributed discrete variables. The multi-layer architecture
              implements a hierarchical data representation that, via belief
              propagation, can be used for learning and inference. Typical uses
              are pattern completion, correction and classification. The FGrn
              paradigm provides great flexibility and modularity and appears as a
              promising candidate for building deep networks: the system can be
              easily extended by introducing new and different (in cardinality
              and in type) variables. Prior knowledge, or supervised information,
              can be introduced at different scales. The FGrn paradigm provides a
              handy way for building all kinds of architectures by
              interconnecting only three types of units: Single Input Single
              Output (SISO) blocks, Sources and Replicators. The network is
              designed like a circuit diagram and the belief messages flow
              bidirectionally in the whole system. The learning algorithms
              operate only locally within each block. The framework is
              demonstrated in this paper in a three-layer structure applied to
              images extracted from a standard data set.},
  urldate  = {2015-03-04},
  journal  = {arXiv:1502.04492 [cs]},
  author   = {Buonanno, Amedeo and Palmieri, Francesco A. N.},
  month    = feb,
  year     = {2015},
  keywords = {Computer Science - Learning, Computer Science - Computer Vision
              and Pattern Recognition},
  file     = {arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/M98TCSHJ/1502.html:text/html;Buonanno
              and Palmieri - 2015 - Towards Building Deep Networks with Bayesian
              Facto.pdf:/Users/apodusenko/Zotero/storage/HF4SRUNZ/Buonanno and
              Palmieri - 2015 - Towards Building Deep Networks with Bayesian
              Facto.pdf:application/pdf}
}

@article{hu_evaluation_2008,
  title   = {Evaluation of {Objective} {Quality} {Measures} for {Speech} {
             Enhancement}},
  volume  = {16},
  number  = {1},
  urldate = {2016-03-03},
  journal = {IEEE Transactions on Audio, Speech, and Language Processing},
  author  = {Hu, Yi and Loizou, Philipos C.},
  year    = {2008},
  file    = {Hu and Loizou - Evaluation of Objective Quality Measures for
             Speec.pdf:/Users/apodusenko/Zotero/storage/5PKMNH78/Hu and Loizou -
             Evaluation of Objective Quality Measures for Speec.pdf:application/pdf}
}

@inproceedings{turner_modeling_2008,
  title     = {Modeling natural sounds with modulation cascade processes},
  publisher = {Advances in Neural Information Processing Systems (NIPS)},
  author    = {Turner, Richard and Sahani, Maneesh},
  year      = {2008},
  file      = {Turner and Sahani - 2008 - Modeling natural sounds with modulation
               cascade pr.pdf:/Users/apodusenko/Zotero/storage/QJ94F2BG/Turner and
               Sahani - 2008 - Modeling natural sounds with modulation cascade
               pr.pdf:application/pdf}
}

@inproceedings{roweis_factorial_2003,
  title     = {Factorial models and refiltering for speech separation and denoising},
  publisher = {Interspeech},
  author    = {Roweis, ST},
  year      = {2003},
  keywords  = {SE, bib-csl2007},
  file      = {Roweis - 2003 - Factorial models and refiltering for speech
               separa.pdf:/Users/apodusenko/Zotero/storage/RQK2ISVC/Roweis - 2003 -
               Factorial models and refiltering for speech separa.pdf:application/pdf}
}

@article{varga_assessment_1993,
  title      = {Assessment for automatic speech recognition: {II}. {NOISEX}-92: {A}
                database and an experiment to study the effect of additive noise on
                speech recognition systems},
  volume     = {12},
  shorttitle = {Assessment for automatic speech recognition},
  number     = {3},
  urldate    = {2016-05-02},
  journal    = {Speech communication},
  author     = {Varga, Andrew and Steeneken, Herman JM},
  year       = {1993},
  pages      = {247--251},
  file       = {
                Snapshot:/Users/apodusenko/Zotero/storage/QJDRPNU5/0167639393900953.html:text/html;Varga
                and Steeneken - 1993 - Assessment for automatic speech recognition II.
                N.pdf:/Users/apodusenko/Zotero/storage/MT75PT6P/Varga and Steeneken -
                1993 - Assessment for automatic speech recognition II.
                N.pdf:application/pdf}
}

@article{deng_enhancement_2004,
  title   = {Enhancement of log mel power spectra of speech using a
             phase-sensitive model of the acoustic environment and sequential
             estimation of the corrupting noise},
  volume  = {12},
  number  = {2},
  urldate = {2016-01-13},
  journal = {IEEE Transactions on Speech and Audio Processing},
  author  = {Deng, Li and Droppo, Jasha and Acero, Alex},
  year    = {2004},
  pages   = {133--143},
  file    = {Deng et al. - 2004 - Enhancement of log mel power spectra of speech
             usi.pdf:/Users/apodusenko/Zotero/storage/T7ZD83TB/Deng et al. - 2004 -
             Enhancement of log mel power spectra of speech
             usi.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/ITKT6VB3/abs_all.html:text/html
             }
}

@article{kochkin_comparison_2014,
  title    = {A {Comparison} of {Consumer} {Satisfaction}, {Subjective} {Benefit},
              and {Quality} of {Life} {Changes} {Associated} with {Traditional} and
              {Direct}-mail {Hearing} {Aid} {Use}},
  volume   = {21},
  abstract = {A survey involving customers of one of the largest and
              longest-established US direct-mail hearing aid companies reveals
              surprising data on benefit, satisfaction, and value.},
  number   = {1},
  urldate  = {2015-10-29},
  journal  = {Hearing Review},
  author   = {Kochkin, Sergey},
  year     = {2014},
  pages    = {16--26},
  file     = {
              Snapshot:/Users/apodusenko/Zotero/storage/D47B9C64/a-comparison-of-consumer-satisfaction-subjective-benefit-and-quality-of-life-changes-associated.html:text/html
              }
}

@inproceedings{rix_perceptual_2001,
  title     = {Perceptual evaluation of speech quality ({PESQ})-a new method for
               speech quality assessment of telephone networks and codecs},
  volume    = {2},
  urldate   = {2015-11-05},
  booktitle = {2001 {IEEE} {International} {Conference} on {Acoustics}, {Speech}
               , and {Signal} {Processing}, 2001. {Proceedings}.},
  publisher = {IEEE},
  author    = {Rix, Antony W. and Beerends, John G. and Hollier, Michael P. and
               Hekstra, Andries P.},
  year      = {2001},
  pages     = {749--752},
  file      = {Rix et al. - 2001 - Perceptual evaluation of speech quality (PESQ)-a
               n.pdf:/Users/apodusenko/Zotero/storage/RJFAV2AF/Rix et al. - 2001 -
               Perceptual evaluation of speech quality (PESQ)-a
               n.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/IF9Q7N3I/login.html:text/html
               }
}

@book{zwicker_psychoacoustics:_2013,
  title      = {Psychoacoustics: {Facts} and models},
  volume     = {22},
  shorttitle = {Psychoacoustics},
  urldate    = {2016-04-18},
  publisher  = {Springer Science \& Business Media},
  author     = {Zwicker, Eberhard and Fastl, Hugo},
  year       = {2013},
  file       = {
                Snapshot:/Users/apodusenko/Zotero/storage/KHCV9NMA/books.html:text/html
                }
}

@misc{noauthor_itu-t_nodate,
  title   = {{ITU}-{T} {Test} {Signals}},
  url     = {
             http://www.itu.int/net/itu-t/sigdb/genaudio/AudioForm-g.aspx?val=1000050
             },
  urldate = {2016-04-14},
  file    = {Audio
             Form:/Users/apodusenko/Zotero/storage/5DGKJFAH/AudioForm-g.html:text/html
             }
}

@misc{noauthor_noisex-92_nodate,
  title   = {{NOISEX}-92},
  url     = {http://www.speech.cs.cmu.edu/comp.speech/Section1/Data/noisex.html},
  urldate = {2016-04-14},
  file    = {NOISEX:/Users/apodusenko/Zotero/storage/XVKSGF9E/noisex.html:text/html
             }
}

@misc{noauthor_mozilla_nodate,
  title   = {Mozilla {Firefox} {Start} {Page}},
  url     = {about:home},
  urldate = {2016-04-12},
  file    = {Mozilla Firefox Start
             Page:/Users/apodusenko/Zotero/storage/3M2RFS9M/home.html:application/xhtml+xml
             }
}

@article{hotelling_analysis_1933,
  title   = {Analysis of a complex of statistical variables into principal
             components.},
  volume  = {24},
  number  = {6},
  journal = {Journal of educational psychology},
  author  = {Hotelling, Harold},
  year    = {1933},
  pages   = {417}
}

@article{bijl_gaussian_2016,
  title    = {Gaussian process optimization through sampling from the maximum
              distribution},
  url      = {http://arxiv.org/abs/1604.00169},
  abstract = {This paper first presents a novel algorithm approximating the
              distribution of the maximum (both its position and its value) of a
              Gaussian process. This algorithm uses particles in a similar way as
              Sequential Monte Carlo samplers. It is subsequently applied to the
              problem of Gaussian Process Optimization (GPO). The resulting GPO
              algorithm does not use an acquisition function, which makes it
              different from other GPO algorithms. Through various example
              problems, including a wind turbine load mitigation example, we find
              that the resulting algorithm on average outperforms existing GPO
              algorithms. In addition, because no acquisition function has to be
              optimized, the algorithm can easily and efficiently be applied to
              problems with high-dimensional input spaces.},
  urldate  = {2016-04-04},
  journal  = {arXiv:1604.00169 [cs, stat]},
  author   = {Bijl, Hildo and Schön, Thomas B. and van Wingerden, Jan-Willem and
              Verhaegen, Michel},
  month    = apr,
  year     = {2016},
  note     = {arXiv: 1604.00169},
  keywords = {Statistics - Machine Learning, Computer Science - Systems and
              Control},
  file     = {Bijl et al. - 2016 - Gaussian process optimization through sampling
              fro.pdf:/Users/apodusenko/Zotero/storage/AABQE3W9/Bijl et al. - 2016 -
              Gaussian process optimization through sampling fro.pdf:application/pdf}
}

@article{eckart_approximation_1936,
  title   = {The approximation of one matrix by another of lower rank},
  volume  = {1},
  number  = {3},
  journal = {Psychometrika},
  author  = {Eckart, Carl and Young, Gale},
  year    = {1936},
  pages   = {211--218}
}

@article{mnih_human-level_2015,
  title     = {Human-level control through deep reinforcement learning},
  volume    = {518},
  copyright = {© 2015 Nature Publishing Group, a division of Macmillan
               Publishers Limited. All Rights Reserved.},
  issn      = {0028-0836},
  url       = {http://www.nature.com/nature/journal/v518/n7540/full/nature14236.html},
  doi       = {10.1038/nature14236},
  abstract  = {The theory of reinforcement learning provides a normative account,
               deeply rooted in psychological and neuroscientific perspectives on
               animal behaviour, of how agents may optimize their control of an
               environment. To use reinforcement learning successfully in
               situations approaching real-world complexity, however, agents are
               confronted with a difficult task: they must derive efficient
               representations of the environment from high-dimensional sensory
               inputs, and use these to generalize past experience to new
               situations. Remarkably, humans and other animals seem to solve this
               problem through a harmonious combination of reinforcement learning
               and hierarchical sensory processing systems, the former evidenced
               by a wealth of neural data revealing notable parallels between the
               phasic signals emitted by dopaminergic neurons and temporal
               difference reinforcement learning algorithms. While reinforcement
               learning agents have achieved some successes in a variety of
               domains, their applicability has previously been limited to domains
               in which useful features can be handcrafted, or to domains with
               fully observed, low-dimensional state spaces. Here we use recent
               advances in training deep neural networks to develop a novel
               artificial agent, termed a deep Q-network, that can learn
               successful policies directly from high-dimensional sensory inputs
               using end-to-end reinforcement learning. We tested this agent on
               the challenging domain of classic Atari 2600 games. We demonstrate
               that the deep Q-network agent, receiving only the pixels and the
               game score as inputs, was able to surpass the performance of all
               previous algorithms and achieve a level comparable to that of a
               professional human games tester across a set of 49 games, using the
               same algorithm, network architecture and hyperparameters. This work
               bridges the divide between high-dimensional sensory inputs and
               actions, resulting in the first artificial agent that is capable of
               learning to excel at a diverse array of challenging tasks.},
  language  = {en},
  number    = {7540},
  urldate   = {2016-03-22},
  journal   = {Nature},
  author    = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu,
               Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex
               and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg
               and Petersen, Stig and Beattie, Charles and Sadik, Amir and
               Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and
               Wierstra, Daan and Legg, Shane and Hassabis, Demis},
  month     = feb,
  year      = {2015},
  keywords  = {Computer Science},
  pages     = {529--533},
  file      = {Mnih et al. - 2015 - Human-level control through deep reinforcement
               lea.pdf:/Users/apodusenko/Zotero/storage/ITR3ID2I/Mnih et al. - 2015 -
               Human-level control through deep reinforcement
               lea.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/7K5I7IN4/nature14236.html:text/html
               }
}

@inproceedings{droppo_noise_2004,
  title     = {Noise robust speech recognition with a switching linear dynamic model
               },
  volume    = {1},
  url       = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1326145},
  urldate   = {2016-03-16},
  booktitle = {Acoustics, {Speech}, and {Signal} {Processing}, 2004. {
               Proceedings}.({ICASSP}'04). {IEEE} {International} {Conference} on
               },
  publisher = {IEEE},
  author    = {Droppo, Jasha and Acero, Alex},
  year      = {2004},
  pages     = {I--953},
  file      = {Droppo and Acero - 2004 - Noise robust speech recognition with a
               switching l.pdf:/Users/apodusenko/Zotero/storage/8C4HVRSJ/Droppo and
               Acero - 2004 - Noise robust speech recognition with a switching
               l.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/WDTUKX9E/abs_all.html:text/html
               }
}

@article{teh_distributed_2015,
  title   = {Distributed {Bayesian} {Learning} with {Stochastic} {Natural}
             -gradient {Expectation} {Propagation} and the {Posterior} {Server}},
  url     = {http://arxiv.org/abs/1512.09327},
  urldate = {2016-03-01},
  journal = {arXiv preprint arXiv:1512.09327},
  author  = {Teh, Yee Whye and Hasenclever, Leonard and Lienart, Thibaut and
             Vollmer, Sebastian and Webb, Stefan and Lakshminarayanan, Balaji and
             Blundell, Charles},
  year    = {2015},
  file    = {Teh et al. - 2015 - Distributed Bayesian Learning with Stochastic
             Natu.pdf:/Users/apodusenko/Zotero/storage/3WZU6RWS/Teh et al. - 2015 -
             Distributed Bayesian Learning with Stochastic Natu.pdf:application/pdf}
}

@article{gelman_expectation_2014,
  title   = {Expectation propagation as a way of life},
  url     = {http://arxiv.org/abs/1412.4869},
  urldate = {2016-03-01},
  journal = {arXiv preprint arXiv:1412.4869},
  author  = {Gelman, Andrew and Vehtari, Aki and Jylänki, Pasi and Robert,
             Christian and Chopin, Nicolas and Cunningham, John P.},
  year    = {2014},
  file    = {Gelman et al. - 2014 - Expectation propagation as a way of
             life.pdf:/Users/apodusenko/Zotero/storage/ZJJ4FJPW/Gelman et al. - 2014
             - Expectation propagation as a way of life.pdf:application/pdf}
}

@article{raymond_expectation_2014,
  title   = {Expectation {Propagation}},
  url     = {http://arxiv.org/abs/1409.6179},
  urldate = {2016-03-01},
  journal = {arXiv preprint arXiv:1409.6179},
  author  = {Raymond, Jack and Manoel, Andre and Opper, Manfred},
  year    = {2014},
  file    = {Raymond et al. - 2014 - Expectation
             Propagation.pdf:/Users/apodusenko/Zotero/storage/FRGAZ9HT/Raymond et
             al. - 2014 - Expectation Propagation.pdf:application/pdf}
}

@article{archer_black_2015,
  title    = {Black box variational inference for state space models},
  url      = {http://arxiv.org/abs/1511.07367},
  abstract = {Latent variable time-series models are among the most heavily used
              tools from machine learning and applied statistics. These models
              have the advantage of learning latent structure both from noisy
              observations and from the temporal ordering in the data, where it
              is assumed that meaningful correlation structure exists across
              time. A few highly-structured models, such as the linear dynamical
              system with linear-Gaussian observations, have closed-form
              inference procedures (e.g. the Kalman Filter), but this case is an
              exception to the general rule that exact posterior inference in
              more complex generative models is intractable. Consequently, much
              work in time-series modeling focuses on approximate inference
              procedures for one particular class of models. Here, we extend
              recent developments in stochastic variational inference to develop
              a `black-box' approximate inference technique for latent variable
              models with latent dynamical structure. We propose a structured
              Gaussian variational approximate posterior that carries the same
              intuition as the standard Kalman filter-smoother but, importantly,
              permits us to use the same inference approach to approximate the
              posterior of much more general, nonlinear latent variable
              generative models. We show that our approach recovers accurate
              estimates in the case of basic models with closed-form posteriors,
              and more interestingly performs well in comparison to variational
              approaches that were designed in a bespoke fashion for specific
              non-conjugate models.},
  urldate  = {2016-03-01},
  journal  = {arXiv:1511.07367 [stat]},
  author   = {Archer, Evan and Park, Il Memming and Buesing, Lars and Cunningham,
              John and Paninski, Liam},
  month    = nov,
  year     = {2015},
  note     = {arXiv: 1511.07367},
  keywords = {Statistics - Machine Learning},
  file     = {Archer et al. - 2015 - Black box variational inference for state space
              mo.pdf:/Users/apodusenko/Zotero/storage/VZVS4KG8/Archer et al. - 2015 -
              Black box variational inference for state space
              mo.pdf:application/pdf;arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/QVUD43HK/1511.html:text/html}
}

@inproceedings{schmidt_minimizing_nodate,
  title    = {Minimizing {Finite} {Sums} with the {Stochastic} {Average} {Gradient}
              },
  url      = {http://arxiv.org/pdf/1309.2388v1.pdf},
  abstract = {We propose the stochastic average gradient (SAG) method for
              optimizing the sum of a finite number of smooth convex functions.
              Like stochastic gradient (SG) methods, the SAG method's iteration
              cost is independent of the number of terms in the sum. However, by
              incorporating a memory of previous gradient values the SAG method
              achieves a faster convergence rate than black-box SG methods. The
              convergence rate is improved from O(1/k{\textasciicircum}\{1/2\})
              to O(1/k) in general, and when the sum is strongly-convex the
              convergence rate is improved from the sub-linear O(1/k) to a linear
              convergence rate of the form O(p{\textasciicircum}k) for p {
              \textless} 1. Further, in many cases the convergence rate of the
              new method is also faster than black-box deterministic gradient
              methods, in terms of the number of gradient evaluations. Numerical
              experiments indicate that the new algorithm often dramatically
              outperforms existing SG and deterministic gradient methods, and
              that the performance may be further improved through the use of
              non-uniform sampling strategies.},
  author   = {Schmidt, M and Roux, N and Bach, F},
  file     = {Schmidt et al. - Minimizing Finite Sums with the Stochastic
              Average.pdf:/Users/apodusenko/Zotero/storage/6VBQI6ZJ/Schmidt et al. -
              Minimizing Finite Sums with the Stochastic
              Average.pdf:application/pdf;Schmidt et al. - Minimizing Finite Sums
              with the Stochastic
              Average.pdf:/Users/apodusenko/Zotero/storage/D59XDVPP/Schmidt et al. -
              Minimizing Finite Sums with the Stochastic Average.pdf:application/pdf}
}

@inproceedings{wang_bayesian_nodate,
  title    = {Bayesian {Optimization} in {High} {Dimensions} via {Random} {
              Embeddings}},
  url      = {http://www.cs.ubc.ca/~ziyuw/papers/rembo.pdf},
  abstract = {Bayesian optimization techniques have been successfully applied to
              robotics, planning, sensor placement, recommendation, advertising,
              intelligent user interfaces and automatic algorithm con-
              figuration. Despite these successes, the approach is restricted to
              problems of moderate dimension, and several workshops on Bayesian
              optimization have identified its scaling to high dimensions as one
              of the holy grails of the field. In this paper, we introduce a
              novel random embedding idea to attack this problem. The resulting
              Random EMbedding Bayesian Optimization (REMBO) algorithm is very
              simple and applies to domains with both categorical and continuous
              variables. The experiments demonstrate that REMBO can effectively
              solve high-dimensional problems, including automatic parameter
              configuration of a popular mixed integer linear programming solver.
              },
  author   = {Wang, Z and Zoghi, M and Hutter, F and Matheson, D and Freitas, N},
  file     = {
              long.pdf:/Users/apodusenko/Zotero/storage/SFB6Z5S7/1301.1942v2.pdf:application/pdf;short.pdf:/Users/apodusenko/Zotero/storage/J3BGDE8X/short.pdf:application/pdf
              }
}

@incollection{schwartz_natural_2001,
  title     = {Natural {Sound} {Statistics} and {Divisive} {Normalization} in the {
               Auditory} {System}},
  url       = {
               http://papers.nips.cc/paper/1860-natural-sound-statistics-and-divisive-normalization-in-the-auditory-system.pdf
               },
  urldate   = {2016-02-17},
  booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 13},
  publisher = {MIT Press},
  author    = {Schwartz, Odelia and Simoncelli, Eero P.},
  editor    = {Leen, T. K. and Dietterich, T. G. and Tresp, V.},
  year      = {2001},
  pages     = {166--172},
  file      = {NIPS
               Snapshort:/Users/apodusenko/Zotero/storage/8M84KSZC/1860-natural-sound-statistics-and-divisive-normalization-in-the-auditory-system.html:text/html;Schwartz
               and Simoncelli - 2001 - Natural Sound Statistics and Divisive
               Normalizatio.pdf:/Users/apodusenko/Zotero/storage/3UXB6JBW/Schwartz and
               Simoncelli - 2001 - Natural Sound Statistics and Divisive
               Normalizatio.pdf:application/pdf}
}

@book{johnson_matrix_1990,
  title     = {Matrix theory and applications},
  volume    = {40},
  isbn      = {0-8218-0154-6},
  publisher = {American Mathematical Soc.},
  author    = {Johnson, Charles R.},
  year      = {1990}
}

@article{kneissler_simultaneous_2015,
  title      = {Simultaneous {Learning} and {Filtering} without {Delusions}: {A} {
                Bayes}-{Optimal} {Derivation} of {Combining} {Predictive} {Inference}
                and {Adaptive} {Filtering}},
  volume     = {9},
  shorttitle = {Simultaneous {Learning} and {Filtering} without {Delusions}},
  url        = {
                http://journal.frontiersin.org/article/10.3389/fncom.2015.00047/abstract
                },
  doi        = {10.3389/fncom.2015.00047},
  urldate    = {2015-04-20},
  journal    = {Frontiers in Computational Neuroscience},
  author     = {Kneissler, Jan and Drugowitsch, Jan and Friston, Karl and Butz,
                Martin V.},
  year       = {2015},
  keywords   = {predictive coding, Recursive least squares, Bayesian Information
                Processing, forward model, Illusions, Kalman filtering},
  pages      = {47},
  file       = {Kneissler et al. - 2015 - Simultaneous Learning and Filtering without
                Delusi.pdf:/Users/apodusenko/Zotero/storage/RJCDB5TW/Kneissler et al. -
                2015 - Simultaneous Learning and Filtering without
                Delusi.pdf:application/pdf;Kneissler et al. - 2015 - supplementary
                material:/Users/apodusenko/Zotero/storage/CXVHG94H/Kneissler et al. -
                2015 - supplementary material.pdf:application/pdf}
}

@article{zhao_online_2008,
  title    = {Online {Noise} {Estimation} {Using} {Stochastic}-{Gain} {HMM} for {
              Speech} {Enhancement}},
  volume   = {16},
  issn     = {1558-7916},
  doi      = {10.1109/TASL.2008.916055},
  abstract = {We propose a noise estimation algorithm for single-channel noise
              suppression in dynamic noisy environments. A stochastic-gain hidden
              Markov model (SG-HMM) is used to model the statistics of
              nonstationary noise with time-varying energy. The noise model is
              adaptive and the model parameters are estimated online from noisy
              observations using a recursive estimation algorithm. The parameter
              estimation is derived for the maximum-likelihood criterion and the
              algorithm is based on the recursive expectation maximization (EM)
              framework. The proposed method facilitates continuous adaptation to
              changes of both noise spectral shapes and noise energy levels, e.g.
              , due to movement of the noise source. Using the estimated noise
              model, we also develop an estimator of the noise power spectral
              density (PSD) based on recursive averaging of estimated noise
              sample spectra. We demonstrate that the proposed scheme achieves
              more accurate estimates of the noise model and noise PSD, and as
              part of a speech enhancement system facilitates a lower level of
              residual noise.},
  number   = {4},
  journal  = {IEEE Transactions on Audio, Speech, and Language Processing},
  author   = {Zhao, D.Y. and Kleijn, W.B. and Ypma, Alexander and de Vries, Bert},
  month    = may,
  year     = {2008},
  keywords = {expectation-maximisation algorithm, Gain modeling, noise
              suppression, nonstationary noise, Recursive estimation, Speech
              enhancement, hidden Markov models, dynamic noisy environment,
              interference suppression, maximum-likelihood criterion, noise
              energy level, noise estimation, noise model adaptation, noise
              spectral shape, online noise estimation, power spectral density,
              recursive expectation maximization, single-channel noise
              suppression, spectral analysis, stochastic-gain hidden Markov model
              , stochastic-gain hidden Markov model (SG-HMM)},
  pages    = {835--846},
  file     = {IEEE Xplore Abstract
              Record:/Users/apodusenko/Zotero/storage/4AXXZVK4/abs_all.html:text/html;Zhao
              et al. - 2008 - Online Noise Estimation Using Stochastic-Gain HMM
              .pdf:/Users/apodusenko/Zotero/storage/4ZPBRBX9/Zhao et al. - 2008 -
              Online Noise Estimation Using Stochastic-Gain HMM .pdf:application/pdf}
}

@phdthesis{bruderer_lukas_input_2015,
  address = {Zurich, CH},
  title   = {Input {Estimation} and {Dynamical} {System} {Identification}: {New} {
             Algorithms} and {Results}},
  url     = {http://e-collection.library.ethz.ch/eserv/eth:48178/eth-48178-02.pdf},
  school  = {ETH Zurich},
  author  = {Bruderer, Lukas},
  year    = {2015},
  file    = {Bruderer, Lukas - 2015 - Input Estimation and Dynamical System
             Identificati.pdf:/Users/apodusenko/Zotero/storage/GK7AD4ZG/Bruderer,
             Lukas - 2015 - Input Estimation and Dynamical System
             Identificati.pdf:application/pdf}
}

@article{da_he_ear-worn_2015,
  title    = {An {Ear}-{Worn} {Vital} {Signs} {Monitor}},
  volume   = {62},
  issn     = {0018-9294},
  doi      = {10.1109/TBME.2015.2459061},
  abstract = {This paper presents a wearable vital signs monitor at the ear. The
              monitor measures the electrocardiogram (ECG), ballistocardiogram
              (BCG), and photoplethysmogram (PPG) to obtain pre-ejection period
              (PEP), stroke volume (SV), cardiac output (CO), and pulse transit
              time (PTT). The ear is demonstrated as a natural anchoring point
              for the integrated sensing of physiological signals. All three
              signals measured can be used to obtain heart rate (HR). Combining
              the ECG and BCG allows for the estimation of the PEP, while
              combining the BCG and PPG allows for the measurement of PTT.
              Additionally, the J-wave amplitude of the BCG is correlated with
              the SV and, when combined with HR, yields CO. Results from a
              clinical human study on 13 subjects demonstrate this
              proof-of-concept device.},
  number   = {11},
  journal  = {IEEE Transactions on Biomedical Engineering},
  author   = {Da He, D. and Winokur, E.S. and Sodini, C.G.},
  month    = nov,
  year     = {2015},
  keywords = {ballistocardiogram, Ballistocardiogram (BCG), Biomedical
              monitoring, blood pressure, blood pressure measurement, cardiac
              output, cardiac output (CO), continuous monitoring, ear,
              electrocardiogram, electrocardiogram (ECG), electrocardiography,
              heart rate, heart rate (HR), J-wave amplitude, Monitoring,
              photoplethysmogram, photoplethysmogram (PPG), photoplethysmography,
              pre-ejection period, pre-ejection period (PEP), preejection period,
              pulse transit time, pulse transit time (PTT), stroke volume, stroke
              volume (SV), wearable vital signs monitor},
  pages    = {2547--2552},
  file     = {IEEE Xplore Abstract
              Record:/Users/apodusenko/Zotero/storage/425K79II/abs_all.html:text/html
              }
}

@book{stuber_principles_2011,
  title     = {Principles of mobile communication},
  url       = {
               https://books.google.nl/books?hl=en&lr=&id=rSz1-HtBQQkC&oi=fnd&pg=PR1&dq=Principles+of+Mobile+Communications&ots=Gz-k1744N7&sig=lVLT56aI9MiCRpYzqSLrhDtoxd4
               },
  urldate   = {2016-01-27},
  publisher = {Springer Science \& Business Media},
  author    = {Stüber, Gordon L.},
  year      = {2011},
  file      = {
               Snapshot:/Users/apodusenko/Zotero/storage/59HD6B9H/books.html:text/html;Stüber
               - 2011 - Principles of mobile
               communication.pdf:/Users/apodusenko/Zotero/storage/9TGBXXT3/Stüber -
               2011 - Principles of mobile communication.pdf:application/pdf}
}

@inproceedings{wu_flexible_2005,
  title     = {Flexible lognormal sum approximation method},
  volume    = {6},
  urldate   = {2015-12-30},
  booktitle = {{GLOBECOM}'05},
  publisher = {IEEE},
  author    = {Wu, Jingxian and Mehta, Neelesh B. and Zhang, Jin},
  year      = {2005},
  pages     = {3413--3417},
  file      = {
               Snapshot:/Users/apodusenko/Zotero/storage/V52FJK22/abs_all.html:text/html;Wu
               et al. - 2005 - Flexible lognormal sum approximation
               method.pdf:/Users/apodusenko/Zotero/storage/GPJ32RZE/Wu et al. - 2005 -
               Flexible lognormal sum approximation method.pdf:application/pdf}
}

@article{lake_human-level_2015,
  title     = {Human-level concept learning through probabilistic program induction},
  volume    = {350},
  copyright = {Copyright © 2015, American Association for the Advancement of
               Science},
  issn      = {0036-8075, 1095-9203},
  url       = {http://science.sciencemag.org/content/350/6266/1332},
  doi       = {10.1126/science.aab3050},
  abstract  = {Handwritten characters drawn by a model Not only do children learn
               effortlessly, they do so quickly and with a remarkable ability to
               use what they have learned as the raw material for creating new
               stuff. Lake et al. describe a computational model that learns in a
               similar fashion and does so better than current deep learning
               algorithms. The model classifies, parses, and recreates handwritten
               characters, and can generate new letters of the alphabet that look
               “right” as judged by Turing-like tests of the model's output in
               comparison to what real humans produce. Science, this issue p. 1332
               People learning new concepts can often generalize successfully from
               just a single example, yet machine learning algorithms typically
               require tens or hundreds of examples to perform with similar
               accuracy. People can also use learned concepts in richer ways than
               conventional algorithms—for action, imagination, and explanation.
               We present a computational model that captures these human learning
               abilities for a large class of simple visual concepts: handwritten
               characters from the world’s alphabets. The model represents
               concepts as simple programs that best explain observed examples
               under a Bayesian criterion. On a challenging one-shot
               classification task, the model achieves human-level performance
               while outperforming recent deep learning approaches. We also
               present several “visual Turing tests” probing the model’s creative
               generalization abilities, which in many cases are indistinguishable
               from human behavior. Combining the capacity to handle noise with
               probabilistic learning yields humanlike performance in a
               computational model. Combining the capacity to handle noise with
               probabilistic learning yields humanlike performance in a
               computational model.},
  language  = {en},
  number    = {6266},
  urldate   = {2016-01-25},
  journal   = {Science},
  author    = {Lake, Brenden M. and Salakhutdinov, Ruslan and Tenenbaum, Joshua B.},
  month     = dec,
  year      = {2015},
  pmid      = {26659050},
  pages     = {1332--1338},
  file      = {Lake et al. - 2015 - Human-level concept learning through
               probabilistic.pdf:/Users/apodusenko/Zotero/storage/6KRZR8FH/Lake et al.
               - 2015 - Human-level concept learning through
               probabilistic.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/44XU4Q4X/1332.html:text/html
               }
}

@article{kates_understanding_2010,
  title      = {Understanding compression: modeling the effects of dynamic-range
                compression in hearing aids},
  volume     = {49},
  issn       = {1708-8186},
  shorttitle = {Understanding compression},
  doi        = {10.3109/14992020903426256},
  abstract   = {The purpose of this paper is to study the effects of dynamic-range
                compression and linear amplification on speech intelligibility and
                quality for hearing-impaired listeners. The paper focuses on the
                relative benefit of compression compared to linear amplification
                and the effect of varying the number of compression channels and
                the compression time constants. The stimuli are sentences in a
                background of stationary speech-shaped noise. Speech
                intelligibility and quality indices are used to predict the
                listener responses for a mild, moderate sloping, and
                moderate/severe hearing loss. The results show a strong interaction
                between signal processing, speech intensity, and hearing loss. The
                results are interpreted in terms of the two major effects of
                compression on speech: the increase in audibility and the decrease
                in temporal and spectral envelope contrast.},
  language   = {eng},
  number     = {6},
  journal    = {International Journal of Audiology},
  author     = {Kates, James M.},
  month      = jun,
  year       = {2010},
  pmid       = {20225931},
  keywords   = {Humans, Computer Simulation, Speech Perception, Auditory Threshold
                , Signal Processing, Computer-Assisted, Hearing Loss, Sensorineural
                , Perceptual Distortion, Perceptual Masking, Prosthesis Design,
                Sound Spectrography, Speech Acoustics},
  pages      = {395--409}
}

@book{crow_lognormal_1988,
  title     = {Lognormal distributions: {Theory} and applications},
  volume    = {88},
  publisher = {Dekker New York},
  author    = {Crow, Edwin L. and Shimizu, Kunio},
  year      = {1988}
}

@inproceedings{rennie_matched-condition_2011,
  title     = {Matched-condition robust {Dynamic} {Noise} {Adaptation}},
  doi       = {10.1109/ASRU.2011.6163919},
  abstract  = {In this paper we describe how the model-based noise robustness
               algorithm for previously unseen noise conditions, Dynamic Noise
               Adaptation (DNA), can be made robust to matched data, without the
               need to do any system re-training. The approach is to do online
               model selection and averaging between two DNA models of noise: one
               that is tracking the evolving state of the background noise, and
               one clamped to the null mis-match hypothesis. The approach, which
               we call DNA with (matched) condition detection (DNA-CD), improves
               the performance of a commerical-grade speech recognizer that
               utilizes feature-space Maximum Mutual Information (fMMI), boosted
               MMI (bMMI), and feature-space Maximum Likelihood Linear Regression
               (fMLLR) compensation by 15\% relative at signal-to-noise ratios
               (SNRs) below 10 dB, and over 8\% relative overall.},
  booktitle = {2011 {IEEE} {Workshop} on {Automatic} {Speech} {Recognition} and
               {Understanding} ({ASRU})},
  author    = {Rennie, S.J. and Dognin, P.L. and Fousek, P.},
  month     = dec,
  year      = {2011},
  keywords  = {Noise, Regression Analysis, speech recognition, Speech, Acoustics,
               Adaptation models, Noise robustness, hidden Markov models,
               Algonquin, bMMI, boosted MMI, commerical-grade speech recognizer,
               condition detection, DNA, DNA-CD, Dynamic Noise Adaptation (DNA),
               feature-space maximum likelihood linear regression, feature-space
               Maximum Mutual Information, fMLLR, fMMI, matched-condition robust
               dynamic noise adaptation, Maximum likelihood estimation, Model
               Adaptation, model-based noise robustness algorithm, null mis-match
               hypothesis, signal-to-noise ratios, SNR, Spectral Subtraction},
  pages     = {137--140},
  file      = {IEEE Xplore Abstract
               Record:/Users/apodusenko/Zotero/storage/SB9J29GA/abs_all.html:text/html;Rennie
               et al. - 2011 - Matched-condition robust Dynamic Noise
               Adaptation.pdf:/Users/apodusenko/Zotero/storage/UNJ4CIUV/Rennie et al.
               - 2011 - Matched-condition robust Dynamic Noise
               Adaptation.pdf:application/pdf}
}

@article{yoshioka_noise_2013,
  title      = {Noise model transfer: {Novel} approach to robustness against
                nonstationary noise},
  volume     = {21},
  shorttitle = {Noise model transfer},
  url        = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6563155},
  number     = {10},
  urldate    = {2016-01-15},
  journal    = {Audio, Speech, and Language Processing, IEEE Transactions on},
  author     = {Yoshioka, Takashi and Nakatani, Takeshi},
  year       = {2013},
  pages      = {2182--2192},
  file       = {
                Snapshot:/Users/apodusenko/Zotero/storage/PZJ6XMUC/abs_all.html:text/html;Yoshioka
                and Nakatani - 2013 - Noise model transfer Novel approach to
                robustness.pdf:/Users/apodusenko/Zotero/storage/PSRXU6NQ/Yoshioka and
                Nakatani - 2013 - Noise model transfer Novel approach to
                robustness.pdf:application/pdf}
}

@article{deng_estimating_2004,
  title   = {Estimating cepstrum of speech under the presence of noise using a
             joint prior of static and dynamic features},
  volume  = {12},
  number  = {3},
  urldate = {2016-01-12},
  journal = {Speech and Audio Processing, IEEE Transactions on},
  author  = {Deng, Li and Droppo, Jasha and Acero, Alex},
  year    = {2004},
  pages   = {218--233},
  file    = {Deng et al. - 2004 - Estimating cepstrum of speech under the presence
             o.pdf:/Users/apodusenko/Zotero/storage/C52NI4WS/Deng et al. - 2004 -
             Estimating cepstrum of speech under the presence
             o.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/CUS678RF/abs_all.html:text/html
             }
}

@inproceedings{kristjansson_single_2004,
  title     = {Single microphone source separation using high resolution signal
               reconstruction},
  volume    = {2},
  urldate   = {2016-01-12},
  booktitle = {Acoustics, {Speech}, and {Signal} {Processing}, 2004. {
               Proceedings}.({ICASSP}'04). {IEEE} {International} {Conference} on
               },
  publisher = {IEEE},
  author    = {Kristjansson, Trausti and Attias, Hagai and Hershey, John},
  year      = {2004},
  pages     = {ii--817},
  file      = {Kristjansson et al. - 2004 - Single microphone source separation using
               high res.pdf:/Users/apodusenko/Zotero/storage/JM3KSENS/Kristjansson et
               al. - 2004 - Single microphone source separation using high
               res.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/2UXQ9U2S/abs_all.html:text/html
               }
}

@book{virtanen_techniques_2012,
  title     = {Techniques for noise robustness in automatic speech recognition},
  urldate   = {2015-12-31},
  publisher = {John Wiley \& Sons},
  author    = {Virtanen, Tuomas and Singh, Rita and Raj, Bhiksha},
  year      = {2012},
  file      = {
               Snapshot:/Users/apodusenko/Zotero/storage/8TP5DJJV/books.html:text/html;Virtanen
               et al. - 2012 - Techniques for noise robustness in automatic
               speec.pdf:/Users/apodusenko/Zotero/storage/ZHTHWAJK/Virtanen et al. -
               2012 - Techniques for noise robustness in automatic
               speec.pdf:application/pdf}
}

@inproceedings{afify_sequential_2001,
  title     = {Sequential noise estimation with optimal forgetting for robust speech
               recognition},
  volume    = {1},
  url       = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=940809},
  urldate   = {2015-12-30},
  booktitle = {Acoustics, {Speech}, and {Signal} {Processing}, 2001. {
               Proceedings}.({ICASSP}'01). 2001 {IEEE} {International} {
               Conference} on},
  publisher = {IEEE},
  author    = {Afify, Mohamed and Siohan, Olivier},
  year      = {2001},
  pages     = {229--232},
  file      = {Afify and Siohan - 2001 - Sequential noise estimation with optimal
               forgettin.pdf:/Users/apodusenko/Zotero/storage/8SI5HD4X/Afify and
               Siohan - 2001 - Sequential noise estimation with optimal
               forgettin.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/9S4M5M7F/abs_all.html:text/html
               }
}

@inproceedings{rennie_robust_2011,
  title     = {Robust speech recognition using dynamic noise adaptation},
  url       = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5947377},
  urldate   = {2015-12-29},
  booktitle = {Acoustics, {Speech} and {Signal} {Processing} ({ICASSP}), 2011 {
               IEEE} {International} {Conference} on},
  publisher = {IEEE},
  author    = {Rennie, Steven and Dognin, Pierre and Fousek, Petr},
  year      = {2011},
  pages     = {4592--4595},
  file      = {Rennie et al. - 2011 - Robust speech recognition using dynamic noise
               adap.pdf:/Users/apodusenko/Zotero/storage/QWH96NDS/Rennie et al. - 2011
               - Robust speech recognition using dynamic noise
               adap.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/WF5G4TDB/abs_all.html:text/html
               }
}

@article{hao_speech_2009,
  title   = {Speech enhancement, gain, and noise spectrum adaptation using
             approximate {Bayesian} estimation},
  volume  = {17},
  url     = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4740160},
  number  = {1},
  urldate = {2015-12-29},
  journal = {Audio, Speech, and Language Processing, IEEE Transactions on},
  author  = {Hao, Jiucang and Attias, Hagai and Nagarajan, Sasi and Lee, Te-Won
             and Sejnowski, Terrence J.},
  year    = {2009},
  pages   = {24--37},
  file    = {[HTML] van
             nih.gov:/Users/apodusenko/Zotero/storage/PEMC87VM/PMC2860321.html:text/html;Hao
             et al. - 2009 - Speech enhancement, gain, and noise spectrum
             adapt.pdf:/Users/apodusenko/Zotero/storage/XPJV32GF/Hao et al. - 2009 -
             Speech enhancement, gain, and noise spectrum
             adapt.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/BWNTU5TA/abs_all.html:text/html
             }
}

@article{ephraim_bayesian_1992,
  title   = {A {Bayesian} estimation approach for speech enhancement using hidden
             {Markov} models},
  volume  = {40},
  url     = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=127947},
  number  = {4},
  urldate = {2015-12-29},
  journal = {Signal processing, IEEE transactions on},
  author  = {Ephraim, Yariv},
  year    = {1992},
  pages   = {725--735},
  file    = {Ephraim - 1992 - A Bayesian estimation approach for speech
             enhancem.pdf:/Users/apodusenko/Zotero/storage/86XNAD7A/Ephraim - 1992 -
             A Bayesian estimation approach for speech
             enhancem.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/6I96394R/login.html:text/html
             }
}

@article{burshtein_speech_2002,
  title   = {Speech enhancement using a mixture-maximum model},
  volume  = {10},
  url     = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1040258},
  number  = {6},
  urldate = {2015-12-29},
  journal = {Speech and Audio Processing, IEEE Transactions on},
  author  = {Burshtein, David and Gannot, Sharon},
  year    = {2002},
  pages   = {341--351},
  file    = {Burshtein and Gannot - 2002 - Speech enhancement using a
             mixture-maximum
             model.pdf:/Users/apodusenko/Zotero/storage/TT32JPHB/Burshtein and
             Gannot - 2002 - Speech enhancement using a mixture-maximum
             model.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/NE7M2NDI/login.html:text/html
             }
}

@article{gemmeke_exemplar-based_2011,
  title   = {Exemplar-based sparse representations for noise robust automatic
             speech recognition},
  volume  = {19},
  url     = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5710402},
  number  = {7},
  urldate = {2015-12-28},
  journal = {Audio, Speech, and Language Processing, IEEE Transactions on},
  author  = {Gemmeke, Jort F. and Virtanen, Tuomas and Hurmalainen, Antti},
  year    = {2011},
  pages   = {2067--2080},
  file    = {Gemmeke et al. - 2011 - Exemplar-based sparse representations for
             noise ro.pdf:/Users/apodusenko/Zotero/storage/STCATQTP/Gemmeke et al. -
             2011 - Exemplar-based sparse representations for noise
             ro.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/57JISVCR/login.html:text/html
             }
}

@article{virtanen_monaural_2007,
  title   = {Monaural sound source separation by nonnegative matrix factorization
             with temporal continuity and sparseness criteria},
  volume  = {15},
  url     = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4100700},
  number  = {3},
  urldate = {2015-12-28},
  journal = {Audio, Speech, and Language Processing, IEEE Transactions on},
  author  = {Virtanen, Tuomas},
  year    = {2007},
  pages   = {1066--1074},
  file    = {
             Snapshot:/Users/apodusenko/Zotero/storage/QI7CUN96/login.html:text/html;Virtanen
             - 2007 - Monaural sound source separation by nonnegative
             ma.pdf:/Users/apodusenko/Zotero/storage/BUXBPMVR/Virtanen - 2007 -
             Monaural sound source separation by nonnegative ma.pdf:application/pdf}
}

@article{ortega_information-theoretic_2015,
  title    = {Information-{Theoretic} {Bounded} {Rationality}},
  url      = {http://arxiv.org/abs/1512.06789},
  abstract = {Bounded rationality, that is, decision-making and planning under
              resource limitations, is widely regarded as an important open
              problem in artificial intelligence, reinforcement learning,
              computational neuroscience and economics. This paper offers a
              consolidated presentation of a theory of bounded rationality based
              on information-theoretic ideas. We provide a conceptual
              justification for using the free energy functional as the objective
              function for characterizing bounded-rational decisions. This
              functional possesses three crucial properties: it controls the size
              of the solution space; it has Monte Carlo planners that are exact,
              yet bypass the need for exhaustive search; and it captures model
              uncertainty arising from lack of evidence or from interacting with
              other agents having unknown intentions. We discuss the single-step
              decision-making case, and show how to extend it to sequential
              decisions using equivalence transformations. This extension yields
              a very general class of decision problems that encompass classical
              decision rules (e.g. EXPECTIMAX and MINIMAX) as limit cases, as
              well as trust- and risk-sensitive planning.},
  urldate  = {2015-12-25},
  journal  = {arXiv:1512.06789 [cs, math, stat]},
  author   = {Ortega, Pedro A. and Braun, Daniel A. and Dyer, Justin and Kim,
              Kee-Eung and Tishby, Naftali},
  month    = dec,
  year     = {2015},
  note     = {arXiv: 1512.06789},
  keywords = {Statistics - Machine Learning, Computer Science - Artificial
              Intelligence, Computer Science - Systems and Control, Mathematics -
              Optimization and Control},
  file     = {arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/2KPCFFM3/1512.html:text/html;Ortega
              et al. - 2015 - Information-Theoretic Bounded
              Rationality.pdf:/Users/apodusenko/Zotero/storage/GDFN9PZG/Ortega et al.
              - 2015 - Information-Theoretic Bounded Rationality.pdf:application/pdf}
}

@article{hensman_fast_2015,
  title    = {Fast {Nonparametric} {Clustering} of {Structured} {Time}-{Series}},
  volume   = {37},
  issn     = {0162-8828},
  doi      = {10.1109/TPAMI.2014.2318711},
  abstract = {In this publication, we combine two Bayesian nonparametric models:
              the Gaussian Process (GP) and the Dirichlet Process (DP). Our
              innovation in the GP model is to introduce a variation on the GP
              prior which enables us to model structured time-series data, i.e.,
              data containing groups where we wish to model inter- and
              intra-group variability. Our innovation in the DP model is an
              implementation of a new fast collapsed variational inference
              procedure which enables us to optimize our variational
              approximation significantly faster than standard VB approaches. In
              a biological time series application we show how our model better
              captures salient features of the data, leading to better
              consistency with existing biological classifications, while the
              associated inference algorithm provides a significant speed-up over
              EM-based variational inference.},
  number   = {2},
  journal  = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  author   = {Hensman, J. and Rattray, M. and Lawrence, N.D.},
  month    = feb,
  year     = {2015},
  keywords = {Variational Bayes, Vectors, Gaussian processes, Time series
              analysis, computational modeling, Biological system modeling, Data
              models, gene expression, Optimization, structured time series},
  pages    = {383--393},
  file     = {IEEE Xplore Abstract
              Record:/Users/apodusenko/Zotero/storage/86G9ZJES/abs_all.html:text/html;IEEE
              Xplore Full Text PDF:/Users/apodusenko/Zotero/storage/QSG2N5D8/Hensman
              et al. - 2015 - Fast Nonparametric Clustering of Structured
              Time-S.pdf:application/pdf}
}

@inproceedings{berouti_enhancement_1979,
  title     = {Enhancement of speech corrupted by acoustic noise},
  volume    = {4},
  urldate   = {2015-11-16},
  booktitle = {{IEEE} {International} {Conference} on {ICASSP}'79.},
  author    = {Berouti, M. and Schwartz, R. and Makhoul, John},
  year      = {1979},
  pages     = {208--211},
  file      = {Berouti et al. - 1979 - Enhancement of speech corrupted by acoustic
               noise.pdf:/Users/apodusenko/Zotero/storage/GAJUDZBB/Berouti et al. -
               1979 - Enhancement of speech corrupted by acoustic
               noise.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/P7TJ5X74/abs_all.html:text/html
               }
}

@incollection{roweis_automatic_2005,
  title     = {Automatic {Speech} {Processing} by {Inference} in {Generative} {
               Models}},
  copyright = {©2005 Springer Science + Business Media, Inc.},
  isbn      = {978-1-4020-8001-2 978-0-387-22794-8},
  url       = {http://link.springer.com/chapter/10.1007/0-387-22794-6_8},
  abstract  = {Summary In this chapter, we have explored the use of inference in
               probabilistic generative models as a powerful signal processing
               tool for speech and audio. The basic paradigm explored was to
               design a simple model for the data we observe in which the key
               quantities that we would eventually like to compute appear as
               hidden (latent) variables. By executing probabilistic inference in
               such models, we automatically estimating the hidden quantities and
               thus perform our desired computation. In a sense, the rules of
               probability derive for us, automatically, the optimal signal
               processing algorithm for our desired outputs given our inputs under
               the model assumptions. Crucially, even though the generative model
               may be quite simple and may not capture all of the variability
               present in the data, the results of inference can still be
               extremely informative. We gave several examples showing how
               inference in very simple generative models can be used to perform
               surprisingly complex speech processing tasks including denoising,
               source separation, pitch tracking, timescale modification and
               estimation of articulatory movements from audio.},
  language  = {en},
  urldate   = {2015-12-16},
  booktitle = {Speech {Separation} by {Humans} and {Machines}},
  publisher = {Springer US},
  author    = {Roweis, Sam T.},
  editor    = {Divenyi, Pierre},
  year      = {2005},
  doi       = {10.1007/0-387-22794-6_8},
  keywords  = {User Interfaces and Human Computer Interaction, Signal, Image and
               Speech Processing, Engineering, general},
  pages     = {97--133},
  file      = {Roweis - 2005 - Automatic Speech Processing by Inference in
               Genera.pdf:/Users/apodusenko/Zotero/storage/WV8U7C5P/Roweis - 2005 -
               Automatic Speech Processing by Inference in
               Genera.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/D46NQSUW/10.html:text/html
               }
}

@article{gales_predictive_1998,
  title   = {Predictive model-based compensation schemes for robust speech
             recognition},
  volume  = {25},
  number  = {1},
  urldate = {2015-12-16},
  journal = {Speech communication},
  author  = {Gales, M. J. F.},
  year    = {1998},
  pages   = {49--74},
  file    = {Gales - 1998 - Predictive model-based compensation schemes for
             ro.pdf:/Users/apodusenko/Zotero/storage/3MU8JK9H/Gales - 1998 -
             Predictive model-based compensation schemes for
             ro.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/65GUDMX6/S0167639398000296.html:text/html
             }
}

@inproceedings{moreno_vector_1996,
  title     = {A vector {Taylor} series approach for environment-independent speech
               recognition},
  volume    = {2},
  urldate   = {2015-12-16},
  booktitle = {{IEEE} {International} {Conference} on {Acoustics}, {Speech}, and
               {Signal} {Processing}, 1996},
  publisher = {IEEE},
  author    = {Moreno, Pedro J. and Raj, Bhiksha and Stern, Richard M.},
  year      = {1996},
  pages     = {733--736},
  file      = {Moreno et al. - 1996 - A vector Taylor series approach for
               environment-in.pdf:/Users/apodusenko/Zotero/storage/ZH2SR3VG/Moreno et
               al. - 1996 - A vector Taylor series approach for
               environment-in.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/9ES9BTRU/abs_all.html:text/html
               }
}

@article{rennie_single-channel_2010,
  title    = {Single-{Channel} {Multitalker} {Speech} {Recognition}},
  volume   = {27},
  issn     = {1053-5888},
  doi      = {10.1109/MSP.2010.938081},
  abstract = {We have described some of the problems with modeling mixed
              acoustic signals in the log spectral domain using graphical models,
              as well as some current approaches to handling these problems for
              multitalker speech separation and recognition. We have also
              reviewed methods for inference on FHMMs (factorial hidden Markov
              model) and methods for handling the nonlinear interaction function
              in the log spectral domain. These methods are capable of separating
              and recognizing speech better than human listeners on the SSC task.
              },
  number   = {6},
  journal  = {IEEE Signal Processing Magazine},
  author   = {Rennie, S.J. and Hershey, J.R. and Olsen, P.A.},
  month    = nov,
  year     = {2010},
  keywords = {graphical model, speech recognition, acoustic signal processing,
              Acoustics, hidden Markov models, computational modeling, spectral
              analysis, Data models, Complexity theory, factorial hidden Markov
              model, log spectral domain, mixed acoustic signal, multitalker
              speech separation, nonlinear interaction function, single-channel
              multitalker speech recognition, speech synthesis, SSC task},
  pages    = {66--80},
  file     = {IEEE Xplore Abstract
              Record:/Users/apodusenko/Zotero/storage/23UX74HH/login.html:text/html;Rennie
              et al. - 2010 - Single-Channel Multitalker Speech
              Recognition.pdf:/Users/apodusenko/Zotero/storage/UAXTAM2H/Rennie et al.
              - 2010 - Single-Channel Multitalker Speech
              Recognition.pdf:application/pdf}
}

@article{smith_efficient_2006,
  title   = {Efficient auditory coding},
  url     = {http://psych.stanford.edu/~jlm/Presentations/SmithLewicki.pdf},
  urldate = {2015-12-15},
  author  = {Smith, Evan C. and Lewicki, Michael S.},
  year    = {2006},
  file    = {Smith and Lewicki - 2006 - Efficient auditory
             coding.pdf:/Users/apodusenko/Zotero/storage/32ZSDW7K/Smith and Lewicki
             - 2006 - Efficient auditory coding.pdf:application/pdf}
}

@article{lewicki_computational_2015,
  title    = {Computational issues in natural auditory scene analysis},
  volume   = {137},
  issn     = {0001-4966},
  url      = {
              http://scitation.aip.org/content/asa/journal/jasa/137/4/10.1121/1.4920202
              },
  doi      = {10.1121/1.4920202},
  abstract = {Scene analysis is a complex process involving a hierarchy of
              computational problems ranging from sensory representation to
              feature extraction to active perception. It is studied in a wide
              range of fields using different approaches, but we still have only
              limited insight into the computations used by biological systems.
              Experimental approaches often implicitly assume a feature detection
              paradigm without addressing the true complexity of the problem.
              Computational approaches are now capable of solving complex scene
              analysis problems, but these often defined in a way that is of
              limited relevance to biology. The challenge is to develop
              approaches for deducing computational principles relevant to
              biological systems. I will present the view that scene analysis is
              a universal problem solved by all animals, and that we can gain new
              insights by studying the problems that animals face in complex
              natural environments. From this, I will present framework for
              studying scene analysis comprising four essential properties: (1)
              the ability to solve ill-posed problems, (2) the ability to
              integrate and store information across time and modality, (3)
              efficient recovery and representation of 3D scene structure, and
              (4) the use of optimal motor actions for acquiring information to
              progress toward behavioral goals.},
  number   = {4},
  urldate  = {2015-12-15},
  journal  = {The Journal of the Acoustical Society of America},
  author   = {Lewicki, Michael S. and Olshausen, Bruno A. and Surlykke, Annemarie
              and Moss, Cynthia F.},
  month    = apr,
  year     = {2015},
  keywords = {Psychological acoustics, Anatomy, Information integration,
              Materials analysis},
  pages    = {2249--2249},
  file     = {Snapshot:/Users/apodusenko/Zotero/storage/2USQ2EUK/1.html:text/html}
}

@inproceedings{deng_incremental_2003,
  title     = {Incremental {Bayes} learning with prior evolution for tracking
               nonstationary noise statistics from noisy speech data},
  volume    = {1},
  doi       = {10.1109/ICASSP.2003.1198870},
  abstract  = {A new approach to sequential estimation of the time-varying prior
               parameters of nonstationary noise is presented using the
               log-spectral or cepstral data of corrupted noisy speech.
               Incremental Bayes learning is developed to provide a basis for
               noise prior evolution, recursively updating the noise prior
               statistics (mean and variance) using the approximate Gaussian
               posterior computed at the preceding time step. The algorithm for
               noise prior evolution is derived in detail, and is evaluated using
               the Aurora2 database with the root-mean-square (RMS) error measure.
               Experimental results show that when the time-varying variance and
               mean of the nonstationary noise prior are estimated and exploited,
               superior performance is achieved compared with using either no
               noise prior information or using the time-invariant, fixed mean and
               variance in the noise prior distribution.},
  booktitle = {2003 {IEEE} {International} {Conference} on {Acoustics}, {Speech}
               , and {Signal} {Processing}, 2003. {Proceedings}.},
  author    = {Deng, Li and Droppo, J. and Acero, A.},
  month     = apr,
  year      = {2003},
  keywords  = {Bayes methods, Gaussian distribution, Statistics, Acoustic noise,
               Recursive estimation, Speech enhancement, Working environment noise
               , Noise measurement, learning (artificial intelligence), SE,
               cepstral analysis, Maximum likelihood estimation, approximate
               Gaussian posterior, Aurora2 database, cepstral data, Databases,
               error statistics, Gaussian noise, incremental Bayes learning,
               log-spectral data, noise prior statistics, noisy speech data,
               nonstationary noise statistics tracking, prior evolution, random
               noise, RMS error measure, sequential estimation, time-varying mean,
               time-varying prior parameters estimation, time-varying variance},
  pages     = {I--672--I--675},
  file      = {Deng et al. - 2003 - Incremental Bayes learning with prior evolution
               fo.pdf:/Users/apodusenko/Zotero/storage/U7FISB9J/Deng et al. - 2003 -
               Incremental Bayes learning with prior evolution
               fo.pdf:application/pdf;IEEE Xplore Abstract
               Record:/Users/apodusenko/Zotero/storage/SXW32NRP/login.html:text/html}
}

@article{sivaprasad_survey_2014,
  title   = {A {Survey} on {Statistical} {Based} {Single} {Channel} {Speech} {
             Enhancement} {Techniques}},
  volume  = {12},
  urldate = {2015-04-28},
  journal = {I.J. Intelligent Systems and Applications},
  author  = {Sivaprasad, N. and Kumar, T. Kishore},
  year    = {2014},
  pages   = {69--85},
  file    = {Sivaprasad and Kumar - 2014 - A Survey on Statistical Based Single
             Channel Speec.pdf:/Users/apodusenko/Zotero/storage/7EJ3QM4B/Sivaprasad
             and Kumar - 2014 - A Survey on Statistical Based Single Channel
             Speec.pdf:application/pdf}
}

@phdthesis{srinivasan_knowledge-based_2005,
  address = {Stockholm},
  title   = {Knowledge-based speech enhancement},
  urldate = {2015-11-18},
  school  = {KTH - Royal Institute of Technology},
  author  = {Srinivasan, Sriram},
  year    = {2005},
  file    = {Srinivasan - 2005 - Knowledge-based speech
             enhancement.pdf:/Users/apodusenko/Zotero/storage/7FNF69PS/Srinivasan -
             2005 - Knowledge-based speech enhancement.pdf:application/pdf}
}

@phdthesis{reller_state-space_2012,
  title   = {State-{Space} {Methods} in {Statistical} {Signal} {Processing}: {New}
             {Ideas} and {Applications}},
  urldate = {2014-04-10},
  school  = {ETH Zurich},
  author  = {Reller, Christoph},
  year    = {2012},
  file    = {Reller - 2012 - State-Space Methods in Statistical Signal
             Processi.pdf:/Users/apodusenko/Zotero/storage/7PS6ADCM/Reller - 2012 -
             State-Space Methods in Statistical Signal Processi.pdf:application/pdf}
}

@article{martin_speech_2005,
  title   = {Speech enhancement based on minimum mean-square error estimation and
             supergaussian priors},
  volume  = {13},
  number  = {5},
  urldate = {2015-11-23},
  journal = {Speech and Audio Processing, IEEE Transactions on},
  author  = {Martin, Rainer},
  year    = {2005},
  pages   = {845--856},
  file    = {Martin - 2005 - Speech enhancement based on minimum mean-square
             er.pdf:/Users/apodusenko/Zotero/storage/WVE9T5V6/Martin - 2005 - Speech
             enhancement based on minimum mean-square
             er.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/RQWR9XQH/abs_all.html:text/html
             }
}

@article{ephraim_speech_1984,
  title   = {Speech enhancement using a minimum-mean square error short-time
             spectral amplitude estimator},
  volume  = {32},
  number  = {6},
  urldate = {2015-11-23},
  journal = {Acoustics, Speech and Signal Processing, IEEE Transactions on},
  author  = {Ephraim, Yariv and Malah, David},
  year    = {1984},
  pages   = {1109--1121},
  file    = {Ephraim and Malah - 1984 - Speech enhancement using a minimum-mean
             square err.pdf:/Users/apodusenko/Zotero/storage/NRGGGSQK/Ephraim and
             Malah - 1984 - Speech enhancement using a minimum-mean square
             err.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/WI2CZRMJ/abs_all.html:text/html
             }
}

@phdthesis{liao_uncertainty_2009,
  title   = {Uncertainty decoding for noise robust speech recognition},
  urldate = {2015-11-17},
  school  = {University of Cambridge},
  author  = {Liao, Hank and Gales, Mark JF},
  year    = {2009},
  file    = {Liao and Gales - 2009 - Uncertainty decoding for noise robust speech
             recog.pdf:/Users/apodusenko/Zotero/storage/S7WDNCB4/Liao and Gales -
             2009 - Uncertainty decoding for noise robust speech
             recog.pdf:application/pdf}
}

@phdthesis{moreno_speech_1996,
  title   = {Speech recognition in noisy environments},
  urldate = {2015-11-16},
  school  = {Carnegie Mellon University Pittsburgh},
  author  = {Moreno, Pedro J.},
  year    = {1996},
  file    = {Moreno - 1996 - Speech recognition in noisy
             environments.pdf:/Users/apodusenko/Zotero/storage/7IZZCPNA/Moreno -
             1996 - Speech recognition in noisy environments.pdf:application/pdf}
}

@article{nadas_speech_1989,
  title   = {Speech recognition using noise-adaptive prototypes},
  volume  = {37},
  number  = {10},
  urldate = {2015-11-19},
  journal = {Acoustics, Speech and Signal Processing, IEEE Transactions on},
  author  = {Nádas, Arthur and Nahamoo, David and Picheny, Michael and {others}},
  year    = {1989},
  pages   = {1495--1503},
  file    = {Nádas et al. - 1989 - Speech recognition using noise-adaptive
             prototypes.pdf:/Users/apodusenko/Zotero/storage/P39HPPQ4/Nádas et al. -
             1989 - Speech recognition using noise-adaptive
             prototypes.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/ASD8HXM9/abs_all.html:text/html
             }
}

@misc{noauthor_julia_nodate-2,
  title    = {Julia},
  url      = {https://github.com/JuliaLang/julia},
  abstract = {julia - The Julia Language: A fresh approach to technical
              computing.},
  urldate  = {2015-10-22},
  file     = {
              Snapshot:/Users/apodusenko/Zotero/storage/MWVH5ZID/julia.html:text/html
              }
}

@article{penny_efficient_2013,
  title    = {Efficient {Posterior} {Probability} {Mapping} {Using} {Savage}-{
              Dickey} {Ratios}},
  volume   = {8},
  doi      = {10.1371/journal.pone.0059655},
  abstract = {Statistical Parametric Mapping (SPM) is the dominant paradigm for
              mass-univariate analysis of neuroimaging data. More recently, a
              Bayesian approach termed Posterior Probability Mapping (PPM) has
              been proposed as an alternative. PPM offers two advantages: (i)
              inferences can be made about effect size thus lending a precise
              physiological meaning to activated regions, (ii) regions can be
              declared inactive. This latter facility is most parsimoniously
              provided by PPMs based on Bayesian model comparisons. To date these
              comparisons have been implemented by an Independent Model
              Optimization (IMO) procedure which separately fits null and
              alternative models. This paper proposes a more computationally
              efficient procedure based on Savage-Dickey approximations to the
              Bayes factor, and Taylor-series approximations to the voxel-wise
              posterior covariance matrices. Simulations show the accuracy of
              this Savage-Dickey-Taylor (SDT) method to be comparable to that of
              IMO. Results on fMRI data show excellent agreement between SDT and
              IMO for second-level models, and reasonable agreement for
              first-level models. This Savage-Dickey test is a Bayesian analogue
              of the classical SPM-F and allows users to implement model
              comparison in a truly interactive manner.},
  number   = {3},
  urldate  = {2014-12-02},
  journal  = {PLoS ONE},
  author   = {Penny, William D. and Ridgway, Gerard R.},
  month    = mar,
  year     = {2013},
  file     = {Penny and Ridgway - 2013 - Efficient Posterior Probability Mapping
              Using Sava.pdf:/Users/apodusenko/Zotero/storage/WVED7W4H/Penny and
              Ridgway - 2013 - Efficient Posterior Probability Mapping Using
              Sava.pdf:application/pdf;PLoS
              Snapshot:/Users/apodusenko/Zotero/storage/QJ9R7UTQ/infodoi10.1371journal.pone.html:text/html
              }
}

@phdthesis{acero_acoustical_1990,
  title   = {Acoustical and environmental robustness in automatic speech
             recognition},
  urldate = {2015-11-16},
  school  = {Carnegie Mellon University Pittsburgh},
  author  = {Acero, Alejandro},
  year    = {1990},
  file    = {[PDF] from
             microsoft.com:/Users/apodusenko/Zotero/storage/4XWP6QPJ/acero_thesis.pdf:application/pdf
             }
}

@inproceedings{martin_speech_2008,
  title     = {Speech enhancement in hearing aids - from noise suppression to
               rendering of auditory scenes},
  isbn      = {978-1-4244-2481-8},
  doi       = {10.1109/EEEI.2008.4736547},
  urldate   = {2015-10-07},
  publisher = {IEEE},
  author    = {Martin, Rainer and Enzner, Gerald},
  month     = mar,
  year      = {2008},
  pages     = {363--367},
  file      = {Martin and Enzner - 2008 - Speech enhancement in hearing aids - from
               noise su.pdf:/Users/apodusenko/Zotero/storage/NUHT273N/Martin and
               Enzner - 2008 - Speech enhancement in hearing aids - from noise
               su.pdf:application/pdf}
}

@inproceedings{maas_bayesian_2014,
  title     = {A {Bayesian} network view on linear and nonlinear acoustic echo
               cancellation},
  doi       = {10.1109/ChinaSIP.2014.6889292},
  abstract  = {In this contribution, we provide a new derivation of the
               normalized least mean square (NLMS) algorithm from a machine
               learning perspective. By applying the inference rules of Bayesian
               networks to a linear observation model, the NLMS can be shown to
               arise as a modification of the Kalman filter equations. Based on a
               nonlinear observation model, we exemplify the benefit of the
               Bayesian point of view by employing the technique of particle
               filtering to realize a tractable algorithm for nonlinear acoustic
               echo cancellation. Experiments carried out on real smartphone
               recordings reveal the remarkable performance of the new approach.},
  booktitle = {2014 {IEEE} {China} {Summit} {International} {Conference} on {
               Signal} and {Information} {Processing} ({ChinaSIP})},
  author    = {Maas, R. and Huemmer, C. and Schwarz, A. and Hofmann, C. and
               Kellermann, W.},
  month     = jul,
  year      = {2014},
  keywords  = {Kalman filters, Bayes methods, Random variables, least mean
               squares methods, Speech, Bayesian networks, Bayesian network,
               acoustic signal processing, Acoustics, echo suppression, Vectors,
               Speech processing, adaptive filtering, particle filtering
               (numerical methods), Kalman filter equations, linear acoustic echo
               cancellation, linear observation model, machine learning for signal
               processing, machine learning perspective, NLMS, nonlinear acoustic
               echo cancellation, nonlinear observation model, normalized least
               mean square algorithm, particle filtering technique, real
               smartphone recordings, smart phones, system identification,
               tractable algorithm},
  pages     = {495--499},
  file      = {IEEE Xplore Abstract
               Record:/Users/apodusenko/Zotero/storage/3NQSK3JP/abstractReferences.html:text/html;Maas
               et al. - 2014 - A Bayesian network viewon linear and nonlinear
               aco.pdf:/Users/apodusenko/Zotero/storage/TARC3ZHT/Maas et al. - 2014 -
               A Bayesian network viewon linear and nonlinear aco.pdf:application/pdf}
}

@unpublished{de_vries_situ_2013,
  type    = {inaugural lecture report},
  title   = {In situ personalization of signal processing systems},
  url     = {http://repository.tue.nl/758255},
  urldate = {2015-10-07},
  author  = {de Vries, Bert},
  month   = sep,
  year    = {2013},
  file    = {In situ personalization of signal processing
             systems:/Users/apodusenko/Zotero/storage/AJJDUCHA/758255.html:text/html
             }
}

@misc{cremer_very_2011,
  title  = {a very minimal introduction to tikz bibtex - {Google} {Search}},
  url    = {
            https://www.google.nl/search?client=ubuntu&hs=hvK&q=a+very+minimal+introduction+to+tikz+bibtex&oq=a+very+minimal+introduction+to+tikz+bibtex&gs_l=serp.3...6324.9633.0.10505.9.8.1.0.0.0.71.521.8.8.0....0...1c.1.64.serp..0.3.197...0i22i30j33i21.7ZRrlX1z9eA
            },
  author = {Cremer, Jacques},
  year   = {2011},
  note   = {(Accessed on 07/24/2016)},
  file   = {Cremer - 2011 - a very minimal introduction to tikz bibtex -
            Googl.pdf:/Users/apodusenko/Zotero/storage/BKR9G9FU/Cremer - 2011 - a
            very minimal introduction to tikz bibtex - Googl.pdf:application/pdf}
}

@article{kleijn_simple_2015,
  title    = {A {Simple} {Model} of {Speech} {Communication} and its {Application}
              to {Intelligibility} {Enhancement}},
  volume   = {22},
  issn     = {1070-9908},
  doi      = {10.1109/LSP.2014.2351784},
  abstract = {We introduce a model of communication that includes noise inherent
              in the message production process as well as noise inherent in the
              message interpretation process. The production and interpretation
              noise processes have a fixed signal-to-noise ratio. The resulting
              system is a simple but effective model of human communication. The
              model naturally leads to a method to enhance the intelligibility of
              speech rendered in a noisy environment. State-of-the-art
              experimental results confirm the practical value of the model.},
  number   = {3},
  journal  = {IEEE Signal Processing Letters},
  author   = {Kleijn, W.B. and Hendriks, R.C.},
  month    = mar,
  year     = {2015},
  keywords = {Auditory system, intelligibility, Noise measurement, Signal to
              noise ratio, Speech, Enhancement, Indexes, Production},
  pages    = {303--307},
  file     = {IEEE Xplore Abstract
              Record:/Users/apodusenko/Zotero/storage/9QFIEQHD/abstractReferences.html:text/html;Kleijn
              and Hendriks - 2015 - A Simple Model of Speech Communication and its
              App.pdf:/Users/apodusenko/Zotero/storage/S8SIEQ7V/Kleijn and Hendriks -
              2015 - A Simple Model of Speech Communication and its
              App.pdf:application/pdf}
}

@inproceedings{jozefowicz_empirical_2015,
  title   = {An {Empirical} {Exploration} of {Recurrent} {Network} {Architectures}
             },
  url     = {http://machinelearning.wustl.edu/mlpapers/papers/icml2015_jozefowicz15},
  urldate = {2016-08-09},
  author  = {Jozefowicz, Rafal and Zaremba, Wojciech and Sutskever, Ilya},
  year    = {2015},
  pages   = {2342--2350},
  file    = {Jozefowicz et al. - 2015 - An Empirical Exploration of Recurrent
             Network Arch.pdf:/Users/apodusenko/Zotero/storage/5Y4AQZCJ/Jozefowicz
             et al. - 2015 - An Empirical Exploration of Recurrent Network
             Arch.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/HXT3RHV6/icml2015_jozefowicz15.html:text/html
             }
}

@article{ortega_minimum_2010,
  title    = {A {Minimum} {Relative} {Entropy} {Principle} for {Learning} and {
              Acting}},
  abstract = {This paper proposes a method to construct an adaptive agent that
              is universal with respect to a given class of experts, where each
              expert is designed specifically for a particular environment. This
              adaptive control problem is formalized as the problem of minimizing
              the relative entropy of the adaptive agent from the expert that is
              most suitable for the unknown environment. If the agent is a
              passive observer, then the optimal solution is the well-known
              Bayesian predictor. However, if the agent is active, then its past
              actions need to be treated as causal interventions on the I/O
              stream rather than normal probability conditions. Here it is shown
              that the solution to this new variational problem is given by a
              stochastic controller called the Bayesian control rule, which
              implements adaptive behavior as a mixture of experts. Furthermore,
              it is shown that under mild assumptions, the Bayesian control rule
              converges to the control law of the most suitable expert. 1.},
  journal  = {J. Artif. Intell. Res. 2010},
  author   = {Ortega, Pedro A. and Braun, Daniel A.},
  year     = {2010},
  pages    = {475--511},
  file     = {Citeseer -
              Snapshot:/Users/apodusenko/Zotero/storage/CSD6CS5H/summary.html:text/html;Ortega
              and Braun - 2010 - A Minimum Relative Entropy Principle for Learning
              .pdf:/Users/apodusenko/Zotero/storage/TEY58G3T/Ortega and Braun - 2010
              - A Minimum Relative Entropy Principle for Learning
              .pdf:application/pdf}
}

@article{kulick_active_2014,
  title      = {Active {Learning} of {Hyperparameters}: {An} {Expected} {Cross} {
                Entropy} {Criterion} for {Active} {Model} {Selection}},
  shorttitle = {Active {Learning} of {Hyperparameters}},
  url        = {http://arxiv.org/abs/1409.7552},
  abstract   = {In standard active learning, the learner's goal is to reduce the
                predictive uncertainty with as little data as possible. We consider
                a slightly different problem: the learner's goal is to uncover
                latent properties of the model---e.g., which features are relevant
                ("active feature selection"), or the choice of hyper
                parameters---with as little data as possible. While the two goals
                are clearly related, we give examples where following the
                predictive uncertainty objective is suboptimal for uncovering
                latent parameters. We propose novel measures of information gain
                about the latent parameter, based on the divergence between the
                prior and expected posterior distribution over the latent parameter
                in question. Notably, this is different from applying Bayesian
                experimental design to latent variables: we give explicit examples
                showing that the latter objective is prone to get stuck in local
                minima, unlike its application the standard predictive uncertainty.
                Extensive evaluations show that active learning using our measures
                significantly accelerates the uncovering of latent model parameters
                , as compared to standard version space approaches
                (Query-by-committee) as well as predictive uncertainty measures.},
  urldate    = {2015-04-01},
  journal    = {arXiv:1409.7552 [cs, stat]},
  author     = {Kulick, Johannes and Lieck, Robert and Toussaint, Marc},
  month      = sep,
  year       = {2014},
  note       = {arXiv: 1409.7552},
  keywords   = {Computer Science - Learning, Statistics - Machine Learning},
  file       = {arXiv.org
                Snapshot:/Users/apodusenko/Zotero/storage/X3XRTGB6/1409.html:text/html;Kulick
                et al. - 2014 - Active Learning of Hyperparameters An Expected
                Cr.pdf:/Users/apodusenko/Zotero/storage/TIWFNCX3/Kulick et al. - 2014 -
                Active Learning of Hyperparameters An Expected Cr.pdf:application/pdf}
}

@article{ortega_conversion_2009,
  title    = {A conversion between utility and information},
  url      = {http://arxiv.org/abs/0911.5106},
  abstract = {Rewards typically express desirabilities or preferences over a set
              of alternatives. Here we propose that rewards can be defined for
              any probability distribution based on three desiderata, namely that
              rewards should be real-valued, additive and order-preserving, where
              the latter implies that more probable events should also be more
              desirable. Our main result states that rewards are then uniquely
              determined by the negative information content. To analyze
              stochastic processes, we define the utility of a realization as its
              reward rate. Under this interpretation, we show that the expected
              utility of a stochastic process is its negative entropy rate.
              Furthermore, we apply our results to analyze agent-environment
              interactions. We show that the expected utility that will actually
              be achieved by the agent is given by the negative cross-entropy
              from the input-output (I/O) distribution of the coupled interaction
              system and the agent's I/O distribution. Thus, our results allow
              for an information-theoretic interpretation of the notion of
              utility and the characterization of agent-environment interactions
              in terms of entropy dynamics.},
  urldate  = {2015-03-04},
  journal  = {arXiv:0911.5106 [cs, math]},
  author   = {Ortega, Pedro A. and Braun, Daniel A.},
  month    = nov,
  year     = {2009},
  note     = {arXiv: 0911.5106},
  keywords = {Computer Science - Artificial Intelligence, Computer Science -
              Information Theory},
  file     = {arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/QFRTC75X/0911.html:text/html;Ortega
              and Braun - 2009 - A conversion between utility and
              information.pdf:/Users/apodusenko/Zotero/storage/QFDT5JK2/Ortega and
              Braun - 2009 - A conversion between utility and
              information.pdf:application/pdf}
}

@article{ram_git_2013,
  title     = {Git can facilitate greater reproducibility and increased transparency
               in science},
  volume    = {8},
  copyright = {2013 Ram; licensee BioMed Central Ltd.},
  issn      = {1751-0473},
  url       = {http://www.scfbm.org/content/8/1/7/abstract},
  doi       = {10.1186/1751-0473-8-7},
  abstract  = {PMID: 23448176},
  language  = {en},
  number    = {1},
  urldate   = {2015-03-25},
  journal   = {Source Code for Biology and Medicine},
  author    = {Ram, Karthik},
  month     = feb,
  year      = {2013},
  pmid      = {23448176},
  keywords  = {Open science, Reproducible research, Version control},
  pages     = {7},
  file      = {Ram - 2013 - Git can facilitate greater reproducibility and
               inc.pdf:/Users/apodusenko/Zotero/storage/PAAT2B4C/Ram - 2013 - Git can
               facilitate greater reproducibility and
               inc.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/LD5YXJNF/7.html:text/html
               }
}

@article{van_waterschoot_fifty_2011,
  title      = {Fifty years of acoustic feedback control: state of the art and future
                challenges},
  shorttitle = {Fifty years of acoustic feedback control},
  url        = {
                ftp://ftp.esat.kuleuven.ac.be/pub/pub/stadius/vanwaterschoot/reports/08-13.pdf
                },
  number     = {99},
  urldate    = {2015-12-16},
  journal    = {Proceedings of the IEEE},
  author     = {Van Waterschoot, Toon and Moonen, Marc},
  year       = {2011},
  pages      = {1--40},
  file       = {Van Waterschoot and Moonen - 2011 - Fifty years of acoustic feedback
                control state of.pdf:/Users/apodusenko/Zotero/storage/K8UQSDBB/Van
                Waterschoot and Moonen - 2011 - Fifty years of acoustic feedback
                control state of.pdf:application/pdf}
}

@article{schwartenbeck_computational_2016,
  title      = {Computational phenotyping in psychiatry: a worked example},
  copyright  = {Copyright © 2016 Schwartenbeck and Friston},
  issn       = {2373-2822},
  shorttitle = {Computational phenotyping in psychiatry},
  url        = {http://eneuro.org/content/early/2016/07/18/ENEURO.0049-16.2016},
  doi        = {10.1523/ENEURO.0049-16.2016},
  abstract   = {Computational Psychiatry is a rapidly emerging field that uses
                model-based quantities to infer the behavioural and neuronal
                abnormalities that underlie psychopathology. If successful, this
                approach promises key insights into (pathological) brain function
                as well as a more mechanistic and quantitative approach to
                psychiatric nosology – structuring therapeutic interventions and
                predicting response and relapse. The basic procedure in
                computational psychiatry is to build a computational model that
                formalises a behavioural or neuronal process. Measured behavioural
                (or neuronal) responses are then used to infer the model parameters
                of a single subject or a group of subjects. Here, we provide an
                illustrative overview over this process, starting from the
                modelling of choice behaviour in a specific task, simulating data
                and then inverting that model to estimate group effects. Finally,
                we illustrate cross-validation to assess whether between-subject
                variables (e.g., diagnosis) can be recovered successfully. Our
                worked example uses a simple two-step maze task and a model of
                choice behaviour based on (active) inference and Markov decision
                processes. The procedural steps and routines we illustrate are not
                restricted to a specific field of research or particular
                computational model but can, in principle, be applied in many
                domains of computational psychiatry. Significance Statement: We
                provide an overview over the process of using formal models to
                understand psychiatric conditions, which is central in the emerging
                research field of ‘Computational Psychiatry’. This approach
                promises key insights into both healthy and pathological brain
                function as well as a more mechanistic understanding of psychiatric
                nosology, which may have important consequences for therapeutic
                interventions or predicting response and relapse. In a worked
                example, we discuss the generic aspects of using a computational
                model to formalise a task, simulating data and estimating
                parameters as well as inferring group effects between patients and
                healthy controls. We also provide routines that can be used for
                these steps and are freely available in the academic software SPM.},
  language   = {en},
  urldate    = {2016-08-03},
  journal    = {eneuro},
  author     = {Schwartenbeck, Philipp and Friston, Karl},
  month      = jul,
  year       = {2016},
  pages      = {ENEURO.0049--16.2016},
  file       = {Schwartenbeck and Friston - 2016 - Computational phenotyping in
                psychiatry a worked
                .pdf:/Users/apodusenko/Zotero/storage/M8KDV3J6/Schwartenbeck and
                Friston - 2016 - Computational phenotyping in psychiatry a worked
                .pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/33YQYF44/ENEURO.0049-16.2016.html:text/html
                }
}

@article{boettiger_introduction_2015,
  title    = {An introduction to {Docker} for reproducible research, with examples
              from the {R} environment},
  volume   = {49},
  issn     = {01635980},
  url      = {http://arxiv.org/abs/1410.0846},
  doi      = {10.1145/2723872.2723882},
  abstract = {As computational work becomes more and more integral to many
              aspects of scientific research, computational reproducibility has
              become an issue of increasing importance to computer systems
              researchers and domain scientists alike. Though computational
              reproducibility seems more straight forward than replicating
              physical experiments, the complex and rapidly changing nature of
              computer environments makes being able to reproduce and extend such
              work a serious challenge. In this paper, I explore common reasons
              that code developed for one research project cannot be successfully
              executed or extended by subsequent researchers. I review current
              approaches to these issues, including virtual machines and workflow
              systems, and their limitations. I then examine how the popular
              emerging technology Docker combines several areas from systems
              research - such as operating system virtualization, cross-platform
              portability, modular re-usable elements, versioning, and a `DevOps'
              philosophy, to address these challenges. I illustrate this with
              several examples of Docker use with a focus on the R statistical
              environment.},
  number   = {1},
  urldate  = {2016-01-30},
  journal  = {ACM SIGOPS Operating Systems Review},
  author   = {Boettiger, Carl},
  month    = jan,
  year     = {2015},
  note     = {arXiv: 1410.0846},
  keywords = {Computer Science - Software Engineering},
  pages    = {71--79},
  file     = {arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/UL67JZ4C/1410.html:text/html;Boettiger
              - 2015 - An introduction to Docker for reproducible
              researc.pdf:/Users/apodusenko/Zotero/storage/6MUEH2TQ/Boettiger - 2015
              - An introduction to Docker for reproducible
              researc.pdf:application/pdf}
}

@article{sato_bayesian_2011,
  title    = {A {Bayesian} {Model} of {Sensory} {Adaptation}},
  volume   = {6},
  url      = {http://dx.doi.org/10.1371/journal.pone.0019377},
  doi      = {10.1371/journal.pone.0019377},
  abstract = {Recent studies reported two opposite types of adaptation in
              temporal perception. Here, we propose a Bayesian model of sensory
              adaptation that exhibits both types of adaptation. We regard
              adaptation as the adaptive updating of estimations of time-evolving
              variables, which determine the mean value of the likelihood
              function and that of the prior distribution in a Bayesian model of
              temporal perception. On the basis of certain assumptions, we can
              analytically determine the mean behavior in our model and identify
              the parameters that determine the type of adaptation that actually
              occurs. The results of our model suggest that we can control the
              type of adaptation by controlling the statistical properties of the
              stimuli presented.},
  number   = {4},
  urldate  = {2015-03-29},
  journal  = {PLoS ONE},
  author   = {Sato, Yoshiyuki and Aihara, Kazuyuki},
  month    = apr,
  year     = {2011},
  pages    = {e19377},
  file     = {Sato and Aihara - 2011 - A Bayesian Model of Sensory
              Adaptation.pdf:/Users/apodusenko/Zotero/storage/MTYLXWPV/Sato and
              Aihara - 2011 - A Bayesian Model of Sensory
              Adaptation.pdf:application/pdf}
}

@article{aponte_mpdcm:_2016,
  title      = {mpdcm: {A} toolbox for massively parallel dynamic causal modeling},
  volume     = {257},
  issn       = {0165-0270},
  shorttitle = {mpdcm},
  url        = {http://www.sciencedirect.com/science/article/pii/S0165027015003428},
  doi        = {10.1016/j.jneumeth.2015.09.009},
  abstract   = {Background Dynamic causal modeling (DCM) for fMRI is an
                established method for Bayesian system identification and inference
                on effective brain connectivity. DCM relies on a biophysical model
                that links hidden neuronal activity to measurable BOLD signals.
                Currently, biophysical simulations from DCM constitute a serious
                computational hindrance. Here, we present Massively Parallel
                Dynamic Causal Modeling (mpdcm), a toolbox designed to address this
                bottleneck. New method mpdcm delegates the generation of
                simulations from DCM's biophysical model to graphical processing
                units (GPUs). Simulations are generated in parallel by implementing
                a low storage explicit Runge–Kutta's scheme on a GPU architecture.
                mpdcm is publicly available under the GPLv3 license. Results We
                found that mpdcm efficiently generates large number of simulations
                without compromising their accuracy. As applications of mpdcm, we
                suggest two computationally expensive sampling algorithms:
                thermodynamic integration and parallel tempering. Comparison with
                existing method(s) mpdcm is up to two orders of magnitude more
                efficient than the standard implementation in the software package
                SPM. Parallel tempering increases the mixing properties of the
                traditional Metropolis–Hastings algorithm at low computational cost
                given efficient, parallel simulations of a model. Conclusions
                Future applications of DCM will likely require increasingly large
                computational resources, for example, when the likelihood landscape
                of a model is multimodal, or when implementing sampling methods for
                multi-subject analysis. Due to the wide availability of GPUs,
                algorithmic advances can be readily available in the absence of
                access to large computer grids, or when there is a lack of
                expertise to implement algorithms in such grids.},
  urldate    = {2017-02-21},
  journal    = {Journal of Neuroscience Methods},
  author     = {Aponte, Eduardo A. and Raman, Sudhir and Sengupta, Biswa and Penny,
                Will D. and Stephan, Klaas E. and Heinzle, Jakob},
  month      = jan,
  year       = {2016},
  keywords   = {Bayesian model comparison, Dynamic causal modeling, GPU, Markov
                chain Monte Carlo, Model evidence, Model inversion, Parallel
                tempering, Thermodynamic integration},
  pages      = {7--16},
  file       = {ScienceDirect
                Snapshot:/Users/apodusenko/Zotero/storage/PTRE3FQR/S0165027015003428.html:text/html
                }
}

@article{spratling_review_2017,
  series   = {Perspectives on {Human} {Probabilistic} {Inferences} and the '{
              Bayesian} {Brain}'},
  title    = {A review of predictive coding algorithms},
  volume   = {112},
  issn     = {0278-2626},
  url      = {http://www.sciencedirect.com/science/article/pii/S027826261530035X},
  doi      = {10.1016/j.bandc.2015.11.003},
  abstract = {Predictive coding is a leading theory of how the brain performs
              probabilistic inference. However, there are a number of distinct
              algorithms which are described by the term “predictive coding”.
              This article provides a concise review of these different
              predictive coding algorithms, highlighting their similarities and
              differences. Five algorithms are covered: linear predictive coding
              which has a long and influential history in the signal processing
              literature; the first neuroscience-related application of
              predictive coding to explaining the function of the retina; and
              three versions of predictive coding that have been proposed to
              model cortical function. While all these algorithms aim to fit a
              generative model to sensory data, they differ in the type of
              generative model they employ, in the process used to optimise the
              fit between the model and sensory data, and in the way that they
              are related to neurobiology.},
  urldate  = {2017-03-26},
  journal  = {Brain and Cognition},
  author   = {Spratling, M. W.},
  month    = mar,
  year     = {2017},
  keywords = {predictive coding, Free energy, Cortex, Neural networks, Retina},
  pages    = {92--97},
  file     = {ScienceDirect
              Snapshot:/Users/apodusenko/Zotero/storage/ZRF5CY9T/S027826261530035X.html:text/html;Spratling
              - 2017 - A review of predictive coding
              algorithms.pdf:/Users/apodusenko/Zotero/storage/BXBI7P9C/Spratling -
              2017 - A review of predictive coding algorithms.pdf:application/pdf}
}

@incollection{friston_variational_2017,
  series    = {Nonlinear {Systems} and {Complexity}},
  title     = {The {Variational} {Principles} of {Cognition}},
  copyright = {©2017 Springer International Publishing AG},
  isbn      = {978-3-319-53672-9 978-3-319-53673-6},
  url       = {http://link.springer.com/chapter/10.1007/978-3-319-53673-6_12},
  abstract  = {This chapter provides a theoretical perspective on a dynamics,
               from the perspective of the free-energy principle. This variational
               principle offers a natural explanation for neuronal activity that
               is formulated in terms of dynamical systems and attracting sets. We
               will see that the free-energy principle emerges when we consider
               the ensemble dynamics of any pattern forming, self-organizing
               system. When we look closely what this principle implies for the
               behavior of systems like the brain, one finds a fairly simple
               explanation for active inference and the Bayesian brain hypothesis.
               Within the Bayesian brain framework, the ensuing dynamics can be
               separated, in a principled way, into those serving perceptual
               inference, learning and behavior. Dynamics here are central; not
               only to an understanding the nature of self-organizing systems but
               also to explain the adaptive nature of neuronal dynamics and
               plasticity in terms of optimization. The special focus of this
               chapter is on the pre-eminent role of heteroclinic cycles in
               providing deep and dynamic (generative) models of the sensorium;
               particularly, the sensations that we generate ourselves through
               movement. In what follows, we will briefly rehearse the basic
               theory and illustrate its implications using simulations of action
               (handwriting)—and its observation.},
  language  = {en},
  number    = {20},
  urldate   = {2017-05-09},
  booktitle = {Advances in {Dynamics}, {Patterns}, {Cognition}},
  publisher = {Springer International Publishing},
  author    = {Friston, Karl},
  editor    = {Aranson, Igor S. and Pikovsky, Arkady and Rulkov, Nikolai F. and
               Tsimring, Lev S.},
  year      = {2017},
  doi       = {10.1007/978-3-319-53673-6_12},
  keywords  = {Free energy, Entropy, Pattern Recognition, Bayesian brain,
               Applications of Nonlinear Dynamics and Chaos Theory, Complex
               Systems, Complexity, Dynamics, Neural activity},
  pages     = {189--211},
  file      = {Friston - 2017 - The Variational Principles of
               Cognition.pdf:/Users/apodusenko/Zotero/storage/JJA5JXIM/Friston - 2017
               - The Variational Principles of
               Cognition.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/YXGQXCQR/978-3-319-53673-6_12.html:text/html
               }
}

@article{robert_expected_2016,
  series   = {Bayes {Factors} for {Testing} {Hypotheses} in {Psychological} {
              Research}: {Practical} {Relevance} and {New} {Developments}},
  title    = {The expected demise of the {Bayes} factor},
  volume   = {72},
  issn     = {0022-2496},
  url      = {http://www.sciencedirect.com/science/article/pii/S0022249615000504},
  doi      = {10.1016/j.jmp.2015.08.002},
  abstract = {This note is a discussion commenting on the paper by Ly et al. on
              “Harold Jeffreys’s Default Bayes Factor Hypothesis Tests:
              Explanation, Extension, and Application in Psychology” and on the
              perceived shortcomings of the classical Bayesian approach to
              testing, while reporting on an alternative approach advanced by
              Kamary et al. (2014) as a solution to this quintessential inference
              problem.},
  urldate  = {2016-11-09},
  journal  = {Journal of Mathematical Psychology},
  author   = {Robert, Christian P.},
  month    = jun,
  year     = {2016},
  keywords = {Bayesian inference, loss function, Bayes factor, Consistency,
              Decision theory, evidence, Mixtures of distributions, Testing of
              hypotheses},
  pages    = {33--37},
  file     = {Robert - 2016 - The expected demise of the Bayes
              factor.pdf:/Users/apodusenko/Zotero/storage/FU2U9RVT/Robert - 2016 -
              The expected demise of the Bayes
              factor.pdf:application/pdf;ScienceDirect
              Snapshot:/Users/apodusenko/Zotero/storage/DH5EKGSN/S0022249615000504.html:text/html
              }
}

@article{sedley_neural_2016,
  title     = {Neural signatures of perceptual inference},
  volume    = {5},
  copyright = {© 2016, Sedley et al. This article is distributed under the terms
               of the Creative Commons Attribution License, which permits
               unrestricted use and redistribution provided that the original
               author and source are credited.},
  issn      = {2050-084X},
  url       = {https://elifesciences.org/content/5/e11476v2},
  doi       = {10.7554/eLife.11476},
  abstract  = {Changes to sensory predictions are encoded by beta oscillations,
               surprise due to prediction violations by gamma oscillations, and
               alpha oscillations may have a role in controlling the precision of
               predictions.},
  language  = {en},
  urldate   = {2017-02-26},
  journal   = {eLife},
  author    = {Sedley, William and Gander, Phillip E. and Kumar, Sukhbinder and
               Kovach, Christopher K. and Oya, Hiroyuki and Kawasaki, Hiroto and Iii
               , Matthew A. Howard and Griffiths, Timothy D.},
  month     = mar,
  year      = {2016},
  pmid      = {26949254},
  keywords  = {predictive coding, Perception, Prediction error, auditory cortex,
               Human, predictions, surprise},
  pages     = {e11476},
  file      = {Sedley et al. - 2016 - Neural signatures of perceptual
               inference.pdf:/Users/apodusenko/Zotero/storage/MGZ7262T/Sedley et al. -
               2016 - Neural signatures of perceptual
               inference.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/UIZE64D4/e11476.html:text/html
               }
}

@article{obleser_tell_2016,
  title     = {Tell me something {I} don’t know},
  volume    = {5},
  copyright = {© 2016, Obleser. This article is distributed under the terms of
               the Creative Commons Attribution License, which permits
               unrestricted use and redistribution provided that the original
               author and source are credited.},
  issn      = {2050-084X},
  url       = {https://elifesciences.org/content/5/e15853v1},
  doi       = {10.7554/eLife.15853},
  abstract  = {The roles that neural oscillations play in the auditory cortex of
               the human brain are becoming clearer.},
  language  = {en},
  urldate   = {2017-02-26},
  journal   = {eLife},
  author    = {Obleser, Jonas},
  month     = apr,
  year      = {2016},
  pmid      = {27090088},
  keywords  = {predictive coding, Perception, Prediction error, auditory cortex,
               Human, predictions, surprise},
  pages     = {e15853},
  file      = {Obleser - 2016 - Tell me something I don’t
               know.pdf:/Users/apodusenko/Zotero/storage/GUBD666V/Obleser - 2016 -
               Tell me something I don’t
               know.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/YCUIE3YB/e15853.html:text/html
               }
}

@article{friston_embodied_2015,
  title      = {Embodied {Inference}: or “{I} think therefore {I} am, if {I} am what
                {I} think”},
  shorttitle = {Embodied {Inference}},
  abstract   = {This chapter considers situated and embodied cognition in terms of
                the free-energy principle. The free-energy formulation starts with
                the premise that biological agents must actively resist a natural
                tendency to disorder. It appeals to the idea that agents are
                essentially inference machines that},
  author     = {Friston, Karl},
  year       = {2015},
  file       = {Citeseer -
                Snapshot:/Users/apodusenko/Zotero/storage/E7NJCTME/summary.html:text/html;Friston
                - Embodied Inference or “I think therefore I am,
                if.pdf:/Users/apodusenko/Zotero/storage/NBXDZU8N/Friston - Embodied
                Inference or “I think therefore I am, if.pdf:application/pdf}
}

@article{mcgraw_personalized_2016,
  title    = {Personalized {Speech} recognition on mobile devices},
  url      = {http://arxiv.org/abs/1603.03185},
  abstract = {We describe a large vocabulary speech recognition system that is
              accurate, has low latency, and yet has a small enough memory and
              computational footprint to run faster than real-time on a Nexus 5
              Android smartphone. We employ a quantized Long Short-Term Memory
              (LSTM) acoustic model trained with connectionist temporal
              classification (CTC) to directly predict phoneme targets, and
              further reduce its memory footprint using an SVD-based compression
              scheme. Additionally, we minimize our memory footprint by using a
              single language model for both dictation and voice command domains,
              constructed using Bayesian interpolation. Finally, in order to
              properly handle device-specific information, such as proper names
              and other context-dependent information, we inject vocabulary items
              into the decoder graph and bias the language model on-the-fly. Our
              system achieves 13.5\% word error rate on an open-ended dictation
              task, running with a median speed that is seven times faster than
              real-time.},
  urldate  = {2016-11-02},
  journal  = {arXiv:1603.03185 [cs]},
  author   = {McGraw, Ian and Prabhavalkar, Rohit and Alvarez, Raziel and Arenas,
              Montse Gonzalez and Rao, Kanishka and Rybach, David and Alsharif,
              Ouais and Sak, Hasim and Gruenstein, Alexander and Beaufays,
              Francoise and Parada, Carolina},
  month    = mar,
  year     = {2016},
  note     = {arXiv: 1603.03185},
  keywords = {Computer Science - Learning, Computer Science - Sound, Computer
              Science - Computation and Language},
  file     = {arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/DM8FF9ZX/1603.html:text/html;McGraw
              et al. - 2016 - Personalized Speech recognition on mobile
              devices.pdf:/Users/apodusenko/Zotero/storage/V8X4X29U/McGraw et al. -
              2016 - Personalized Speech recognition on mobile
              devices.pdf:application/pdf}
}

@inproceedings{pereyra_comparing_2016,
  title  = {Comparing {Bayesian} models in the absence of ground truth},
  author = {Pereyra, Marcelo and McLaughlin, Steve},
  year   = {2016},
  file   = {Pereyra and McLaughlin - 2016 - Comparing Bayesian models in the
            absence of ground.pdf:/Users/apodusenko/Zotero/storage/IHJH3Y2H/Pereyra
            and McLaughlin - 2016 - Comparing Bayesian models in the absence of
            ground.pdf:application/pdf}
}

@article{abbeel_toward_2016,
  title      = {Toward a {Science} of {Autonomy} for {Physical} {Systems}: {Paths}},
  shorttitle = {Toward a {Science} of {Autonomy} for {Physical} {Systems}},
  url        = {http://arxiv.org/abs/1609.05814},
  abstract   = {An Autonomous Physical System (APS) will be expected to reliably
                and independently evaluate, execute, and achieve goals while
                respecting surrounding rules, laws, or conventions. In doing so, an
                APS must rely on a broad spectrum of dynamic, complex, and often
                imprecise information about its surroundings, the task it is to
                perform, and its own sensors and actuators. For example, cleaning
                in a home or commercial setting requires the ability to perceive,
                grasp, and manipulate many physical objects, the ability to
                reliably perform a variety of subtasks such as washing, folding,
                and stacking, and knowledge about local conventions such as how
                objects are classified and where they should be stored. The
                information required for reliable autonomous operation may come
                from external sources and from the robot's own sensor observations
                or in the form of direct instruction by a trainer. Similar
                considerations apply across many domains - construction,
                manufacturing, in-home assistance, and healthcare. For example,
                surgeons spend many years learning about physiology and anatomy
                before they touch a patient. They then perform roughly 1000
                surgeries under the tutelage of an expert surgeon, and they
                practice basic maneuvers such as suture tying thousands of times
                outside the operating room. All of these elements come together to
                achieve expertise at this task. Endowing a system with robust
                autonomy by traditional programming methods has thus far had
                limited success. Several promising new paths to acquiring and
                processing such data are emerging. This white paper outlines three
                promising research directions for enabling an APS to learn the
                physical and information skills necessary to perform tasks with
                independence and flexibility: Deep Reinforcement Learning,
                Human-Robot Interaction, and Cloud Robotics.},
  urldate    = {2016-11-10},
  journal    = {arXiv:1609.05814 [cs]},
  author     = {Abbeel, Pieter and Goldberg, Ken and Hager, Gregory and Shah, Julie},
  month      = sep,
  year       = {2016},
  note       = {arXiv: 1609.05814},
  keywords   = {Computer Science - Computers and Society, Computer Science -
                Robotics},
  file       = {Abbeel et al. - 2016 - Toward a Science of Autonomy for Physical
                Systems.pdf:/Users/apodusenko/Zotero/storage/UXZ9B7SP/Abbeel et al. -
                2016 - Toward a Science of Autonomy for Physical
                Systems.pdf:application/pdf;arXiv.org
                Snapshot:/Users/apodusenko/Zotero/storage/LHPYVIFU/1609.html:text/html}
}

@article{wostmann_tracking_2016,
  title      = {Tracking the signal, cracking the code: speech and speech
                comprehension in non-invasive human electrophysiology},
  volume     = {0},
  issn       = {2327-3798},
  shorttitle = {Tracking the signal, cracking the code},
  url        = {http://dx.doi.org/10.1080/23273798.2016.1262051},
  doi        = {10.1080/23273798.2016.1262051},
  abstract   = {Magneto- and electroencephalographic (M/EEG) signals recorded from
                the human scalp have allowed for substantial advances for neural
                models of speech comprehension over the past decades. These methods
                are currently advancing rapidly and continue to offer unparalleled
                insight in the near-to-real-time neural dynamics of speech
                processing. We provide a historically informed overview over
                dependent measures in the time and frequency domain and highlight
                recent advances resulting from these measures. We discuss the
                notorious challenges (and solutions) speech and language
                researchers are faced with when studying auditory brain responses
                in M/EEG. We argue that a key to understanding the neural basis of
                speech comprehension will lie in studying interactions between the
                neural tracking of speech and the functional neural network
                dynamics. This article is intended for both, non-experts who want
                to learn how to use M/EEG to study speech comprehension and
                scholars aiming for an overview of state-of-the-art M/EEG analysis
                methods.},
  number     = {0},
  urldate    = {2017-02-26},
  journal    = {Language, Cognition and Neuroscience},
  author     = {Wöstmann, Malte and Fiedler, Lorenz and Obleser, Jonas},
  month      = dec,
  year       = {2016},
  keywords   = {Speech, Electroencephalogram, language, magnetoencephalogram},
  pages      = {1--15},
  file       = {
                Snapshot:/Users/apodusenko/Zotero/storage/HJ3SZKHS/23273798.2016.html:text/html;Wöstmann
                et al. - 2016 - Tracking the signal, cracking the code speech
                and.pdf:/Users/apodusenko/Zotero/storage/X8U96ILL/Wöstmann et al. -
                2016 - Tracking the signal, cracking the code speech
                and.pdf:application/pdf}
}

@article{shapiro_21st_2005,
  title      = {A 21st century view of evolution: genome system architecture,
                repetitive {DNA}, and natural genetic engineering},
  volume     = {345},
  issn       = {0378-1119},
  shorttitle = {A 21st century view of evolution},
  doi        = {10.1016/j.gene.2004.11.020},
  abstract   = {The last 50 years of molecular genetics have produced an abundance
                of new discoveries and data that make it useful to revisit some
                basic concepts and assumptions in our thinking about genomes and
                evolution. Chief among these observations are the complex
                modularity of genome organization, the biological ubiquity of
                mobile and repetitive DNA sequences, and the fundamental importance
                of DNA rearrangements in the evolution of sequenced genomes. This
                review will take a broad overview of these developments and suggest
                some new ways of thinking about genomes as sophisticated informatic
                storage systems and about evolution as a systems engineering
                process.},
  language   = {eng},
  number     = {1},
  journal    = {Gene},
  author     = {Shapiro, James A.},
  month      = jan,
  year       = {2005},
  pmid       = {15716117},
  keywords   = {Humans, Animals, DNA Transposable Elements, Evolution, Molecular,
                Gene Rearrangement, Genes, Immunoglobulin, Genome, Mutagenesis,
                Insertional, Repetitive Sequences, Nucleic Acid},
  pages      = {91--100},
  file       = {Shapiro - 2005 - A 21st century view of evolution genome system
                ar.pdf:/Users/apodusenko/Zotero/storage/SKNRHKLF/Shapiro - 2005 - A
                21st century view of evolution genome system ar.pdf:application/pdf}
}

@article{duan_rl$^2$:_2016,
  title      = {{RL}\${\textasciicircum}2\$: {Fast} {Reinforcement} {Learning} via {
                Slow} {Reinforcement} {Learning}},
  shorttitle = {{RL}\${\textasciicircum}2\$},
  url        = {http://arxiv.org/abs/1611.02779},
  abstract   = {Deep reinforcement learning (deep RL) has been successful in
                learning sophisticated behaviors automatically; however, the
                learning process requires a huge number of trials. In contrast,
                animals can learn new tasks in just a few trials, benefiting from
                their prior knowledge about the world. This paper seeks to bridge
                this gap. Rather than designing a "fast" reinforcement learning
                algorithm, we propose to represent it as a recurrent neural network
                (RNN) and learn it from data. In our proposed method, RL\${
                \textasciicircum}2\$, the algorithm is encoded in the weights of
                the RNN, which are learned slowly through a general-purpose ("slow"
                ) RL algorithm. The RNN receives all information a typical RL
                algorithm would receive, including observations, actions, rewards,
                and termination flags; and it retains its state across episodes in
                a given Markov Decision Process (MDP). The activations of the RNN
                store the state of the "fast" RL algorithm on the current
                (previously unseen) MDP. We evaluate RL\${\textasciicircum}2\$
                experimentally on both small-scale and large-scale problems. On the
                small-scale side, we train it to solve randomly generated multi-arm
                bandit problems and finite MDPs. After RL\${\textasciicircum}2\$ is
                trained, its performance on new MDPs is close to human-designed
                algorithms with optimality guarantees. On the large-scale side, we
                test RL\${\textasciicircum}2\$ on a vision-based navigation task
                and show that it scales up to high-dimensional problems.},
  urldate    = {2016-11-10},
  journal    = {arXiv:1611.02779 [cs, stat]},
  author     = {Duan, Yan and Schulman, John and Chen, Xi and Bartlett, Peter and
                Sutskever, Ilya and Abbeel, Pieter},
  month      = nov,
  year       = {2016},
  note       = {arXiv: 1611.02779},
  keywords   = {Computer Science - Learning, Statistics - Machine Learning,
                Computer Science - Artificial Intelligence, Computer Science -
                Neural and Evolutionary Computing},
  file       = {arXiv.org
                Snapshot:/Users/apodusenko/Zotero/storage/S84UZTAH/1611.html:text/html;Duan
                et al. - 2016 - RL\$^2\$ Fast Reinforcement Learning via Slow
                Reinf.pdf:/Users/apodusenko/Zotero/storage/29652DH7/Duan et al. - 2016
                - RL\$^2\$ Fast Reinforcement Learning via Slow
                Reinf.pdf:application/pdf}
}

@inproceedings{weber_reinforced_2015,
  title     = {Reinforced {Variational} {Inference}},
  url       = {http://approximateinference.org/accepted/WeberEtAl2015.pdf},
  urldate   = {2016-11-21},
  booktitle = {Advances in {Approximate} {Bayesian} {Inference}},
  author    = {Weber, Theophane and Heess, Nicolas and Eslami, S. M. Ali and
               Schulman, John and Wingate, David and Silver, David},
  year      = {2015},
  file      = {Weber et al. - 2015 - Reinforced Variational
               Inference.pdf:/Users/apodusenko/Zotero/storage/APXDXLMK/Weber et al. -
               2015 - Reinforced Variational Inference.pdf:application/pdf}
}

@article{chung_empirical_2014,
  title    = {Empirical {Evaluation} of {Gated} {Recurrent} {Neural} {Networks} on
              {Sequence} {Modeling}},
  url      = {http://arxiv.org/abs/1412.3555},
  abstract = {In this paper we compare different types of recurrent units in
              recurrent neural networks (RNNs). Especially, we focus on more
              sophisticated units that implement a gating mechanism, such as a
              long short-term memory (LSTM) unit and a recently proposed gated
              recurrent unit (GRU). We evaluate these recurrent units on the
              tasks of polyphonic music modeling and speech signal modeling. Our
              experiments revealed that these advanced recurrent units are indeed
              better than more traditional recurrent units such as tanh units.
              Also, we found GRU to be comparable to LSTM.},
  urldate  = {2016-11-21},
  journal  = {arXiv:1412.3555 [cs]},
  author   = {Chung, Junyoung and Gulcehre, Caglar and Cho, KyungHyun and Bengio,
              Yoshua},
  month    = dec,
  year     = {2014},
  note     = {arXiv: 1412.3555},
  keywords = {Computer Science - Learning, Computer Science - Neural and
              Evolutionary Computing},
  file     = {arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/HZ2DMWAA/1412.html:text/html;Chung
              et al. - 2014 - Empirical Evaluation of Gated Recurrent Neural
              Net.pdf:/Users/apodusenko/Zotero/storage/KLWPSEUE/Chung et al. - 2014 -
              Empirical Evaluation of Gated Recurrent Neural Net.pdf:application/pdf}
}

@article{sedley_integrative_2017,
  title    = {An {Integrative} {Tinnitus} {Model} {Based} on {Sensory} {Precision}},
  issn     = {0166-2236},
  url      = {http://www.sciencedirect.com/science/article/pii/S0166223616301412},
  doi      = {10.1016/j.tins.2016.10.004},
  abstract = {Tinnitus is a common disorder that often complicates hearing loss.
              Its mechanisms are incompletely understood. Current theories
              proposing pathophysiology from the ear to the cortex cannot
              individually – or collectively – explain the range of experimental
              evidence available. We propose a new framework, based on predictive
              coding, in which spontaneous activity in the subcortical auditory
              pathway constitutes a ‘tinnitus precursor’ which is normally
              ignored as imprecise evidence against the prevailing percept of
              ‘silence’. Extant models feature as contributory mechanisms acting
              to increase either the intensity of the precursor or its precision.
              If precision (i.e., postsynaptic gain) rises sufficiently then
              tinnitus is perceived. Perpetuation arises through focused
              attention, which further increases the precision of the precursor,
              and resetting of the default prediction to expect tinnitus.},
  urldate  = {2016-11-23},
  journal  = {Trends in Neurosciences},
  author   = {Sedley, William and Friston, Karl J. and Gander, Phillip E. and
              Kumar, Sukhbinder and Griffiths, Timothy D.},
  year     = {2017},
  keywords = {predictive coding, auditory cortex, precision, tinnitus},
  file     = {ScienceDirect
              Snapshot:/Users/apodusenko/Zotero/storage/KPNFU68D/S0166223616301412.html:text/html;Sedley
              et al. - 2017 - An Integrative Tinnitus Model Based on Sensory
              Pre.pdf:/Users/apodusenko/Zotero/storage/A5Q7JM28/Sedley et al. - 2017
              - An Integrative Tinnitus Model Based on Sensory
              Pre.pdf:application/pdf}
}

@article{kim_structured_2017,
  title    = {Structured {Attention} {Networks}},
  url      = {http://arxiv.org/abs/1702.00887},
  abstract = {Attention networks have proven to be an effective approach for
              embedding categorical inference within a deep neural network.
              However, for many tasks we may want to model richer structural
              dependencies without abandoning end-to-end training. In this work,
              we experiment with incorporating richer structural distributions,
              encoded using graphical models, within deep networks. We show that
              these structured attention networks are simple extensions of the
              basic attention procedure, and that they allow for extending
              attention beyond the standard soft-selection approach, such as
              attending to partial segmentations or to subtrees. We experiment
              with two different classes of structured attention networks: a
              linear-chain conditional random field and a graph-based parsing
              model, and describe how these models can be practically implemented
              as neural network layers. Experiments show that this approach is
              effective for incorporating structural biases, and structured
              attention networks outperform baseline attention models on a
              variety of synthetic and real tasks: tree transduction, neural
              machine translation, question answering, and natural language
              inference. We further find that models trained in this way learn
              interesting unsupervised hidden representations that generalize
              simple attention.},
  urldate  = {2017-02-16},
  journal  = {arXiv:1702.00887 [cs]},
  author   = {Kim, Yoon and Denton, Carl and Hoang, Luong and Rush, Alexander M.},
  month    = feb,
  year     = {2017},
  note     = {arXiv: 1702.00887},
  keywords = {Computer Science - Learning, Computer Science - Computation and
              Language, Computer Science - Neural and Evolutionary Computing},
  file     = {arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/WPAG53WF/1702.html:text/html;Kim
              et al. - 2017 - Structured Attention
              Networks.pdf:/Users/apodusenko/Zotero/storage/GPRQBVMX/Kim et al. -
              2017 - Structured Attention Networks.pdf:application/pdf}
}

@article{kolbaek_speech_2017,
  title    = {Speech {Intelligibility} {Potential} of {General} and {Specialized} {
              Deep} {Neural} {Network} {Based} {Speech} {Enhancement} {Systems}},
  volume   = {25},
  issn     = {2329-9290},
  doi      = {10.1109/TASLP.2016.2628641},
  abstract = {In this paper, we study aspects of single microphone speech
              enhancement (SE) based on deep neural networks (DNNs). Specifically
              , we explore the generalizability capabilities of state-of-the-art
              DNN-based SE systems with respect to the background noise type, the
              gender of the target speaker, and the signal-to-noise ratio (SNR).
              Furthermore, we investigate how specialized DNN-based SE systems,
              which have been trained to be either noise type specific, speaker
              specific or SNR specific, perform relative to DNN based SE systems
              that have been trained to be noise type general, speaker general,
              and SNR general. Finally, we compare how a DNN-based SE system
              trained to be noise type general, speaker general, and SNR general
              performs relative to a state-of-the-art short-time spectral
              amplitude minimum mean square error (STSA-MMSE) based SE algorithm.
              We show that DNN-based SE systems, when trained specifically to
              handle certain speakers, noise types and SNRs, are capable of
              achieving large improvements in estimated speech quality (SQ) and
              speech intelligibility (SI), when tested in matched conditions.
              Furthermore, we show that improvements in estimated SQ and SI can
              be achieved by a DNN-based SE system when exposed to unseen
              speakers, genders and noise types, given a large number of speakers
              and noise types have been used in the training of the system. In
              addition, we show that a DNN-based SE system that has been trained
              using a large number of speakers and a wide range of noise types
              outperforms a state-of-the-art STSA-MMSE based SE method, when
              tested using a range of unseen speakers and noise types. Finally, a
              listening test using several DNN-based SE systems tested in unseen
              speaker conditions show that these systems can improve SI for some
              SNR and noise type configurations but degrade SI for others.},
  number   = {1},
  journal  = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  author   = {Kolbæk, M. and Tan, Z. H. and Jensen, J.},
  month    = jan,
  year     = {2017},
  keywords = {Signal processing algorithms, Speech enhancement, Deep neural
              networks, DNN-based SE systems, generalizability, ideal ratio mask,
              intelligibility, least mean squares methods, neural nets, Noise
              measurement, short-time spectral amplitude minimum mean square
              error based SE algorithm, SI, Signal to noise ratio, Silicon,
              single microphone speech enhancement, specialized deep neural
              network based speech enhancement systems, Speech, speech
              intelligibility, speech quality, SQ, STSA-MMSE based SE method,
              Training},
  pages    = {153--167},
  file     = {IEEE Xplore Abstract
              Record:/Users/apodusenko/Zotero/storage/VMBTP5AH/7744475.html:text/html;Kolbæk
              et al. - 2017 - Speech Intelligibility Potential of General and
              Sp.pdf:/Users/apodusenko/Zotero/storage/FDN4C92C/Kolbæk et al. - 2017 -
              Speech Intelligibility Potential of General and Sp.pdf:application/pdf}
}

@article{zhang_deep_2016,
  title    = {A {Deep} {Ensemble} {Learning} {Method} for {Monaural} {Speech} {
              Separation}},
  volume   = {24},
  issn     = {2329-9290},
  url      = {http://dl.acm.org/citation.cfm?id=2992480.2992491},
  abstract = {Monaural speech separation is a fundamental problem in robust
              speech processing. Recently, deep neural network (DNN)-based speech
              separation methods, which predict either clean speech or an ideal
              time-frequency mask, have demonstrated remarkable performance
              improvement. However, a single DNN with a given window length does
              not leverage contextual information sufficiently, and the
              differences between the two optimization objectives are not well
              understood. In this paper, we propose a deep ensemble method, named
              multicontext networks, to address monaural speech separation. The
              first multicontext network averages the outputs of multiple DNNs
              whose inputs employ different window lengths. The second
              multicontext network is a stack of multiple DNNs. Each DNN in a
              module of the stack takes the concatenation of original acoustic
              features and expansion of the soft output of the lower module as
              its input, and predicts the ratio mask of the target speaker; the
              DNNs in the same module employ different contexts. We have
              conducted extensive experiments with three speech corpora. The
              results demonstrate the effectiveness of the proposed method. We
              have also compared the two optimization objectives systematically
              and found that predicting the ideal time-frequency mask is more
              efficient in utilizing clean training speech, while predicting
              clean speech is less sensitive to SNR variations.},
  number   = {5},
  urldate  = {2017-03-07},
  journal  = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
  author   = {Zhang, Xiao-Lei and Wang, DeLiang},
  month    = may,
  year     = {2016},
  keywords = {Deep neural networks, ensemble learning, mapping-based separation,
              masking-based separation, monaural speech separation, multicontext
              networks},
  pages    = {967--977},
  file     = {Zhang and Wang - 2016 - A Deep Ensemble Learning Method for Monaural
              Speec.pdf:/Users/apodusenko/Zotero/storage/F9NLIWKP/Zhang and Wang -
              2016 - A Deep Ensemble Learning Method for Monaural
              Speec.pdf:application/pdf}
}

@article{chennu_expectation_2013,
  title     = {Expectation and {Attention} in {Hierarchical} {Auditory} {Prediction}
               },
  volume    = {33},
  copyright = {Copyright © 2013 the authors 0270-6474/13/3311194-12\$15.00/0},
  issn      = {0270-6474, 1529-2401},
  url       = {http://www.jneurosci.org/content/33/27/11194},
  doi       = {10.1523/JNEUROSCI.0114-13.2013},
  abstract  = {Hierarchical predictive coding suggests that attention in humans
               emerges from increased precision in probabilistic inference,
               whereas expectation biases attention in favor of contextually
               anticipated stimuli. We test these notions within auditory
               perception by independently manipulating top-down expectation and
               attentional precision alongside bottom-up stimulus predictability.
               Our findings support an integrative interpretation of commonly
               observed electrophysiological signatures of neurodynamics, namely
               mismatch negativity (MMN), P300, and contingent negative variation
               (CNV), as manifestations along successive levels of predictive
               complexity. Early first-level processing indexed by the MMN was
               sensitive to stimulus predictability: here, attentional precision
               enhanced early responses, but explicit top-down expectation
               diminished it. This pattern was in contrast to later, second-level
               processing indexed by the P300: although sensitive to the degree of
               predictability, responses at this level were contingent on
               attentional engagement and in fact sharpened by top-down
               expectation. At the highest level, the drift of the CNV was a
               fine-grained marker of top-down expectation itself. Source
               reconstruction of high-density EEG, supported by intracranial
               recordings, implicated temporal and frontal regions differentially
               active at early and late levels. The cortical generators of the CNV
               suggested that it might be involved in facilitating the
               consolidation of context-salient stimuli into conscious perception.
               These results provide convergent empirical support to promising
               recent accounts of attention and expectation in predictive coding.},
  language  = {en},
  number    = {27},
  urldate   = {2017-04-15},
  journal   = {Journal of Neuroscience},
  author    = {Chennu, Srivas and Noreika, Valdas and Gueorguiev, David and
               Blenkmann, Alejandro and Kochen, Silvia and Ibáñez, Agustín and Owen,
               Adrian M. and Bekinschtein, Tristan A.},
  month     = jul,
  year      = {2013},
  pmid      = {23825422},
  pages     = {11194--11205},
  file      = {Chennu et al. - 2013 - Expectation and Attention in Hierarchical
               Auditory.pdf:/Users/apodusenko/Zotero/storage/TMH9QXBW/Chennu et al. -
               2013 - Expectation and Attention in Hierarchical
               Auditory.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/AXA2WLN6/11194.html:text/html
               }
}

@incollection{friston_variational_2017-1,
  series    = {Springer {Tracts} in {Advanced} {Robotics}},
  title     = {The {Variational} {Principles} of {Action}},
  copyright = {©2017 Springer International Publishing AG},
  isbn      = {978-3-319-51546-5 978-3-319-51547-2},
  url       = {http://link.springer.com/chapter/10.1007/978-3-319-51547-2_10},
  abstract  = {This chapter provides a theoretical perspective on action and the
               control of movement from the point of view of the free-energy
               principle. This variational principle offers an explanation for
               neuronal activity and ensuing behavior that is formulated in terms
               of dynamical systems and attracting sets. We will see that the
               free-energy principle emerges when considering the ensemble
               dynamics of biological systems like ourselves. When we look closely
               what this principle implies for the behavior of systems like the
               brain, one finds a fairly straightforward explanation for many
               aspects of action and perception; in particular, their
               (approximately Bayesian) optimality. Within the Bayesian brain
               framework, the ensuing dynamics can be separated into those serving
               perceptual inference, learning and behavior. Variational principles
               play a key role in what follows; both in understanding the nature
               of self-organizing systems but also in explaining the adaptive
               nature of neuronal dynamics and plasticity in terms of
               optimization—and the process theories that mediate optimal
               inference and motor control. A special focus of this chapter is the
               pre-eminent role of heteroclinic cycles in providing deep and
               dynamic (generative) models of the sensorium; particularly the
               sensations that we generate ourselves through action. In what
               follows, we will briefly rehearse the basic theory and illustrate
               its implications using simulations of action (handwriting)—and its
               observation.},
  language  = {en},
  number    = {117},
  urldate   = {2017-05-09},
  booktitle = {Geometric and {Numerical} {Foundations} of {Movements}},
  publisher = {Springer International Publishing},
  author    = {Friston, Karl},
  editor    = {Laumond, Jean-Paul and Mansard, Nicolas and Lasserre, Jean-Bernard},
  year      = {2017},
  doi       = {10.1007/978-3-319-51547-2_10},
  keywords  = {Neurosciences, Artificial Intelligence (incl. Robotics), Geometry,
               Numeric Computing, Robotics and Automation},
  pages     = {207--235},
  file      = {Friston - 2017 - The Variational Principles of
               Action.pdf:/Users/apodusenko/Zotero/storage/JLX4NTHR/Friston - 2017 -
               The Variational Principles of
               Action.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/FWW5VAXN/978-3-319-51547-2_10.html:text/html
               }
}

@article{roelfsema_perceptual_2010,
  title    = {Perceptual learning rules based on reinforcers and attention},
  volume   = {14},
  issn     = {1364-6613},
  url      = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2835467/},
  doi      = {10.1016/j.tics.2009.11.005},
  abstract = {How does the brain learn those visual features that are relevant
              for behavior? In this article, we focus on two factors that guide
              plasticity of visual representations. First, reinforcers cause the
              global release of diffusive neuromodulatory signals that gate
              plasticity. Second, attentional feedback signals highlight the
              chain of neurons between sensory and motor cortex responsible for
              the selected action. We here propose that the attentional feedback
              signals guide learning by suppressing plasticity of irrelevant
              features while permitting the learning of relevant ones. By
              hypothesizing that sensory signals that are too weak to be
              perceived can escape from this inhibitory feedback, we bring
              attentional learning theories and theories that emphasized the
              importance of neuromodulatory signals into a single, unified
              framework.},
  number   = {2},
  urldate  = {2015-09-17},
  journal  = {Trends in cognitive sciences},
  author   = {Roelfsema, Pieter R. and van Ooyen, Arjen and Watanabe, Takeo},
  month    = feb,
  year     = {2010},
  pmid     = {20060771},
  pmcid    = {PMC2835467},
  pages    = {64--71},
  file     = {Roelfsema et al. - 2010 - Perceptual learning rules based on
              reinforcers and.pdf:/Users/apodusenko/Zotero/storage/EIYGPJI7/Roelfsema
              et al. - 2010 - Perceptual learning rules based on reinforcers
              and.pdf:application/pdf}
}

@incollection{gershman_neural_2010,
  title     = {The {Neural} {Costs} of {Optimal} {Control}},
  url       = {
               http://papers.nips.cc/paper/4167-the-neural-costs-of-optimal-control.pdf
               },
  urldate   = {2015-11-26},
  booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 23},
  publisher = {Curran Associates, Inc.},
  author    = {Gershman, Samuel and Wilson, Robert},
  editor    = {Lafferty, J. D. and Williams, C. K. I. and Shawe-Taylor, J. and
               Zemel, R. S. and Culotta, A.},
  year      = {2010},
  pages     = {712--720},
  file      = {Gershman and Wilson - 2010 - The Neural Costs of Optimal
               Control.pdf:/Users/apodusenko/Zotero/storage/4IPW5ACS/Gershman and
               Wilson - 2010 - The Neural Costs of Optimal
               Control.pdf:application/pdf;NIPS
               Snapshort:/Users/apodusenko/Zotero/storage/VFKRRAKZ/4167-the-neural-costs-of-optimal-control.html:text/html
               }
}

@article{henaff_model-based_2017,
  title    = {Model-{Based} {Planning} in {Discrete} {Action} {Spaces}},
  url      = {http://arxiv.org/abs/1705.07177},
  abstract = {Planning actions using learned and differentiable forward models
              of the world is a general approach which has a number of desirable
              properties, including improved sample complexity over model-free RL
              methods, reuse of learned models across different tasks, and the
              ability to perform efficient gradient-based optimization in
              continuous action spaces. However, this approach does not apply
              straightforwardly when the action space is discrete, which may have
              limited its adoption. In this work, we introduce two discrete
              planning tasks inspired by existing question-answering datasets and
              show that it is in fact possible to effectively perform planning
              via backprop in discrete action spaces using two simple yet
              principled modifications. Our experiments show that this approach
              can significantly outperform model-free RL based methods and
              supervised imitation learners.},
  urldate  = {2017-05-26},
  journal  = {arXiv:1705.07177 [cs]},
  author   = {Henaff, Mikael and Whitney, William F. and LeCun, Yann},
  month    = may,
  year     = {2017},
  note     = {arXiv: 1705.07177},
  keywords = {Computer Science - Artificial Intelligence},
  file     = {arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/7F457CJI/1705.html:text/html;Henaff
              et al. - 2017 - Model-Based Planning in Discrete Action
              Spaces.pdf:/Users/apodusenko/Zotero/storage/WLKCF5Y7/Henaff et al. -
              2017 - Model-Based Planning in Discrete Action
              Spaces.pdf:application/pdf}
}

@article{santurkar_generative_2017,
  title    = {Generative {Compression}},
  url      = {http://arxiv.org/abs/1703.01467},
  abstract = {Traditional image and video compression algorithms rely on
              hand-crafted encoder/decoder pairs (codecs) that lack adaptability
              and are agnostic to the data being compressed. Here we describe the
              concept of generative compression, the compression of data using
              generative models, and suggest that it is a direction worth
              pursuing to produce more accurate and visually pleasing
              reconstructions at much deeper compression levels for both image
              and video data. We also demonstrate that generative compression is
              orders-of-magnitude more resilient to bit error rates (e.g. from
              noisy wireless channels) than traditional variable-length coding
              schemes.},
  journal  = {arXiv:1703.01467 [cs]},
  author   = {Santurkar, Shibani and Budden, David and Shavit, Nir},
  month    = mar,
  year     = {2017},
  note     = {arXiv: 1703.01467},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file     = {arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/A2DBY3XT/1703.html:text/html;Santurkar
              et al. - 2017 - Generative
              Compression.pdf:/Users/apodusenko/Zotero/storage/F8Z2ECEX/Santurkar et
              al. - 2017 - Generative Compression.pdf:application/pdf}
}

@article{swingler_narrowband_1994,
  title      = {Narrowband line-array beamforming: practically achievable resolution
                limit of unbiased estimators},
  volume     = {19},
  issn       = {0364-9059},
  shorttitle = {Narrowband line-array beamforming},
  doi        = {10.1109/48.286645},
  abstract   = {Based on an approximation to the Cramer-Rao lower bound, it is
                demonstrated that meaningful resolution with an unbiased narrowband
                estimator requires a source separation of at least about 1/10 of a
                Rayleigh beamwidth, even under ideal circumstances, with 1/4
                beamwidth being a more practically achievable figure},
  number     = {2},
  journal    = {IEEE Journal of Oceanic Engineering},
  author     = {Swingler, D. N.},
  month      = apr,
  year       = {1994},
  keywords   = {Signal to noise ratio, Narrowband, Gaussian noise, Array signal
                processing, Cramer-Rao lower bound, Frequency estimation,
                narrowband line-array beamforming, Phased arrays, Rayleigh
                beamwidth, resolution limit, sensor arrays, signal detection,
                Signal resolution, source separation, Spatial resolution, unbiased
                narrowband estimator},
  pages      = {225--226},
  file       = {IEEE Xplore Abstract
                Record:/Users/apodusenko/Zotero/storage/G2RW2Q67/286645.html:text/html;Swingler
                - 1994 - Narrowband line-array beamforming practically
                ach.pdf:/Users/apodusenko/Zotero/storage/8KMTU9E4/Swingler - 1994 -
                Narrowband line-array beamforming practically ach.pdf:application/pdf}
}

@inproceedings{johnson_stochastic_2014,
  title     = {Stochastic variational inference for {Bayesian} time series models},
  url       = {http://www.jmlr.org/proceedings/papers/v32/johnson14.pdf},
  urldate   = {2017-06-06},
  booktitle = {International {Conference} on {Machine} {Learning}},
  author    = {Johnson, Matthew and Willsky, Alan},
  year      = {2014},
  pages     = {1854--1862},
  file      = {Johnson and Willsky - 2014 - Stochastic variational inference for
               Bayesian time.pdf:/Users/apodusenko/Zotero/storage/X6TKZAS4/Johnson and
               Willsky - 2014 - Stochastic variational inference for Bayesian
               time.pdf:application/pdf}
}

@inproceedings{salamon_unsupervised_2015,
  title     = {Unsupervised feature learning for urban sound classification},
  url       = {http://ieeexplore.ieee.org/abstract/document/7177954/},
  urldate   = {2017-03-16},
  booktitle = {Acoustics, {Speech} and {Signal} {Processing} ({ICASSP}), 2015 {
               IEEE} {International} {Conference} on},
  publisher = {IEEE},
  author    = {Salamon, Justin and Bello, Juan Pablo},
  year      = {2015},
  pages     = {171--175},
  file      = {Salamon and Bello - 2015 - Unsupervised feature learning for urban
               sound clas.pdf:/Users/apodusenko/Zotero/storage/MQDDRN24/Salamon and
               Bello - 2015 - Unsupervised feature learning for urban sound
               clas.pdf:application/pdf}
}

@article{santoro_one-shot_2016,
  title   = {One-shot learning with memory-augmented neural networks},
  url     = {https://arxiv.org/abs/1605.06065},
  urldate = {2017-03-16},
  journal = {arXiv preprint arXiv:1605.06065},
  author  = {Santoro, Adam and Bartunov, Sergey and Botvinick, Matthew and
             Wierstra, Daan and Lillicrap, Timothy},
  year    = {2016},
  file    = {Santoro et al. - 2016 - One-shot learning with memory-augmented neural
             net.pdf:/Users/apodusenko/Zotero/storage/RVTT7T3P/Santoro et al. - 2016
             - One-shot learning with memory-augmented neural
             net.pdf:application/pdf}
}

@article{fei-fei_one-shot_2006,
  title   = {One-shot learning of object categories},
  volume  = {28},
  url     = {http://ieeexplore.ieee.org/abstract/document/1597116/},
  number  = {4},
  urldate = {2017-03-16},
  journal = {IEEE transactions on pattern analysis and machine intelligence},
  author  = {Fei-Fei, Li and Fergus, Rob and Perona, Pietro},
  year    = {2006},
  pages   = {594--611},
  file    = {Fei-Fei et al. - 2006 - One-shot learning of object
             categories.pdf:/Users/apodusenko/Zotero/storage/3FN3IPHN/Fei-Fei et al.
             - 2006 - One-shot learning of object categories.pdf:application/pdf}
}

@article{duan_one-shot_2017,
  title   = {One-{Shot} {Imitation} {Learning}},
  url     = {https://arxiv.org/abs/1703.07326},
  urldate = {2017-03-31},
  journal = {arXiv preprint arXiv:1703.07326},
  author  = {Duan, Yan and Andrychowicz, Marcin and Stadie, Bradly and Ho,
             Jonathan and Schneider, Jonas and Sutskever, Ilya and Abbeel, Pieter
             and Zaremba, Wojciech},
  year    = {2017},
  file    = {Duan et al. - 2017 - One-Shot Imitation
             Learning.pdf:/Users/apodusenko/Zotero/storage/QZMJA7DQ/Duan et al. -
             2017 - One-Shot Imitation Learning.pdf:application/pdf}
}

@article{fei-fei_learning_2007,
  title      = {Learning generative visual models from few training examples: {An}
                incremental bayesian approach tested on 101 object categories},
  volume     = {106},
  shorttitle = {Learning generative visual models from few training examples},
  url        = {http://www.sciencedirect.com/science/article/pii/S1077314206001688},
  number     = {1},
  urldate    = {2017-03-16},
  journal    = {Computer vision and Image understanding},
  author     = {Fei-Fei, Li and Fergus, Rob and Perona, Pietro},
  year       = {2007},
  pages      = {59--70},
  file       = {Fei-Fei et al. - 2007 - Learning generative visual models from few
                trainin.pdf:/Users/apodusenko/Zotero/storage/NAUVHJWV/Fei-Fei et al. -
                2007 - Learning generative visual models from few
                trainin.pdf:application/pdf}
}

@phdthesis{johnson_bayesian_2014,
  title   = {Bayesian time series models and scalable inference},
  url     = {
             http://ssg.mit.edu/ssg_theses/ssg_theses_2010_present/JohnsonM_PhD_6_14.pdf
             },
  urldate = {2017-06-07},
  school  = {Massachusetts Institute of Technology},
  author  = {Johnson, Matthew James},
  year    = {2014},
  file    = {Johnson - 2014 - Bayesian time series models and scalable
             inference.pdf:/Users/apodusenko/Zotero/storage/99IUCBQT/Johnson - 2014
             - Bayesian time series models and scalable
             inference.pdf:application/pdf}
}

@article{johnson_bayesian_2013,
  title   = {Bayesian nonparametric hidden semi-{Markov} models},
  volume  = {14},
  url     = {http://www.jmlr.org/papers/v14/johnson13a.html},
  number  = {Feb},
  urldate = {2017-04-12},
  journal = {Journal of Machine Learning Research},
  author  = {Johnson, Matthew J. and Willsky, Alan S.},
  year    = {2013},
  pages   = {673--701},
  file    = {Johnson and Willsky - 2013 - Bayesian nonparametric hidden semi-Markov
             models.pdf:/Users/apodusenko/Zotero/storage/CIQZHK7U/Johnson and
             Willsky - 2013 - Bayesian nonparametric hidden semi-Markov
             models.pdf:application/pdf}
}

@phdthesis{koch_siamese_2015,
  title   = {Siamese neural networks for one-shot image recognition},
  url     = {
             https://pdfs.semanticscholar.org/e669/55e4a24b611c54f9e7f6b178e7cbaddd0fbb.pdf
             },
  urldate = {2017-03-31},
  school  = {University of Toronto},
  author  = {Koch, Gregory},
  year    = {2015},
  file    = {Koch - 2015 - Siamese neural networks for one-shot image
             recogni.pdf:/Users/apodusenko/Zotero/storage/NBWFM989/Koch - 2015 -
             Siamese neural networks for one-shot image recogni.pdf:application/pdf}
}

@article{dai_recurrent_nodate,
  title   = {Recurrent {Hidden} {Semi}-{Markov} {Model}},
  url     = {http://www.cc.gatech.edu/~lsong/papers/DaiDaiZhaLietal17.pdf},
  urldate = {2017-05-08},
  author  = {Dai, Hanjun and Dai, Bo and Zhang, Yan-Ming and Li, Shuang and Song,
             Le},
  file    = {Dai et al. - Recurrent Hidden Semi-Markov
             Model.pdf:/Users/apodusenko/Zotero/storage/PJ66RJ4H/Dai et al. -
             Recurrent Hidden Semi-Markov Model.pdf:application/pdf}
}

@inproceedings{salakhutdinov_one-shot_2012,
  title     = {One-{Shot} {Learning} with a {Hierarchical} {Nonparametric} {Bayesian
               } {Model}.},
  url       = {
               http://www.jmlr.org/proceedings/papers/v27/salakhutdinov12a/salakhutdinov12a.pdf
               },
  urldate   = {2017-03-16},
  booktitle = {{ICML} {Unsupervised} and {Transfer} {Learning}},
  author    = {Salakhutdinov, Ruslan and Tenenbaum, Joshua B. and Torralba, Antonio
               },
  year      = {2012},
  pages     = {195--206},
  file      = {Salakhutdinov et al. - 2012 - One-Shot Learning with a Hierarchical
               Nonparametri.pdf:/Users/apodusenko/Zotero/storage/56RI2J4D/Salakhutdinov
               et al. - 2012 - One-Shot Learning with a Hierarchical
               Nonparametri.pdf:application/pdf}
}

@article{rezende_one-shot_2016,
  title   = {One-shot generalization in deep generative models},
  url     = {https://arxiv.org/abs/1603.05106},
  urldate = {2017-03-31},
  journal = {arXiv preprint arXiv:1603.05106},
  author  = {Rezende, Danilo Jimenez and Mohamed, Shakir and Danihelka, Ivo and
             Gregor, Karol and Wierstra, Daan},
  year    = {2016},
  file    = {Rezende et al. - 2016 - One-shot generalization in deep generative
             models.pdf:/Users/apodusenko/Zotero/storage/ZQAVCH93/Rezende et al. -
             2016 - One-shot generalization in deep generative
             models.pdf:application/pdf}
}

@inproceedings{vinyals_matching_2016,
  title     = {Matching networks for one shot learning},
  url       = {
               http://papers.nips.cc/paper/6385-matching-networks-for-one-shot-learning
               },
  urldate   = {2017-03-17},
  booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
  author    = {Vinyals, Oriol and Blundell, Charles and Lillicrap, Tim and Wierstra
               , Daan and {others}},
  year      = {2016},
  pages     = {3630--3638},
  file      = {Vinyals et al. - 2016 - Matching networks for one shot
               learning.pdf:/Users/apodusenko/Zotero/storage/82EMBV97/Vinyals et al. -
               2016 - Matching networks for one shot learning.pdf:application/pdf}
}

@inproceedings{lee_nonparametric_2012,
  title     = {A nonparametric {Bayesian} approach to acoustic model discovery},
  url       = {http://dl.acm.org/citation.cfm?id=2390531},
  urldate   = {2017-04-03},
  booktitle = {Proceedings of the 50th {Annual} {Meeting} of the {Association}
               for {Computational} {Linguistics}: {Long} {Papers}-{Volume} 1},
  publisher = {Association for Computational Linguistics},
  author    = {Lee, Chia-ying and Glass, James},
  year      = {2012},
  pages     = {40--49},
  file      = {Lee and Glass - 2012 - A nonparametric Bayesian approach to acoustic
               mode.pdf:/Users/apodusenko/Zotero/storage/CDGIBUHK/Lee and Glass - 2012
               - A nonparametric Bayesian approach to acoustic
               mode.pdf:application/pdf}
}

@article{bissiri_general_2016,
  title   = {A general framework for updating belief distributions},
  journal = {Journal of the Royal Statistical Society: Series B (Statistical
             Methodology)},
  author  = {Bissiri, Pier Giovanni and Holmes, C. C. and Walker, Stephen G.},
  year    = {2016}
}

@article{turner_demodulation_2011,
  title    = {Demodulation as {Probabilistic} {Inference}},
  volume   = {19},
  issn     = {1558-7916},
  doi      = {10.1109/TASL.2011.2135852},
  abstract = {Demodulation is an ill-posed problem whenever both carrier and
              envelope signals are broadband and unknown. Here, we approach this
              problem using the methods of probabilistic inference. The new
              approach, called Probabilistic Amplitude Demodulation (PAD), is
              computationally challenging but improves on existing methods in a
              number of ways. By contrast to previous approaches to demodulation,
              it satisfies five key desiderata: PAD has soft constraints because
              it is probabilistic; PAD is able to automatically adjust to the
              signal because it learns parameters; PAD is user-steerable because
              the solution can be shaped by user-specific prior information; PAD
              is robust to broad-band noise because this is modeled explicitly;
              and PAD's solution is self-consistent, empirically satisfying a
              Carrier Identity property. Furthermore, the probabilistic view
              naturally encompasses noise and uncertainty, allowing PAD to cope
              with missing data and return error bars on carrier and envelope
              estimates. Finally, we show that when PAD is applied to a
              bandpass-filtered signal, the stop-band energy of the inferred
              carrier is minimal, making PAD well-suited to sub-band
              demodulation.},
  number   = {8},
  journal  = {IEEE Transactions on Audio, Speech, and Language Processing},
  author   = {Turner, R. E. and Sahani, M.},
  month    = nov,
  year     = {2011},
  keywords = {Noise, inference mechanisms, Learning, Inference, Mathematical
              model, Probabilistic inference, amplitude modulation, band-pass
              filters, bandpass-filtered signal, broadband noise, Carrier,
              carrier identity property, demodulation, envelope, Frequency
              modulation, Helium, PAD, probabilistic amplitude demodulation,
              Probabilistic logic, subband demodulation, telecommunication
              computing},
  pages    = {2398--2411},
  file     = {IEEE Xplore Abstract
              Record:/Users/apodusenko/Zotero/storage/TRJ34W52/5741712.html:text/html;IEEE
              Xplore Full Text PDF:/Users/apodusenko/Zotero/storage/4WQ32WRV/Turner
              and Sahani - 2011 - Demodulation as Probabilistic
              Inference.pdf:application/pdf}
}

@article{lotter_deep_2016,
  title   = {Deep {Predictive} {Coding} {Networks} for {Video} {Prediction} and {
             Unsupervised} {Learning}},
  url     = {https://openreview.net/forum?id=B1ewdt9xe&noteId=B1ewdt9xe},
  urldate = {2017-04-29},
  author  = {Lotter, William and Kreiman, Gabriel and Cox, David},
  month   = nov,
  year    = {2016},
  file    = {Lotter et al. - 2016 - Deep Predictive Coding Networks for Video
             Predicti.pdf:/Users/apodusenko/Zotero/storage/R7CH2FKJ/Lotter et al. -
             2016 - Deep Predictive Coding Networks for Video
             Predicti.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/XP6ZZX2V/forum.html:text/html
             }
}

@article{whittington_approximation_2017,
  title    = {An {Approximation} of the {Error} {Backpropagation} {Algorithm} in a
              {Predictive} {Coding} {Network} with {Local} {Hebbian} {Synaptic} {
              Plasticity}},
  issn     = {0899-7667},
  url      = {http://dx.doi.org/10.1162/NECO_a_00949},
  doi      = {10.1162/NECO_a_00949},
  abstract = {To efficiently learn from feedback, cortical networks need to
              update synaptic weights on multiple levels of cortical hierarchy.
              An effective and well-known algorithm for computing such changes in
              synaptic weights is the error backpropagation algorithm. However,
              in this algorithm, the change in synaptic weights is a complex
              function of weights and activities of neurons not directly
              connected with the synapse being modified, whereas the changes in
              biological synapses are determined only by the activity of
              presynaptic and postsynaptic neurons. Several models have been
              proposed that approximate the backpropagation algorithm with local
              synaptic plasticity, but these models require complex external
              control over the network or relatively complex plasticity rules.
              Here we show that a network developed in the predictive coding
              framework can efficiently perform supervised learning fully
              autonomously, employing only simple local Hebbian plasticity.
              Furthermore, for certain parameters, the weight change in the
              predictive coding model converges to that of the backpropagation
              algorithm. This suggests that it is possible for cortical networks
              with simple Hebbian synaptic plasticity to implement efficient
              learning algorithms in which synapses in areas on multiple levels
              of hierarchy are modified to minimize the error on the output.},
  urldate  = {2017-03-25},
  journal  = {Neural Computation},
  author   = {Whittington, James C. R. and Bogacz, Rafal},
  month    = mar,
  year     = {2017},
  pages    = {1--34},
  file     = {Neural Computation
              Snapshot:/Users/apodusenko/Zotero/storage/GGXJ545V/NECO_a_00949.html:text/html;Whittington
              and Bogacz - 2017 - An Approximation of the Error Backpropagation
              Algo.pdf:/Users/apodusenko/Zotero/storage/KNEXBWVJ/Whittington and
              Bogacz - 2017 - An Approximation of the Error Backpropagation
              Algo.pdf:application/pdf}
}

@article{hsu_learning_2017,
  title    = {Learning {Latent} {Representations} for {Speech} {Generation} and {
              Transformation}},
  url      = {http://arxiv.org/abs/1704.04222},
  abstract = {An ability to model a generative process and learn a latent
              representation for speech in an unsupervised fashion will be
              crucial to process vast quantities of unlabelled speech data.
              Recently, deep probabilistic generative models such as Variational
              Autoencoders (VAEs) have achieved tremendous success in modeling
              natural images. In this paper, we apply a convolutional VAE to
              model the generative process of natural speech. We derive latent
              space arithmetic operations to disentangle learned latent
              representations. We demonstrate the capability of our model to
              modify the phonetic content or the speaker identity for speech
              segments using the derived operations, without the need for
              parallel supervisory data.},
  urldate  = {2017-05-25},
  journal  = {arXiv:1704.04222 [cs, stat]},
  author   = {Hsu, Wei-Ning and Zhang, Yu and Glass, James},
  month    = apr,
  year     = {2017},
  note     = {arXiv: 1704.04222},
  keywords = {Computer Science - Learning, Statistics - Machine Learning,
              Computer Science - Computation and Language},
  file     = {arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/2VN2A573/1704.html:text/html;Hsu
              et al. - 2017 - Learning Latent Representations for Speech
              Generat.pdf:/Users/apodusenko/Zotero/storage/RKV6UQEF/Hsu et al. - 2017
              - Learning Latent Representations for Speech
              Generat.pdf:application/pdf}
}

@article{andrychowicz_learning_2016,
  title    = {Learning to learn by gradient descent by gradient descent},
  url      = {http://arxiv.org/abs/1606.04474},
  abstract = {The move from hand-designed features to learned features in
              machine learning has been wildly successful. In spite of this,
              optimization algorithms are still designed by hand. In this paper
              we show how the design of an optimization algorithm can be cast as
              a learning problem, allowing the algorithm to learn to exploit
              structure in the problems of interest in an automatic way. Our
              learned algorithms, implemented by LSTMs, outperform generic,
              hand-designed competitors on the tasks for which they are trained,
              and also generalize well to new tasks with similar structure. We
              demonstrate this on a number of tasks, including simple convex
              problems, training neural networks, and styling images with neural
              art.},
  urldate  = {2017-04-27},
  journal  = {arXiv:1606.04474 [cs]},
  author   = {Andrychowicz, Marcin and Denil, Misha and Gomez, Sergio and Hoffman,
              Matthew W. and Pfau, David and Schaul, Tom and Shillingford, Brendan
              and de Freitas, Nando},
  month    = jun,
  year     = {2016},
  note     = {arXiv: 1606.04474},
  keywords = {Computer Science - Learning, Computer Science - Neural and
              Evolutionary Computing},
  file     = {Andrychowicz et al. - 2016 - Learning to learn by gradient descent by
              gradient .pdf:/Users/apodusenko/Zotero/storage/7AESTGNU/Andrychowicz et
              al. - 2016 - Learning to learn by gradient descent by gradient
              .pdf:application/pdf;arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/SRQURMIB/1606.html:text/html}
}

@incollection{friston_policies_2012,
  series    = {Springer {Series} in {Computational} {Neuroscience}},
  title     = {Policies and {Priors}},
  copyright = {©2012 Springer Science+Business Media, LLC},
  isbn      = {978-1-4614-0750-8 978-1-4614-0751-5},
  url       = {http://link.springer.com/chapter/10.1007/978-1-4614-0751-5_9},
  abstract  = {This chapter considers addiction from a purely theoretical point
               of view. It tries to substantiate the idea that addictive behaviour
               is a natural consequence of abnormal perceptual learning. In short,
               addictive behaviours emerge when behaviour confounds its own
               acquisition. Specifically, we consider what would happen if
               behaviour interfered with the neurotransmitter systems responsible
               for optimising the conditional certainty or precision of inferences
               about causal structure in the world. We will pursue this within a
               rather abstract framework provided by free-energy formulations of
               action and perception. Although this treatment does not touch upon
               many of the neurobiological or psychosocial issues in addiction
               research, it provides a principled framework within which to
               understand exchanges with the environment and how they can be
               disturbed. Our focus will be on behaviour as active inference and
               the key role of prior expectations. These priors play the role of
               policies in reinforcement learning and place crucial constraints on
               perceptual inference and subsequent action. A dynamical treatment
               of these policies suggests a fundamental distinction between
               fixed-point policies that lead to a single attractive state and
               itinerant policies that support wandering behavioural orbits among
               sets of attractive states. Itinerant policies may provide a useful
               metaphor for many forms of behaviour and, in particular, addiction.
               Under these sorts of policies, neuromodulatory (e.g., dopaminergic)
               perturbations can lead to false inference and consequent learning,
               which produce addictive and preservative behaviour.},
  language  = {en},
  number    = {10},
  urldate   = {2016-11-24},
  booktitle = {Computational {Neuroscience} of {Drug} {Addiction}},
  publisher = {Springer New York},
  author    = {Friston, Karl},
  editor    = {Gutkin, Boris and Ahmed, Serge H.},
  year      = {2012},
  doi       = {10.1007/978-1-4614-0751-5_9},
  keywords  = {Computer Appl. in Life Sciences, Neurosciences},
  pages     = {237--283},
  file      = {Friston - 2012 - Policies and
               Priors.pdf:/Users/apodusenko/Zotero/storage/MFUMRRSK/Friston - 2012 -
               Policies and
               Priors.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/IIKC2QKD/978-1-4614-0751-5_9.html:text/html
               }
}

@article{shao_computational_2010,
  title    = {A computational auditory scene analysis system for speech segregation
              and robust speech recognition},
  volume   = {24},
  issn     = {08852308},
  url      = {http://linkinghub.elsevier.com/retrieve/pii/S088523080800020X},
  doi      = {10.1016/j.csl.2008.03.004},
  language = {en},
  number   = {1},
  urldate  = {2017-04-13},
  journal  = {Computer Speech \& Language},
  author   = {Shao, Yang and Srinivasan, Soundararajan and Jin, Zhaozhang and Wang
              , DeLiang},
  month    = jan,
  year     = {2010},
  pages    = {77--93},
  file     = {
              SSJW.csl10.pdf:/Users/apodusenko/Zotero/storage/TCJWF6DC/SSJW.csl10.pdf:application/pdf
              }
}

@book{jaynes_probability_2003,
  title     = {Probability {Theory}: {The} {Logic} of {Science}},
  url       = {http://bayes.wustl.edu/etj/prob/book.pdf},
  urldate   = {2017-04-11},
  publisher = {Cambridge University Press},
  author    = {Jaynes, E.T.},
  year      = {2003},
  file      = {Jaynes - Probability Theory The Logic of
               Science.pdf:/Users/apodusenko/Zotero/storage/R9K9YMX8/Jaynes -
               Probability Theory The Logic of Science.pdf:application/pdf}
}

@article{palmer_bayesian_2017,
  title      = {Bayesian {Approaches} to {Autism}: {Towards} {Volatility}, {Action},
                and {Behavior}.},
  issn       = {1939-1455 (Electronic), 0033-2909 (Print)},
  shorttitle = {Bayesian {Approaches} to {Autism}},
  url        = {http://psycnet.apa.org/psycarticles/2017-12911-001},
  doi        = {10.1037/bul0000097},
  abstract   = {Autism spectrum disorder currently lacks an explanation that
                bridges cognitive, computational, and neural domains. In the past 5
                years, progress has been sought in this area by drawing on Bayesian
                probability theory to describe both social and nonsocial aspects of
                autism in terms of systematic differences in the processing of
                sensory information in the brain. The present article begins by
                synthesizing the existing literature in this regard, including an
                introduction to the topic for unfamiliar readers. The key proposal
                is that autism is characterized by a greater weighting of sensory
                information in updating probabilistic representations of the
                environment. Here, we unpack further how the hierarchical setting
                of Bayesian inference in the brain (i.e., predictive processing)
                adds significant depth to this approach. In particular, autism may
                relate to finer mechanisms involved in the context-sensitive
                adjustment of sensory weightings, such as in how neural
                representations of environmental volatility inform perception.
                Crucially, in light of recent sensorimotor treatments of predictive
                processing (i.e., active inference), hypotheses regarding atypical
                sensory weighting in autism have direct implications for the
                regulation of action and behavior. Given that core features of
                autism relate to how the individual interacts with and samples the
                world around them (e.g., reduced social responding, repetitive
                behaviors, motor impairments, and atypical visual sampling), the
                extension of Bayesian theories of autism to action will be critical
                for yielding insights into this condition. (PsycINFO Database
                Record (c) 2017 APA, all rights reserved)},
  urldate    = {2017-04-12},
  journal    = {APA PsycNET},
  author     = {Palmer, Colin J. and Lawson, Rebecca P. and Hohwy, Jakob},
  month      = mar,
  year       = {2017},
  file       = {Palmer et al. - 2017 - Bayesian Approaches to Autism Towards
                Volatility,.pdf:/Users/apodusenko/Zotero/storage/PT99KA7T/Palmer et al.
                - 2017 - Bayesian Approaches to Autism Towards Volatility,
                .pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/V3ES69C4/2017-12911-001.html:text/html
                }
}

@techreport{carl_tutorial_nodate,
  title    = {Tutorial on {Variational} {Autoencoders}},
  url      = {https://arxiv.org/abs/1606.05908},
  abstract = {In just three years, Variational Autoencoders (VAEs) have emerged
              as one of the most popular approaches to unsupervised learning of
              complicated distributions. VAEs are appealing because they are
              built on top of standard function approximators (neural networks),
              and can be trained with stochastic gradient descent. VAEs have
              already shown promise in generating many kinds of complicated data,
              including handwritten digits, faces, house numbers, CIFAR images,
              physical models of scenes, segmentation, and predicting the future
              from static images. This tutorial introduces the intuitions behind
              VAEs, explains the mathematics behind them, and describes some
              empirical behavior. No prior knowledge of variational Bayesian
              methods is assumed.},
  author   = {Carl, Doersch},
  file     = {Carl - Tutorial on Variational
              Autoencoders.pdf:/Users/apodusenko/Zotero/storage/M8CSZJ3S/Carl -
              Tutorial on Variational Autoencoders.pdf:application/pdf}
}

@book{chatfield_analysis_1975,
  address    = {Boston, MA},
  title      = {The {Analysis} of {Time} {Series}: {Theory} and {Practice}},
  isbn       = {978-0-412-14180-5 978-1-4899-2925-9},
  shorttitle = {The {Analysis} of {Time} {Series}},
  url        = {http://link.springer.com/10.1007/978-1-4899-2925-9},
  language   = {en},
  urldate    = {2017-04-11},
  publisher  = {Springer US},
  author     = {Chatfield, C.},
  year       = {1975},
  doi        = {10.1007/978-1-4899-2925-9},
  file       = {Chatfield - 1975 - The Analysis of Time Series Theory and
                Practice.pdf:/Users/apodusenko/Zotero/storage/USD8R6QE/Chatfield - 1975
                - The Analysis of Time Series Theory and Practice.pdf:application/pdf}
}

@inproceedings{nguyen_probabilistic_2017,
  title    = {Probabilistic {Inference}-based {Reinforcement} {Learning}},
  abstract = {We introduce probabilistic inference-based reinforcement learning
              (PIReL), an approach to solve decision making problems by treating
              them as probabilistic inference tasks. Unlike classical
              reinforcement learning, which requires explicitly defined reward
              functions and discount factors, in PIReL they are implicitly
              interpreted as probabilistic assumptions of the model. This would
              enable a fundamental way to design the reward function and discount
              factor by model selection as well as bring the potential to apply
              existing probabilistic modeling techniques to reinforcement
              learning problems.},
  author   = {Nguyen, Quan and de Vries, Bert and Tjalkens, Tjalling},
  year     = {2017},
  file     = {Nguyen et al. - 2017 - Probabilistic Inference-based Reinforcement
              Learni.pdf:/Users/apodusenko/Zotero/storage/P2WPBXS4/Nguyen et al. -
              2017 - Probabilistic Inference-based Reinforcement
              Learni.pdf:application/pdf}
}

@inproceedings{mesaros_tut_2016,
  title     = {{TUT} database for acoustic scene classification and sound event
               detection},
  url       = {http://ieeexplore.ieee.org/abstract/document/7760424/},
  urldate   = {2017-04-04},
  booktitle = {Signal {Processing} {Conference} ({EUSIPCO}), 2016 24th {European
               }},
  publisher = {IEEE},
  author    = {Mesaros, Annamaria and Heittola, Toni and Virtanen, Tuomas},
  year      = {2016},
  pages     = {1128--1132},
  file      = {Mesaros et al. - 2016 - TUT database for acoustic scene classification
               and.pdf:/Users/apodusenko/Zotero/storage/X6PA6WPS/Mesaros et al. - 2016
               - TUT database for acoustic scene classification
               and.pdf:application/pdf}
}

@inproceedings{wildhaber_signal_2017,
  title     = {Signal {Detection} and {Discrimination} for {Medical} {Devices} {
               Using} {Windowed} {State} {Space} {Filters}},
  url       = {http://www.actapress.com/PaperInfo.aspx?paperId=456429},
  doi       = {10.2316/P.2017.852-020},
  abstract  = {We introduce a model-based approach for computationally efficient
               signal detection and discrimination, which is relevant for
               biological signals.},
  urldate   = {2017-03-08},
  publisher = {ACTA Press},
  author    = {Wildhaber, Reto Andreas and Zalmai, Nour and Jacomet, Marcel and
               Loeliger, Hans-Andrea},
  month     = mar,
  year      = {2017},
  file      = {
               Snapshot:/Users/apodusenko/Zotero/storage/S8HF2SHG/PaperInfo.html:text/html;Wildhaber
               et al. - 2017 - Signal Detection and Discrimination for Medical
               De.pdf:/Users/apodusenko/Zotero/storage/7PN5NWNQ/Wildhaber et al. -
               2017 - Signal Detection and Discrimination for Medical
               De.pdf:application/pdf}
}

@article{lake_building_2016,
  title    = {Building {Machines} {That} {Learn} and {Think} {Like} {People}},
  url      = {http://arxiv.org/abs/1604.00289},
  abstract = {Recent progress in artificial intelligence (AI) has renewed
              interest in building systems that learn and think like people. Many
              advances have come from using deep neural networks trained
              end-to-end in tasks such as object recognition, video games, and
              board games, achieving performance that equals or even beats humans
              in some respects. Despite their biological inspiration and
              performance achievements, these systems differ from human
              intelligence in crucial ways. We review progress in cognitive
              science suggesting that truly human-like learning and thinking
              machines will have to reach beyond current engineering trends in
              both what they learn, and how they learn it. Specifically, we argue
              that these machines should (a) build causal models of the world
              that support explanation and understanding, rather than merely
              solving pattern recognition problems; (b) ground learning in
              intuitive theories of physics and psychology, to support and enrich
              the knowledge that is learned; and (c) harness compositionality and
              learning-to-learn to rapidly acquire and generalize knowledge to
              new tasks and situations. We suggest concrete challenges and
              promising routes towards these goals that can combine the strengths
              of recent neural network advances with more structured cognitive
              models.},
  urldate  = {2017-03-07},
  journal  = {arXiv:1604.00289 [cs, stat]},
  author   = {Lake, Brenden M. and Ullman, Tomer D. and Tenenbaum, Joshua B. and
              Gershman, Samuel J.},
  month    = apr,
  year     = {2016},
  note     = {arXiv: 1604.00289},
  keywords = {Computer Science - Learning, Statistics - Machine Learning,
              Computer Science - Artificial Intelligence, Computer Science -
              Neural and Evolutionary Computing, Computer Science - Computer
              Vision and Pattern Recognition},
  file     = {arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/VS8RK2NW/1604.html:text/html;Lake
              et al. - 2016 - Building Machines That Learn and Think Like
              People.pdf:/Users/apodusenko/Zotero/storage/5I8D5R3G/Lake et al. - 2016
              - Building Machines That Learn and Think Like
              People.pdf:application/pdf}
}

@article{gerstoft_multisnapshot_2016,
  title    = {Multisnapshot {Sparse} {Bayesian} {Learning} for {DOA}},
  volume   = {23},
  issn     = {1070-9908},
  doi      = {10.1109/LSP.2016.2598550},
  abstract = {The directions of arrival (DOA) of plane waves are estimated from
              multisnapshot sensor array data using sparse Bayesian learning
              (SBL). The prior for the source amplitudes is assumed independent
              zero-mean complex Gaussian distributed with hyperparameters, the
              unknown variances (i.e., the source powers). For a complex Gaussian
              likelihood with hyperparameter, the unknown noise variance, the
              corresponding Gaussian posterior distribution is derived. The
              hyperparameters are automatically selected by maximizing the
              evidence and promoting sparse DOA estimates. The SBL scheme for DOA
              estimation is discussed and evaluated competitively against LASSO
              (l(1)-regularization), conventional beamforming, and MUSIC.},
  number   = {10},
  journal  = {I E E E Signal Processing Letters},
  author   = {Gerstoft, Peter and Mecklenbrauker, Christoph F. and Xenaki,
              Angeliki and Nannuru, Santosh},
  year     = {2016},
  keywords = {Estimation, Signal processing algorithms, Bayes methods, Gaussian
              distribution, learning (artificial intelligence), Array signal
              processing, sensor arrays, Array processing, Arrayprocessing,
              Arrays, Beamforming, Complex Gaussian, complex Gaussian likelihood,
              compressive beamforming, compressivebeamforming, Conventional
              beamforming, Covariance matrices, Digital signal processing,
              Direction of arrival, Direction-of-arrival estimation, directions
              of arrival, directions of arrival (DOA) estimation, DOA estimation,
              hyperparameters, independent zero-mean complex Gaussian
              distribution, Knowledge engineering techniques, multisnapshot
              sensor array data, Other topics in statistics, plane waves,
              Posterior distributions, relevance vector machine, SBL, Sensing
              devices and transducers, Signal processing and detection, source
              amplitudes, source powers, Sparse Bayesian learning, Sparse
              Bayesian learning (SBL), sparse reconstruction, Zero-mean complex},
  pages    = {1469--1473},
  file     = {Gerstoft et al. - 2016 - Multisnapshot Sparse Bayesian Learning for
              DOA.pdf:/Users/apodusenko/Zotero/storage/BPQZWW9I/Gerstoft et al. -
              2016 - Multisnapshot Sparse Bayesian Learning for
              DOA.pdf:application/pdf}
}

@article{xenaki_block-sparse_2016,
  title    = {Block-sparse beamforming for spatially extended sources in a {
              Bayesian} formulation},
  volume   = {140},
  issn     = {1520-8524},
  doi      = {10.1121/1.4962325},
  abstract = {Direction-of-arrival (DOA) estimation refers to the localization
              of sound sources on an angular grid from noisy measurements of the
              associated wavefield with an array of sensors. For accurate
              localization, the number of angular look-directions is much larger
              than the number of sensors, hence, the problem is underdetermined
              and requires regularization. Traditional methods use an ℓ2-norm
              regularizer, which promotes minimum-power (smooth) solutions, while
              regularizing with ℓ1-norm promotes sparsity. Sparse signal
              reconstruction improves the resolution in DOA estimation in the
              presence of a few point sources, but cannot capture spatially
              extended sources. The DOA estimation problem is formulated in a
              Bayesian framework where regularization is imposed through prior
              information on the source spatial distribution which is then
              reconstructed as the maximum a posteriori estimate. A composite
              prior is introduced, which simultaneously promotes a piecewise
              constant profile and sparsity in the solution. Simulations and
              experimental measurements show that this choice of regularization
              provides high-resolution DOA estimation in a general framework,
              i.e., in the presence of spatially extended sources.},
  language = {eng},
  number   = {3},
  journal  = {The Journal of the Acoustical Society of America},
  author   = {Xenaki, Angeliki and Fernandez-Grande, Efren and Gerstoft, Peter},
  month    = sep,
  year     = {2016},
  pmid     = {27914408},
  pages    = {1828},
  file     = {Xenaki et al. - 2016 - Block-sparse beamforming for spatially extended
              so.pdf:/Users/apodusenko/Zotero/storage/UMGDU326/Xenaki et al. - 2016 -
              Block-sparse beamforming for spatially extended so.pdf:application/pdf}
}

@article{xenaki_compressive_2014,
  title    = {Compressive beamforming},
  volume   = {136},
  issn     = {1520-8524},
  doi      = {10.1121/1.4883360},
  abstract = {Sound source localization with sensor arrays involves the
              estimation of the direction-of-arrival (DOA) from a limited number
              of observations. Compressive sensing (CS) solves such
              underdetermined problems achieving sparsity, thus improved
              resolution, and can be solved efficiently with convex optimization.
              The DOA estimation problem is formulated in the CS framework and it
              is shown that CS has superior performance compared to traditional
              DOA estimation methods especially under challenging scenarios such
              as coherent arrivals and single-snapshot data. An offset and
              resolution analysis is performed to indicate the limitations of CS.
              It is shown that the limitations are related to the beampattern,
              thus can be predicted. The high-resolution capabilities and the
              robustness of CS are demonstrated on experimental array data from
              ocean acoustic measurements for source tracking with
              single-snapshot data.},
  language = {eng},
  number   = {1},
  journal  = {The Journal of the Acoustical Society of America},
  author   = {Xenaki, Angeliki and Gerstoft, Peter and Mosegaard, Klaus},
  month    = jul,
  year     = {2014},
  pmid     = {24993212},
  pages    = {260--271},
  file     = {Xenaki et al. - 2014 - Compressive
              beamforming.pdf:/Users/apodusenko/Zotero/storage/4NJVX96R/Xenaki et al.
              - 2014 - Compressive beamforming.pdf:application/pdf}
}

@inproceedings{broderick_streaming_2013,
  title     = {Streaming variational bayes},
  url       = {http://papers.nips.cc/paper/4980-streaming-variational-bayes},
  urldate   = {2017-02-22},
  booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
  author    = {Broderick, Tamara and Boyd, Nicholas and Wibisono, Andre and Wilson,
               Ashia C. and Jordan, Michael I.},
  year      = {2013},
  pages     = {1727--1735},
  file      = {Broderick et al. - 2013 - Streaming variational
               bayes.pdf:/Users/apodusenko/Zotero/storage/ZXDEC5CJ/Broderick et al. -
               2013 - Streaming variational bayes.pdf:application/pdf}
}

@article{gemici_generative_2017,
  title    = {Generative {Temporal} {Models} with {Memory}},
  url      = {http://arxiv.org/abs/1702.04649},
  abstract = {We consider the general problem of modeling temporal data with
              long-range dependencies, wherein new observations are fully or
              partially predictable based on temporally-distant, past
              observations. A sufficiently powerful temporal model should
              separate predictable elements of the sequence from unpredictable
              elements, express uncertainty about those unpredictable elements,
              and rapidly identify novel elements that may help to predict the
              future. To create such models, we introduce Generative Temporal
              Models augmented with external memory systems. They are developed
              within the variational inference framework, which provides both a
              practical training methodology and methods to gain insight into the
              models' operation. We show, on a range of problems with sparse,
              long-term temporal dependencies, that these models store
              information from early in a sequence, and reuse this stored
              information efficiently. This allows them to perform substantially
              better than existing models based on well-known recurrent neural
              networks, like LSTMs.},
  urldate  = {2017-02-16},
  journal  = {arXiv:1702.04649 [cs]},
  author   = {Gemici, Mevlana and Hung, Chia-Chun and Santoro, Adam and Wayne,
              Greg and Mohamed, Shakir and Rezende, Danilo J. and Amos, David and
              Lillicrap, Timothy},
  month    = feb,
  year     = {2017},
  note     = {arXiv: 1702.04649},
  keywords = {Computer Science - Learning, Computer Science - Neural and
              Evolutionary Computing},
  file     = {arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/KWI9845G/1702.html:text/html;Gemici
              et al. - 2017 - Generative Temporal Models with
              Memory.pdf:/Users/apodusenko/Zotero/storage/I7NPF4I3/Gemici et al. -
              2017 - Generative Temporal Models with Memory.pdf:application/pdf}
}

@article{ghahramani_propagation_2001,
  title  = {Propagation algorithms for variational {Bayesian} learning},
  author = {Ghahramani, Zoubin and Beal, Matthew J.},
  year   = {2001}
}

@inproceedings{kulhavy_tracking_1984,
  title     = {Tracking of slowly varying parameters by directional forgetting},
  booktitle = {Proc. 9th {IFAC} {World} {Congress}, {Budapest}, {Hungary}},
  author    = {Kulhavý, Rudolf and Kárný, Miroslav},
  year      = {1984},
  pages     = {78--83}
}

@article{kulhavy_duality_1996,
  title   = {On {Duality} of {Exponential} and {Linear} {Forgetting}},
  volume  = {32},
  number  = {10},
  urldate = {2016-04-06},
  journal = {Automatica},
  author  = {Kulhavý, R. and Kraus, F. J.},
  year    = {1996},
  pages   = {1403--1415},
  file    = {Kulhavỳ and Kraus - On Duality of Exponential and Linear
             Forgetting.pdf:/Users/apodusenko/Zotero/storage/8QRBRHEB/Kulhavỳ and
             Kraus - On Duality of Exponential and Linear
             Forgetting.pdf:application/pdf}
}

@article{kulhavy_general_1993,
  title    = {On a general concept of forgetting},
  volume   = {58},
  issn     = {0020-7179},
  url      = {http://dx.doi.org/10.1080/00207179308923034},
  doi      = {10.1080/00207179308923034},
  abstract = {Practice leads us to seek a simple method which would make
              parameter estimation (and subsequent control or signal processing)
              reliably adaptive. Unfortunately, in most applications we lack
              sufficient information to specify a complete model of parameter
              variations. In other words, the problem is ‘under-determined’ which
              prevents us from employing standard equations of probability
              calculus. In this paper we apply known principles of rational
              behaviour in such situations to propose a plausible and well
              justified solution. The result we get is close to classical
              exponential forgetting, but regularized by available prior
              information. We demonstrate the practical implications of this
              feature.},
  number   = {4},
  urldate  = {2016-10-17},
  journal  = {International Journal of Control},
  author   = {Kulhavý, R. and Zarrop, M. B.},
  month    = oct,
  year     = {1993},
  pages    = {905--924},
  file     = {Kulhavy - 1993 - On a general concept of
              forgetting.pdf:/Users/apodusenko/Zotero/storage/U8RXKJXU/Kulhavy - 1993
              - On a general concept of
              forgetting.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/JG2UQI6J/00207179308923034.html:text/html
              }
}

@article{karny_use_2009,
  title   = {Use of {Kullback}-{Leibler} divergence for forgetting},
  volume  = {23},
  number  = {10},
  journal = {International Journal of Adaptive Control and Signal Processing},
  author  = {Kárný, Miroslav and Andrýsek, Josef},
  year    = {2009},
  pages   = {961--975}
}

@article{karny_mixturebased_2003,
  title   = {Mixture‐based adaptive probabilistic control},
  volume  = {17},
  number  = {2},
  journal = {International Journal of adaptive control and signal processing},
  author  = {Kárný, Miroslav and Böhm, Josef and Guy, Tatiana V. and Nedoma, Petr
             },
  year    = {2003},
  pages   = {119--132}
}

@inproceedings{alvarado_gaussian_2016,
  address  = {SALERNO, ITALY},
  title    = {Gaussian {Processes} for {Music} {Audio} {Modelling} and {Content} {
              Analysis}},
  abstract = {Real music signals are highly variable, yet they have strong
              statistical structure. Prior information about the underlying
              physical mechanisms by which sounds are generated and rules by
              which complex sound structure is constructed (notes, chords, a
              complete musical score), can be naturally unified using Bayesian
              modelling techniques. Typically algorithms for Automatic Music
              Transcription independently carry out individual tasks such as
              multiple-F0 detection and beat tracking. The challenge remains to
              perform joint estimation of all parameters. We present a Bayesian
              approach for modelling music audio, and content analysis. The
              proposed methodology based on Gaussian processes seeks joint
              estimation of multiple music concepts by incorporating into the
              kernel prior information about non-stationary behaviour, dynamics,
              and rich spectral content present in the modelled music signal. We
              illustrate the benefits of this approach via two tasks: pitch
              estimation, and inferring missing segments in a polyphonic audio
              recording.},
  author   = {Alvarado, Pablo A. and Stowell, Dan},
  month    = sep,
  year     = {2016},
  file     = {Alvarado and Stowell - 2016 - Gaussian Processes for Music Audio
              Modelling and C.pdf:/Users/apodusenko/Zotero/storage/B5MCE9IM/Alvarado
              and Stowell - 2016 - Gaussian Processes for Music Audio Modelling and
              C.pdf:application/pdf}
}

@article{karklin_hierarchical_2005,
  title   = {A hierarchical {Bayesian} model for learning nonlinear statistical
             regularities in nonstationary natural signals},
  volume  = {17},
  number  = {2},
  journal = {Neural computation},
  author  = {Karklin, Yan and Lewicki, Michael S.},
  year    = {2005},
  pages   = {397--423}
}

@inproceedings{kraus_stabilized_1989,
  title     = {Stabilized least squares estimators for time-variant processes},
  booktitle = {Decision and {Control}, 1989., {Proceedings} of the 28th {IEEE} {
               Conference} on},
  publisher = {IEEE},
  author    = {Kraus, F. J.},
  year      = {1989},
  pages     = {1803--1804}
}

@inproceedings{milek_stabilized_1991,
  title     = {Stabilized least squares estimators for time variant processes},
  volume    = {1},
  booktitle = {Proceedings of the 1st {IFAC} {Symposium} on {Design} {Methods}
               of {Control} {Systems}},
  author    = {Milek, J. J. and Kraus, F. J.},
  year      = {1991},
  pages     = {430--435}
}

@book{milek_stabilized_1995,
  title     = {Stabilized adaptive forgetting in recursive parameter estimation},
  isbn      = {3-7281-2304-8},
  publisher = {vdf Hochschulverlag AG},
  author    = {Milek, J. T.},
  year      = {1995}
}

@misc{david_blei_et_al._nips_nodate,
  title  = {{NIPS} 2016 {VI} tutorial},
  author = {{David Blei et al.}},
  file   = {
            2016_NIPS_VI_tutorial.pdf:/Users/apodusenko/Zotero/storage/E8S65IGP/2016_NIPS_VI_tutorial.pdf:application/pdf
            }
}

@article{kording_ten_2016,
  title     = {Ten simple rules for structuring papers},
  copyright = {© 2016, Posted by Cold Spring Harbor Laboratory Press. This
               pre-print is available under a Creative Commons License
               (Attribution-NonCommercial 4.0 International), CC BY-NC 4.0, as
               described at http://creativecommons.org/licenses/by-nc/4.0/},
  url       = {http://biorxiv.org/content/early/2016/11/28/088278},
  doi       = {10.1101/088278},
  abstract  = {Good scientific writing is essential to career development and to
               the progress of science. A well-structured manuscript allows
               readers and reviewers to get excited about the subject matter, to
               understand and verify the paper's contributions, and to integrate
               them into a broader context. However, many scientists struggle with
               producing high-quality manuscripts and are typically given little
               training in paper writing. Focusing on how readers consume
               information, we present a set of 10 simple rules to help you get
               across the main idea of your paper. These rules are designed to
               make your paper more influential and the process of writing more
               efficient and pleasurable.},
  language  = {en},
  urldate   = {2016-12-12},
  journal   = {bioRxiv},
  author    = {Kording, Konrad P. and Mensh, Brett},
  month     = nov,
  year      = {2016},
  pages     = {088278},
  file      = {Kording and Mensh - 2016 - Ten simple rules for structuring
               papers.pdf:/Users/apodusenko/Zotero/storage/WI5HF45Q/Kording and Mensh
               - 2016 - Ten simple rules for structuring
               papers.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/WK9CFM7T/088278.html:text/html
               }
}

@article{friston_bayesian_2016,
  title    = {Bayesian model reduction and empirical {Bayes} for group ({DCM})
              studies},
  volume   = {128},
  issn     = {1053-8119},
  url      = {http://www.sciencedirect.com/science/article/pii/S105381191501037X},
  doi      = {10.1016/j.neuroimage.2015.11.015},
  abstract = {This technical note describes some Bayesian procedures for the
              analysis of group studies that use nonlinear models at the first
              (within-subject) level – e.g., dynamic causal models – and linear
              models at subsequent (between-subject) levels. Its focus is on
              using Bayesian model reduction to finesse the inversion of multiple
              models of a single dataset or a single (hierarchical or empirical
              Bayes) model of multiple datasets. These applications of Bayesian
              model reduction allow one to consider parametric random effects and
              make inferences about group effects very efficiently (in a few
              seconds). We provide the relatively straightforward theoretical
              background to these procedures and illustrate their application
              using a worked example. This example uses a simulated mismatch
              negativity study of schizophrenia. We illustrate the robustness of
              Bayesian model reduction to violations of the (commonly used)
              Laplace assumption in dynamic causal modelling and show how its
              recursive application can facilitate both classical and Bayesian
              inference about group differences. Finally, we consider the
              application of these empirical Bayesian procedures to
              classification and prediction.},
  urldate  = {2016-06-28},
  journal  = {NeuroImage},
  author   = {Friston, Karl J. and Litvak, Vladimir and Oswal, Ashwini and Razi,
              Adeel and Stephan, Klaas E. and van Wijk, Bernadette C. M. and
              Ziegler, Gabriel and Zeidman, Peter},
  month    = mar,
  year     = {2016},
  keywords = {Random effects, classification, Bayesian model reduction, Dynamic
              causal modelling, Empirical Bayes, Fixed effects, Hierarchical
              modelling},
  pages    = {413--431},
  file     = {Friston et al. - 2016 - Bayesian model reduction and empirical Bayes
              for g.pdf:/Users/apodusenko/Zotero/storage/I7ZR5DC3/Friston et al. -
              2016 - Bayesian model reduction and empirical Bayes for
              g.pdf:application/pdf;ScienceDirect
              Snapshot:/Users/apodusenko/Zotero/storage/QZDG4JX4/S105381191501037X.html:text/html
              }
}

@article{friston_active_2017,
  title   = {Active {Inference}, artificial curiosity and insight},
  journal = {Psychological Review},
  author  = {Friston, Karl and Lin, Marco and Frith, Christopher and Pezzulo,
             Giovanni and Hobson, J Allen and Ondobaka, Sasha},
  year    = {2017},
  file    = {Friston et al. - 2017 - Active Inference, artificial curiosity and
             insight.pdf:/Users/apodusenko/Zotero/storage/VP625MTG/Friston et al. -
             2017 - Active Inference, artificial curiosity and
             insight.pdf:application/pdf}
}

@inproceedings{gonzalez_glasses:_2016,
  title      = {{GLASSES}: {Relieving} {The} {Myopia} {Of} {Bayesian} {Optimisation}},
  shorttitle = {{GLASSES}},
  url        = {http://jmlr.org/proceedings/papers/v51/gonzalez16b.html},
  urldate    = {2016-11-10},
  author     = {Gonzalez, Javier and Osborne, Michael and Lawrence, Neil},
  year       = {2016},
  pages      = {790--799},
  file       = {Gonzalez et al. - 2016 - GLASSES Relieving The Myopia Of Bayesian
                Optimisa.pdf:/Users/apodusenko/Zotero/storage/AC8G9KC4/Gonzalez et al.
                - 2016 - GLASSES Relieving The Myopia Of Bayesian
                Optimisa.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/5BIG6TWI/gonzalez16b.html:text/html
                }
}

@article{mathys_bayesian_2011,
  title    = {A {Bayesian} foundation for individual learning under uncertainty},
  volume   = {5},
  issn     = {16625161},
  url      = {http://www.frontiersin.org/Journal/10.3389/fnhum.2011.00039/full},
  doi      = {10.3389/fnhum.2011.00039},
  urldate  = {2014-04-10},
  journal  = {Frontiers in Human Neuroscience},
  author   = {Mathys, Christoph D. and Daunizeau, Jean and Friston, Karl J. and
              Klaas, Stephan E.},
  year     = {2011},
  keywords = {neuroscience, variational Bayes},
  file     = {Mathys - 2011 - A Bayesian foundation for individual learning
              unde.pdf:/Users/apodusenko/Zotero/storage/2BNAPCFU/Mathys - 2011 - A
              Bayesian foundation for individual learning unde.pdf:application/pdf}
}

@inproceedings{roger_bioacoustic_2017,
  title   = {Bioacoustic {Sound} {Scene} {Representation} by {Hierarchical} {
             Dirichlet} {Process} for {Hidden} {Markov} {Model}},
  url     = {https://openreview.net/forum?id=Hk1l9Xqxe},
  urldate = {2016-11-29},
  author  = {Roger, Vincent and Bartcus, Marius and Chamroukhi, Faicel and Glotin
             , Herve},
  year    = {2017},
  file    = {Roger et al. - 2017 - Bioacoustic Sound Scene Representation by
             Hierarch.pdf:/Users/apodusenko/Zotero/storage/CJK2756F/Roger et al. -
             2017 - Bioacoustic Sound Scene Representation by
             Hierarch.pdf:application/pdf}
}

@article{chen_learning_2016,
  title    = {Learning to {Learn} for {Global} {Optimization} of {Black} {Box} {
              Functions}},
  url      = {http://arxiv.org/abs/1611.03824},
  abstract = {We present a learning to learn approach for training recurrent
              neural networks to perform black-box global optimization. In the
              meta-learning phase we use a large set of smooth target functions
              to learn a recurrent neural network (RNN) optimizer, which is
              either a long-short term memory network or a differentiable neural
              computer. After learning, the RNN can be applied to learn policies
              in reinforcement learning, as well as other black-box learning
              tasks, including continuous correlated bandits and experimental
              design. We compare this approach to Bayesian optimization, with
              emphasis on the issues of computation speed, horizon length, and
              exploration-exploitation trade-offs.},
  urldate  = {2016-11-25},
  journal  = {arXiv:1611.03824 [cs, stat]},
  author   = {Chen, Yutian and Hoffman, Matthew W. and Colmenarejo, Sergio Gomez
              and Denil, Misha and Lillicrap, Timothy P. and de Freitas, Nando},
  month    = nov,
  year     = {2016},
  note     = {arXiv: 1611.03824},
  keywords = {Computer Science - Learning, Statistics - Machine Learning},
  file     = {arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/Q6CUD6FA/1611.html:text/html;Chen
              et al. - 2016 - Learning to Learn for Global Optimization of
              Black.pdf:/Users/apodusenko/Zotero/storage/94FV7NI2/Chen et al. - 2016
              - Learning to Learn for Global Optimization of
              Black.pdf:application/pdf}
}

@article{friston_deep_2017,
  title   = {Deep temporal models and active inference},
  journal = {Neuroscience \& Biobehavioral Reviews},
  author  = {Friston, Karl and Rosch, Richard and Parr, Thomas and Price, Cathy
             and Bowman, Howard},
  year    = {2017},
  file    = {Friston et al. - 2017 - Deep temporal models and active
             inference.pdf:/Users/apodusenko/Zotero/storage/CFW9E5BW/Friston et al.
             - 2017 - Deep temporal models and active inference.pdf:application/pdf}
}

@article{mcdermott_sound_2011,
  title      = {Sound {Texture} {Perception} via {Statistics} of the {Auditory} {
                Periphery}: {Evidence} from {Sound} {Synthesis}},
  volume     = {71},
  issn       = {0896-6273},
  shorttitle = {Sound {Texture} {Perception} via {Statistics} of the {Auditory}
                {Periphery}},
  url        = {http://www.cell.com/neuron/abstract/S0896-6273(11)00562-9},
  doi        = {10.1016/j.neuron.2011.06.032},
  language   = {English},
  number     = {5},
  urldate    = {2016-11-21},
  journal    = {Neuron},
  author     = {McDermott, Josh H. and Simoncelli, Eero P.},
  month      = sep,
  year       = {2011},
  pmid       = {21903084},
  pages      = {926--940},
  file       = {McDermott and Simoncelli - 2011 - Sound Texture Perception via
                Statistics of the
                Aud.pdf:/Users/apodusenko/Zotero/storage/SQJ8EJZF/McDermott and
                Simoncelli - 2011 - Sound Texture Perception via Statistics of the
                Aud.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/JTTKA9FK/S0896-6273(11)00562-9.html:text/html
                }
}

@inproceedings{turner_statistical_2010,
  title     = {Statistical inference for single- and multi-band {Probabilistic} {
               Amplitude} {Demodulation}},
  url       = {
               https://www.researchgate.net/publication/224149610_Statistical_inference_for_single-_and_multi-band_Probabilistic_Amplitude_Demodulation
               },
  doi       = {10.1109/ICASSP.2010.5494909},
  abstract  = {Amplitude demodulation is an ill-posed problem and so it is
               natural to treat it from a Bayesian viewpoint, inferring the most
               likely carrier and envelope under probabilistic constraints. One
               such...},
  urldate   = {2016-11-21},
  booktitle = {{ResearchGate}},
  author    = {Turner, Richard E. and Sahani, Maneesh},
  month     = apr,
  year      = {2010},
  pages     = {5466--5469},
  file      = {
               Snapshot:/Users/apodusenko/Zotero/storage/BDXVPUUV/224149610_Statistical_inference_for_single-_and_multi-band_Probabilistic_Amplitude_Demodulation.html:text/html;Turner
               and Sahani - 2010 - Statistical inference for single- and multi-band
               P.pdf:/Users/apodusenko/Zotero/storage/8RF24UB6/Turner and Sahani -
               2010 - Statistical inference for single- and multi-band
               P.pdf:application/pdf}
}

@article{deng_deep_2015,
  title    = {Deep {Discriminative} and {Generative} {Models} for {Pattern} {
              Recognition}},
  url      = {
              https://www.microsoft.com/en-us/research/publication/deep-discriminative-and-generative-models-for-pattern-recognition/
              },
  abstract = {In this chapter we describe deep generative and discriminative
              models as they have been applied to speech recognition. The former
              models describe the distribution of data, whereas the latter models
              describe the distribution of targets conditioned on data. Both
              models are characterized as being ‘deep’ as they use layers of
              latent or hidden variables. Understanding …},
  urldate  = {2016-11-21},
  journal  = {Microsoft Research},
  author   = {Deng, Li and Jaitly, Navdeep},
  month    = nov,
  year     = {2015},
  file     = {Deng and Jaitly - 2015 - Deep Discriminative and Generative Models for
              Patt.pdf:/Users/apodusenko/Zotero/storage/DCN5JR8H/Deng and Jaitly -
              2015 - Deep Discriminative and Generative Models for
              Patt.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/8Z22TR2S/deep-discriminative-and-generative-models-for-pattern-recognition.html:text/html
              }
}

@article{johnson_composing_2016,
  title    = {Composing graphical models with neural networks for structured
              representations and fast inference},
  url      = {http://arxiv.org/abs/1603.06277},
  abstract = {We propose a general modeling and inference framework that
              composes probabilistic graphical models with deep learning methods
              and combines their respective strengths. Our model family augments
              graphical structure in latent variables with neural network
              observation models. For inference, we extend variational
              autoencoders to use graphical model approximating distributions
              with recognition networks that output conjugate potentials. All
              components of these models are learned simultaneously with a single
              objective, giving a scalable algorithm that leverages stochastic
              variational inference, natural gradients, graphical model message
              passing, and the reparameterization trick. We illustrate this
              framework with several example models and an application to mouse
              behavioral phenotyping.},
  urldate  = {2016-11-03},
  journal  = {arXiv:1603.06277 [stat]},
  author   = {Johnson, Matthew J. and Duvenaud, David and Wiltschko, Alexander B.
              and Datta, Sandeep R. and Adams, Ryan P.},
  month    = mar,
  year     = {2016},
  note     = {arXiv: 1603.06277},
  keywords = {Statistics - Machine Learning},
  file     = {arXiv\:1603.06277
              PDF:/Users/apodusenko/Zotero/storage/DFQMQUH8/Johnson et al. - 2016 -
              Composing graphical models with neural networks
              fo.pdf:application/pdf;arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/S47WEXQ5/1603.html:text/html}
}

@article{linderman_recurrent_2016,
  title    = {Recurrent switching linear dynamical systems},
  url      = {http://arxiv.org/abs/1610.08466},
  abstract = {Many natural systems, such as neurons firing in the brain or
              basketball teams traversing a court, give rise to time series data
              with complex, nonlinear dynamics. We can gain insight into these
              systems by decomposing the data into segments that are each
              explained by simpler dynamic units. Building on switching linear
              dynamical systems (SLDS), we present a new model class that not
              only discovers these dynamical units, but also explains how their
              switching behavior depends on observations or continuous latent
              states. These "recurrent" switching linear dynamical systems
              provide further insight by discovering the conditions under which
              each unit is deployed, something that traditional SLDS models fail
              to do. We leverage recent algorithmic advances in approximate
              inference to make Bayesian inference in these models easy, fast,
              and scalable.},
  urldate  = {2016-11-02},
  journal  = {arXiv:1610.08466 [stat]},
  author   = {Linderman, Scott W. and Miller, Andrew C. and Adams, Ryan P. and
              Blei, David M. and Paninski, Liam and Johnson, Matthew J.},
  month    = oct,
  year     = {2016},
  note     = {arXiv: 1610.08466},
  keywords = {Statistics - Machine Learning},
  file     = {arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/CI4VH65M/1610.html:text/html;Linderman
              et al. - 2016 - Recurrent switching linear dynamical
              systems.pdf:/Users/apodusenko/Zotero/storage/RBF8Q95I/Linderman et al.
              - 2016 - Recurrent switching linear dynamical
              systems.pdf:application/pdf}
}

@phdthesis{turner_statistical_2010-1,
  title    = {Statistical {Models} for {Natural} {Sounds}},
  abstract = {It is important to understand the rich structure of natural sounds
              in order to solve important tasks, like automatic speech
              recognition, and to understand auditory processing in the brain.
              This thesis takes a step in this direction by characterising the
              statistics of simple natural sounds. We focus on the statistics
              because perception often appears to depend on them, rather than on
              the raw waveform. For example the perception of auditory textures,
              like running water, wind, fire and rain, depends on
              summary-statistics, like the rate of falling rain droplets, rather
              than on the exact details of the physical source. In order to
              analyse the statistics of sounds accurately it is necessary to
              improve a number of traditional signal processing methods,
              including those for amplitude demodulation, time-frequency analysis
              , and sub-band demodulation. These estimation tasks are ill-posed
              and therefore it is natural to treat them as Bayesian inference
              problems. The new probabilistic versions of these methods have
              several advantages. For example, they perform more accurately on
              natural signals and are more robust to noise, they can also fill-in
              missing sections of data, and provide error-bars. Furthermore,
              free-parameters can be learned from the signal. Using these new
              algorithms we demonstrate that the energy, sparsity, modulation
              depth and modulation time-scale in each sub-band of a signal are
              critical statistics, together with the dependencies between the
              sub-band modulators. In order to validate this claim, a model
              containing co-modulated coloured noise carriers is shown to be
              capable of generating a range of realistic sounding auditory
              textures. Finally, we explored the connection between the
              statistics of natural sounds and perception. We demonstrate that
              inference in the model for auditory textures qualitatively
              replicates the primitive grouping rules that listeners use to
              understand simple acoustic scenes. This suggests that the auditory
              system is optimised for the statistics of natural sounds.},
  school   = {Gatsby Computational Neuroscience Unit, UCL},
  author   = {Turner, Richard E.},
  year     = {2010},
  file     = {Turner - 2010 - Statistical Models for Natural
              Sounds.pdf:/Users/apodusenko/Zotero/storage/3MI735HA/Turner - 2010 -
              Statistical Models for Natural Sounds.pdf:application/pdf}
}

@book{jazwinski_stochastic_2007,
  title     = {Stochastic processes and filtering theory},
  isbn      = {0-486-46274-9},
  publisher = {Courier Corporation},
  author    = {Jazwinski, Andrew H.},
  year      = {2007}
}

@inproceedings{masegosa_bayesian_2017,
  address   = {Sydney, Australia},
  title     = {Bayesian {Models} of {Data} {Streams} with {Hierarchical} {Power} {
               Priors}},
  url       = {http://proceedings.mlr.press/v70/masegosa17a/masegosa17a.pdf},
  booktitle = {Proceedings of the 34 th {International} {Conference} on {Machine
               } {Learning}},
  author    = {Masegosa, Andres and Nielsen, Thomas and Ramos-Lopez, Darıo and
               Salmeron, Antonio and Madsen, Anders},
  year      = {2017},
  file      = {Masegosa et al. - 2017 - Bayesian Models of Data Streams with
               Hierarchical .pdf:/Users/apodusenko/Zotero/storage/S4NSSQEP/Masegosa et
               al. - 2017 - Bayesian Models of Data Streams with Hierarchical
               .pdf:application/pdf}
}

@inproceedings{wadehn_outlier-insensitive_2016,
  title     = {Outlier-insensitive {Kalman} smoothing and marginal message passing},
  doi       = {10.1109/EUSIPCO.2016.7760447},
  abstract  = {We propose a new approach to outlier-insensitive Kalman smoothing
               based on normal priors with unknown variance (NUV). In contrast to
               prior work, the actual computations amount essentially to
               iterations of a standard Kalman smoother (with few extra
               computations). The proposed approach is easily extended to
               nonlinear estimation problems by combining the outlier detection
               with an extended Kalman smoother. For the Kalman smoothing, we
               consider both a Modified Bryson-Frasier smoother and the recently
               proposed Backward Information Filter Forward Marginal smoother,
               neither of which requires matrix inversions.},
  booktitle = {2016 24th {European} {Signal} {Processing} {Conference} ({EUSIPCO
               })},
  author    = {Wadehn, F. and Bruderer, L. and Dauwels, J. and Sahdeva, V. and Yu,
               H. and Loeliger, H. A.},
  month     = aug,
  year      = {2016},
  keywords  = {backward information filter, Estimation, Europe, extended Kalman
               smoother, forward marginal smoother, Kalman filters, marginal
               message passing, Message passing, modified Bryson-Frasier smoother,
               outlier-insensitive Kalman smoothing, smoothing methods, standard
               Kalman smoother, Standards, unknown variance},
  pages     = {1242--1246},
  file      = {IEEE Xplore Abstract
               Record:/Users/apodusenko/Zotero/storage/J97XHJW2/7760447.html:text/html;Wadehn
               e.a. - 2016 - Outlier-insensitive Kalman smoothing and marginal
               .pdf:/Users/apodusenko/Zotero/storage/EWBXKZZ4/Wadehn e.a. - 2016 -
               Outlier-insensitive Kalman smoothing and marginal .pdf:application/pdf}
}

@article{kong_deep_2016,
  title   = {Deep neural network baseline for {DCASE} challenge 2016},
  url     = {http://epubs.surrey.ac.uk/813518/},
  urldate = {2017-09-28},
  journal = {Proceedings of DCASE 2016},
  author  = {Kong, Qiuqiang and Sobieraj, Iwnoa and Wang, Wenwu and Plumbley,
             Mark D.},
  year    = {2016},
  file    = {Kong et al. - 2016 - Deep neural network baseline for DCASE challenge
             2.pdf:/Users/apodusenko/Zotero/storage/NP6WSW9U/Kong et al. - 2016 -
             Deep neural network baseline for DCASE challenge 2.pdf:application/pdf}
}

@inproceedings{bae_acoustic_2016,
  title     = {Acoustic scene classification using parallel combination of {LSTM}
               and {CNN}},
  url       = {
               https://www.cs.tut.fi/sgn/arg/dcase2016/documents/workshop/Bae-DCASE2016workshop.pdf
               },
  urldate   = {2017-09-28},
  booktitle = {Proceedings of the {Detection} and {Classification} of {Acoustic}
               {Scenes} and {Events} 2016 {Workshop} ({DCASE2016})},
  author    = {Bae, Soo Hyun and Choi, Inkyu and Kim, Nam Soo},
  year      = {2016},
  file      = {Bae et al. - 2016 - Acoustic scene classification using parallel
               combi.pdf:/Users/apodusenko/Zotero/storage/VE2FGK78/Bae et al. - 2016 -
               Acoustic scene classification using parallel combi.pdf:application/pdf}
}

@article{park_score_2016,
  title   = {Score fusion of classification systems for acoustic scene
             classification},
  url     = {
             http://www.cs.tut.fi/sgn/arg/dcase2016/documents/challenge_technical_reports/Task1/Ko_2016_1_task1.pdf
             },
  urldate = {2017-09-28},
  journal = {IEEE AASP Challenge on Detection and Classification of Acoustic
             Scenes and Events (DCASE)},
  author  = {Park, Sangwook and Mun, Seongkyu and Lee, Younglo and Ko, Hanseok},
  year    = {2016},
  file    = {
             Ko_2016_1_task1.pdf:/Users/apodusenko/Zotero/storage/KNXFXIDQ/Ko_2016_1_task1.pdf:application/pdf
             }
}

@inproceedings{ebbers_hidden_2017,
  title     = {Hidden {Markov} {Model} {Variational} {Autoencoder} for {Acoustic} {
               Unit} {Discovery}},
  url       = {http://www.isca-speech.org/archive/Interspeech_2017/abstracts/1160.html
               },
  doi       = {10.21437/Interspeech.2017-1160},
  language  = {en},
  urldate   = {2017-09-27},
  publisher = {ISCA},
  author    = {Ebbers, Janek and Heymann, Jahn and Drude, Lukas and Glarner, Thomas
               and Haeb-Umbach, Reinhold and Raj, Bhiksha},
  month     = aug,
  year      = {2017},
  pages     = {488--492},
  file      = {
               1160.PDF:/Users/apodusenko/Zotero/storage/QM8DNDC9/1160.PDF:application/pdf
               }
}

@inproceedings{sonowal_audio_2017,
  title     = {Audio {Classification} {Using} {Class}-{Specific} {Learned} {
               Descriptors}},
  url       = {http://www.isca-speech.org/archive/Interspeech_2017/abstracts/0982.html
               },
  doi       = {10.21437/Interspeech.2017-982},
  language  = {en},
  urldate   = {2017-09-27},
  publisher = {ISCA},
  author    = {Sonowal, Sukanya and Sandhan, Tushar and Choi, Inkyu and Kim, Nam
               Soo},
  month     = aug,
  year      = {2017},
  pages     = {484--487},
  file      = {
               0982.PDF:/Users/apodusenko/Zotero/storage/UWUM43B2/0982.PDF:application/pdf
               }
}

@article{hsu_unsupervised_2017,
  title    = {Unsupervised {Learning} of {Disentangled} and {Interpretable} {
              Representations} from {Sequential} {Data}},
  url      = {http://arxiv.org/abs/1709.07902},
  abstract = {We present a factorized hierarchical variational autoencoder,
              which learns disentangled and interpretable representations from
              sequential data without supervision. Specifically, we exploit the
              multi-scale nature of information in sequential data by formulating
              it explicitly within a factorized hierarchical graphical model that
              imposes sequence-dependent priors and sequence-independent priors
              to different sets of latent variables. The model is evaluated on
              two speech corpora to demonstrate, qualitatively, its ability to
              transform speakers or linguistic content by manipulating different
              sets of latent variables; and quantitatively, its ability to
              outperform an i-vector baseline for speaker verification and reduce
              the word error rate by as much as 35\% in mismatched train/test
              scenarios for automatic speech recognition tasks.},
  urldate  = {2017-09-27},
  journal  = {arXiv:1709.07902 [cs, eess, stat]},
  author   = {Hsu, Wei-Ning and Zhang, Yu and Glass, James},
  month    = sep,
  year     = {2017},
  note     = {arXiv: 1709.07902},
  keywords = {Computer Science - Learning, Computer Science - Sound, Statistics
              - Machine Learning, Computer Science - Computation and Language,
              Electrical Engineering and Systems Science - Audio and Speech
              Processing},
  file     = {arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/W8STT2E4/1709.html:text/html;Hsu
              et al. - 2017 - Unsupervised Learning of Disentangled and
              Interpre.pdf:/Users/apodusenko/Zotero/storage/N39LDIFR/Hsu et al. -
              2017 - Unsupervised Learning of Disentangled and
              Interpre.pdf:application/pdf}
}

@article{shi_zhusuan:_2017,
  title      = {{ZhuSuan}: {A} {Library} for {Bayesian} {Deep} {Learning}},
  shorttitle = {{ZhuSuan}},
  url        = {http://arxiv.org/abs/1709.05870},
  abstract   = {In this paper we introduce ZhuSuan, a python probabilistic
                programming library for Bayesian deep learning, which conjoins the
                complimentary advantages of Bayesian methods and deep learning.
                ZhuSuan is built upon Tensorflow. Unlike existing deep learning
                libraries, which are mainly designed for deterministic neural
                networks and supervised tasks, ZhuSuan is featured for its deep
                root into Bayesian inference, thus supporting various kinds of
                probabilistic models, including both the traditional hierarchical
                Bayesian models and recent deep generative models. We use running
                examples to illustrate the probabilistic programming on ZhuSuan,
                including Bayesian logistic regression, variational auto-encoders,
                deep sigmoid belief networks and Bayesian recurrent neural
                networks.},
  urldate    = {2017-09-21},
  journal    = {arXiv:1709.05870 [cs, stat]},
  author     = {Shi, Jiaxin and Chen, Jianfei and Zhu, Jun and Sun, Shengyang and
                Luo, Yucen and Gu, Yihong and Zhou, Yuhao},
  month      = sep,
  year       = {2017},
  note       = {arXiv: 1709.05870},
  keywords   = {Computer Science - Learning, Statistics - Machine Learning,
                Computer Science - Artificial Intelligence, Statistics -
                Computation, Computer Science - Neural and Evolutionary Computing},
  file       = {arXiv.org
                Snapshot:/Users/apodusenko/Zotero/storage/WVHLDIMQ/1709.html:text/html;Shi
                e.a. - 2017 - ZhuSuan A Library for Bayesian Deep
                Learning.pdf:/Users/apodusenko/Zotero/storage/2FUILEB4/Shi e.a. - 2017
                - ZhuSuan A Library for Bayesian Deep Learning.pdf:application/pdf}
}

@article{chachada_environmental_2014,
  title      = {Environmental sound recognition: a survey},
  volume     = {3},
  issn       = {2048-7703},
  shorttitle = {Environmental sound recognition},
  url        = {http://www.journals.cambridge.org/abstract_S2048770314000122},
  doi        = {10.1017/ATSIP.2014.12},
  language   = {en},
  urldate    = {2017-09-19},
  journal    = {APSIPA Transactions on Signal and Information Processing},
  author     = {Chachada, Sachin and Kuo, C.-C. Jay},
  year       = {2014},
  file       = {Chachada and Kuo - 2014 - Environmental sound recognition a
                survey.pdf:/Users/apodusenko/Zotero/storage/QFC55RNJ/Chachada and Kuo -
                2014 - Environmental sound recognition a survey.pdf:application/pdf}
}

@inproceedings{qian_speech_2017,
  title  = {Speech {Enhancement} {Using} {Bayesian} {Wavenet}},
  author = {Qian, Kaizhi and Zhang, Yang and Chang, Shiyu and Yang, Xuesong and
            Florêncio, Dinei and Hasegawa-Johnson, Mark},
  year   = {2017},
  file   = {Qian e.a. - 2017 - Speech Enhancement Using Bayesian
            Wavenet.pdf:/Users/apodusenko/Zotero/storage/UTNZX6EW/Qian e.a. - 2017
            - Speech Enhancement Using Bayesian Wavenet.pdf:application/pdf}
}

@misc{noauthor_speech_nodate,
  title    = {Speech {Enhancement} {Using} {Bayesian} {Wavenet} - {Semantic} {
              Scholar}},
  url      = {
              /paper/Speech-Enhancement-Using-Bayesian-Wavenet-Qian-Zhang/5e7481a1254b02f220b3c2b61397309193de0fba
              },
  abstract = {In recent years, deep learning has achieved great success in
              speech enhancement. However, there are two major limitations
              regarding existing works. First, the Bayesian framework is not
              adopted in many such deep-learning-based algorithms. In particular,
              the prior distribution for speech in the Bayesian framework has
              been shown useful by regularizing the output to be in the speech
              space, and thus improving the performance. Second, the majority of
              the existing methods operate on the frequency domain of the noisy
              speech, such as spectrogram and its variations. The clean speech is
              then reconstructed using the approach of overlap-add, which is
              limited by its inherent performance upper bound. This paper
              presents a Bayesian speech enhancement framework, called BaWN
              (Bayesian WaveNet), which directly operates on raw audio samples.
              It adopts the recently announced WaveNet, which is shown to be
              effective in modeling conditional distributions of speech samples
              while generating natural speech. Experiments show that BaWN is able
              to recover clean and natural speech.},
  urldate  = {2017-09-18},
  file     = {
              Snapshot:/Users/apodusenko/Zotero/storage/2TRKT4WE/5e7481a1254b02f220b3c2b61397309193de0fba.html:text/html
              }
}

@article{vakman_amplitude_nodate,
  title   = {Amplitude, phase, frequency—fundamental concepts of oscillation
             theory},
  url     = {http://iopscience.iop.org/article/10.1070/PU1977v020n12ABEH005479/meta},
  urldate = {2017-09-18},
  author  = {Vakman, D. E. and {Vaĭ}},
  file    = {
             pdf.pdf:/Users/apodusenko/Zotero/storage/JP9APTVH/pdf.pdf:application/pdf
             }
}

@inproceedings{shiran_enhanced_2009,
  title     = {Enhanced {PESQ} algorithm for objective assessment of speech quality
               at a continuous varying delay},
  doi       = {10.1109/QOMEX.2009.5246960},
  abstract  = {ITU-T P.862 - ldquoPerceptual Evaluation of Speech Quality
               (PESQ)rdquo is well known as an intrusive objective speech quality
               assessment method. Some reports have found that the PESQ time
               alignment mechanism fails to estimate delay where signals with high
               packet loss rate and dynamic time processing are present. A new
               time-alignment algorithm to improve the PESQ accuracy for
               time-scale modified voice transmission is suggested here. In the
               propose model, the time alignment of reference and degraded speech
               is estimated using Dynamic Time- Warping (DTW) in contrast to
               correlation and splitting methods used in the standard PESQ.
               Comparative results versus subjective Mean Opinion Score (MOS) show
               improvement in cases where dynamic time processing of signals is
               present.},
  booktitle = {2009 {International} {Workshop} on {Quality} of {Multimedia} {
               Experience}},
  author    = {Shiran, N. and Shallom, I. D.},
  month     = jul,
  year      = {2009},
  keywords  = {Speech enhancement, Speech analysis, Testing, Psychology,
               Degradation, Speech processing, Psychoacoustic models, Quality
               assessment, continuous varying delay, Delay effects, Delay
               estimation, DTW, dynamic time-warping, intrusive objective speech
               quality assessment method, ITU-T P.862, mean opinion score, MOS,
               objective assessment, perceptual evaluation of speech quality, PESQ
               , Time-Alignment, time-scale modified voice transmission},
  pages     = {157--162},
  file      = {IEEE Xplore Abstract
               Record:/Users/apodusenko/Zotero/storage/BD7DXVQZ/5246960.html:text/html;IEEE
               Xplore Full Text PDF:/Users/apodusenko/Zotero/storage/XJAQ6SXC/Shiran
               and Shallom - 2009 - Enhanced PESQ algorithm for objective assessment
               o.pdf:application/pdf}
}

@inproceedings{kressner_robustness_2011,
  title     = {Robustness of the {Hearing} {Aid} {Speech} {Quality} {Index} ({HASQI}
               )},
  doi       = {10.1109/ASPAA.2011.6082343},
  abstract  = {Objective measures of speech quality have been the subject of
               significant prior work, particularly in the areas of speech codecs
               and communication channels for normal-hearing listeners. One of the
               primary concerns of researchers in this area is how these metrics
               generalize to datasets or listener studies which are “unknown” to
               the measures. Another growing concern is how these metrics perform
               for the hearing-impaired community. Researchers working with the
               this community need to be able to predict how hearing-impaired
               listeners will perceive the quality of speech, as well as how they
               will perceive the quality of speech processed specifically by
               hearing aids. A relatively recent metric, the Hearing Aid Speech
               Quality Index (HASQI), is a model-based objective measure of
               quality developed in the context of hearing aids for normal-hearing
               and hearing-impaired listeners (Kates \& Arehart, Journal of the
               Audio Engineering Society, 2010). As such, HASQI makes substantial
               progress on some of the generalization issues. However, HASQI has
               not been tested thus far on any datasets other than the one on
               which it was trained. The objective of this study is to demonstrate
               the robustness of HASQI in predicting subjective quality. We use an
               “unknown” dataset of noisy speech processed by noise suppression
               algorithms, along with a corresponding set of subjective quality
               scores from normal-hearing listeners, to demonstrate HASQI's
               prediction performance. Furthermore, we compare HASQI's performance
               with that of several other objective measures in order to provide a
               point of reference.},
  booktitle = {2011 {IEEE} {Workshop} on {Applications} of {Signal} {Processing}
               to {Audio} and {Acoustics} ({WASPAA})},
  author    = {Kressner, A. A. and Anderson, D. V. and Rozell, C. J.},
  month     = oct,
  year      = {2011},
  keywords  = {Signal to noise ratio, Speech, Indexes, cepstral analysis, Speech
               processing, interference suppression, communication channel,
               Correlation, handicapped aids, HASQI, hearing aid speech quality
               index, Hearing Aid Speech Quality Index (HASQI), hearing-impaired
               community, model-based objective measure, noise suppression
               algorithm, normal-hearing listener, objective measure, speech
               codecs, speech processed quality, speech quality assessment,
               telecommunication channels},
  pages     = {209--212},
  file      = {IEEE Xplore Abstract
               Record:/Users/apodusenko/Zotero/storage/UHH9GRJD/6082343.html:text/html;IEEE
               Xplore Full Text PDF:/Users/apodusenko/Zotero/storage/JMFEMVWQ/Kressner
               et al. - 2011 - Robustness of the Hearing Aid Speech Quality
               Index.pdf:application/pdf}
}

@article{shannon_speech_1995,
  title    = {Speech recognition with primarily temporal cues},
  volume   = {270},
  issn     = {0036-8075},
  abstract = {Nearly perfect speech recognition was observed under conditions of
              greatly reduced spectral information. Temporal envelopes of speech
              were extracted from broad frequency bands and were used to modulate
              noises of the same bandwidths. This manipulation preserved temporal
              envelope cues in each band but restricted the listener to severely
              degraded information on the distribution of spectral energy. The
              identification of consonants, vowels, and words in simple sentences
              improved markedly as the number of bands increased; high speech
              recognition performance was obtained with only three bands of
              modulated noise. Thus, the presentation of a dynamic temporal
              pattern in only a few broad spectral regions is sufficient for the
              recognition of speech.},
  language = {eng},
  number   = {5234},
  journal  = {Science (New York, N.Y.)},
  author   = {Shannon, R. V. and Zeng, F. G. and Kamath, V. and Wygonski, J. and
              Ekelid, M.},
  month    = oct,
  year     = {1995},
  pmid     = {7569981},
  keywords = {Noise, Humans, Speech Perception, hearing, Auditory Threshold,
              Cues, Temporal Lobe},
  pages    = {303--304}
}

@incollection{darwin_chapter_1995,
  address   = {San Diego},
  series    = {Handbook of {Perception} and {Cognition}},
  title     = {Chapter 11 - {Auditory} {Grouping}},
  isbn      = {978-0-12-505626-7},
  url       = {http://www.sciencedirect.com/science/article/pii/B9780125056267500133},
  booktitle = {Hearing},
  publisher = {Academic Press},
  author    = {Darwin, C. J. and Carlyon, R. P.},
  editor    = {Moore, Brian C. J.},
  year      = {1995},
  doi       = {10.1016/B978-012505626-7/50013-3},
  pages     = {387--424},
  file      = {ScienceDirect
               Snapshot:/Users/apodusenko/Zotero/storage/P4WAR7JM/B9780125056267500133.html:text/html
               }
}

@book{bregman_auditory_1990,
  title     = {Auditory scene analysis},
  volume    = {10},
  url       = {
               http://webpages.mcgill.ca/staff/Group2/abregm1/web/pdf/2004_%20Encyclopedia-Soc-Behav-Sci.pdf
               },
  urldate   = {2017-09-16},
  publisher = {Cambridge, ma: mit press},
  author    = {Bregman, Albert S. and {others}},
  year      = {1990},
  file      = {
               988064c7be5bb180b8220aa07a43b061c780.pdf:/Users/apodusenko/Zotero/storage/2CWAGPHE/988064c7be5bb180b8220aa07a43b061c780.pdf:application/pdf
               }
}

@inproceedings{attias_temporal_1997,
  title     = {Temporal low-order statistics of natural sounds},
  url       = {
               http://papers.nips.cc/paper/1262-temporal-low-order-statistics-of-natural-sounds.pdf
               },
  urldate   = {2017-09-16},
  booktitle = {Advances in neural information processing systems},
  author    = {Attias, Hagai and Schreiner, Christoph E.},
  year      = {1997},
  pages     = {27--33},
  file      = {
               1262-temporal-low-order-statistics-of-natural-sounds.pdf:/Users/apodusenko/Zotero/storage/KH4M9AQ4/1262-temporal-low-order-statistics-of-natural-sounds.pdf:application/pdf
               }
}

@article{helmholtz_concerning_1866,
  title   = {Concerning the perceptions in general},
  journal = {Treatise on physiological optics,},
  author  = {Helmholtz, H. von},
  year    = {1866}
}

@article{grunwald_inconsistency_2014,
  title   = {Inconsistency of {Bayesian} inference for misspecified linear models,
             and a proposal for repairing it},
  journal = {arXiv preprint arXiv:1412.3730},
  author  = {Grünwald, Peter and van Ommen, Thijs},
  year    = {2014}
}

@article{box_science_1976,
  title   = {Science and {Statistics}},
  journal = {Journal of the American Statistical Association},
  author  = {Box, George EP},
  year    = {1976},
  pages   = {791--799}
}

@article{box_sampling_1980,
  title   = {Sampling and {Bayes}' inference in scientific modelling and
             robustness},
  journal = {Journal of the Royal Statistical Society. Series A (General)},
  author  = {Box, George EP},
  year    = {1980},
  pages   = {383--430}
}

@article{hafner_tensorflow_2017,
  title      = {{TensorFlow} {Agents}: {Efficient} {Batched} {Reinforcement} {
                Learning} in {TensorFlow}},
  shorttitle = {{TensorFlow} {Agents}},
  url        = {https://scirate.com/arxiv/1709.02878},
  abstract   = {We introduce TensorFlow Agents, an efficient infrastructure
                paradigm for building parallel reinforcement learning algorithms in
                TensorFlow. We simulate multiple environments in parallel, and
                group them to perform the neural network computation on a batch
                rather than individual observations. This allows the TensorFlow
                execution engine to parallelize computation, without the need for
                manual synchronization. Environments are stepped in separate Python
                processes to progress them in parallel without interference of the
                global interpreter lock. As part of this project, we introduce
                BatchPPO, an efficient implementation of the proximal policy
                optimization algorithm. By open sourcing TensorFlow Agents, we hope
                to provide a flexible starting point for future projects that
                accelerates future research in the field.},
  urldate    = {2017-09-12},
  journal    = {SciRate},
  author     = {Hafner, Danijar and Davidson, James and Vanhoucke, Vincent},
  month      = sep,
  year       = {2017},
  file       = {Hafner e.a. - 2017 - TensorFlow Agents Efficient Batched
                Reinforcement.pdf:/Users/apodusenko/Zotero/storage/KFGH8EIW/Hafner e.a.
                - 2017 - TensorFlow Agents Efficient Batched
                Reinforcement.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/TD7USDB5/1709.html:text/html
                }
}

@article{russo_tutorial_2017,
  title    = {A {Tutorial} on {Thompson} {Sampling}},
  url      = {http://arxiv.org/abs/1707.02038},
  abstract = {Thompson sampling is an algorithm for online decision problems
              where actions are taken sequentially in a manner that must balance
              between exploiting what is known to maximize immediate performance
              and investing to accumulate new information that may improve future
              performance. The algorithm addresses a broad range of problems in a
              computationally efficient manner and is therefore enjoying wide
              use. This tutorial covers the algorithm and its application,
              illustrating concepts through a range of examples, including
              Bernoulli bandit problems, shortest path problems, dynamic pricing,
              recommendation, active learning with neural networks, and
              reinforcement learning in Markov decision processes. Most of these
              problems involve complex information structures, where information
              revealed by taking an action informs beliefs about other actions.
              We will also discuss when and why Thompson sampling is or is not
              effective and relations to alternative algorithms.},
  urldate  = {2017-09-07},
  journal  = {arXiv:1707.02038 [cs]},
  author   = {Russo, Daniel and Van Roy, Benjamin and Kazerouni, Abbas and Osband,
              Ian},
  month    = jul,
  year     = {2017},
  note     = {arXiv: 1707.02038},
  keywords = {Computer Science - Learning},
  file     = {arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/4UEV2IWU/1707.html:text/html;Russo
              e.a. - 2017 - A Tutorial on Thompson
              Sampling.pdf:/Users/apodusenko/Zotero/storage/J63357II/Russo e.a. -
              2017 - A Tutorial on Thompson Sampling.pdf:application/pdf}
}

@article{ortega_bayesian_2009,
  title    = {A {Bayesian} {Rule} for {Adaptive} {Control} based on {Causal} {
              Interventions}},
  url      = {http://arxiv.org/abs/0911.5104},
  abstract = {Explaining adaptive behavior is a central problem in artificial
              intelligence research. Here we formalize adaptive agents as mixture
              distributions over sequences of inputs and outputs (I/O). Each
              distribution of the mixture constitutes a `possible world', but the
              agent does not know which of the possible worlds it is actually
              facing. The problem is to adapt the I/O stream in a way that is
              compatible with the true world. A natural measure of adaptation can
              be obtained by the Kullback-Leibler (KL) divergence between the I/O
              distribution of the true world and the I/O distribution expected by
              the agent that is uncertain about possible worlds. In the case of
              pure input streams, the Bayesian mixture provides a well-known
              solution for this problem. We show, however, that in the case of
              I/O streams this solution breaks down, because outputs are issued
              by the agent itself and require a different probabilistic syntax as
              provided by intervention calculus. Based on this calculus, we
              obtain a Bayesian control rule that allows modeling adaptive
              behavior with mixture distributions over I/O streams. This rule
              might allow for a novel approach to adaptive control based on a
              minimum KL-principle.},
  urldate  = {2017-09-04},
  journal  = {arXiv:0911.5104 [cs]},
  author   = {Ortega, Pedro A. and Braun, Daniel A.},
  month    = nov,
  year     = {2009},
  note     = {arXiv: 0911.5104},
  keywords = {Computer Science - Learning, Computer Science - Artificial
              Intelligence},
  file     = {arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/FK4JTS2E/0911.html:text/html;Ortega
              and Braun - 2009 - A Bayesian Rule for Adaptive Control based on
              Caus.pdf:/Users/apodusenko/Zotero/storage/9I4JS3N4/Ortega and Braun -
              2009 - A Bayesian Rule for Adaptive Control based on
              Caus.pdf:application/pdf}
}

@article{adams_predictions_2013,
  title      = {Predictions not commands: active inference in the motor system},
  volume     = {218},
  issn       = {1863-2661},
  shorttitle = {Predictions not commands},
  doi        = {10.1007/s00429-012-0475-5},
  abstract   = {The descending projections from motor cortex share many features
                with top-down or backward connections in visual cortex; for example
                , corticospinal projections originate in infragranular layers, are
                highly divergent and (along with descending cortico-cortical
                projections) target cells expressing NMDA receptors. This is
                somewhat paradoxical because backward modulatory characteristics
                would not be expected of driving motor command signals. We resolve
                this apparent paradox using a functional characterisation of the
                motor system based on Helmholtz's ideas about perception; namely,
                that perception is inference on the causes of visual sensations. We
                explain behaviour in terms of inference on the causes of
                proprioceptive sensations. This explanation appeals to active
                inference, in which higher cortical levels send descending
                proprioceptive predictions, rather than motor commands. This
                process mirrors perceptual inference in sensory cortex, where
                descending connections convey predictions, while ascending
                connections convey prediction errors. The anatomical substrate of
                this recurrent message passing is a hierarchical system consisting
                of functionally asymmetric driving (ascending) and modulatory
                (descending) connections: an arrangement that we show is almost
                exactly recapitulated in the motor system, in terms of its laminar,
                topographic and physiological characteristics. This perspective
                casts classical motor reflexes as minimising prediction errors and
                may provide a principled explanation for why motor cortex is
                agranular.},
  language   = {eng},
  number     = {3},
  journal    = {Brain Structure \& Function},
  author     = {Adams, Rick A. and Shipp, Stewart and Friston, Karl J.},
  month      = may,
  year       = {2013},
  pmid       = {23129312},
  pmcid      = {PMC3637647},
  keywords   = {Humans, Neurons, Animals, Brain Mapping, Visual Perception,
                Cerebral Cortex, Motor Activity, Nerve Net, Neural Pathways,
                Predictive Value of Tests, Reflex, Models, Neurological},
  pages      = {611--643},
  file       = {Adams et al. - 2013 - Predictions not commands active inference in the
                .pdf:/Users/apodusenko/Zotero/storage/VECQENAW/Adams et al. - 2013 -
                Predictions not commands active inference in the .pdf:application/pdf}
}

@article{karl_deep_2016,
  title      = {Deep {Variational} {Bayes} {Filters}: {Unsupervised} {Learning} of {
                State} {Space} {Models} from {Raw} {Data}},
  shorttitle = {Deep {Variational} {Bayes} {Filters}},
  url        = {http://arxiv.org/abs/1605.06432},
  abstract   = {We introduce Deep Variational Bayes Filters (DVBF), a new method
                for unsupervised learning and identification of latent Markovian
                state space models. Leveraging recent advances in Stochastic
                Gradient Variational Bayes, DVBF can overcome intractable inference
                distributions via variational inference. Thus, it can handle highly
                nonlinear input data with temporal and spatial dependencies such as
                image sequences without domain knowledge. Our experiments show that
                enabling backpropagation through transitions enforces state space
                assumptions and significantly improves information content of the
                latent embedding. This also enables realistic long-term prediction.
                },
  urldate    = {2017-08-30},
  journal    = {arXiv:1605.06432 [cs, stat]},
  author     = {Karl, Maximilian and Soelch, Maximilian and Bayer, Justin and van
                der Smagt, Patrick},
  month      = may,
  year       = {2016},
  note       = {arXiv: 1605.06432},
  keywords   = {Computer Science - Learning, Statistics - Machine Learning,
                Computer Science - Systems and Control},
  file       = {arXiv.org
                Snapshot:/Users/apodusenko/Zotero/storage/VYHU3UWN/1605.html:text/html;Karl
                et al. - 2016 - Deep Variational Bayes Filters Unsupervised
                Learn.pdf:/Users/apodusenko/Zotero/storage/VXZD6VN9/Karl et al. - 2016
                - Deep Variational Bayes Filters Unsupervised Learn.pdf:application/pdf
                }
}

@article{zhao_recursive_2017,
  title    = {Recursive {Variational} {Bayesian} {Dual} {Estimation} for {Nonlinear
              } {Dynamics} and {Non}-{Gaussian} {Observations}},
  url      = {http://arxiv.org/abs/1707.09049},
  abstract = {State space models provide an interpretable framework for complex
              time series by combining an intuitive dynamical system model with a
              probabilistic observation model. We developed a flexible online
              learning framework for latent nonlinear state dynamics and filtered
              latent states. Our method utilizes the stochastic gradient
              variational Bayes method to jointly optimize the parameters of the
              nonlinear dynamics, observation model, and the recognition model.
              Unlike previous approaches, our framework can incorporate
              non-trivial observation noise models and infer in real-time. We
              test our method on point process observations driven by continuous
              attractor dynamics, demonstrating its ability to recover the phase
              portrait, filtered trajectory, and produce long-term predictions
              for neuroscience applications.},
  urldate  = {2017-08-30},
  journal  = {arXiv:1707.09049 [stat]},
  author   = {Zhao, Yuan and Park, Il Memming},
  month    = jul,
  year     = {2017},
  note     = {arXiv: 1707.09049},
  keywords = {Statistics - Machine Learning},
  file     = {arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/YMRHX9US/1707.html:text/html;Zhao
              and Park - 2017 - Recursive Variational Bayesian Dual Estimation
              for.pdf:/Users/apodusenko/Zotero/storage/R3R4G974/Zhao and Park - 2017
              - Recursive Variational Bayesian Dual Estimation
              for.pdf:application/pdf}
}

@article{kosiorek_hierarchical_2017,
  title    = {Hierarchical {Attentive} {Recurrent} {Tracking}},
  url      = {http://arxiv.org/abs/1706.09262},
  abstract = {Class-agnostic object tracking is particularly difficult in
              cluttered environments as target specific discriminative models
              cannot be learned a priori. Inspired by how the human visual cortex
              employs spatial attention and separate "where" and "what"
              processing pathways to actively suppress irrelevant visual features
              , this work develops a hierarchical attentive recurrent model for
              single object tracking in videos. The first layer of attention
              discards the majority of background by selecting a region
              containing the object of interest, while the subsequent layers tune
              in on visual features particular to the tracked object. This
              framework is fully differentiable and can be trained in a purely
              data driven fashion by gradient methods. To improve training
              convergence, we augment the loss function with terms for a number
              of auxiliary tasks relevant for tracking. Evaluation of the
              proposed model is performed on two datasets of increasing
              difficulty: pedestrian tracking on the KTH activity recognition
              dataset and the KITTI object tracking dataset.},
  urldate  = {2017-08-30},
  journal  = {arXiv:1706.09262 [cs]},
  author   = {Kosiorek, Adam R. and Bewley, Alex and Posner, Ingmar},
  month    = jun,
  year     = {2017},
  note     = {arXiv: 1706.09262},
  keywords = {Computer Science - Artificial Intelligence, Computer Science -
              Neural and Evolutionary Computing, Computer Science - Computer
              Vision and Pattern Recognition},
  file     = {arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/SGM2TP3Z/1706.html:text/html;Kosiorek
              et al. - 2017 - Hierarchical Attentive Recurrent
              Tracking.pdf:/Users/apodusenko/Zotero/storage/7ZJU8G48/Kosiorek et al.
              - 2017 - Hierarchical Attentive Recurrent Tracking.pdf:application/pdf}
}

@inproceedings{grubb_boosted_2010,
  address   = {USA},
  series    = {{ICML}'10},
  title     = {Boosted {Backpropagation} {Learning} for {Training} {Deep} {Modular}
               {Networks}},
  isbn      = {978-1-60558-907-7},
  url       = {http://dl.acm.org/citation.cfm?id=3104322.3104375},
  abstract  = {Divide-and-conquer is key to building sophisticated learning
               machines: hard problems are solved by composing a network of
               modules that solve simpler problems (LeCun et al., 1998; Rohde,
               2002; Bradley, 2009). Many such existing systems rely on learning
               algorithms which are based on simple parametric gradient descent
               where the parametrization must be predetermined, or more
               specialized per-application algorithms which are usually ad-hoc and
               complicated. We present a novel approach for training generic
               modular networks that uses two existing techniques: the error
               propagation strategy of backpropagation and more recent research on
               descent in spaces of functions (Mason et al., 1999; Scholkopf \&
               Smola, 2001). Combining these two methods of optimization gives a
               simple algorithm for training heterogeneous networks of functional
               modules using simple gradient propagation mechanics and established
               learning algorithms. The resulting separation of concerns between
               learning individual modules and error propagation mechanics eases
               implementation, enables a larger class of modular learning
               strategies, and allows per-module control of
               complexity/regularization. We derive and demonstrate this
               functional backpropagation and contrast it with traditional
               gradient descent in parameter space, observing that in our example
               domain the method is significantly more robust to local optima.},
  urldate   = {2017-08-30},
  booktitle = {Proceedings of the 27th {International} {Conference} on {
               International} {Conference} on {Machine} {Learning}},
  publisher = {Omnipress},
  author    = {Grubb, Alexander and Bagnell, J. Andrew},
  year      = {2010},
  pages     = {407--414},
  file      = {Grubb and Bagnell - 2010 - Boosted Backpropagation Learning for
               Training Deep.pdf:/Users/apodusenko/Zotero/storage/BNL2JPY8/Grubb and
               Bagnell - 2010 - Boosted Backpropagation Learning for Training
               Deep.pdf:application/pdf}
}

@article{gershman_amortized_2014,
  title    = {Amortized {Inference} in {Probabilistic} {Reasoning}},
  volume   = {36},
  url      = {http://escholarship.org/uc/item/34j1h7k5},
  abstract = {Amortized Inference in Probabilistic Reasoning Samuel J. Gershman
              1 (sjgershm@mit.edu) and Noah D. Goodman 2 (ngoodman@stanford.edu)
              1 Department of Brain and Cognitive Sciences, MIT of Psychology,
              Stanford University 2 Department Abstract similar or related
              queries. For example, as you view an im- age, your head and eyes
              are continuously moving, generating an infinitude of slightly
              different queries. For these queries, it may be inaccurate to reuse
              a stored inference without modifi- cation. This raises the problem
              of amortized inference: how to flexibly reuse inferences so as to
              answer a variety of re- lated queries. Recently, Stuhlm¨uller et
              al. (2013) addressed this problem by using stored samples to
              estimate local condi- tional distributions, and then approximating
              answers to more complex queries by composing the local
              distributions. The work described in this paper seeks experimental
              evidence for a similar kind of flexible reuse in human reasoning.
              We presented subjects with a simple Bayesian network and asked them
              to answer a series of queries about it. One of these queries (the
              “target”) could be answered by reusing the answer to another query
              (the “sub-query”). We hypothesized that the effects of reuse would
              be evident compared to an in- ference with the same structure but
              no re-usable sub-query. Further, we hypothesized that this effect
              would be present only if the target was presented after the
              sub-query. Accord- ingly, we manipulated (between subjects) whether
              the target came before or after the sub-query. This design allowed
              us to look for two key signatures of reuse: correlations between
              related inferences (Experiment 1) and faster responses for in-
              ferences that exploit reuse (Experiment 2). Recent studies of
              probabilistic reasoning have postulated general-purpose inference
              algorithms that can be used to an- swer arbitrary queries. These
              algorithms are memoryless, in the sense that each query is
              processed independently, without reuse of earlier computation. We
              argue that the brain oper- ates in the setting of amortized
              inference, where numerous related queries must be answered (e.g.,
              recognizing a scene from multiple viewpoints); in this setting,
              memoryless algo- rithms can be computationally wasteful. We propose
              a simple form of flexible reuse, according to which shared
              inferences are cached and composed together to answer new queries.
              We present experimental evidence that humans exploit this form of
              reuse: the answer to a complex query can be systematically
              predicted from a person’s response to a simpler query if the
              simpler query was presented first and entails a sub-inference (i.e.
              , a sub-component of the more complex query). People are also
              faster at answering a complex query when it is preceded by a
              sub-inference. Our results suggest that the astonishing ef-
              ficiency of human probabilistic reasoning may be supported by
              interactions between inference and memory. Keywords: induction,
              Bayesian inference, memory “Cognition is recognition.” – Hofstadter
              (1995) Introduction One view of probabilistic reasoning holds that
              our brains are equipped with general-purpose inference algorithms
              that can be used to answer arbitrary queries (Griffiths et al.,
              2012; Pouget et al., 2013). An under-appreciated property of such
              algorithms borrowed from computer science is that they are
              memoryless: each query is (at least in principle) processed
              independently of others. While this property guarantees that
              inferences will not interfere with one another, it can also lead to
              gross computational inefficiency, since inferences are never
              reused; memorylessness implies that answering the same query twice
              requires the same amount of computation as answer two unique
              queries. 1 Whatever inference algorithms the brain uses, they are
              un- likely to be memoryless. Consider, for example, the image in
              Figure 1 (Gregory, 1970). Upon viewing it for the first time, most
              observers find it extremely difficult to identify what the image
              depicts. 2 However, once the image has been deciphered, all
              subsequent views are instantly recognized. Clearly, the visual
              system is not running a computationally expensive inference
              algorithm upon each viewing; the infer- ence is simply reused. In
              reality, it is rare to be faced with the exact same query multiple
              times. Much more pervasive is the appearance of Figure 1: What does
              this image depict? Amortized inference in Bayesian networks 1 To be
              fair, inference algorithms for dynamical systems, like Kalman
              filtering, involve reuse in a certain sense. However, these
              algorithms are not designed to reuse inferences when applied to
              sev- eral independent time series (even if the time series are
              identical). 2 Answer: a dalmatian. In this paper, we will restrict
              our attention to amortized in- ference for Bayesian networks. Let
              p(x) denote a probability distribution on variables x = \{x 1 , . .
              . , x M \}. A Bayesian net- work G is a directed acyclic graph with
              nodes corresponding},
  number   = {36},
  urldate  = {2017-08-30},
  journal  = {Proceedings of the Cognitive Science Society},
  author   = {Gershman, Samuel and Goodman, Noah},
  month    = jan,
  year     = {2014},
  file     = {Gershman and Goodman - 2014 - Amortized Inference in Probabilistic
              Reasoning.pdf:/Users/apodusenko/Zotero/storage/P36WP5EL/Gershman and
              Goodman - 2014 - Amortized Inference in Probabilistic
              Reasoning.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/2K983WE5/34j1h7k5.html:text/html
              }
}

@article{rezende_stochastic_2014,
  title    = {Stochastic {Backpropagation} and {Approximate} {Inference} in {Deep}
              {Generative} {Models}},
  url      = {http://arxiv.org/abs/1401.4082},
  abstract = {We marry ideas from deep neural networks and approximate Bayesian
              inference to derive a generalised class of deep, directed
              generative models, endowed with a new algorithm for scalable
              inference and learning. Our algorithm introduces a recognition
              model to represent approximate posterior distributions, and that
              acts as a stochastic encoder of the data. We develop stochastic
              back-propagation -- rules for back-propagation through stochastic
              variables -- and use this to develop an algorithm that allows for
              joint optimisation of the parameters of both the generative and
              recognition model. We demonstrate on several real-world data sets
              that the model generates realistic samples, provides accurate
              imputations of missing data and is a useful tool for
              high-dimensional data visualisation.},
  urldate  = {2017-08-30},
  journal  = {arXiv:1401.4082 [cs, stat]},
  author   = {Rezende, Danilo Jimenez and Mohamed, Shakir and Wierstra, Daan},
  month    = jan,
  year     = {2014},
  note     = {arXiv: 1401.4082},
  keywords = {Computer Science - Learning, Statistics - Machine Learning,
              Computer Science - Artificial Intelligence, Statistics -
              Computation, Statistics - Methodology},
  file     = {arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/2BSGQ6UJ/1401.html:text/html;Rezende
              et al. - 2014 - Stochastic Backpropagation and Approximate
              Inferen.pdf:/Users/apodusenko/Zotero/storage/877KHHGW/Rezende et al. -
              2014 - Stochastic Backpropagation and Approximate
              Inferen.pdf:application/pdf}
}

@article{salimans_fixed-form_2013,
  title    = {Fixed-{Form} {Variational} {Posterior} {Approximation} through {
              Stochastic} {Linear} {Regression}},
  volume   = {8},
  issn     = {1936-0975, 1931-6690},
  url      = {https://projecteuclid.org/euclid.ba/1386166315},
  doi      = {10.1214/13-BA858},
  abstract = {We propose a general algorithm for approximating nonstandard
              Bayesian posterior distributions. The algorithm minimizes the
              Kullback-Leibler divergence of an approximating distribution to the
              intractable posterior distribution. Our method can be used to
              approximate any posterior distribution, provided that it is given
              in closed form up to the proportionality constant. The
              approximation can be any distribution in the exponential family or
              any mixture of such distributions, which means that it can be made
              arbitrarily precise. Several examples illustrate the speed and
              accuracy of our approximation method in practice.},
  language = {EN},
  number   = {4},
  urldate  = {2017-08-30},
  journal  = {Bayesian Analysis},
  author   = {Salimans, Tim and Knowles, David A.},
  month    = dec,
  year     = {2013},
  mrnumber = {MR3150471},
  zmnumber = {1329.62142},
  keywords = {Variational Bayes, approximate inference, stochastic approximation
              },
  pages    = {837--882},
  file     = {Salimans and Knowles - 2013 - Fixed-Form Variational Posterior
              Approximation
              thr.pdf:/Users/apodusenko/Zotero/storage/8UH8TJMI/Salimans and Knowles
              - 2013 - Fixed-Form Variational Posterior Approximation
              thr.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/UBFUWJ87/1386166315.html:text/html
              }
}

@article{salimans_evolution_2017,
  title    = {Evolution {Strategies} as a {Scalable} {Alternative} to {
              Reinforcement} {Learning}},
  url      = {http://arxiv.org/abs/1703.03864},
  abstract = {We explore the use of Evolution Strategies, a class of black box
              optimization algorithms, as an alternative to popular RL techniques
              such as Q-learning and Policy Gradients. Experiments on MuJoCo and
              Atari show that ES is a viable solution strategy that scales
              extremely well with the number of CPUs available: By using hundreds
              to thousands of parallel workers, ES can solve 3D humanoid walking
              in 10 minutes and obtain competitive results on most Atari games
              after one hour of training time. In addition, we highlight several
              advantages of ES as a black box optimization technique: it is
              invariant to action frequency and delayed rewards, tolerant of
              extremely long horizons, and does not need temporal discounting or
              value function approximation.},
  urldate  = {2017-08-29},
  journal  = {arXiv:1703.03864 [cs, stat]},
  author   = {Salimans, Tim and Ho, Jonathan and Chen, Xi and Sutskever, Ilya},
  month    = mar,
  year     = {2017},
  note     = {arXiv: 1703.03864},
  keywords = {Computer Science - Learning, Statistics - Machine Learning,
              Computer Science - Artificial Intelligence, Computer Science -
              Neural and Evolutionary Computing},
  file     = {arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/FA893B5D/1703.html:text/html;Salimans
              et al. - 2017 - Evolution Strategies as a Scalable Alternative to
              .pdf:/Users/apodusenko/Zotero/storage/2NFXZWXC/Salimans et al. - 2017 -
              Evolution Strategies as a Scalable Alternative to .pdf:application/pdf}
}

@inproceedings{gal_rapid_2015,
  title     = {Rapid {Prototyping} of {Probabilistic} {Models}: {Emerging} {
               Challenges} in {Variational} {Inference}},
  booktitle = {Advances in {Approximate} {Bayesian} {Inference} workshop, {NIPS}
               },
  author    = {Gal, Yarin},
  year      = {2015},
  file      = {Gal - 2015 - Rapid Prototyping of Probabilistic Models
               Emergin.pdf:/Users/apodusenko/Zotero/storage/IKGTYJSF/Gal - 2015 -
               Rapid Prototyping of Probabilistic Models Emergin.pdf:application/pdf}
}

@inproceedings{murphy_linear_2001,
  address   = {Cambridge, MA, USA},
  series    = {{NIPS}'01},
  title     = {Linear {Time} {Inference} in {Hierarchical} {HMMs}},
  url       = {http://dl.acm.org/citation.cfm?id=2980539.2980647},
  abstract  = {The hierarchical hidden Markov model (HHMM) is a generalization of
               the hidden Markov model (HMM) that models sequences with structure
               at many length/time scales [FST98]. Unfortunately, the original
               inference algorithm is rather complicated, and takes O(T3) time,
               where is the length of the sequence, making it impractical for many
               domains. In this paper, we show how HHMMs are a special kind of
               dynamic Bayesian network (DBN), and thereby derive a much simpler
               inference algorithm, which only takes O(T) time. Furthermore, by
               drawing the connection between HHMMs and DBNs, we enable the
               application of many standard approximation techniques to further
               speed up inference.},
  urldate   = {2017-08-27},
  booktitle = {Proceedings of the 14th {International} {Conference} on {Neural}
               {Information} {Processing} {Systems}: {Natural} and {Synthetic}},
  publisher = {MIT Press},
  author    = {Murphy, Kevin P. and Paskin, Mark A.},
  year      = {2001},
  pages     = {833--840},
  file      = {Murphy and Paskin - 2001 - Linear Time Inference in Hierarchical
               HMMs.pdf:/Users/apodusenko/Zotero/storage/CHQ2J2VF/Murphy and Paskin -
               2001 - Linear Time Inference in Hierarchical HMMs.pdf:application/pdf}
}

@article{wang_general_2015,
  title    = {A {General} {Method} for {Robust} {Bayesian} {Modeling}},
  url      = {http://arxiv.org/abs/1510.05078},
  abstract = {Robust Bayesian models are appealing alternatives to standard
              models, providing protection from data that contains outliers or
              other departures from the model assumptions. Historically, robust
              models were mostly developed on a case-by-case basis; examples
              include robust linear regression, robust mixture models, and bursty
              topic models. In this paper we develop a general approach to robust
              Bayesian modeling. We show how to turn an existing Bayesian model
              into a robust model, and then develop a generic strategy for
              computing with it. We use our method to study robust variants of
              several models, including linear regression, Poisson regression,
              logistic regression, and probabilistic topic models. We discuss the
              connections between our methods and existing approaches, especially
              empirical Bayes and James-Stein estimation.},
  urldate  = {2017-08-27},
  journal  = {arXiv:1510.05078 [stat]},
  author   = {Wang, Chong and Blei, David M.},
  month    = oct,
  year     = {2015},
  note     = {arXiv: 1510.05078},
  keywords = {Statistics - Machine Learning},
  file     = {arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/2RQLF9JP/1510.html:text/html;Wang
              and Blei - 2015 - A General Method for Robust Bayesian
              Modeling.pdf:/Users/apodusenko/Zotero/storage/UPNDFRFW/Wang and Blei -
              2015 - A General Method for Robust Bayesian
              Modeling.pdf:application/pdf}
}

@article{loeliger_factor_2015,
  title    = {Factor {Graphs} for {Quantum} {Probabilities}},
  url      = {http://arxiv.org/abs/1508.00689},
  abstract = {A factor-graph representation of quantum-mechanical probabilities
              (involving any number of measurements) is proposed. Unlike standard
              statistical models, the proposed representation uses auxiliary
              variables (state variables) that are not random variables. All
              joint probability distributions are marginals of some
              complex-valued function \$q\$, and it is demonstrated how the basic
              concepts of quantum mechanics relate to factorizations and
              marginals of \$q\$.},
  urldate  = {2017-08-27},
  journal  = {arXiv:1508.00689 [quant-ph, stat]},
  author   = {Loeliger, Hans-Andrea and Vontobel, Pascal O.},
  month    = aug,
  year     = {2015},
  note     = {arXiv: 1508.00689},
  keywords = {Computer Science - Artificial Intelligence, Computer Science -
              Information Theory, Mathematics - Statistics Theory, Quantum
              Physics},
  file     = {arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/JEWSMBGP/1508.html:text/html;Loeliger
              and Vontobel - 2015 - Factor Graphs for Quantum
              Probabilities.pdf:/Users/apodusenko/Zotero/storage/BFSRYNHI/Loeliger
              and Vontobel - 2015 - Factor Graphs for Quantum
              Probabilities.pdf:application/pdf}
}

@article{sengupta_approximate_2016,
  title    = {Approximate {Bayesian} inference as a gauge theory},
  volume   = {14},
  issn     = {1545-7885},
  url      = {http://arxiv.org/abs/1705.06614},
  doi      = {10.1371/journal.pbio.1002400},
  abstract = {In a published paper {\textbackslash}cite\{Sengupta2016\}, we have
              proposed that the brain (and other self-organized biological and
              artificial systems) can be characterized via the mathematical
              apparatus of a gauge theory. The picture that emerges from this
              approach suggests that any biological system (from a neuron to an
              organism) can be cast as resolving uncertainty about its external
              milieu, either by changing its internal states or its relationship
              to the environment. Using formal arguments, we have shown that a
              gauge theory for neuronal dynamics -- based on approximate Bayesian
              inference -- has the potential to shed new light on phenomena that
              have thus far eluded a formal description, such as attention and
              the link between action and perception. Here, we describe the
              technical apparatus that enables such a variational inference on
              manifolds.},
  number   = {3},
  urldate  = {2017-08-26},
  journal  = {PLOS Biology},
  author   = {Sengupta, Biswa and Friston, Karl},
  month    = mar,
  year     = {2016},
  note     = {arXiv: 1705.06614},
  keywords = {Quantitative Biology - Neurons and Cognition, Computer Science -
              Neural and Evolutionary Computing},
  pages    = {e1002400},
  file     = {arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/T367NCEF/1705.html:text/html;Sengupta
              and Friston - 2016 - Approximate Bayesian inference as a gauge
              theory.pdf:/Users/apodusenko/Zotero/storage/V3J65EQ3/Sengupta and
              Friston - 2016 - Approximate Bayesian inference as a gauge
              theory.pdf:application/pdf}
}

@article{cui_continuous_2016,
  title    = {Continuous {Online} {Sequence} {Learning} with an {Unsupervised} {
              Neural} {Network} {Model}},
  volume   = {28},
  issn     = {0899-7667},
  url      = {http://www.mitpressjournals.org/doi/full/10.1162/NECO_a_00893},
  doi      = {10.1162/NECO_a_00893},
  abstract = {The ability to recognize and predict temporal sequences of sensory
              inputs is vital for survival in natural environments. Based on many
              known properties of cortical neurons, hierarchical temporal memory
              (HTM) sequence memory recently has been proposed as a theoretical
              framework for sequence learning in the cortex. In this letter, we
              analyze properties of HTM sequence memory and apply it to sequence
              learning and prediction problems with streaming data. We show the
              model is able to continuously learn a large number of variable
              order temporal sequences using an unsupervised Hebbian-like
              learning rule. The sparse temporal codes formed by the model can
              robustly handle branching temporal sequences by maintaining
              multiple predictions until there is sufficient disambiguating
              evidence. We compare the HTM sequence memory with other sequence
              learning algorithms, including statistical methods—autoregressive
              integrated moving average; feedforward neural networks—time delay
              neural network and online sequential extreme learning machine; and
              recurrent neural networks—long short-term memory and echo-state
              networks on sequence prediction problems with both artificial and
              real-world data. The HTM model achieves comparable accuracy to
              other state-of-the-art algorithms. The model also exhibits
              properties that are critical for sequence learning, including
              continuous online learning, the ability to handle multiple
              predictions and branching sequences with high-order statistics,
              robustness to sensor noise and fault tolerance, and good
              performance without task-specific hyperparameter tuning. Therefore,
              the HTM sequence memory not only advances our understanding of how
              the brain may solve the sequence learning problem but is also
              applicable to real-world sequence learning problems from continuous
              data streams.},
  number   = {11},
  urldate  = {2017-08-26},
  journal  = {Neural Computation},
  author   = {Cui, Yuwei and Ahmad, Subutai and Hawkins, Jeff},
  month    = sep,
  year     = {2016},
  pages    = {2474--2504},
  file     = {Cui et al. - 2016 - Continuous Online Sequence Learning with an
              Unsupe.pdf:/Users/apodusenko/Zotero/storage/KF476SCI/Cui et al. - 2016
              - Continuous Online Sequence Learning with an
              Unsupe.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/HDWFM5T3/NECO_a_00893.html:text/html
              }
}

@article{gelman_prior_2017,
  title    = {The prior can generally only be understood in the context of the
              likelihood},
  url      = {http://arxiv.org/abs/1708.07487},
  abstract = {A key sticking point of Bayesian analysis is the choice of prior
              distribution, and there is a vast literature on potential defaults
              including uniform priors, Jeffreys' priors, reference priors,
              maximum entropy priors, and weakly informative priors. These
              methods, however, often manifest a key conceptual tension in prior
              modeling: a model encoding true prior information should be chosen
              without reference to the model of the measurement process, but
              almost all common prior modeling techniques are implicitly
              motivated by a reference likelihood. In this paper we resolve this
              apparent paradox by placing the choice of prior into the context of
              the entire Bayesian analysis, from inference to prediction to model
              evaluation.},
  urldate  = {2017-08-26},
  journal  = {arXiv:1708.07487 [stat]},
  author   = {Gelman, Andrew and Simpson, Daniel and Betancourt, Michael},
  month    = aug,
  year     = {2017},
  note     = {arXiv: 1708.07487},
  keywords = {Statistics - Methodology},
  file     = {arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/YXCPSW45/1708.html:text/html;Gelman
              et al. - 2017 - The prior can generally only be understood in the
              .pdf:/Users/apodusenko/Zotero/storage/B8RVFZHQ/Gelman et al. - 2017 -
              The prior can generally only be understood in the .pdf:application/pdf}
}

@article{arulkumaran_brief_2017,
  title    = {A {Brief} {Survey} of {Deep} {Reinforcement} {Learning}},
  url      = {http://arxiv.org/abs/1708.05866},
  abstract = {Deep reinforcement learning is poised to revolutionise the field
              of AI and represents a step towards building autonomous systems
              with a higher level understanding of the visual world. Currently,
              deep learning is enabling reinforcement learning to scale to
              problems that were previously intractable, such as learning to play
              video games directly from pixels. Deep reinforcement learning
              algorithms are also applied to robotics, allowing control policies
              for robots to be learned directly from camera inputs in the real
              world. In this survey, we begin with an introduction to the general
              field of reinforcement learning, then progress to the main streams
              of value-based and policy-based methods. Our survey will cover
              central algorithms in deep reinforcement learning, including the
              deep \$Q\$-network, trust region policy optimisation, and
              asynchronous advantage actor-critic. In parallel, we highlight the
              unique advantages of deep neural networks, focusing on visual
              understanding via reinforcement learning. To conclude, we describe
              several current areas of research within the field.},
  urldate  = {2017-08-25},
  journal  = {arXiv:1708.05866 [cs, stat]},
  author   = {Arulkumaran, Kai and Deisenroth, Marc Peter and Brundage, Miles and
              Bharath, Anil Anthony},
  month    = aug,
  year     = {2017},
  note     = {arXiv: 1708.05866},
  keywords = {Computer Science - Learning, Statistics - Machine Learning,
              Computer Science - Artificial Intelligence},
  file     = {Arulkumaran et al. - 2017 - A Brief Survey of Deep Reinforcement
              Learning.pdf:/Users/apodusenko/Zotero/storage/M5QL6M9L/Arulkumaran et
              al. - 2017 - A Brief Survey of Deep Reinforcement
              Learning.pdf:application/pdf;arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/PV3T8RPF/1708.html:text/html}
}

@article{sandhu_bayesian_2017,
  title    = {Bayesian model selection using automatic relevance determination for
              nonlinear dynamical systems},
  volume   = {320},
  issn     = {0045-7825},
  url      = {http://www.sciencedirect.com/science/article/pii/S0045782516313020},
  doi      = {10.1016/j.cma.2017.01.042},
  abstract = {Bayesian model selection is augmented with automatic relevance
              determination (ARD) to perform model reduction of complex dynamical
              systems modelled by nonlinear, stochastic ordinary differential
              equations (ODE). Given noisy measurement data, a parametrically
              flexible model is envisioned to represent the dynamical system. A
              Bayesian model selection problem is posed to find the best model
              nested under the envisioned model. This model selection problem is
              transferred from the model space to hyper-parameter space by
              regularizing the parameter posterior space through a parametrized
              prior distribution called the ARD prior. The resulting joint prior
              pdf is the combination of parametrized ARD priors assigned to
              parameters whose relevance to the system dynamics is questionable
              and the known prior pdf for parameters whose relevance is known a
              priori. The hyper-parameter of each ARD prior explicitly represents
              the relevance of the corresponding model parameter. The
              hyper-parameters are estimated using the measurement data by
              performing evidence maximization or type-II maximum likelihood.
              Superfluous model parameters are switched off during evidence
              maximization by the corresponding ARD prior, forcing the model
              parameter to be irrelevant for prediction purposes. An efficient
              numerical implementation for evidence computation using Markov
              Chain Monte Carlo sampling of the parameter posterior distribution
              is presented for the case when the analytical evaluation of
              evidence is not possible. The ARD approach is validated with
              synthetic measurements generated from a nonlinear, unsteady
              aeroelastic oscillator consisting of a NACA0012 airfoil undergoing
              limit cycle oscillation. A set of intentionally flexible stochastic
              ODEs having different state-space formulation is proposed to model
              the synthetic data. ARD is used to obtain an optimal nested model
              corresponding to each proposed model. The optimal nested model with
              the maximum posterior model probability is chosen as the overall
              optimal model. ARD provides a flexible Bayesian platform to find
              the optimal nested model by eliminating the need to propose
              candidate nested models and its prior pdfs.},
  urldate  = {2017-08-21},
  journal  = {Computer Methods in Applied Mechanics and Engineering},
  author   = {Sandhu, Rimple and Pettit, Chris and Khalil, Mohammad and Poirel,
              Dominique and Sarkar, Abhijit},
  month    = jun,
  year     = {2017},
  keywords = {Kalman filter, Automatic relevance determination, Bayesian model
              selection, Markov Chain Monte Carlo simulation},
  pages    = {237--260},
  file     = {Sandhu et al. - 2017 - Bayesian model selection using automatic
              relevance.pdf:/Users/apodusenko/Zotero/storage/DSYX3BN2/Sandhu et al. -
              2017 - Bayesian model selection using automatic
              relevance.pdf:application/pdf;ScienceDirect
              Snapshot:/Users/apodusenko/Zotero/storage/W6ZZK4AT/S0045782516313020.html:text/html
              }
}

@article{ilin_recurrent_2017,
  title    = {Recurrent {Ladder} {Networks}},
  url      = {http://arxiv.org/abs/1707.09219},
  abstract = {We propose a recurrent extension of the Ladder network, which is
              motivated by the inference required in hierarchical latent variable
              models. We demonstrate that the recurrent Ladder is able to handle
              a wide variety of complex learning tasks that benefit from
              iterative inference and temporal modeling. The architecture shows
              close-to-optimal results on temporal modeling of video data,
              competitive results on music modeling, and improved perceptual
              grouping based on higher order abstractions, such as stochastic
              textures and motion cues. We present results for fully supervised,
              semi-supervised, and unsupervised tasks. The results suggest that
              the proposed architecture and principles are powerful tools for
              learning a hierarchy of abstractions, handling temporal information
              , modeling relations and interactions between objects.},
  urldate  = {2017-08-18},
  journal  = {arXiv:1707.09219 [cs, stat]},
  author   = {Ilin, Alexander and Prémont-Schwarz, Isabeau and Hao, Tele Hotloo
              and Rasmus, Antti and Boney, Rinu and Valpola, Harri},
  month    = jul,
  year     = {2017},
  note     = {arXiv: 1707.09219},
  keywords = {Computer Science - Learning, Statistics - Machine Learning,
              Computer Science - Neural and Evolutionary Computing},
  file     = {arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/HE69WG42/1707.html:text/html;Ilin
              et al. - 2017 - Recurrent Ladder
              Networks.pdf:/Users/apodusenko/Zotero/storage/2GKR56VA/Ilin et al. -
              2017 - Recurrent Ladder Networks.pdf:application/pdf}
}

@article{khan_conjugate-computation_2017,
  title      = {Conjugate-{Computation} {Variational} {Inference} : {Converting} {
                Variational} {Inference} in {Non}-{Conjugate} {Models} to {Inferences}
                in {Conjugate} {Models}},
  shorttitle = {Conjugate-{Computation} {Variational} {Inference}},
  url        = {http://arxiv.org/abs/1703.04265},
  abstract   = {Variational inference is computationally challenging in models
                that contain both conjugate and non-conjugate terms. Methods
                specifically designed for conjugate models, even though
                computationally efficient, find it difficult to deal with
                non-conjugate terms. On the other hand, stochastic-gradient methods
                can handle the non-conjugate terms but they usually ignore the
                conjugate structure of the model which might result in slow
                convergence. In this paper, we propose a new algorithm called
                Conjugate-computation Variational Inference (CVI) which brings the
                best of the two worlds together -- it uses conjugate computations
                for the conjugate terms and employs stochastic gradients for the
                rest. We derive this algorithm by using a stochastic mirror-descent
                method in the mean-parameter space, and then expressing each
                gradient step as a variational inference in a conjugate model. We
                demonstrate our algorithm's applicability to a large class of
                models and establish its convergence. Our experimental results show
                that our method converges much faster than the methods that ignore
                the conjugate structure of the model.},
  urldate    = {2017-08-18},
  journal    = {arXiv:1703.04265 [cs]},
  author     = {Khan, Mohammad Emtiyaz and Lin, Wu},
  month      = mar,
  year       = {2017},
  note       = {arXiv: 1703.04265},
  keywords   = {Computer Science - Learning},
  file       = {arXiv.org
                Snapshot:/Users/apodusenko/Zotero/storage/RGJG2R2H/1703.html:text/html;Khan
                and Lin - 2017 - Conjugate-Computation Variational Inference
                Conv.pdf:/Users/apodusenko/Zotero/storage/TYBU8VH4/Khan and Lin - 2017
                - Conjugate-Computation Variational Inference Conv.pdf:application/pdf}
}

@article{peters_uncertainty_2017,
  title      = {Uncertainty and stress: {Why} it causes diseases and how it is
                mastered by the brain},
  volume     = {156},
  issn       = {0301-0082},
  shorttitle = {Uncertainty and stress},
  url        = {http://www.sciencedirect.com/science/article/pii/S0301008217300369},
  doi        = {10.1016/j.pneurobio.2017.05.004},
  abstract   = {The term ‘stress’ – coined in 1936 – has many definitions, but
                until now has lacked a theoretical foundation. Here we present an
                information-theoretic approach – based on the ‘free energy
                principle’ – defining the essence of stress; namely, uncertainty.
                We address three questions: What is uncertainty? What does it do to
                us? What are our resources to master it? Mathematically speaking,
                uncertainty is entropy or ‘expected surprise’. The ‘free energy
                principle’ rests upon the fact that self-organizing biological
                agents resist a tendency to disorder and must therefore minimize
                the entropy of their sensory states. Applied to our everyday life,
                this means that we feel uncertain, when we anticipate that outcomes
                will turn out to be something other than expected – and that we are
                unable to avoid surprise. As all cognitive systems strive to reduce
                their uncertainty about future outcomes, they face a critical
                constraint: Reducing uncertainty requires cerebral energy. The
                characteristic of the vertebrate brain to prioritize its own high
                energy is captured by the notion of the ‘selfish brain’.
                Accordingly, in times of uncertainty, the selfish brain demands
                extra energy from the body. If, despite all this, the brain cannot
                reduce uncertainty, a persistent cerebral energy crisis may develop
                , burdening the individual by ‘allostatic load’ that contributes to
                systemic and brain malfunction (impaired memory, atherogenesis,
                diabetes and subsequent cardio- and cerebrovascular events). Based
                on the basic tenet that stress originates from uncertainty, we
                discuss the strategies our brain uses to avoid surprise and thereby
                resolve uncertainty.},
  urldate    = {2017-08-17},
  journal    = {Progress in Neurobiology},
  author     = {Peters, Achim and McEwen, Bruce S. and Friston, Karl},
  month      = sep,
  year       = {2017},
  keywords   = {Learning, Attention, Variational free energy, Uncertainty,
                Allostatic load, Atherosclerosis, Bayesian brain, Brain energy
                metabolism, mortality, Selfish Brain, Stress definition, Stress
                habituation},
  pages      = {164--188},
  file       = {Peters e.a. - 2017 - Uncertainty and stress Why it causes diseases
                and.pdf:/Users/apodusenko/Zotero/storage/7L3PSGYA/Peters e.a. - 2017 -
                Uncertainty and stress Why it causes diseases
                and.pdf:application/pdf;ScienceDirect
                Snapshot:/Users/apodusenko/Zotero/storage/2UUAJBX6/S0301008217300369.html:text/html
                }
}

@article{powers_pavlovian_2017,
  title     = {Pavlovian conditioning–induced hallucinations result from
               overweighting of perceptual priors},
  volume    = {357},
  copyright = {Copyright © 2017 The Authors, some rights reserved; exclusive
               licensee American Association for the Advancement of Science. No
               claim to original U.S. Government Works.
               http://www.sciencemag.org/about/science-licenses-journal-article-reuseThis
               is an article distributed under the terms of the Science Journals
               Default License.},
  issn      = {0036-8075, 1095-9203},
  url       = {http://science.sciencemag.org/content/357/6351/596},
  doi       = {10.1126/science.aan3458},
  abstract  = {Neural mechanisms for hallucinations Pairing a stimulus in one
               modality (vision) with a stimulus in another (sound) can lead to
               task-induced hallucinations in healthy individuals. After many
               trials, people eventually report perceiving a nonexistent stimulus
               contingent on the presence of the previously paired stimulus.
               Powers et al. investigated how different groups of volunteers and
               patients respond to this conditioning paradigm. They used behavior,
               neuroimaging, and computational modeling to dissect the effect of
               perceptual priors versus sensory evidence on such induced
               hallucinations. People who are more prone to hear voices were more
               susceptible to the induced auditory hallucinations. The network of
               brain regions that was active during the conditioned hallucinations
               resembled the network observed during clinical symptom capture in
               individuals who hallucinate while in a brain scanner. Science, this
               issue p. 596 Some people hear voices that others do not, but only
               some of those people seek treatment. Using a Pavlovian learning
               task, we induced conditioned hallucinations in four groups of
               people who differed orthogonally in their voice-hearing and
               treatment-seeking statuses. People who hear voices were
               significantly more susceptible to the effect. Using functional
               neuroimaging and computational modeling of perception, we
               identified processes that differentiated voice-hearers from
               non–voice-hearers and treatment-seekers from non–treatment-seekers
               and characterized a brain circuit that mediated the conditioned
               hallucinations. These data demonstrate the profound and sometimes
               pathological impact of top-down cognitive processes on perception
               and may represent an objective means to discern people with a need
               for treatment from those without. Perceptual beliefs, stimulus
               associations, and belief volatility drive task-induced
               hallucinations in voice-hearers. Perceptual beliefs, stimulus
               associations, and belief volatility drive task-induced
               hallucinations in voice-hearers.},
  language  = {en},
  number    = {6351},
  journal   = {Science},
  author    = {Powers, A. R. and Mathys, C. and Corlett, P. R.},
  month     = aug,
  year      = {2017},
  pmid      = {28798131},
  pages     = {596--600},
  file      = {
               aan3458-Powers-SM.pdf:/Users/apodusenko/Zotero/storage/936J2TNK/aan3458-Powers-SM.pdf:application/pdf;Powers
               e.a. - 2017 - Pavlovian conditioning–induced hallucinations
               resu.pdf:/Users/apodusenko/Zotero/storage/2XPLAKDA/Powers e.a. - 2017 -
               Pavlovian conditioning–induced hallucinations
               resu.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/9TSBBJLA/tab-pdf.html:text/html
               }
}

@inproceedings{wang_robust_2017,
  title     = {Robust {Probabilistic} {Modeling} with {Bayesian} {Data} {Reweighting
               }},
  url       = {http://proceedings.mlr.press/v70/wang17g.html},
  abstract  = {Probabilistic models analyze data by relying on a set of
               assumptions. Data that exhibit deviations from these assumptions
               can undermine inference and prediction quality. Robust models offer
               protect...},
  language  = {en},
  booktitle = {{PMLR}},
  author    = {Wang, Yixin and Kucukelbir, Alp and Blei, David M.},
  month     = jul,
  year      = {2017},
  pages     = {3646--3655},
  file      = {
               Snapshot:/Users/apodusenko/Zotero/storage/HH2IPPJQ/wang17g.html:text/html;Wang
               et al. - 2017 - Robust Probabilistic Modeling with Bayesian Data
               R.pdf:/Users/apodusenko/Zotero/storage/5QKQZDZT/Wang et al. - 2017 -
               Robust Probabilistic Modeling with Bayesian Data
               R.pdf:application/pdf;wang17g-supp.pdf:/Users/apodusenko/Zotero/storage/W4P332KX/wang17g-supp.pdf:application/pdf
               }
}

@article{ouden_how_2012,
  title    = {How {Prediction} {Errors} {Shape} {Perception}, {Attention}, and {
              Motivation}},
  volume   = {3},
  issn     = {1664-1078},
  url      = {http://journal.frontiersin.org/article/10.3389/fpsyg.2012.00548/full},
  doi      = {10.3389/fpsyg.2012.00548},
  abstract = {Prediction errors are a central notion in theoretical models of
              reinforcement learning, perceptual inference, decision-making and
              cognition, and prediction error signals have been reported across a
              wide range of brain regions and experimental paradigms. Here, we
              will make an attempt to see the forest for the trees, considering
              the commonalities and differences of reported prediction errors
              signals in light of recent suggestions that the computation of
              prediction errors forms a fundamental mode of brain function. We
              discuss where different types of prediction errors are encoded, how
              they are generated, and the different functional roles they fulfil.
              We suggest that while encoding of prediction errors is a common
              computation across brain regions, the content and function of these
              error signals can be very different, and are determined by the
              afferent and efferent connections within the neural circuitry in
              which they arise.},
  language = {English},
  journal  = {Frontiers in Psychology},
  author   = {Ouden, Den and Em, Hanneke and Kok, Peter and Lange, De and P,
              Floris},
  year     = {2012},
  keywords = {predictive coding, Learning, decision-making, Attention,
              expectation, Motivation, Perception, perceptual inference,
              prediction, Prediction error}
}

@incollection{yedidia_understanding_2001,
  author    = {Yedidia, J.S. and Freeman, W.T. and Weiss, Y.},
  title     = {Understanding Belief Propagation and Its Generalizations},
  booktitle = {Exploring Artificial Intelligence in the New Millennium},
  year      = 2003,
  editor    = {Lakemeyer, G. and Nebel, B.},
  chapter   = 8,
  pages     = {239--236},
  month     = jan,
  publisher = {Morgan Kaufmann Publishers},
  isbn      = {1-55860-811-7},
  url       = {https://www.merl.com/publications/TR2001-22}
}

@inproceedings{ranganath_black_2014,
  title     = {Black {Box} {Variational} {Inference}},
  url       = {http://proceedings.mlr.press/v33/ranganath14.html},
  abstract  = {Variational inference has become a widely used method to
               approximate posteriors in complex latent variables models. However,
               deriving a variational inference algorithm generally requires
               significa...},
  booktitle = {{PMLR}},
  author    = {Ranganath, Rajesh and Gerrish, Sean and Blei, David},
  month     = apr,
  year      = {2014},
  pages     = {814--822},
  file      = {Full Text PDF:/Users/apodusenko/Zotero/storage/FUPXUNDE/Ranganath e.a.
               - 2014 - Black Box Variational Inference.pdf:application/pdf;Full Text
               PDF:/Users/apodusenko/Zotero/storage/TVE8PUI4/Ranganath e.a. - 2014 -
               Black Box Variational
               Inference.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/QVHIQLU4/ranganath14.html:text/html;Snapshot:/Users/apodusenko/Zotero/storage/DMAIR52V/ranganath14.html:text/html
               }
}

@incollection{moffitt_julia_2014,
  title     = {julia},
  booktitle = {Seven {More} {Languages} in seven weeks},
  author    = {Moffitt, Jack and Tate, Bruce},
  year      = {2014},
  file      = {Moffitt and Tate - 2014 -
               julia.pdf:/Users/apodusenko/Zotero/storage/76HDQ7NS/Moffitt and Tate -
               2014 - julia.pdf:application/pdf;Moffitt and Tate - 2014 -
               julia.pdf:/Users/apodusenko/Zotero/storage/QIMDE74K/Moffitt and Tate -
               2014 - julia.pdf:application/pdf}
}

@article{zhao_hmm-based_2007,
  title    = {{HMM}-based gain modeling for enhancement of speech in noise},
  volume   = {15},
  url      = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4100677},
  number   = {3},
  urldate  = {2016-02-23},
  journal  = {Audio, Speech, and Language Processing, IEEE Transactions on},
  author   = {Zhao, David Y. and Kleijn, W. Bastiaan},
  year     = {2007},
  keywords = {expectation-maximisation algorithm, Bayes methods, Statistical
              distributions, Acoustic noise, Background noise, Bayesian speech
              estimator, data-driven prior models, energy variation, explicit
              gain modeling, Gain modeling, hidden Markov model, hidden Markov
              modeling (HMM), Hidden Markov models, HMM-based gain modeling,
              noise gains, noise suppression, nonstationary noise, offline
              estimation, Performance gain, Recursive estimation, recursive
              expectation-maximization algorithm, Speech enhancement, speech gain
              estimation, speech recognition, stochastic gain variables,
              Stochastic resonance, time-invariant model parameters, time-varying
              model parameters, Working environment noise},
  pages    = {882--892},
  file     = {[HTML] from
              diva-portal.org:/Users/apodusenko/Zotero/storage/3NGUVGFA/record.html:text/html;[HTML]
              from
              diva-portal.org:/Users/apodusenko/Zotero/storage/R2B3J52G/record.html:text/html;IEEE
              Xplore Abstract
              Record:/Users/apodusenko/Zotero/storage/WBZSDB5G/abs_all.html:text/html;Snapshot:/Users/apodusenko/Zotero/storage/DNTMUNAW/abs_all.html:text/html;Snapshot:/Users/apodusenko/Zotero/storage/7BC3D6SN/abs_all.html:text/html;Zhao
              and Kleijn - 2007 - HMM-based gain modeling for enhancement of speech
              .pdf:/Users/apodusenko/Zotero/storage/TTIWJXTE/Zhao and Kleijn - 2007 -
              HMM-based gain modeling for enhancement of speech
              .pdf:application/pdf;Zhao and Kleijn - 2007 - HMM-Based Gain Modeling
              for Enhancement of Speech
              .pdf:/Users/apodusenko/Zotero/storage/TZ38G3QM/Zhao and Kleijn - 2007 -
              HMM-Based Gain Modeling for Enhancement of Speech
              .pdf:application/pdf;Zhao and Kleijn - 2007 - HMM-based gain modeling
              for enhancement of speech
              .pdf:/Users/apodusenko/Zotero/storage/6XVEQXK4/Zhao and Kleijn - 2007 -
              HMM-based gain modeling for enhancement of speech .pdf:application/pdf}
}

@incollection{yedidia_idiosyncratic_2000,
  title     = {An {Idiosyncratic} {Journey} {Beyond} {Mean} {Field} {Theory}},
  url       = {http://ieeexplore.ieee.org/articleDetails.jsp?arnumber=6281795},
  language  = {English},
  urldate   = {2014-07-31},
  booktitle = {Advanced {Mean} {Field} {Methods}},
  author    = {Yedidia, Jonathan S.},
  year      = {2000},
  pages     = {37--49},
  file      = {
               Snapshot:/Users/apodusenko/Zotero/storage/V9CV382I/login.html:text/html;Yedidia
               - 2000 - An Idiosyncratic Journey Beyond Mean Field
               Theory.pdf:/Users/apodusenko/Zotero/storage/ZC42BAFJ/Yedidia - 2000 -
               An Idiosyncratic Journey Beyond Mean Field Theory.pdf:application/pdf}
}

@phdthesis{winn_variational_2003,
  title    = {Variational message passing and its applications},
  url      = {http://johnwinn.org/Publications/thesis/Winn03_thesis.pdf},
  urldate  = {2014-04-10},
  author   = {Winn, John},
  year     = {2003},
  keywords = {variational Bayes, factor graphs},
  file     = {Winn - 2003 - Variational message passing and its
              applications.pdf:/Users/apodusenko/Zotero/storage/HH5H6HP4/Winn - 2003
              - Variational message passing and its applications.pdf:application/pdf}
}

@article{wilson_software_2013,
  title      = {Software {Carpentry}: {Lessons} {Learned}},
  shorttitle = {Software {Carpentry}},
  url        = {http://arxiv.org/abs/1307.5448},
  abstract   = {Over the last 15 years, Software Carpentry has evolved from a
                week-long training course at the US national laboratories into a
                worldwide volunteer effort to raise standards in scientific
                computing. This article explains what we have learned along the way
                the challenges we now face, and our plans for the future.},
  urldate    = {2014-07-03},
  journal    = {arXiv:1307.5448 [physics]},
  author     = {Wilson, Greg},
  month      = jul,
  year       = {2013},
  note       = {arXiv: 1307.5448},
  keywords   = {Computer Science - Computers and Society, Computer Science -
                General Literature, Physics - Physics Education},
  file       = {arXiv.org
                Snapshot:/Users/apodusenko/Zotero/storage/N5PAW45K/1307.html:text/html;arXiv.org
                Snapshot:/Users/apodusenko/Zotero/storage/MXAMFBJM/1307.html:text/html;Wilson
                - 2013 - Software Carpentry Lessons
                Learned.pdf:/Users/apodusenko/Zotero/storage/TSG7DT4E/Wilson - 2013 -
                Software Carpentry Lessons Learned.pdf:application/pdf;Wilson - 2013 -
                Software Carpentry Lessons
                Learned.pdf:/Users/apodusenko/Zotero/storage/5XBGI5HG/Wilson - 2013 -
                Software Carpentry Lessons Learned.pdf:application/pdf}
}

@article{whiteley_attention_2012,
  title   = {Attention in a {Bayesian} {Framework}},
  volume  = {6},
  issn    = {1662-5161},
  url     = {http://www.frontiersin.org/Journal/10.3389/fnhum.2012.00100/full},
  doi     = {10.3389/fnhum.2012.00100},
  urldate = {2014-04-10},
  journal = {Frontiers in Human Neuroscience},
  author  = {Whiteley, Louise and Sahani, Maneesh},
  year    = {2012},
  file    = {Whiteley and Sahani - 2012 - Attention in a Bayesian
             Framework.pdf:/Users/apodusenko/Zotero/storage/IE2ZHXKT/Whiteley and
             Sahani - 2012 - Attention in a Bayesian
             Framework.pdf:application/pdf;Whiteley and Sahani - 2012 - Attention in
             a Bayesian
             Framework.pdf:/Users/apodusenko/Zotero/storage/BZ66D3RK/Whiteley and
             Sahani - 2012 - Attention in a Bayesian Framework.pdf:application/pdf}
}

@article{watanabe_alternative_2012,
  title    = {An alternative view of variational {Bayes} and asymptotic
              approximations of free energy},
  volume   = {86},
  issn     = {0885-6125, 1573-0565},
  url      = {http://link.springer.com/article/10.1007/s10994-011-5264-5},
  doi      = {10.1007/s10994-011-5264-5},
  abstract = {Bayesian learning, widely used in many applied data-modeling
              problems, is often accomplished with approximation schemes because
              it requires intractable computation of the posterior distributions.
              In this study, we focus on two approximation methods, variational
              Bayes and local variational approximation. We show that the
              variational Bayes approach for statistical models with latent
              variables can be viewed as a special case of local variational
              approximation, where the log-sum-exp function is used to form the
              lower bound of the log-likelihood. The minimum variational free
              energy, the objective function of variational Bayes, is analyzed
              and related to the asymptotic theory of Bayesian learning. This
              analysis additionally implies a relationship between the
              generalization performance of the variational Bayes approach and
              the minimum variational free energy.},
  language = {en},
  number   = {2},
  urldate  = {2014-11-30},
  journal  = {Machine Learning},
  author   = {Watanabe, Kazuho},
  month    = feb,
  year     = {2012},
  keywords = {Variational Bayes, variational Bayes, Artificial Intelligence
              (incl. Robotics), Asymptotic analysis, Computing Methodologies,
              Control, Generalization error, Language Translation and Linguistics
              , Local variational approximation, Mechatronics, Robotics,
              Simulation and Modeling, Variational free energy, Control, Robotics
              , Mechatronics},
  pages    = {273--293},
  file     = {
              Snapshot:/Users/apodusenko/Zotero/storage/R6D3D8JI/10.html:text/html;Snapshot:/Users/apodusenko/Zotero/storage/2XG8GFJM/10.html:text/html;Watanabe
              - 2012 - An alternative view of variational Bayes and
              asymp.pdf:/Users/apodusenko/Zotero/storage/VUXZCGNT/Watanabe - 2012 -
              An alternative view of variational Bayes and
              asymp.pdf:application/pdf;Watanabe - 2012 - An alternative view of
              variational Bayes and
              asymp.pdf:/Users/apodusenko/Zotero/storage/U555TFUD/Watanabe - 2012 -
              An alternative view of variational Bayes and asymp.pdf:application/pdf}
}

@article{wagenmakers_bayesian_2010,
  title      = {Bayesian hypothesis testing for psychologists: {A} tutorial on the {
                Savage}–{Dickey} method},
  volume     = {60},
  issn       = {0010-0285},
  shorttitle = {Bayesian hypothesis testing for psychologists},
  url        = {http://www.sciencedirect.com/science/article/pii/S0010028509000826},
  doi        = {10.1016/j.cogpsych.2009.12.001},
  abstract   = {In the field of cognitive psychology, the p-value hypothesis test
                has established a stranglehold on statistical reporting. This is
                unfortunate, as the p-value provides at best a rough estimate of
                the evidence that the data provide for the presence of an
                experimental effect. An alternative and arguably more appropriate
                measure of evidence is conveyed by a Bayesian hypothesis test,
                which prefers the model with the highest average likelihood. One of
                the main problems with this Bayesian hypothesis test, however, is
                that it often requires relatively sophisticated numerical methods
                for its computation. Here we draw attention to the Savage–Dickey
                density ratio method, a method that can be used to compute the
                result of a Bayesian hypothesis test for nested models and under
                certain plausible restrictions on the parameter priors. Practical
                examples demonstrate the method’s validity, generality, and
                flexibility.},
  number     = {3},
  urldate    = {2015-01-20},
  journal    = {Cognitive Psychology},
  author     = {Wagenmakers, Eric-Jan and Lodewyckx, Tom and Kuriyal, Himanshu and
                Grasman, Raoul},
  month      = may,
  year       = {2010},
  keywords   = {Humans, Bayes Theorem, Bayes factor, Behavioral Research,
                Cognitive science, hierarchical modeling, Likelihood Functions,
                Model selection, Model Selection, Order-restrictions, Random
                effects, Statistical evidence, Models, Statistical, Data
                Interpretation, Statistical},
  pages      = {158--189},
  file       = {ScienceDirect
                Snapshot:/Users/apodusenko/Zotero/storage/DNBUJCNP/S0010028509000826.html:text/html;ScienceDirect
                Snapshot:/Users/apodusenko/Zotero/storage/37PLVVGW/S0010028509000826.html:text/html;Wagenmakers
                et al. - 2010 - Bayesian hypothesis testing for psychologists A
                t.pdf:/Users/apodusenko/Zotero/storage/UAN83BVT/Wagenmakers et al. -
                2010 - Bayesian hypothesis testing for psychologists A
                t.pdf:application/pdf}
}

@article{velimir_m._ilic_entropy_2009,
  title      = {The {Entropy} {Message} {Passing}: {A} {New} {Algorithm} {Over} {
                Factor} {Graphs}},
  shorttitle = {The {Entropy} {Message} {Passing}},
  author     = {Velimir M. Ilic, Miomir S. Stankovic},
  year       = {2009},
  file       = {The Entropy Message Passing\: A New Algorithm Over Factor Graphs -
                ResearchGate:/Users/apodusenko/Zotero/storage/3WM57M64/45857027_The_Entropy_Message_Passing_A_New_Algorithm_Over_Factor_Graphs.html:text/html;Velimir
                M. Ilic - 2009 - The Entropy Message Passing A New Algorithm Over
                .pdf:/Users/apodusenko/Zotero/storage/YYJ7J8ZF/Velimir M. Ilic - 2009 -
                The Entropy Message Passing A New Algorithm Over .pdf:application/pdf}
}

@mastersthesis{van_witteveen_adaptive_2014,
  title    = {Adaptive {Reinforcement} {Learning}},
  abstract = {This thesis investigates the applicability of the Probabilistic
              Inference for Learning COntrol (PILCO) algorithm to large systems
              and systems with time varying measurement noise. PILCO is a
              state-of-the-art model-learning Reinforcement Learning (RL)
              algorithm that uses a Gaussian Process (GP) model to average over
              uncertainties during learning. Simulated case studies on a
              second-order system and a cart-pole system show that both the
              Radial Basis Function (RBF) controller and the GP controller ﬁnd
              good solutions when the number of basis functions is chosen
              correctly. However, when a high number of basis functions is
              selected, the RBF controller fails completely, while the GP
              controller is able ﬁnd a suboptimal solution. In order to reduce
              the computational time for large systems is the identiﬁcation of
              the GP model parallelized. For a four dimensional model the
              parallelization results in a 20 to 40 percent reduction of the
              identiﬁcation computational time. A simulated case study of a
              cart-pole system shows a strong decrease in performance when
              increasing the measurement noise variance or kurtosis. The
              controller is robust for changing skewness of the measurement
              noise. Furthermore is the variance of the measurement noise an
              important parameter, because it has to be selected as a ﬁxed
              parameter of the GP controller prior to learning. Therefore
              Adaptive-Probabilistic Inference for Learning COntrol (A-PILCO) is
              proposed. This is a framework that initiates a new learning process
              when the measurement noise variance exceeds its conﬁdence bounds.
              By reducing the computational time signiﬁcantly for large and/or
              complex systems and by implementing the A-PILCO framework the PILCO
              algorithm becomes applicable larger set of systems.},
  school   = {TU Delft},
  author   = {Van Witteveen, Kees},
  year     = {2014},
  file     = {Van Witteveen - 2014 - Adaptive Reinforcement
              Learning.pdf:/Users/apodusenko/Zotero/storage/98HJ8QK4/Van Witteveen -
              2014 - Adaptive Reinforcement Learning.pdf:application/pdf;Van
              Witteveen - 2014 - Adaptive Reinforcement
              Learning.pdf:/Users/apodusenko/Zotero/storage/YLEKBGVD/Van Witteveen -
              2014 - Adaptive Reinforcement Learning.pdf:application/pdf}
}

@article{vanlier_optimal_2014,
  title     = {Optimal experiment design for model selection in biochemical networks
               },
  volume    = {8},
  copyright = {2014 Vanlier et al.; licensee BioMed Central Ltd.},
  issn      = {1752-0509},
  url       = {http://www.biomedcentral.com/1752-0509/8/20/abstract},
  doi       = {10.1186/1752-0509-8-20},
  abstract  = {Mathematical modeling is often used to formalize hypotheses on how
               a biochemical network operates by discriminating between competing
               models. Bayesian model selection offers a way to determine the
               amount of evidence that data provides to support one model over the
               other while favoring simple models. In practice, the amount of
               experimental data is often insufficient to make a clear distinction
               between competing models. Often one would like to perform a new
               experiment which would discriminate between competing hypotheses.
               PMID: 24555498},
  language  = {en},
  number    = {1},
  urldate   = {2014-11-30},
  journal   = {BMC Systems Biology},
  author    = {Vanlier, Joep and Tiemann, Christian A. and Hilbers, Peter AJ and
               Riel, Natal AW van},
  month     = feb,
  year      = {2014},
  pmid      = {24555498},
  keywords  = {Bayes factor, Model selection, Model Selection, inference,
               uncertainty, Uncertainty},
  pages     = {20},
  file      = {
               Snapshot:/Users/apodusenko/Zotero/storage/AG5BWW6Z/20.html:text/html;Snapshot:/Users/apodusenko/Zotero/storage/L75RPA6G/20.html:text/html;Vanlier
               et al. - 2014 - Optimal experiment design for model selection in
               b.pdf:/Users/apodusenko/Zotero/storage/2VJKC63G/Vanlier et al. - 2014 -
               Optimal experiment design for model selection in
               b.pdf:application/pdf;Vanlier et al. - 2014 - Optimal experiment design
               for model selection in
               b.pdf:/Users/apodusenko/Zotero/storage/P8VYSTRI/Vanlier et al. - 2014 -
               Optimal experiment design for model selection in
               b.pdf:application/pdf;Vanlier et al. - 2014 - Supplement - Optimal
               Experimental Design:/Users/apodusenko/Zotero/storage/XW4Z6ZCW/Vanlier
               et al. - 2014 - Supplement - Optimal Experimental
               Design.pdf:application/pdf;Vanlier et al. - 2014 - Supplement - Optimal
               Experimental Design:/Users/apodusenko/Zotero/storage/CLDR3NSQ/Vanlier
               et al. - 2014 - Supplement - Optimal Experimental
               Design.pdf:application/pdf}
}

@inproceedings{turner_state-space_2010,
  title     = {State-space inference and learning with {Gaussian} processes},
  url       = {
               http://machinelearning.wustl.edu/mlpapers/paper_files/AISTATS2010_TurnerDR10.pdf
               },
  urldate   = {2014-04-10},
  booktitle = {International {Conference} on {Artificial} {Intelligence} and {
               Statistics}},
  author    = {Turner, Ryan D. and Deisenroth, Marc P. and Rasmussen, Carl E.},
  year      = {2010},
  pages     = {868--875},
  file      = {Turner et al. - 2010 - State-space inference and learning with
               Gaussian p.pdf:/Users/apodusenko/Zotero/storage/IT4ZMUG2/Turner et al.
               - 2010 - State-space inference and learning with Gaussian
               p.pdf:application/pdf;Turner et al. - 2010 - State-space inference and
               learning with Gaussian
               p.pdf:/Users/apodusenko/Zotero/storage/EQMSFJ3L/Turner et al. - 2010 -
               State-space inference and learning with Gaussian p.pdf:application/pdf}
}

@article{tran_edward:_2016,
  title      = {Edward: {A} library for probabilistic modeling, inference, and
                criticism},
  shorttitle = {Edward},
  url        = {https://arxiv.org/abs/1610.09787},
  urldate    = {2017-06-07},
  journal    = {arXiv preprint arXiv:1610.09787},
  author     = {Tran, Dustin and Kucukelbir, Alp and Dieng, Adji B. and Rudolph,
                Maja and Liang, Dawen and Blei, David M.},
  year       = {2016},
  file       = {Tran et al. - 2016 - Edward A library for probabilistic modeling,
                infe.pdf:/Users/apodusenko/Zotero/storage/NXZJ57EI/Tran et al. - 2016 -
                Edward A library for probabilistic modeling, infe.pdf:application/pdf}
}

@article{sun_framework_2012,
  title    = {A framework for {Bayesian} optimality of psychophysical laws},
  volume   = {56},
  issn     = {00222496},
  url      = {
              http://yadda.icm.edu.pl/yadda/element/bwmeta1.element.elsevier-099c0473-a046-305a-8e71-67dab2abf2aa
              },
  doi      = {10.1016/j.jmp.2012.08.002},
  language = {en},
  number   = {6},
  urldate  = {2014-04-25},
  journal  = {Journal of Mathematical Psychology},
  author   = {Sun, John Z. and Wang, Grace I. and Goyal, Vivek K and Varshney, Lav
              R.},
  month    = dec,
  year     = {2012},
  pages    = {495--501},
  file     = {A framework for Bayesian optimality of psychophysical laws - Journal
              of Mathematical Psychology - Tom 56, Numer 6 (2012) - Biblioteka Nauki
              -
              Yadda:/Users/apodusenko/Zotero/storage/C9JPSBW9/bwmeta1.element.html:text/html;A
              framework for Bayesian optimality of psychophysical laws - Journal of
              Mathematical Psychology - Tom 56, Numer 6 (2012) - Biblioteka Nauki -
              Yadda:/Users/apodusenko/Zotero/storage/XS68XZYN/bwmeta1.element.html:text/html;Sun
              et al. - 2012 - A framework for Bayesian optimality of
              psychophysi.pdf:/Users/apodusenko/Zotero/storage/6V37DAM6/Sun et al. -
              2012 - A framework for Bayesian optimality of
              psychophysi.pdf:application/pdf;Sun et al. - 2012 - A framework for
              Bayesian optimality of
              psychophysi.pdf:/Users/apodusenko/Zotero/storage/2YU86WD6/Sun et al. -
              2012 - A framework for Bayesian optimality of
              psychophysi.pdf:application/pdf}
}

@unpublished{stone_eyes_2008,
  title    = {Eyes, {Flies}, and {Information} {Theory}},
  url      = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.210.725},
  abstract = {2 Information: What is it good for? 4},
  author   = {Stone, JV},
  year     = {2008},
  file     = {Citeseer -
              Snapshot:/Users/apodusenko/Zotero/storage/TN8VHJ7I/summary.html:text/html;Citeseer
              -
              Snapshot:/Users/apodusenko/Zotero/storage/6NY8IPCA/summary.html:text/html;Stone
              - 2008 - Eyes, Flies, and Information
              Theory.pdf:/Users/apodusenko/Zotero/storage/I37UZI7N/Stone - 2008 -
              Eyes, Flies, and Information Theory.pdf:application/pdf;Stone - 2008 -
              Eyes, Flies, and Information
              Theory.pdf:/Users/apodusenko/Zotero/storage/TNLS2UGI/Stone - 2008 -
              Eyes, Flies, and Information Theory.pdf:application/pdf}
}

@article{stevens_automated_2013,
  title    = {An automated and reproducible workflow for running and analyzing
              neural simulations using {Lancet} and {IPython} {Notebook}},
  volume   = {7},
  issn     = {1662-5196},
  url      = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3874632/},
  doi      = {10.3389/fninf.2013.00044},
  abstract = {Lancet is a new, simulator-independent Python utility for
              succinctly specifying, launching, and collating results from large
              batches of interrelated computationally demanding program runs.
              This paper demonstrates how to combine Lancet with IPython Notebook
              to provide a flexible, lightweight, and agile workflow for fully
              reproducible scientific research. This informal and pragmatic
              approach uses IPython Notebook to capture the steps in a scientific
              computation as it is gradually automated and made ready for
              publication, without mandating the use of any separate application
              that can constrain scientific exploration and innovation. The
              resulting notebook concisely records each step involved in even
              very complex computational processes that led to a particular
              figure or numerical result, allowing the complete chain of events
              to be replicated automatically. Lancet was originally designed to
              help solve problems in computational neuroscience, such as
              analyzing the sensitivity of a complex simulation to various
              parameters, or collecting the results from multiple runs with
              different random starting points. However, because it is never
              possible to know in advance what tools might be required in future
              tasks, Lancet has been designed to be completely general,
              supporting any type of program as long as it can be launched as a
              process and can return output in the form of files. For instance,
              Lancet is also heavily used by one of the authors in a separate
              research group for launching batches of microprocessor simulations.
              This general design will allow Lancet to continue supporting a
              given research project even as the underlying approaches and tools
              change.},
  urldate  = {2014-04-17},
  journal  = {Frontiers in Neuroinformatics},
  author   = {Stevens, Jean-Luc R. and Elver, Marco and Bednar, James A.},
  month    = dec,
  year     = {2013},
  pmid     = {24416014},
  pmcid    = {PMC3874632},
  file     = {Stevens et al. - 2013 - An automated and reproducible workflow for
              running.pdf:/Users/apodusenko/Zotero/storage/2KPBRMF2/Stevens et al. -
              2013 - An automated and reproducible workflow for
              running.pdf:application/pdf;Stevens et al. - 2013 - An automated and
              reproducible workflow for
              running.pdf:/Users/apodusenko/Zotero/storage/E28QZNJS/Stevens et al. -
              2013 - An automated and reproducible workflow for
              running.pdf:application/pdf}
}

@inproceedings{smidl_bayesian_2004,
  title     = {Bayesian estimation of non-stationary {AR} model parameters via an
               unknown forgetting factor},
  isbn      = {0-7803-8434-2},
  booktitle = {Digital {Signal} {Processing} {Workshop}, 2004 and the 3rd {IEEE}
               {Signal} {Processing} {Education} {Workshop}. 2004 {IEEE} 11th},
  publisher = {IEEE},
  author    = {Smidl, V. and Quinn, A.},
  year      = {2004},
  keywords  = {Bayes methods, Bayesian methods, autoregressive process,
               autoregressive processes, Educational institutions, Equations,
               exponential forgetting, Gaussian distribution, History, matrices,
               nonstationary AR model parameters, parameter tracking, posterior
               distribution, Random variables, Statistical distributions,
               Statistics, time-varying channels, time-varying parameter Bayesian
               estimation, unknown forgetting factor, variational techniques,
               variational-Bayes method, White noise},
  pages     = {221--225},
  file      = {IEEE Xplore Abstract
               Record:/Users/apodusenko/Zotero/storage/SUXC7L2D/abs_all.html:text/html;Smidl
               and Quinn - 2004 - Bayesian estimation of non-stationary AR model
               par.pdf:/Users/apodusenko/Zotero/storage/TZI7JEN6/Smidl and Quinn -
               2004 - Bayesian estimation of non-stationary AR model
               par.pdf:application/pdf}
}

@article{sengupta_information_2013,
  title    = {Information and {Efficiency} in the {Nervous} {System}—{A} {Synthesis
              }},
  volume   = {9},
  url      = {http://dx.doi.org/10.1371/journal.pcbi.1003157},
  doi      = {10.1371/journal.pcbi.1003157},
  abstract = {In systems biology, questions concerning the molecular and
              cellular makeup of an organism are of utmost importance, especially
              when trying to understand how unreliable components—like genetic
              circuits, biochemical cascades, and ion channels, among
              others—enable reliable and adaptive behaviour. The repertoire and
              speed of biological computations are limited by thermodynamic or
              metabolic constraints: an example can be found in neurons, where
              fluctuations in biophysical states limit the information they can
              encode—with almost 20–60\% of the total energy allocated for the
              brain used for signalling purposes, either via action potentials or
              by synaptic transmission. Here, we consider the imperatives for
              neurons to optimise computational and metabolic efficiency, wherein
              benefits and costs trade-off against each other in the context of
              self-organised and adaptive behaviour. In particular, we try to
              link information theoretic (variational) and thermodynamic
              (Helmholtz) free-energy formulations of neuronal processing and
              show how they are related in a fundamental way through a complexity
              minimisation lemma.},
  number   = {7},
  urldate  = {2014-08-25},
  journal  = {PLoS Comput Biol},
  author   = {Sengupta, Biswa and Stemmler, Martin B. and Friston, Karl J.},
  month    = jul,
  year     = {2013},
  pages    = {e1003157},
  file     = {PLoS
              Snapshot:/Users/apodusenko/Zotero/storage/DMKRIQ53/infodoi10.1371journal.pcbi.html:text/html;Sengupta
              et al. - 2013 - Information and Efficiency in the Nervous
              System—A.pdf:/Users/apodusenko/Zotero/storage/HUYP6PQF/Sengupta et al.
              - 2013 - Information and Efficiency in the Nervous
              System—A.pdf:application/pdf}
}

@article{schwartenbeck_exploration_2013,
  title   = {Exploration, novelty, surprise, and free energy minimization},
  volume  = {4},
  issn    = {1664-1078},
  url     = {http://www.frontiersin.org/Journal/10.3389/fpsyg.2013.00710/full},
  doi     = {10.3389/fpsyg.2013.00710},
  urldate = {2014-04-10},
  journal = {Frontiers in Psychology},
  author  = {Schwartenbeck, Philipp and FitzGerald, Thomas and Dolan, Raymond J.
             and Friston, Karl},
  year    = {2013},
  file    = {Schwartenbeck et al. - 2013 - Exploration, novelty, surprise, and free
             energy mi.pdf:/Users/apodusenko/Zotero/storage/JDW53QIR/Schwartenbeck
             et al. - 2013 - Exploration, novelty, surprise, and free energy
             mi.pdf:application/pdf;Schwartenbeck et al. - 2013 - Exploration,
             novelty, surprise, and free energy
             mi.pdf:/Users/apodusenko/Zotero/storage/ANRWI6M5/Schwartenbeck et al. -
             2013 - Exploration, novelty, surprise, and free energy
             mi.pdf:application/pdf}
}

@article{sato_how_2014,
  title      = {How much to trust the senses: {Likelihood} learning},
  volume     = {14},
  issn       = {1534-7362},
  shorttitle = {How much to trust the senses},
  url        = {http://jov.arvojournals.org/article.aspx?articleid=2213024},
  doi        = {10.1167/14.13.13},
  number     = {13},
  urldate    = {2016-11-04},
  journal    = {Journal of Vision},
  author     = {Sato, Yoshiyuki and Kording, Konrad P.},
  month      = nov,
  year       = {2014},
  keywords   = {Bayesian models, context-dependent learning, likelihood learning,
                sensorimotor integration},
  pages      = {13--13},
  file       = {Sato and Kording - 2014 - How much to trust the senses Likelihood
                learning.pdf:/Users/apodusenko/Zotero/storage/W7GIJDDW/Sato and Kording
                - 2014 - How much to trust the senses Likelihood
                learning.pdf:application/pdf;Sato and Kording - 2014 - How much to
                trust the senses Likelihood
                learning.pdf:/Users/apodusenko/Zotero/storage/KW3D2Y3Q/Sato and Kording
                - 2014 - How much to trust the senses Likelihood
                learning.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/N4SWWKQE/article.html:text/html;Snapshot:/Users/apodusenko/Zotero/storage/ISL6726F/13.html:text/html
                }
}

@article{salvatier_probabilistic_2016,
  title    = {Probabilistic programming in {Python} using {PyMC3}},
  volume   = {2},
  issn     = {2376-5992},
  url      = {https://peerj.com/articles/cs-55},
  doi      = {10.7717/peerj-cs.55},
  language = {en},
  urldate  = {2017-06-07},
  journal  = {PeerJ Computer Science},
  author   = {Salvatier, John and Wiecki, Thomas V. and Fonnesbeck, Christopher},
  month    = apr,
  year     = {2016},
  pages    = {e55},
  file     = {Salvatier et al. - 2016 - Probabilistic programming in Python using
              PyMC3.pdf:/Users/apodusenko/Zotero/storage/42JDVGW9/Salvatier et al. -
              2016 - Probabilistic programming in Python using
              PyMC3.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/ZDM4R9D8/cs-55.html:text/html;Snapshot:/Users/apodusenko/Zotero/storage/C88ZQVGC/cs-55.html:text/html
              }
}

@article{riegler_merging_2013,
  title      = {Merging {Belief} {Propagation} and the {Mean} {Field} {Approximation}
                : {A} {Free} {Energy} {Approach}},
  volume     = {59},
  issn       = {0018-9448, 1557-9654},
  shorttitle = {Merging {Belief} {Propagation} and the {Mean} {Field} {
                Approximation}},
  url        = {
                http://vbn.aau.dk/en/publications/merging-belief-propagation-and-the-mean-field-approximation-a-free-energy-approach(933cdedc-336e-4db1-9e17-b5918181727d).html
                },
  doi        = {10.1109/TIT.2012.2218573},
  number     = {1},
  urldate    = {2014-04-19},
  journal    = {IEEE Transactions on Information Theory},
  author     = {Riegler, Erwin and Kirkelund, Gunvor Elisabeth and Manchon, Carles
                Navarro and Badiu, Mihai-Alin and Fleury, Bernard Henri},
  month      = jan,
  year       = {2013},
  pages      = {588--602},
  file       = {Merging Belief Propagation and the Mean Field Approximation\: A Free
                Energy Approach - Research - Aalborg
                University:/Users/apodusenko/Zotero/storage/JBMKKUHP/merging-belief-propagation-and-the-mean-field-approximation-a-free-energy-approach(933cdedc-336.html:text/html;Merging
                Belief Propagation and the Mean Field Approximation\: A Free Energy
                Approach - Research - Aalborg
                University:/Users/apodusenko/Zotero/storage/BGQ7GYGX/merging-belief-propagation-and-the-mean-field-approximation-a-free-energy-approach(933cdedc-336.html:text/html;Merging
                Belief Propagation and the Mean Field Approximation\: A Free Energy
                Approach - Research - Aalborg
                University:/Users/apodusenko/Zotero/storage/IRRFY6PN/merging-belief-propagation-and-the-mean-field-approximation-a-free-energy-approach(933cdedc-336.html:text/html;Riegler
                et al. - 2013 - Merging Belief Propagation and the Mean Field
                Appr.pdf:/Users/apodusenko/Zotero/storage/TPMGFMXX/Riegler et al. -
                2013 - Merging Belief Propagation and the Mean Field
                Appr.pdf:application/pdf;Riegler et al. - 2013 - Merging Belief
                Propagation and the Mean Field
                Appr.pdf:/Users/apodusenko/Zotero/storage/F886GXG4/Riegler et al. -
                2013 - Merging Belief Propagation and the Mean Field
                Appr.pdf:application/pdf;Riegler et al. - 2013 - Merging Belief
                Propagation and the Mean Field
                Appr.pdf:/Users/apodusenko/Zotero/storage/N8FK3NTR/Riegler et al. -
                2013 - Merging Belief Propagation and the Mean Field
                Appr.pdf:application/pdf}
}

@inproceedings{reece_introduction_2010,
  title     = {An introduction to {Gaussian} processes for the {Kalman} filter
               expert},
  url       = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5711863},
  urldate   = {2014-04-10},
  booktitle = {Information {Fusion} ({FUSION}), 2010 13th {Conference} on},
  publisher = {IEEE},
  author    = {Reece, Steven and Roberts, Stephen},
  year      = {2010},
  pages     = {1--9},
  file      = {Reece and Roberts - 2010 - An introduction to Gaussian processes for
               the Kalm.pdf:/Users/apodusenko/Zotero/storage/9QNN925F/Reece and
               Roberts - 2010 - An introduction to Gaussian processes for the
               Kalm.pdf:application/pdf;Reece and Roberts - 2010 - An introduction to
               Gaussian processes for the
               Kalm.pdf:/Users/apodusenko/Zotero/storage/RYL943VT/Reece and Roberts -
               2010 - An introduction to Gaussian processes for the
               Kalm.pdf:application/pdf}
}

@article{raju_inference_2016,
  title    = {Inference by {Reparameterization} in {Neural} {Population} {Codes}},
  url      = {http://arxiv.org/abs/1605.06544},
  abstract = {Behavioral experiments on humans and animals suggest that the
              brain performs probabilistic inference to interpret its
              environment. Here we present a new general-purpose,
              biologically-plausible neural implementation of approximate
              inference. The neural network represents uncertainty using
              Probabilistic Population Codes (PPCs), which are distributed neural
              representations that naturally encode probability distributions,
              and support marginalization and evidence integration in a
              biologically-plausible manner. By connecting multiple PPCs together
              as a probabilistic graphical model, we represent multivariate
              probability distributions. Approximate inference in graphical
              models can be accomplished by message-passing algorithms that
              disseminate local information throughout the graph. An attractive
              and often accurate example of such an algorithm is Loopy Belief
              Propagation (LBP), which uses local marginalization and evidence
              integration operations to perform approximate inference efficiently
              even for complex models. Unfortunately, a subtle feature of LBP
              renders it neurally implausible. However, LBP can be elegantly
              reformulated as a sequence of Tree-based Reparameterizations (TRP)
              of the graphical model. We re-express the TRP updates as a
              nonlinear dynamical system with both fast and slow timescales, and
              show that this produces a neurally plausible solution. By combining
              all of these ideas, we show that a network of PPCs can represent
              multivariate probability distributions and implement the TRP
              updates to perform probabilistic inference. Simulations with
              Gaussian graphical models demonstrate that the neural network
              inference quality is comparable to the direct evaluation of LBP and
              robust to noise, and thus provides a promising mechanism for
              general probabilistic inference in the population codes of the
              brain.},
  urldate  = {2017-07-28},
  journal  = {arXiv:1605.06544 [q-bio]},
  author   = {Raju, Rajkumar Vasudeva and Pitkow, Xaq},
  month    = may,
  year     = {2016},
  note     = {arXiv: 1605.06544},
  keywords = {Quantitative Biology - Neurons and Cognition},
  file     = {arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/INSV5BC8/1605.html:text/html;arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/2Y79375J/1605.html:text/html;Raju
              en Pitkow - 2016 - Inference by Reparameterization in Neural
              Populati.pdf:/Users/apodusenko/Zotero/storage/CDBSBAQU/Raju en Pitkow -
              2016 - Inference by Reparameterization in Neural
              Populati.pdf:application/pdf;Raju en Pitkow - 2016 - Inference by
              Reparameterization in Neural
              Populati.pdf:/Users/apodusenko/Zotero/storage/D3THDFPA/Raju en Pitkow -
              2016 - Inference by Reparameterization in Neural
              Populati.pdf:application/pdf}
}

@article{pezzulo_active_nodate,
  title    = {Active {Inference}, homeostatic regulation and adaptive behavioural
              control},
  issn     = {0301-0082},
  url      = {http://www.sciencedirect.com/science/article/pii/S0301008215000908},
  doi      = {10.1016/j.pneurobio.2015.09.001},
  abstract = {We review a theory of homeostatic regulation and adaptive
              behavioural control within the Active Inference framework. Our aim
              is to connect two research streams that are usually considered
              independently; namely, Active Inference and associative learning
              theories of animal behaviour. The former uses a probabilistic
              (Bayesian) formulation of perception and action, while the latter
              calls on multiple (Pavlovian, habitual, goal-directed) processes
              for homeostatic and behavioural control. We offer a synthesis these
              classical processes and cast them as successive hierarchical
              contextualisations of sensorimotor constructs, using the generative
              models that underpin Active Inference. This dissolves any apparent
              mechanistic distinction between the optimization processes that
              mediate classical control or learning. Furthermore, we generalize
              the scope of Active Inference by emphasizing interoceptive
              inference and homeostatic regulation. The ensuing homeostatic (or
              allostatic) perspective provides an intuitive explanation for how
              priors act as drives or goals to enslave action, and emphasises the
              embodied nature of inference.},
  urldate  = {2015-09-17},
  journal  = {Progress in Neurobiology},
  author   = {Pezzulo, Giovanni and Rigoli, Francesco and Friston, Karl},
  keywords = {Active inference, Adaptive control, Homeostatic regulation,
              Model-based control, Model-free control, Pavlovian control},
  file     = {Pezzulo et al. - 2015 - Active Inference, homeostatic regulation and
              adapt.pdf:/Users/apodusenko/Zotero/storage/R329FAQR/Pezzulo et al. -
              2015 - Active Inference, homeostatic regulation and
              adapt.pdf:application/pdf;ScienceDirect
              Snapshot:/Users/apodusenko/Zotero/storage/4NDBUP4T/S0301008215000908.html:text/html;ScienceDirect
              Snapshot:/Users/apodusenko/Zotero/storage/AWEDV9AH/S0301008215000908.html:text/html
              }
}

@article{penny_bayesian_2012,
  title    = {Bayesian {Models} of {Brain} and {Behaviour}},
  volume   = {2012},
  url      = {http://www.hindawi.com/journals/isrn/2012/785791/abs/},
  doi      = {10.5402/2012/785791},
  abstract = {This paper presents a review of Bayesian models of brain and
              behaviour. We first review the basic principles of Bayesian
              inference. This is followed by descriptions of sampling and
              variational methods for approximate inference, and forward and
              backward recursions in time for inference in dynamical models. The
              review of behavioural models covers work in visual processing,
              sensory integration, sensorimotor integration, and collective
              decision making. The review of brain models covers a range of
              spatial scales from synapses to neurons and population codes, but
              with an emphasis on models of cortical hierarchies. We describe a
              simple hierarchical model which provides a mathematical framework
              relating constructs in Bayesian inference to those in neural
              computation. We close by reviewing recent theoretical developments
              in Bayesian inference for planning and control.},
  language = {en},
  urldate  = {2014-09-02},
  journal  = {International Scholarly Research Notices},
  author   = {Penny, William},
  month    = oct,
  year     = {2012},
  pages    = {e785791},
  file     = {Penny - 2012 - Bayesian Models of Brain and
              Behaviour.pdf:/Users/apodusenko/Zotero/storage/V7SJBXDB/Penny - 2012 -
              Bayesian Models of Brain and Behaviour.pdf:application/pdf;Penny - 2012
              - Bayesian Models of Brain and
              Behaviour.pdf:/Users/apodusenko/Zotero/storage/45EJ4254/Penny - 2012 -
              Bayesian Models of Brain and
              Behaviour.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/IJHA9IXS/785791.html:application/xhtml+xml;Snapshot:/Users/apodusenko/Zotero/storage/8SPM3CZC/785791.html:text/html
              }
}

@inproceedings{pedersen_variational_2011,
  title     = {A variational message passing algorithm for sensor self-localization
               in wireless networks},
  doi       = {10.1109/ISIT.2011.6033940},
  abstract  = {We propose a novel algorithm for sensor self-localization in
               cooperative wireless networks where observations of relative sensor
               distances are available. The variational message passing (VMP)
               algorithm is used to implement a mean field solution to the
               estimation of the posterior probabilities of the sensor positions
               in an R2 scenario. Extension to R3 is straight-forward. Compared to
               non-parametric methods based on belief propagation, the VMP
               algorithm features significantly lower communication overhead
               between sensors. This is supported by performance simulations which
               show that the estimated mean localization error of the algorithm
               stabilizes after approximately 30 iterations.},
  booktitle = {2011 {IEEE} {International} {Symposium} on {Information} {Theory}
               {Proceedings} ({ISIT})},
  author    = {Pedersen, C. and Pedersen, T. and Fleury, B.H.},
  month     = jul,
  year      = {2011},
  keywords  = {Noise, Message passing, Belief propagation, Approximation
               algorithms, Probability, Signal processing algorithms,
               communication overhead, cooperative communication, cooperative
               wireless network, estimated mean localization, iterative methods,
               mean field solution implementation, Mobile communication,
               nonparametric method, performance simulation, posterior
               probabilities estimation, sensor placement, sensor
               self-localization, variational message passing algorithm, VMP
               algorithm, Wireless networks, wireless sensor networks},
  pages     = {2158--2162},
  file      = {IEEE Xplore Abstract
               Record:/Users/apodusenko/Zotero/storage/8HE84B79/login.html:text/html;IEEE
               Xplore Abstract
               Record:/Users/apodusenko/Zotero/storage/8YEPYI3L/login.html:text/html;Pedersen
               et al. - 2011 - A variational message passing algorithm for
               sensor.pdf:/Users/apodusenko/Zotero/storage/WT2MUSI4/Pedersen et al. -
               2011 - A variational message passing algorithm for
               sensor.pdf:application/pdf;Pedersen et al. - 2011 - A variational
               message passing algorithm for
               sensor.pdf:/Users/apodusenko/Zotero/storage/ART4SYCG/Pedersen et al. -
               2011 - A variational message passing algorithm for
               sensor.pdf:application/pdf}
}

@article{mumford_computational_1992,
  title    = {On the computational architecture of the neocortex. {II}. {The} role
              of cortico-cortical loops},
  volume   = {66},
  issn     = {0340-1200},
  abstract = {This paper is a sequel to an earlier paper which proposed an
              active role for the thalamus, integrating multiple hypotheses
              formed in the cortex via the thalamo-cortical loop. In this paper,
              I put forward a hypothesis on the role of the reciprocal,
              topographic pathways between two cortical areas, one often a
              'higher' area dealing with more abstract information about the
              world, the other 'lower', dealing with more concrete data. The
              higher area attempts to fit its abstractions to the data it
              receives from lower areas by sending back to them from its deep
              pyramidal cells a template reconstruction best fitting the lower
              level view. The lower area attempts to reconcile the reconstruction
              of its view that it receives from higher areas with what it knows,
              sending back from its superficial pyramidal cells the features in
              its data which are not predicted by the higher area. The whole
              calculation is done with all areas working simultaneously, but with
              order imposed by synchronous activity in the various top-down,
              bottom-up loops. Evidence for this theory is reviewed and
              experimental tests are proposed. A third part of this paper will
              deal with extensions of these ideas to the frontal lobe.},
  language = {eng},
  number   = {3},
  journal  = {Biological Cybernetics},
  author   = {Mumford, D.},
  year     = {1992},
  pmid     = {1540675},
  keywords = {Humans, Neurons, Models, Neurological, Animals, Cerebral Cortex,
              Cybernetics, Pyramidal Tracts, Synapses, Models, Neurological},
  pages    = {241--251},
  file     = {Mumford - 1992 - On the computational architecture of the
              neocortex.pdf:/Users/apodusenko/Zotero/storage/VZAXUVQY/Mumford - 1992
              - On the computational architecture of the
              neocortex.pdf:application/pdf}
}

@inproceedings{mcintyre_thinking_2007,
  title     = {On thinking probabilistically},
  url       = {
               http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.145.6184&rep=rep1&type=pdf
               },
  urldate   = {2014-04-12},
  booktitle = {Extreme {Events} ({Proceedings} of the 15th {Aha} {Hulikoa} {
               Workshop}), {SOEST}, {Unuiversity} of {Hawaii}},
  publisher = {Citeseer},
  author    = {McIntyre, Michael E.},
  year      = {2007},
  pages     = {153--161},
  file      = {Mcintyre - 2007 - On thinking
               probabilistically.pdf:/Users/apodusenko/Zotero/storage/5985R8UT/Mcintyre
               - 2007 - On thinking probabilistically.pdf:application/pdf;Mcintyre -
               2007 - On thinking
               probabilistically.pdf:/Users/apodusenko/Zotero/storage/UNCKTD39/Mcintyre
               - 2007 - On thinking probabilistically.pdf:application/pdf}
}

@incollection{luttinen_linear_2014,
  series    = {Lecture {Notes} in {Computer} {Science}},
  title     = {Linear {State}-{Space} {Model} with {Time}-{Varying} {Dynamics}},
  copyright = {©2014 Springer-Verlag Berlin Heidelberg},
  isbn      = {978-3-662-44850-2 978-3-662-44851-9},
  url       = {http://link.springer.com/chapter/10.1007/978-3-662-44851-9_22},
  abstract  = {This paper introduces a linear state-space model with time-varying
               dynamics. The time dependency is obtained by forming the state
               dynamics matrix as a time-varying linear combination of a set of
               matrices. The time dependency of the weights in the linear
               combination is modelled by another linear Gaussian dynamical model
               allowing the model to learn how the dynamics of the process
               changes. Previous approaches have used switching models which have
               a small set of possible state dynamics matrices and the model
               selects one of those matrices at each time, thus jumping between
               them. Our model forms the dynamics as a linear combination and the
               changes can be smooth and more continuous. The model is motivated
               by physical processes which are described by linear partial
               differential equations whose parameters vary in time. An example of
               such a process could be a temperature field whose evolution is
               driven by a varying wind direction. The posterior inference is
               performed using variational Bayesian approximation. The experiments
               on stochastic advection-diffusion processes and real-world weather
               processes show that the model with time-varying dynamics can
               outperform previously introduced approaches.},
  language  = {en},
  number    = {8725},
  urldate   = {2014-10-21},
  booktitle = {Machine {Learning} and {Knowledge} {Discovery} in {Databases}},
  publisher = {Springer Berlin Heidelberg},
  author    = {Luttinen, Jaakko and Raiko, Tapani and Ilin, Alexander},
  editor    = {Calders, Toon and Esposito, Floriana and Hüllermeier, Eyke and Meo,
               Rosa},
  month     = jan,
  year      = {2014},
  keywords  = {Artificial Intelligence (incl. Robotics), Data Mining and
               Knowledge Discovery, Information Storage and Retrieval, Pattern
               Recognition},
  pages     = {338--353},
  file      = {Luttinen et al. - 2014 - Linear State-Space Model with Time-Varying
               Dynamic.pdf:/Users/apodusenko/Zotero/storage/547PW3VX/Luttinen et al. -
               2014 - Linear State-Space Model with Time-Varying
               Dynamic.pdf:application/pdf;Luttinen et al. - 2014 - Linear State-Space
               Model with Time-Varying
               Dynamic.pdf:/Users/apodusenko/Zotero/storage/XSYY3YHM/Luttinen et al. -
               2014 - Linear State-Space Model with Time-Varying
               Dynamic.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/NR9WVSAU/978-3-662-44851-9_22.html:text/html;Snapshot:/Users/apodusenko/Zotero/storage/RYTJZAVP/978-3-662-44851-9_22.html:text/html
               }
}

@article{lunn_winbugs_2000,
  title      = {{WinBUGS} - {A} {Bayesian} modelling framework: {Concepts}, structure
                , and extensibility},
  volume     = {10},
  issn       = {0960-3174, 1573-1375},
  shorttitle = {{WinBUGS} - {A} {Bayesian} modelling framework},
  url        = {https://link.springer.com/article/10.1023/A:1008929526011},
  doi        = {10.1023/A:1008929526011},
  abstract   = {WinBUGS is a fully extensible modular framework for constructing
                and analysing Bayesian full probability models. Models may be
                specified either textually via the BUGS language or pictorially
                using a graphical interface called DoodleBUGS. WinBUGS processes
                the model specification and constructs an object-oriented
                representation of the model. The software offers a user-interface,
                based on dialogue boxes and menu commands, through which the model
                may then be analysed using Markov chain Monte Carlo techniques. In
                this paper we discuss how and why various modern computing concepts
                , such as object-orientation and run-time linking, feature in the
                software's design. We also discuss how the framework may be
                extended. It is possible to write specific applications that form
                an apparently seamless interface with WinBUGS for users with
                specialized requirements. It is also possible to interface with
                WinBUGS at a lower level by incorporating new object types that may
                be used by WinBUGS without knowledge of the modules in which they
                are implemented. Neither of these types of extension require access
                to, or even recompilation of, the WinBUGS source-code.},
  number     = {4},
  journal    = {Statistics and Computing},
  author     = {Lunn, David J. and Thomas, Andrew and Best, Nicky and Spiegelhalter,
                David},
  month      = oct,
  year       = {2000},
  pages      = {325--337},
  file       = {
                Snapshot:/Users/apodusenko/Zotero/storage/WSVNSF5J/A1008929526011.html:text/html;Snapshot:/Users/apodusenko/Zotero/storage/MT6233E3/A1008929526011.html:text/html
                }
}

@article{liepe_maximizing_2013,
  title    = {Maximizing the {Information} {Content} of {Experiments} in {Systems}
              {Biology}},
  volume   = {9},
  url      = {http://dx.doi.org/10.1371/journal.pcbi.1002888},
  doi      = {10.1371/journal.pcbi.1002888},
  abstract = {Author SummaryFor most biological signalling and regulatory
              systems we still lack reliable mechanistic models. And where such
              models exist, e.g. in the form of differential equations, we
              typically have only rough estimates for the parameters that
              characterize the biochemical reactions. In order to improve our
              knowledge of such systems we require better estimates for these
              parameters and here we show how judicious choice of experiments,
              based on a combination of simulations and information theoretical
              analysis, can help us. Our approach builds on the available,
              frequently rudimentary information, and identifies which
              experimental set-up provides most additional information about all
              the parameters, or individual parameters. We will also consider the
              related but subtly different problem of which experiments need to
              be performed in order to decrease the uncertainty about the
              behaviour of the system under altered conditions. We develop the
              theoretical framework in the necessary detail before illustrating
              its use and applying it to the repressilator model, the regulation
              of Hes1 and signal transduction in the Akt pathway.},
  number   = {1},
  urldate  = {2014-11-30},
  journal  = {PLoS Comput Biol},
  author   = {Liepe, Juliane and Filippi, Sarah and Komorowski, Michał and Stumpf,
              Michael P. H.},
  month    = jan,
  year     = {2013},
  pages    = {e1002888},
  file     = {Liepe et al. - 2013 - Maximizing the Information Content of
              Experiments .pdf:/Users/apodusenko/Zotero/storage/Q7R5MJK6/Liepe et al.
              - 2013 - Maximizing the Information Content of Experiments
              .pdf:application/pdf;Liepe et al. - 2013 - Maximizing the Information
              Content of Experiments
              .pdf:/Users/apodusenko/Zotero/storage/5U85KUM9/Liepe et al. - 2013 -
              Maximizing the Information Content of Experiments
              .pdf:application/pdf;PLoS
              Snapshot:/Users/apodusenko/Zotero/storage/JBM7GIXQ/infodoi10.1371journal.pcbi.html:text/html;PLoS
              Snapshot:/Users/apodusenko/Zotero/storage/ZZM8WVYJ/infodoi10.1371journal.pcbi.html:text/html
              }
}

@article{liang_generative_2013,
  title    = {A {Generative} {Product}-of-{Filters} {Model} of {Audio}},
  url      = {http://arxiv.org/abs/1312.5857},
  abstract = {We propose the product-of-filters (PoF) model, a generative model
              that decomposes audio spectra as sparse linear combinations of "
              filters" in the log-spectral domain. PoF makes similar assumptions
              to those used in the classic homomorphic filtering approach to
              signal processing, but replaces hand-designed decompositions built
              of basic signal processing operations with a learned decomposition
              based on statistical inference. This paper formulates the PoF model
              and derives a mean-field method for posterior inference and a
              variational EM algorithm to estimate the model's free parameters.
              We demonstrate PoF's potential for audio processing on a bandwidth
              expansion task, and show that PoF can serve as an effective
              unsupervised feature extractor for a speaker identification task.},
  urldate  = {2014-10-16},
  journal  = {arXiv:1312.5857 [cs, stat]},
  author   = {Liang, Dawen and Hoffman, Matthew D. and Mysore, Gautham J.},
  month    = dec,
  year     = {2013},
  note     = {arXiv: 1312.5857},
  keywords = {Computer Science - Learning, Statistics - Machine Learning},
  file     = {arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/H6BEVH3R/1312.html:text/html;arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/ZJVEZIRX/1312.html:text/html;Liang
              et al. - 2013 - A Generative Product-of-Filters Model of
              Audio.pdf:/Users/apodusenko/Zotero/storage/EZ2TXBFE/Liang et al. - 2013
              - A Generative Product-of-Filters Model of
              Audio.pdf:application/pdf;Liang et al. - 2013 - A Generative
              Product-of-Filters Model of
              Audio.pdf:/Users/apodusenko/Zotero/storage/NV8B6ZIH/Liang et al. - 2013
              - A Generative Product-of-Filters Model of Audio.pdf:application/pdf}
}

@article{lee_dynamic_2014,
  series   = {Theoretical and computational neuroscience},
  title    = {Dynamic belief state representations},
  volume   = {25},
  issn     = {0959-4388},
  url      = {http://www.sciencedirect.com/science/article/pii/S0959438814000348},
  doi      = {10.1016/j.conb.2014.01.018},
  abstract = {Perceptual and control systems are tasked with the challenge of
              accurately and efficiently estimating the dynamic states of objects
              in the environment. To properly account for uncertainty, it is
              necessary to maintain a dynamical belief state representation
              rather than a single state vector. In this review, canonical
              algorithms for computing and updating belief states in robotic
              applications are delineated, and connections to biological systems
              are highlighted. A navigation example is used to illustrate the
              importance of properly accounting for correlations between belief
              state components, and to motivate the need for further
              investigations in psychophysics and neurobiology.},
  urldate  = {2014-11-07},
  journal  = {Current Opinion in Neurobiology},
  author   = {Lee, Daniel D and Ortega, Pedro A and Stocker, Alan A},
  month    = apr,
  year     = {2014},
  pages    = {221--227},
  file     = {Lee et al. - 2014 - Dynamic belief state
              representations.pdf:/Users/apodusenko/Zotero/storage/2GKCGN95/Lee et
              al. - 2014 - Dynamic belief state
              representations.pdf:application/pdf;Lee et al. - 2014 - Dynamic belief
              state representations.pdf:/Users/apodusenko/Zotero/storage/4QDLBK6F/Lee
              et al. - 2014 - Dynamic belief state
              representations.pdf:application/pdf;ScienceDirect
              Snapshot:/Users/apodusenko/Zotero/storage/VEW63JQT/S0959438814000348.html:text/html;ScienceDirect
              Snapshot:/Users/apodusenko/Zotero/storage/6G2G75N9/S0959438814000348.html:text/html
              }
}

@inproceedings{kucukelbir_automatic_2015,
  title     = {Automatic {Variational} {Inference} in {Stan}},
  url       = {
               http://papers.nips.cc/paper/5758-automatic-variational-inference-in-stan.pdf
               },
  urldate   = {2017-07-28},
  booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 28},
  publisher = {Curran Associates, Inc.},
  author    = {Kucukelbir, Alp and Ranganath, Rajesh and Gelman, Andrew and Blei,
               David},
  editor    = {Cortes, C. and Lawrence, N. D. and Lee, D. D. and Sugiyama, M. and
               Garnett, R.},
  year      = {2015},
  pages     = {568--576},
  file      = {Kucukelbir e.a. - 2015 - Automatic Variational Inference in
               Stan.pdf:/Users/apodusenko/Zotero/storage/WFHYUSR9/Kucukelbir e.a. -
               2015 - Automatic Variational Inference in Stan.pdf:application/pdf;NIPS
               Snapshort:/Users/apodusenko/Zotero/storage/2GU8ILKD/5758-automatic-variational-inference-in-stan.html:text/html
               }
}

@book{koller_probabilistic_2009,
  title     = {Probabilistic graphical models: principles and techniques},
  isbn      = {0-262-01319-3},
  publisher = {MIT press},
  author    = {Koller, Daphne and Friedman, Nir},
  year      = {2009}
}

@article{katahira_deterministic_2008,
  title    = {Deterministic annealing variant of variational {Bayes} method},
  volume   = {95},
  issn     = {1742-6596},
  url      = {http://iopscience.iop.org/1742-6596/95/1/012015},
  doi      = {10.1088/1742-6596/95/1/012015},
  abstract = {The Variational Bayes (VB) method is widely used as an
              approximation of the Bayesian method. Because the VB method is a
              gradient algorithm, it is often trapped by poor local optimal
              solutions. We introduce deterministic annealing to the VB method to
              overcome such a local optimal problem. A temperature parameter is
              introduced to the free energy for controlling the annealing process
              deterministically. Applying the method to a mixture of Gaussian
              models and hidden Markov models, we show that it can obtain the
              global optimum of the free energy and discover optimal model
              structure.},
  language = {en},
  number   = {1},
  urldate  = {2015-06-23},
  journal  = {Journal of Physics: Conference Series},
  author   = {Katahira, K. and Watanabe, K. and Okada, M.},
  month    = jan,
  year     = {2008},
  pages    = {012015},
  file     = {Katahira et al. - 2008 - Deterministic annealing variant of
              variational Bay.pdf:/Users/apodusenko/Zotero/storage/C6JNEFDE/Katahira
              et al. - 2008 - Deterministic annealing variant of variational
              Bay.pdf:application/pdf;Katahira et al. - 2008 - Deterministic
              annealing variant of variational
              Bay.pdf:/Users/apodusenko/Zotero/storage/VZFR2DF9/Katahira et al. -
              2008 - Deterministic annealing variant of variational
              Bay.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/VJK7TKZG/012015.html:text/html
              }
}

@article{karny_approximate_2014,
  series   = {Processing and {Mining} {Complex} {Data} {Streams}},
  title    = {Approximate {Bayesian} recursive estimation},
  volume   = {285},
  issn     = {0020-0255},
  url      = {http://www.sciencedirect.com/science/article/pii/S0020025514000966},
  doi      = {10.1016/j.ins.2014.01.048},
  abstract = {Bayesian learning provides a firm theoretical basis of the design
              and exploitation of algorithms in data-streams processing
              (preprocessing, change detection, hypothesis testing, clustering,
              etc.). Primarily, it relies on a recursive parameter estimation of
              a firmly bounded complexity. As a rule, it has to approximate the
              exact posterior probability density (pd), which comprises unreduced
              information about the estimated parameter. In the recursive
              treatment of the data stream, the latest approximate pd is usually
              updated using the treated parametric model and the newest data and
              then approximated. The fact that approximation errors may
              accumulate over time course is mostly neglected in the estimator
              design and, at most, checked ex post. The paper inspects the
              estimator design with respect to the error accumulation and
              concludes that a sort of forgetting (pd flattening) is an
              indispensable part of a reliable approximate recursive estimation.
              The conclusion results from a Bayesian problem formulation
              complemented by the minimum Kullback–Leibler divergence principle.
              Claims of the paper are supported by a straightforward analysis, by
              elaboration of the proposed estimator to widely applicable
              parametric models and illustrated numerically.},
  urldate  = {2015-10-12},
  journal  = {Information Sciences},
  author   = {Kárný, Miroslav},
  month    = nov,
  year     = {2014},
  keywords = {Approximate parameter estimation, Bayesian recursive estimation,
              Forgetting, Kullback–Leibler divergence},
  pages    = {100--111},
  file     = {Kárný - 2014 - Approximate Bayesian recursive
              estimation.pdf:/Users/apodusenko/Zotero/storage/6M6W5P8A/Kárný - 2014 -
              Approximate Bayesian recursive estimation.pdf:application/pdf;Kárný -
              2014 - Approximate Bayesian recursive
              estimation.pdf:/Users/apodusenko/Zotero/storage/8YPMEEBC/Kárný - 2014 -
              Approximate Bayesian recursive
              estimation.pdf:application/pdf;ScienceDirect
              Snapshot:/Users/apodusenko/Zotero/storage/87E29VMP/S0020025514000966.html:text/html;ScienceDirect
              Snapshot:/Users/apodusenko/Zotero/storage/BZ8F2AZ8/S0020025514000966.html:text/html
              }
}

@article{kanai_cerebral_2015,
  title      = {Cerebral hierarchies: predictive processing, precision and the
                pulvinar},
  volume     = {370},
  copyright  = {. © 2015 The Authors. Published by the Royal Society under the
                terms of the Creative Commons Attribution License
                http://creativecommons.org/licenses/by/4.0/, which permits
                unrestricted use, provided the original author and source are
                credited.},
  issn       = {0962-8436, 1471-2970},
  shorttitle = {Cerebral hierarchies},
  url        = {http://rstb.royalsocietypublishing.org/content/370/1668/20140169},
  doi        = {10.1098/rstb.2014.0169},
  abstract   = {This paper considers neuronal architectures from a computational
                perspective and asks what aspects of neuroanatomy and
                neurophysiology can be disclosed by the nature of neuronal
                computations? In particular, we extend current formulations of the
                brain as an organ of inference—based upon hierarchical predictive
                coding—and consider how these inferences are orchestrated. In other
                words, what would the brain require to dynamically coordinate and
                contextualize its message passing to optimize its computational
                goals? The answer that emerges rests on the delicate (modulatory)
                gain control of neuronal populations that select and coordinate
                (prediction error) signals that ascend cortical hierarchies. This
                is important because it speaks to a hierarchical anatomy of
                extrinsic (between region) connections that form two distinct
                classes, namely a class of driving (first-order) connections that
                are concerned with encoding the content of neuronal representations
                and a class of modulatory (second-order) connections that establish
                context—in the form of the salience or precision ascribed to
                content. We explore the implications of this distinction from a
                formal perspective (using simulations of feature–ground
                segregation) and consider the neurobiological substrates of the
                ensuing precision-engineered dynamics, with a special focus on the
                pulvinar and attention.},
  language   = {en},
  number     = {1668},
  urldate    = {2015-12-29},
  journal    = {Philosophical Transactions of the Royal Society of London B:
                Biological Sciences},
  author     = {Kanai, Ryota and Komura, Yutaka and Shipp, Stewart and Friston, Karl
                },
  month      = may,
  year       = {2015},
  pmid       = {25823866},
  pages      = {20140169},
  file       = {Kanai et al. - 2015 - Cerebral hierarchies predictive processing,
                preci.pdf:/Users/apodusenko/Zotero/storage/DHGSAVWZ/Kanai et al. - 2015
                - Cerebral hierarchies predictive processing,
                preci.pdf:application/pdf;Kanai et al. - 2015 - Cerebral hierarchies
                predictive processing,
                preci.pdf:/Users/apodusenko/Zotero/storage/58C2DZ68/Kanai et al. - 2015
                - Cerebral hierarchies predictive processing,
                preci.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/A5623UU7/20140169.html:text/html;Snapshot:/Users/apodusenko/Zotero/storage/L4LK5SFC/20140169.html:text/html
                }
}

@article{ishii_control_2002,
  title    = {Control of {Exploitation}-{Exploration} {Meta}-{Parameter} in {
              Reinforcement} {Learning}},
  volume   = {15},
  abstract = {In reinforcement learning, the duality between exploitation and
              exploration has long been an important issue. This paper presents a
              new method that controls the balance between exploitation and
              exploration. Our learning scheme is based on model-based
              reinforcement learning, in which the Bayes inference with
              forgetting effect estimates the state-transition probability of the
              environment. The balance parameter, which corresponds to the
              randomness in action selection, is controlled based on variation of
              action results and perception of environmental change. When applied
              to maze tasks, our method successfully obtains good controls by
              adapting to environmental changes. Recently, Usher et al. [60] has
              suggested that noradrenergic neurons in the locus coeruleus may
              control the exploitation-exploration balance in a real brain and
              that the balance may correspond to the level of animal's selective
              attention. According to this scenario, we also discuss a possible
              implementation in the brain.},
  journal  = {Neural Networks},
  author   = {Ishii, Shin and Yoshida, Wako and Yoshimoto, Junichiro},
  year     = {2002},
  pages    = {665--687},
  file     = {Citeseer -
              Snapshot:/Users/apodusenko/Zotero/storage/3QZW3HQ6/summary.html:text/html;Citeseer
              -
              Snapshot:/Users/apodusenko/Zotero/storage/VHUFWZ7M/summary.html:text/html;Ishii
              et al. - 2002 - Control of Exploitation-Exploration
              Meta-Parameter.pdf:/Users/apodusenko/Zotero/storage/H4Z24K5H/Ishii et
              al. - 2002 - Control of Exploitation-Exploration
              Meta-Parameter.pdf:application/pdf;Ishii et al. - 2002 - Control of
              Exploitation-Exploration
              Meta-Parameter.pdf:/Users/apodusenko/Zotero/storage/INX6W5Q9/Ishii et
              al. - 2002 - Control of Exploitation-Exploration
              Meta-Parameter.pdf:application/pdf}
}

@article{huber_recursive_2014,
  title      = {Recursive {Gaussian} process: {On}-line regression and learning},
  volume     = {45},
  issn       = {01678655},
  shorttitle = {Recursive {Gaussian} process},
  url        = {http://linkinghub.elsevier.com/retrieve/pii/S0167865514000786},
  doi        = {10.1016/j.patrec.2014.03.004},
  language   = {en},
  urldate    = {2014-04-10},
  journal    = {Pattern Recognition Letters},
  author     = {Huber, Marco F.},
  month      = aug,
  year       = {2014},
  pages      = {85--91},
  file       = {Huber - 2014 - Recursive Gaussian process On-line regression
                and.pdf:/Users/apodusenko/Zotero/storage/NAVQXGUE/Huber - 2014 -
                Recursive Gaussian process On-line regression
                and.pdf:application/pdf;Huber - 2014 - Recursive Gaussian process
                On-line regression
                and.pdf:/Users/apodusenko/Zotero/storage/JSP258ZM/Huber - 2014 -
                Recursive Gaussian process On-line regression and.pdf:application/pdf}
}

@article{hines_determination_2014,
  title      = {Determination of parameter identifiability in nonlinear biophysical
                models: {A} {Bayesian} approach},
  volume     = {143},
  issn       = {0022-1295, 1540-7748},
  shorttitle = {Determination of parameter identifiability in nonlinear
                biophysical models},
  url        = {http://jgp.rupress.org/content/143/3/401},
  doi        = {10.1085/jgp.201311116},
  abstract   = {A major goal of biophysics is to understand the physical
                mechanisms of biological molecules and systems. Mechanistic models
                are evaluated based on their ability to explain carefully
                controlled experiments. By fitting models to data, biophysical
                parameters that cannot be measured directly can be estimated from
                experimentation. However, it might be the case that many different
                combinations of model parameters can explain the observations
                equally well. In these cases, the model parameters are not
                identifiable: the experimentation has not provided sufficient
                constraining power to enable unique estimation of their true
                values. We demonstrate that this pitfall is present even in simple
                biophysical models. We investigate the underlying causes of
                parameter non-identifiability and discuss straightforward methods
                for determining when parameters of simple models can be inferred
                accurately. However, for models of even modest complexity, more
                general tools are required to diagnose parameter
                non-identifiability. We present a method based in Bayesian
                inference that can be used to establish the reliability of
                parameter estimates, as well as yield accurate quantification of
                parameter confidence.},
  language   = {en},
  number     = {3},
  urldate    = {2014-07-01},
  journal    = {The Journal of General Physiology},
  author     = {Hines, Keegan E. and Middendorf, Thomas R. and Aldrich, Richard W.},
  month      = jan,
  year       = {2014},
  pmid       = {24516188},
  pages      = {401--416},
  file       = {Hines et al. - 2014 - Determination of parameter identifiability in
                nonl.pdf:/Users/apodusenko/Zotero/storage/ZJWU2SZI/Hines et al. - 2014
                - Determination of parameter identifiability in
                nonl.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/TVITS3BP/401.html:text/html;Snapshot:/Users/apodusenko/Zotero/storage/NW7KXY44/401.html:text/html
                }
}

@article{heylighen_cybernetic_2014,
  title      = {Cybernetic {Principles} of {Aging} and {Rejuvenation}: the
                buffering-challenging strategy for life extension},
  shorttitle = {Cybernetic {Principles} of {Aging} and {Rejuvenation}},
  url        = {http://arxiv.org/abs/1403.8135},
  abstract   = {Aging is analyzed as the spontaneous loss of adaptivity and
                increase in fragility that characterizes dynamic systems.
                Cybernetics defines the general regulatory mechanisms that a system
                can use to prevent or repair the damage produced by disturbances.
                According to the law of requisite variety, disturbances can be held
                in check by maximizing buffering capacity, range of compensatory
                actions, and knowledge about which action to apply to which
                disturbance. This suggests a general strategy for rejuvenating the
                organism by increasing its capabilities of adaptation. Buffering
                can be optimized by providing sufficient rest together with plenty
                of nutrients: amino acids, antioxidants, methyl donors, vitamins,
                minerals, etc. Knowledge and the range of action can be extended by
                subjecting the organism to an as large as possible variety of
                challenges. These challenges are ideally brief so as not to deplete
                resources and produce irreversible damage. However, they should be
                sufficiently intense and unpredictable to induce an overshoot in
                the mobilization of resources for damage repair, and to stimulate
                the organism to build stronger capabilities for tackling future
                challenges. This allows them to override the trade-offs and
                limitations that evolution has built into the organism's repair
                processes in order to conserve potentially scarce resources. Such
                acute, "hormetic" stressors strengthen the organism in part via the
                "order from noise" mechanism that destroys dysfunctional structures
                by subjecting them to strong, random variations. They include heat
                and cold, physical exertion, exposure, stretching, vibration,
                fasting, food toxins, micro-organisms, environmental enrichment and
                psychological challenges.},
  urldate    = {2014-11-21},
  journal    = {arXiv:1403.8135 [nlin, q-bio]},
  author     = {Heylighen, Francis},
  month      = mar,
  year       = {2014},
  note       = {arXiv: 1403.8135},
  keywords   = {Nonlinear Sciences - Adaptation and Self-Organizing Systems,
                Quantitative Biology - Other Quantitative Biology},
  file       = {arXiv.org
                Snapshot:/Users/apodusenko/Zotero/storage/TDHG68E8/1403.html:text/html;Heylighen
                - 2014 - Cybernetic Principles of Aging and Rejuvenation
                t.pdf:/Users/apodusenko/Zotero/storage/KPMD92A8/Heylighen - 2014 -
                Cybernetic Principles of Aging and Rejuvenation t.pdf:application/pdf}
}

@article{hernandez-lobato_predictive_2014,
  title    = {Predictive {Entropy} {Search} for {Efficient} {Global} {Optimization}
              of {Black}-box {Functions}},
  url      = {http://arxiv.org/abs/1406.2541},
  abstract = {We propose a novel information-theoretic approach for Bayesian
              optimization called Predictive Entropy Search (PES). At each
              iteration, PES selects the next evaluation point that maximizes the
              expected information gained with respect to the global maximum. PES
              codifies this intractable acquisition function in terms of the
              expected reduction in the differential entropy of the predictive
              distribution. This reformulation allows PES to obtain
              approximations that are both more accurate and efficient than other
              alternatives such as Entropy Search (ES). Furthermore, PES can
              easily perform a fully Bayesian treatment of the model
              hyperparameters while ES cannot. We evaluate PES in both synthetic
              and real-world applications, including optimization problems in
              machine learning, finance, biotechnology, and robotics. We show
              that the increased accuracy of PES leads to significant gains in
              optimization performance.},
  urldate  = {2014-10-16},
  journal  = {arXiv:1406.2541 [cs, stat]},
  author   = {Hernández-Lobato, José Miguel and Hoffman, Matthew W. and Ghahramani
              , Zoubin},
  month    = jun,
  year     = {2014},
  note     = {arXiv: 1406.2541},
  keywords = {Computer Science - Learning, Statistics - Machine Learning},
  file     = {arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/FKETJ7IC/1406.html:text/html;Hernández-Lobato
              et al. - 2014 - Predictive Entropy Search for Efficient Global
              Opt.pdf:/Users/apodusenko/Zotero/storage/A2MYPDPQ/Hernández-Lobato et
              al. - 2014 - Predictive Entropy Search for Efficient Global
              Opt.pdf:application/pdf}
}

@article{hennig_entropy_2011,
  title    = {Entropy {Search} for {Information}-{Efficient} {Global} {Optimization
              }},
  url      = {http://arxiv.org/abs/1112.1217},
  abstract = {Contemporary global optimization algorithms are based on local
              measures of utility, rather than a probability measure over
              location and value of the optimum. They thus attempt to collect low
              function values, not to learn about the optimum. The reason for the
              absence of probabilistic global optimizers is that the
              corresponding inference problem is intractable in several ways.
              This paper develops desiderata for probabilistic optimization
              algorithms, then presents a concrete algorithm which addresses each
              of the computational intractabilities with a sequence of
              approximations and explicitly adresses the decision problem of
              maximizing information gain from each evaluation.},
  urldate  = {2014-04-11},
  journal  = {arXiv:1112.1217 [cs, stat]},
  author   = {Hennig, Philipp and Schuler, Christian J.},
  month    = dec,
  year     = {2011},
  keywords = {Statistics - Machine Learning, Computer Science - Artificial
              Intelligence},
  file     = {arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/8XQD4PYD/1112.html:text/html;Hennig
              and Schuler - 2011 - Entropy Search for Information-Efficient Global
              Op.pdf:/Users/apodusenko/Zotero/storage/97R3M53Z/Hennig and Schuler -
              2011 - Entropy Search for Information-Efficient Global
              Op.pdf:application/pdf}
}

@article{hassabis_neuroscience-inspired_2017,
  title    = {Neuroscience-{Inspired} {Artificial} {Intelligence}},
  volume   = {95},
  issn     = {0896-6273},
  url      = {http://www.cell.com/neuron/abstract/S0896-6273(17)30509-3},
  doi      = {10.1016/j.neuron.2017.06.011},
  language = {English},
  number   = {2},
  journal  = {Neuron},
  author   = {Hassabis, Demis and Kumaran, Dharshan and Summerfield, Christopher
              and Botvinick, Matthew},
  month    = jul,
  year     = {2017},
  pmid     = {28728020},
  keywords = {Artificial intelligence, Brain, cognition, Learning, neural
              network},
  pages    = {245--258},
  file     = {Hassabis et al. - 2017 - Neuroscience-Inspired Artificial
              Intelligence.pdf:/Users/apodusenko/Zotero/storage/2PI2AR6M/Hassabis et
              al. - 2017 - Neuroscience-Inspired Artificial
              Intelligence.pdf:application/pdf;Hassabis et al. - 2017 -
              Neuroscience-Inspired Artificial
              Intelligence.pdf:/Users/apodusenko/Zotero/storage/B3G3U36R/Hassabis et
              al. - 2017 - Neuroscience-Inspired Artificial
              Intelligence.pdf:application/pdf}
}

@article{guo_lmmse_2008,
  title    = {{LMMSE} turbo equalization based on factor graphs},
  volume   = {26},
  issn     = {0733-8716},
  doi      = {10.1109/JSAC.2008.080208},
  abstract = {In this paper, a vector-form factor graph representation is
              derived for intersymbol interference (ISI) channels. The resultant
              graphs have a tree-structure that avoids the short cycle problem in
              existing graph approaches. Based on a joint Gaussian approximation,
              we establish a connection between the LLR (log-likelihood ratio)
              estimator for a linear system driven by binary inputs and the LMMSE
              (linear minimum mean-square error) estimator for a linear system
              driven by Gaussian inputs. This connection facilitates the
              application of the recently proposed Gaussian message passing
              technique to the cycle-free graphs for ISI channels. We also show
              the equivalence between the proposed approach and the Wang-Poor
              approach based on the LMMSE principle. An attractive advantage of
              the proposed approach is its intrinsic parallel structure.
              Simulation results are provided to demonstrate this property.},
  number   = {2},
  journal  = {IEEE Journal on Selected Areas in Communications},
  author   = {Guo, Q. and Ping, L.},
  month    = feb,
  year     = {2008},
  keywords = {Message passing, graph theory, Parity check codes, equalisers,
              Equalizers, Gaussian approximation, intersymbol interference
              channel, Iterative algorithms, Iterative decoding, joint Gaussian
              approximation, linear minimum mean-square error, Linear systems,
              log-likelihood ratio estimator, mean square error methods,
              Performance loss, Tree graphs, tree-structure, turbo codes, turbo
              equalization, vector-form factor graph representation, Wang-Poor
              approach},
  pages    = {311--319},
  file     = {IEEE Xplore Abstract
              Record:/Users/apodusenko/Zotero/storage/67W5RG6R/4444762.html:text/html;IEEE
              Xplore Abstract
              Record:/Users/apodusenko/Zotero/storage/GG2KUNHW/4444762.html:text/html
              }
}

@inproceedings{ole-christoffer_granmo_bayesian_2008,
  title     = {A {Bayesian} {Learning} {Automaton} for {Solving} {Two}-{Armed} {
               Bernoulli} {Bandit} {Problems}},
  doi       = {10.1109/ICMLA.2008.67},
  abstract  = {The two-armed Bernoulli bandit (TABB) problem is a classical
               optimization problem where an agent sequentially pulls one of two
               arms attached to a gambling machine, with each pull resulting
               either in a reward or a penalty. The reward probabilities of each
               arm are unknown, and thus one must balance between exploiting
               existing knowledge about the arms, and obtaining new information.
               In the last decades, several computationally efficient algorithms
               for tackling this problem have emerged, with learning automata (LA)
               being known for their Â¿-optimality, and confidence interval based
               for logarithmically growing regret. Applications include treatment
               selection in clinical trials, route selection in adaptive routing,
               and plan exploration in games like Go. The TABB has also been
               extensively studied from a Bayesian perspective, however, in
               general, such analysis leads to computationally inefficient
               solution policies. This paper introduces the Bayesian learning
               automaton (BLA). The BLA is inherently Bayesian in nature, yet
               relies simply on counting rewards/penalties and on random sampling
               from a pair of twin beta distributions. Furthermore, we report that
               BLA is self-correcting and converges to only pulling the optimal
               arm with probability 1. Extensive experiments demonstrate that, in
               contrast to most LA, BLA does not rely on external learning
               speed/accuracy control. It also outperforms recently proposed
               confidence interval based algorithms. We thus believe that BLA
               opens up for improved performance in a number of applications,and
               that it forms the basis for a new avenue of research.},
  publisher = {IEEE},
  author    = {Ole-Christoffer Granmo},
  month     = dec,
  year      = {2008},
  keywords  = {machine learning, belief networks, Application software, Arm,
               Bayesian learning automaton, Bayesian methods, Clinical trials,
               learning automata, optimisation, optimization problem, Resource
               management, Routing, Sampling methods, twin beta distributions,
               two-armed Bernoulli bandit problems},
  pages     = {23--30},
  file      = {IEEE Xplore Abstract
               Record:/Users/apodusenko/Zotero/storage/49L9SERT/login.html:text/html;Ole-Christoffer
               Granmo - 2008 - A Bayesian Learning Automaton for Solving
               Two-Arme.pdf:/Users/apodusenko/Zotero/storage/AXSVXDKT/Ole-Christoffer
               Granmo - 2008 - A Bayesian Learning Automaton for Solving
               Two-Arme.pdf:application/pdf}
}

@article{genewein_bounded_2015,
  title      = {Bounded {Rationality}, {Abstraction}, and {Hierarchical} {Decision}-{
                Making}: {An} {Information}-{Theoretic} {Optimality} {Principle}},
  shorttitle = {Bounded {Rationality}, {Abstraction}, and {Hierarchical} {
                Decision}-{Making}},
  url        = {http://journal.frontiersin.org/article/10.3389/frobt.2015.00027/full},
  doi        = {10.3389/frobt.2015.00027},
  abstract   = {Abstraction and hierarchical information processing are hallmarks
                of human and animal intelligence underlying the unrivaled
                flexibility of behavior in biological systems. Achieving such
                flexibility in artificial systems is challenging, even with more
                and more computational power. Here, we investigate the hypothesis
                that abstraction and hierarchical information processing might in
                fact be the consequence of limitations in information-processing
                power. In particular, we study an information-theoretic framework
                of bounded rational decision-making that trades off utility
                maximization against information-processing costs. We apply the
                basic principle of this framework to perception-action systems with
                multiple information-processing nodes and derive bounded-optimal
                solutions. We show how the formation of abstractions and
                decision-making hierarchies depends on information-processing
                costs. We illustrate the theoretical ideas with example simulations
                and conclude by formalizing a mathematically unifying optimization
                principle that could potentially be extended to more complex
                systems.},
  urldate    = {2016-09-12},
  journal    = {Computational Intelligence},
  author     = {Genewein, Tim and Leibfried, Felix and Grau-Moya, Jordi and Braun,
                Daniel Alexander},
  year       = {2015},
  keywords   = {information theory, bounded rationality, computational rationality
                , decision-making, hierarchical architecture, lossy compression,
                perception-action system, rate-distortion},
  pages      = {27},
  file       = {Genewein et al. - 2015 - Bounded Rationality, Abstraction, and
                Hierarchical.pdf:/Users/apodusenko/Zotero/storage/AEIQJTFU/Genewein et
                al. - 2015 - Bounded Rationality, Abstraction, and
                Hierarchical.pdf:application/pdf;Genewein et al. - 2015 - Bounded
                Rationality, Abstraction, and
                Hierarchical.pdf:/Users/apodusenko/Zotero/storage/RFLL28E8/Genewein et
                al. - 2015 - Bounded Rationality, Abstraction, and
                Hierarchical.pdf:application/pdf}
}

@article{friston_active_2012,
  title      = {Active inference and agency: optimal control without cost functions},
  volume     = {106},
  issn       = {0340-1200, 1432-0770},
  shorttitle = {Active inference and agency},
  url        = {http://link.springer.com/article/10.1007/s00422-012-0512-8},
  doi        = {10.1007/s00422-012-0512-8},
  language   = {en},
  number     = {8-9},
  urldate    = {2015-10-08},
  journal    = {Biological Cybernetics},
  author     = {Friston, Karl and Samothrakis, Spyridon and Montague, Read},
  month      = aug,
  year       = {2012},
  keywords   = {Action, Algorithms, agency, Bayes Theorem, Bayesian,
                Bioinformatics, Computer Appl. in Life Sciences, Decision Making,
                Free energy, Inference, Markov Chains, Models, Neurobiology,
                Neurological, Neurosciences, optimal control, Partially observable
                Markov decision processes, Statistical Physics, Dynamical Systems
                and Complexity},
  pages      = {523--541},
  file       = {Friston et al. - 2012 - Active inference and agency optimal control
                witho.pdf:/Users/apodusenko/Zotero/storage/P2V5NBGZ/Friston et al. -
                2012 - Active inference and agency optimal control
                witho.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/CE4MN7PU/10.html:text/html
                }
}

@article{friston_free_2006,
  title    = {A free energy principle for the brain},
  volume   = {100},
  issn     = {0928-4257},
  doi      = {10.1016/j.jphysparis.2006.10.001},
  abstract = {By formulating Helmholtz's ideas about perception, in terms of
              modern-day theories, one arrives at a model of perceptual inference
              and learning that can explain a remarkable range of neurobiological
              facts: using constructs from statistical physics, the problems of
              inferring the causes of sensory input and learning the causal
              structure of their generation can be resolved using exactly the
              same principles. Furthermore, inference and learning can proceed in
              a biologically plausible fashion. The ensuing scheme rests on
              Empirical Bayes and hierarchical models of how sensory input is
              caused. The use of hierarchical models enables the brain to
              construct prior expectations in a dynamic and context-sensitive
              fashion. This scheme provides a principled way to understand many
              aspects of cortical organisation and responses. In this paper, we
              show these perceptual processes are just one aspect of emergent
              behaviours of systems that conform to a free energy principle. The
              free energy considered here measures the difference between the
              probability distribution of environmental quantities that act on
              the system and an arbitrary distribution encoded by its
              configuration. The system can minimise free energy by changing its
              configuration to affect the way it samples the environment or
              change the distribution it encodes. These changes correspond to
              action and perception respectively and lead to an adaptive exchange
              with the environment that is characteristic of biological systems.
              This treatment assumes that the system's state and structure encode
              an implicit and probabilistic model of the environment. We will
              look at the models entailed by the brain and how minimisation of
              its free energy can explain its dynamics and structure.},
  number   = {1-3},
  journal  = {Journal of Physiology, Paris},
  author   = {Friston, Karl and Kilner, James and Harrison, Lee},
  month    = sep,
  year     = {2006},
  pmid     = {17097864},
  keywords = {Humans, Probability, Brain, Learning, Bayes Theorem, Models,
              Neurological, Afferent Pathways, Animals, Attention, Brain Mapping,
              Computer Simulation, Entropy, Visual Perception, Models,
              Neurological},
  pages    = {70--87},
  file     = {Friston et al. - 2006 - A free energy principle for the
              brain.pdf:/Users/apodusenko/Zotero/storage/2KPZJVJA/Friston et al. -
              2006 - A free energy principle for the brain.pdf:application/pdf}
}

@article{friston_statistical_1994,
  title      = {Statistical parametric maps in functional imaging: a general linear
                approach},
  volume     = {2},
  shorttitle = {Statistical parametric maps in functional imaging},
  url        = {http://onlinelibrary.wiley.com/doi/10.1002/hbm.460020402/full, see
                http://www.fil.ion.ucl.ac.uk/spm/software for SPM software},
  number     = {4},
  urldate    = {2017-07-28},
  journal    = {Human brain mapping},
  author     = {Friston, Karl J. and Holmes, Andrew P. and Worsley, Keith J. and
                Poline, J.-P. and Frith, Chris D. and Frackowiak, Richard SJ},
  year       = {1994},
  pages      = {189--210},
  file       = {Friston e.a. - 1994 - Statistical parametric maps in functional
                imaging.pdf:/Users/apodusenko/Zotero/storage/3GVDDJHZ/Friston e.a. -
                1994 - Statistical parametric maps in functional
                imaging.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/NKKNNY9P/full.html:text/html
                }
}

@article{friston_free_2012,
  title    = {A {Free} {Energy} {Principle} for {Biological} {Systems}},
  volume   = {14},
  issn     = {1099-4300},
  url      = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3510653/},
  doi      = {10.3390/e14112100},
  abstract = {This paper describes a free energy principle that tries to explain
              the ability of biological systems to resist a natural tendency to
              disorder. It appeals to circular causality of the sort found in
              synergetic formulations of self-organization (e.g., the slaving
              principle) and models of coupled dynamical systems, using nonlinear
              Fokker Planck equations. Here, circular causality is induced by
              separating the states of a random dynamical system into external
              and internal states, where external states are subject to random
              fluctuations and internal states are not. This reduces the problem
              to finding some (deterministic) dynamics of the internal states
              that ensure the system visits a limited number of external states;
              in other words, the measure of its (random) attracting set, or the
              Shannon entropy of the external states is small. We motivate a
              solution using a principle of least action based on variational
              free energy (from statistical physics) and establish the conditions
              under which it is formally equivalent to the information bottleneck
              method. This approach has proved useful in understanding the
              functional architecture of the brain. The generality of variational
              free energy minimisation and corresponding information theoretic
              formulations may speak to interesting applications beyond the
              neurosciences; e.g., in molecular or evolutionary biology.},
  number   = {11},
  urldate  = {2017-07-28},
  journal  = {Entropy (Basel, Switzerland)},
  author   = {Friston, Karl J.},
  month    = nov,
  year     = {2012},
  pmid     = {23204829},
  pmcid    = {PMC3510653},
  pages    = {2100--2121},
  file     = {Friston - 2012 - A Free Energy Principle for Biological
              Systems.pdf:/Users/apodusenko/Zotero/storage/2EYS9RG6/Friston - 2012 -
              A Free Energy Principle for Biological Systems.pdf:application/pdf}
}

@article{fitzgerald_model_2014,
  title    = {Model averaging, optimal inference, and habit formation},
  volume   = {8},
  url      = {
              http://journal.frontiersin.org/Journal/10.3389/fnhum.2014.00457/abstract
              },
  doi      = {10.3389/fnhum.2014.00457},
  abstract = {Postulating that the brain performs approximate Bayesian inference
              generates principled and empirically testable models of neuronal
              function—the subject of much current interest in neuroscience and
              related disciplines. Current formulations address inference and
              learning under some assumed and particular model. In reality,
              organisms are often faced with an additional challenge—that of
              determining which model or models of their environment are the best
              for guiding behavior. Bayesian model averaging—which says that an
              agent should weight the predictions of different models according
              to their evidence—provides a principled way to solve this problem.
              Importantly, because model evidence is determined by both the
              accuracy and complexity of the model, optimal inference requires
              that these be traded off against one another. This means an agent's
              behavior should show an equivalent balance. We hypothesize that
              Bayesian model averaging plays an important role in cognition,
              given that it is both optimal and realizable within a plausible
              neuronal architecture. We outline model averaging and how it might
              be implemented, and then explore a number of implications for brain
              and behavior. In particular, we propose that model averaging can
              explain a number of apparently suboptimal phenomena within the
              framework of approximate (bounded) Bayesian inference, focusing
              particularly upon the relationship between goal-directed and
              habitual behavior.},
  urldate  = {2014-08-25},
  journal  = {Frontiers in Human Neuroscience},
  author   = {FitzGerald, Thomas H. B. and Dolan, Raymond J. and Friston, Karl},
  year     = {2014},
  keywords = {Bayesian inference, Active inference, habit, interference effect,
              predictive coding},
  pages    = {457},
  file     = {Data Sheet 1.DOCX:/Users/apodusenko/Zotero/storage/7GQ7564E/Data Sheet
              1.DOCX:application/vnd.openxmlformats-officedocument.wordprocessingml.document;Data
              Sheet 1.DOCX:/Users/apodusenko/Zotero/storage/2DE4B53N/Data Sheet
              1.DOCX:application/vnd.openxmlformats-officedocument.wordprocessingml.document;FitzGerald
              et al. - 2014 - Model averaging, optimal inference, and habit
              form.pdf:/Users/apodusenko/Zotero/storage/AWHJ69P2/FitzGerald et al. -
              2014 - Model averaging, optimal inference, and habit
              form.pdf:application/pdf;FitzGerald et al. - 2014 - Model averaging,
              optimal inference, and habit
              form.pdf:/Users/apodusenko/Zotero/storage/MDRTPS4I/FitzGerald et al. -
              2014 - Model averaging, optimal inference, and habit
              form.pdf:application/pdf}
}

@inproceedings{farmani_probabilistic_2014,
  address   = {Reims (Fr)},
  title     = {A {Probabilistic} {Approach} to {Hearing} {Loss} {Compensation}},
  booktitle = {{IEEE} {International} {Workshop} on {Machine} {Learning} for {
               Signal} {Processing} ({MLSP})},
  author    = {Farmani, Mojtaba and De Vries, Bert},
  month     = sep,
  year      = {2014},
  keywords  = {hearing aids, Bayesian inference, inference mechanisms, Abstracts,
               automated Bayesian inference, Bayes methods, DRC, hearing loss,
               hearing loss compensation algorithms, hearing loss models, hearing
               loss problem, HL compensation algorithms, HL model, Kalman filter,
               medical signal processing, probabilistic approach, probabilistic
               model},
  file      = {Farmani and de Vries - 2014 - A Probabilistic Approach to Hearing Loss
               Compensat.pdf:/Users/apodusenko/Zotero/storage/FTDZCPJ5/Farmani and de
               Vries - 2014 - A Probabilistic Approach to Hearing Loss
               Compensat.pdf:application/pdf;van de Laar and de Vries - 2016 - A
               Probabilistic Modeling Approach to Hearing Loss
               .pdf:/Users/apodusenko/Zotero/storage/UH6TGHF7/van de Laar and de Vries
               - 2016 - A Probabilistic Modeling Approach to Hearing Loss
               .pdf:application/pdf}
}

@article{englert_probabilistic_2013,
  title    = {Probabilistic model-based imitation learning},
  volume   = {21},
  issn     = {1059-7123, 1741-2633},
  url      = {http://adb.sagepub.com/content/21/5/388},
  doi      = {10.1177/1059712313491614},
  abstract = {Efficient skill acquisition is crucial for creating versatile
              robots. One intuitive way to teach a robot new tricks is to
              demonstrate a task and enable the robot to imitate the demonstrated
              behavior. This approach is known as imitation learning. Classical
              methods of imitation learning, such as inverse reinforcement
              learning or behavioral cloning, suffer substantially from the
              correspondence problem when the actions (i.e. motor commands,
              torques or forces) of the teacher are not observed or the body of
              the teacher differs substantially, e.g., in the actuation. To
              address these drawbacks we propose to learn a robot-specific
              controller that directly matches robot trajectories with observed
              ones. We present a novel and robust probabilistic model-based
              approach for solving a probabilistic trajectory matching problem
              via policy search. For this purpose, we propose to learn a
              probabilistic model of the system, which we exploit for mental
              rehearsal of the current controller by making predictions about
              future trajectories. These internal simulations allow for learning
              a controller without permanently interacting with the real system,
              which results in a reduced overall interaction time. Using
              long-term predictions from this learned model, we train
              robot-specific controllers that reproduce the expert’s distribution
              of demonstrations without the need to observe motor commands during
              the demonstration. The strength of our approach is that it
              addresses the correspondence problem in a principled way. Our
              method achieves a higher learning speed than both model-based
              imitation learning based on dynamics motor primitives and
              trial-and-error-based learning systems with hand-crafted cost
              functions. We successfully applied our approach to imitating human
              behavior using a tendon-driven compliant robotic arm. Moreover, we
              demonstrate the generalization ability of our approach in a
              multi-task learning setup.},
  language = {en},
  number   = {5},
  urldate  = {2014-08-28},
  journal  = {Adaptive Behavior},
  author   = {Englert, Peter and Paraschos, Alexandros and Deisenroth, Marc Peter
              and Peters, Jan},
  month    = oct,
  year     = {2013},
  pages    = {388--403},
  file     = {Englert et al. - 2013 - Probabilistic model-based imitation
              learning.pdf:/Users/apodusenko/Zotero/storage/G52AKSC2/Englert et al. -
              2013 - Probabilistic model-based imitation
              learning.pdf:application/pdf;Englert et al. - 2013 - Probabilistic
              model-based imitation
              learning.pdf:/Users/apodusenko/Zotero/storage/NDPXSS8N/Englert et al. -
              2013 - Probabilistic model-based imitation
              learning.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/7247STXV/388.html:text/html;Snapshot:/Users/apodusenko/Zotero/storage/2CTL4VUF/388.html:text/html
              }
}

@article{elfwing_scaled_2013,
  title    = {Scaled free-energy based reinforcement learning for robust and
              efficient learning in high-dimensional state spaces},
  volume   = {7},
  url      = {
              http://journal.frontiersin.org/Journal/10.3389/fnbot.2013.00003/abstract
              },
  doi      = {10.3389/fnbot.2013.00003},
  abstract = {Free-energy based reinforcement learning (FERL) was proposed for
              learning in high-dimensional state- and action spaces, which cannot
              be handled by standard function approximation methods. In this
              study, we propose a scaled version of free-energy based
              reinforcement learning to achieve more robust and more efficient
              learning performance. The action-value function is approximated by
              the negative free-energy of a restricted Boltzmann machine, divided
              by a constant scaling factor that is related to the size of the
              Boltzmann machine (the square root of the number of state nodes in
              this study). Our first task is a digit floor gridworld task, where
              the states are represented by images of handwritten digits from the
              MNIST data set. The purpose of the task is to investigate the
              proposed method's ability, through the extraction of task-relevant
              features in the hidden layer, to cluster images of the same digit
              and to cluster images of different digits that corresponds to
              states with the same optimal action. We also test the method's
              robustness with respect to different exploration schedules, i.e.,
              different settings of the initial temperature and the temperature
              discount rate in softmax action selection. Our second task is a
              robot visual navigation task, where the robot can learn its
              position by the different colors of the lower part of four
              landmarks and it can infer the correct corner goal area by the
              color of the upper part of the landmarks. The state space consists
              of binarized camera images with, at most, nine different colors,
              which is equal to 6642 binary states. For both tasks, the learning
              performance is compared with standard FERL and with function
              approximation where the action-value function is approximated by a
              two-layered feedforward neural network.},
  urldate  = {2014-09-09},
  journal  = {Frontiers in Neurorobotics},
  author   = {Elfwing, Stefan and Uchibe, Eiji and Doya, Kenji},
  year     = {2013},
  keywords = {free-energy, function approximation, reinforcement learning,
              restricted Boltzmann machine, robot navigation},
  pages    = {3},
  file     = {Elfwing et al. - 2013 - Scaled free-energy based reinforcement
              learning fo.pdf:/Users/apodusenko/Zotero/storage/JS3WHJZC/Elfwing et
              al. - 2013 - Scaled free-energy based reinforcement learning
              fo.pdf:application/pdf}
}

@article{doersch_tutorial_2016,
  title    = {Tutorial on {Variational} {Autoencoders}},
  url      = {http://arxiv.org/abs/1606.05908},
  abstract = {In just three years, Variational Autoencoders (VAEs) have emerged
              as one of the most popular approaches to unsupervised learning of
              complicated distributions. VAEs are appealing because they are
              built on top of standard function approximators (neural networks),
              and can be trained with stochastic gradient descent. VAEs have
              already shown promise in generating many kinds of complicated data,
              including handwritten digits, faces, house numbers, CIFAR images,
              physical models of scenes, segmentation, and predicting the future
              from static images. This tutorial introduces the intuitions behind
              VAEs, explains the mathematics behind them, and describes some
              empirical behavior. No prior knowledge of variational Bayesian
              methods is assumed.},
  urldate  = {2016-07-22},
  journal  = {arXiv:1606.05908 [cs, stat]},
  author   = {Doersch, Carl},
  month    = jun,
  year     = {2016},
  note     = {arXiv: 1606.05908},
  keywords = {Computer Science - Learning, Statistics - Machine Learning},
  file     = {arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/8PTCRIKH/1606.html:text/html;Doersch
              - 2016 - Tutorial on Variational
              Autoencoders.pdf:/Users/apodusenko/Zotero/storage/IPNFJ4KE/Doersch -
              2016 - Tutorial on Variational Autoencoders.pdf:application/pdf}
}

@inproceedings{deisenroth_pilco:_2011,
  title      = {{PILCO}: {A} {Model}-{Based} and {Data}-{Efficient} {Approach} to {
                Policy} {Search}},
  shorttitle = {{PILCO}},
  abstract   = {In this paper, we introduce pilco, a practical, data-efficient
                model-based policy search method. Pilco reduces model bias, one of
                the key problems of model-based reinforcement learning, in a
                principled way. By learning a probabilistic dynamics model and
                explicitly incorporating model uncertainty into long-term planning,
                pilco can cope with very little data and facilitates learning from
                scratch in only a few trials. Policy evaluation is performed in
                closed form using state-ofthe-art approximate inference.
                Furthermore, policy gradients are computed analytically for policy
                improvement. We report unprecedented learning efficiency on
                challenging and high-dimensional control tasks. 1. Introduction and
                Related},
  booktitle  = {In {Proceedings} of the {International} {Conference} on {Machine}
                {Learning}},
  author     = {Deisenroth, Marc Peter and Rasmussen, Carl Edward},
  year       = {2011},
  file       = {Citeseer -
                Snapshot:/Users/apodusenko/Zotero/storage/UP93NA6N/summary.html:text/html;Citeseer
                -
                Snapshot:/Users/apodusenko/Zotero/storage/3VJHMPUJ/summary.html:text/html;Deisenroth
                and Rasmussen - 2011 - PILCO A Model-Based and Data-Efficient Approach
                t.pdf:/Users/apodusenko/Zotero/storage/BZ56685T/Deisenroth and
                Rasmussen - 2011 - PILCO A Model-Based and Data-Efficient Approach
                t.pdf:application/pdf}
}

@article{deisenroth_gaussian_2014,
  title   = {Gaussian {Processes} for {Data}-{Efficient} {Learning} in {Robotics}
             and {Control}},
  issn    = {0162-8828, 2160-9292},
  url     = {
             http://www.computer.org/portal/web/csdl2/home/-/csdl/trans/tp/preprint/06654139-abs.html
             },
  doi     = {10.1109/TPAMI.2013.218},
  urldate = {2014-08-28},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  author  = {Deisenroth, Marc Peter and Fox, Dieter and Rasmussen, Carl Edward},
  year    = {2014},
  pages   = {1--1},
  file    = {Deisenroth et al. - 2014 - Gaussian Processes for Data-Efficient
             Learning in .pdf:/Users/apodusenko/Zotero/storage/PDLURHMM/Deisenroth
             et al. - 2014 - Gaussian Processes for Data-Efficient Learning in
             .pdf:application/pdf;Gaussian Processes for Data-Efficient Learning in
             Robotics and
             Control:/Users/apodusenko/Zotero/storage/5V77PVJG/06654139-abs.html:text/html
             }
}

@article{daunizeau_optimizing_2011,
  title    = {Optimizing {Experimental} {Design} for {Comparing} {Models} of {Brain
              } {Function}},
  volume   = {7},
  url      = {http://dx.doi.org/10.1371/journal.pcbi.1002280},
  doi      = {10.1371/journal.pcbi.1002280},
  abstract = {Author Summary During the past two decades, brain mapping research
              has undergone a paradigm switch. In addition to localizing brain
              regions that encode specific sensory, motor or cognitive processes,
              neuroimaging data is nowadays further exploited to ask questions
              about how information is transmitted through brain networks. The
              ambition here is to ask questions such as: “what is the nature of
              the information that region A passes on to region B”. This can be
              experimentally addressed by, e.g., showing that the influence that
              A exerts onto B depends upon specific sensory, motor or cognitive
              manipulations. This means one has to compare (in a statistical
              sense) candidate network models of the brain (with different
              modulations of effective connectivity, say), based on experimental
              data. The question we address here is how one should design the
              experiment in order to best discriminate such candidate models. We
              approach the problem from a statistical decision theoretical
              perspective, whereby the optimal design is the one that minimizes
              the model selection error rate. We demonstrate the approach using
              simulated and empirical data and show how it can be applied to any
              experimental question that can be framed as a model comparison
              problem.},
  number   = {11},
  urldate  = {2014-12-01},
  journal  = {PLoS Comput Biol},
  author   = {Daunizeau, Jean and Preuschoff, Kerstin and Friston, Karl and
              Stephan, Klaas},
  month    = nov,
  year     = {2011},
  pages    = {e1002280},
  file     = {Daunizeau et al. - 2011 - Optimizing Experimental Design for Comparing
              Model.pdf:/Users/apodusenko/Zotero/storage/WRCQX4ZA/Daunizeau et al. -
              2011 - Optimizing Experimental Design for Comparing
              Model.pdf:application/pdf;Daunizeau et al. - 2011 - Optimizing
              Experimental Design for Comparing
              Model.pdf:/Users/apodusenko/Zotero/storage/Z75PJVVG/Daunizeau et al. -
              2011 - Optimizing Experimental Design for Comparing
              Model.pdf:application/pdf;PLoS
              Snapshot:/Users/apodusenko/Zotero/storage/KBIU8MT2/infodoi10.1371journal.pcbi.html:text/html;PLoS
              Snapshot:/Users/apodusenko/Zotero/storage/7PNT78X5/infodoi10.1371journal.pcbi.html:text/html
              }
}

@article{daunizeau_vba:_2014,
  title      = {{VBA}: {A} {Probabilistic} {Treatment} of {Nonlinear} {Models} for {
                Neurobiological} and {Behavioural} {Data}},
  volume     = {10},
  issn       = {1553-734X},
  shorttitle = {{VBA}},
  url        = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3900378/},
  doi        = {10.1371/journal.pcbi.1003441},
  abstract   = {This work is in line with an on-going effort tending toward a
                computational (quantitative and refutable) understanding of human
                neuro-cognitive processes. Many sophisticated models for
                behavioural and neurobiological data have flourished during the
                past decade. Most of these models are partly unspecified (i.e. they
                have unknown parameters) and nonlinear. This makes them difficult
                to peer with a formal statistical data analysis framework. In turn,
                this compromises the reproducibility of model-based empirical
                studies. This work exposes a software toolbox that provides generic
                , efficient and robust probabilistic solutions to the three
                problems of model-based analysis of empirical data: (i) data
                simulation, (ii) parameter estimation/model selection, and (iii)
                experimental design optimization.},
  number     = {1},
  urldate    = {2014-07-30},
  journal    = {PLoS Computational Biology},
  author     = {Daunizeau, Jean and Adam, Vincent and Rigoux, Lionel},
  month      = jan,
  year       = {2014},
  pmid       = {24465198},
  pmcid      = {PMC3900378},
  file       = {Daunizeau et al. - 2014 - VBA A Probabilistic Treatment of Nonlinear
                Models.pdf:/Users/apodusenko/Zotero/storage/JWWTPWGE/Daunizeau et al. -
                2014 - VBA A Probabilistic Treatment of Nonlinear
                Models.pdf:application/pdf;Daunizeau et al. - 2014 - VBA A
                Probabilistic Treatment of Nonlinear
                Models.pdf:/Users/apodusenko/Zotero/storage/DWFABEVA/Daunizeau et al. -
                2014 - VBA A Probabilistic Treatment of Nonlinear
                Models.pdf:application/pdf}
}

@article{coelho_bayesian_2011,
  title    = {A {Bayesian} {Framework} for {Parameter} {Estimation} in {Dynamical}
              {Models}},
  volume   = {6},
  url      = {http://dx.doi.org/10.1371/journal.pone.0019616},
  doi      = {10.1371/journal.pone.0019616},
  abstract = {Mathematical models in biology are powerful tools for the study
              and exploration of complex dynamics. Nevertheless, bringing
              theoretical results to an agreement with experimental observations
              involves acknowledging a great deal of uncertainty intrinsic to our
              theoretical representation of a real system. Proper handling of
              such uncertainties is key to the successful usage of models to
              predict experimental or field observations. This problem has been
              addressed over the years by many tools for model calibration and
              parameter estimation. In this article we present a general
              framework for uncertainty analysis and parameter estimation that is
              designed to handle uncertainties associated with the modeling of
              dynamic biological systems while remaining agnostic as to the type
              of model used. We apply the framework to fit an SIR-like influenza
              transmission model to 7 years of incidence data in three European
              countries: Belgium, the Netherlands and Portugal.},
  number   = {5},
  urldate  = {2014-07-01},
  journal  = {PLoS ONE},
  author   = {Coelho, Flávio Codeço and Codeço, Cláudia Torres and Gomes, M.
              Gabriela M.},
  month    = may,
  year     = {2011},
  pages    = {e19616},
  file     = {Coelho et al. - 2011 - A Bayesian Framework for Parameter Estimation
              in D.pdf:/Users/apodusenko/Zotero/storage/755XWUKT/Coelho et al. - 2011
              - A Bayesian Framework for Parameter Estimation in
              D.pdf:application/pdf;Coelho et al. - 2011 - A Bayesian Framework for
              Parameter Estimation in
              D.pdf:/Users/apodusenko/Zotero/storage/JTSBE2YU/Coelho et al. - 2011 -
              A Bayesian Framework for Parameter Estimation in
              D.pdf:application/pdf;PLoS
              Snapshot:/Users/apodusenko/Zotero/storage/FC4W3IXD/infodoi10.1371journal.pone.html:text/html;PLoS
              Snapshot:/Users/apodusenko/Zotero/storage/X3PGAUAJ/infodoi10.1371journal.pone.html:text/html
              }
}

@article{chung_recurrent_2015,
  title    = {A {Recurrent} {Latent} {Variable} {Model} for {Sequential} {Data}},
  url      = {http://arxiv.org/abs/1506.02216},
  abstract = {In this paper, we explore the inclusion of latent random variables
              into the dynamic hidden state of a recurrent neural network (RNN)
              by combining elements of the variational autoencoder. We argue that
              through the use of high-level latent random variables, the
              variational RNN (VRNN)1 can model the kind of variability observed
              in highly structured sequential data such as natural speech. We
              empirically evaluate the proposed model against related sequential
              models on four speech datasets and one handwriting dataset. Our
              results show the important roles that latent random variables can
              play in the RNN dynamic hidden state.},
  urldate  = {2015-12-10},
  journal  = {arXiv:1506.02216 [cs]},
  author   = {Chung, Junyoung and Kastner, Kyle and Dinh, Laurent and Goel,
              Kratarth and Courville, Aaron and Bengio, Yoshua},
  month    = jun,
  year     = {2015},
  note     = {arXiv: 1506.02216},
  keywords = {Computer Science - Learning},
  file     = {arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/HTHA7PGX/1506.html:text/html;arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/I2LDV7FM/1506.html:text/html;Chung
              et al. - 2015 - A Recurrent Latent Variable Model for Sequential
              D.pdf:/Users/apodusenko/Zotero/storage/4WVSRSN4/Chung et al. - 2015 - A
              Recurrent Latent Variable Model for Sequential
              D.pdf:application/pdf;Chung et al. - 2015 - A Recurrent Latent Variable
              Model for Sequential
              D.pdf:/Users/apodusenko/Zotero/storage/6HFIVGC6/Chung et al. - 2015 - A
              Recurrent Latent Variable Model for Sequential D.pdf:application/pdf}
}

@article{chung_hierarchical_2016,
  title    = {Hierarchical {Multiscale} {Recurrent} {Neural} {Networks}},
  url      = {http://arxiv.org/abs/1609.01704},
  abstract = {Learning both hierarchical and temporal representation has been
              among the long-standing challenges of recurrent neural networks.
              Multiscale recurrent neural networks have been considered as a
              promising approach to resolve this issue, yet there has been a lack
              of empirical evidence showing that this type of models can actually
              capture the temporal dependencies by discovering the latent
              hierarchical structure of the sequence. In this paper, we propose a
              novel multiscale approach, called the hierarchical multiscale
              recurrent neural networks, which can capture the latent
              hierarchical structure in the sequence by encoding the temporal
              dependencies with different timescales using a novel update
              mechanism. We show some evidence that our proposed multiscale
              architecture can discover underlying hierarchical structure in the
              sequences without using explicit boundary information. We evaluate
              our proposed model on character-level language modelling and
              handwriting sequence modelling.},
  urldate  = {2016-09-12},
  journal  = {arXiv:1609.01704 [cs]},
  author   = {Chung, Junyoung and Ahn, Sungjin and Bengio, Yoshua},
  month    = sep,
  year     = {2016},
  note     = {arXiv: 1609.01704},
  keywords = {Computer Science - Learning},
  file     = {arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/5648RPBJ/1609.html:text/html;arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/YT5AGFVW/1609.html:text/html;Chung
              et al. - 2016 - Hierarchical Multiscale Recurrent Neural
              Networks.pdf:/Users/apodusenko/Zotero/storage/EXWUJNJ4/Chung et al. -
              2016 - Hierarchical Multiscale Recurrent Neural
              Networks.pdf:application/pdf;Chung et al. - 2016 - Hierarchical
              Multiscale Recurrent Neural
              Networks.pdf:/Users/apodusenko/Zotero/storage/ULXH4ETX/Chung et al. -
              2016 - Hierarchical Multiscale Recurrent Neural
              Networks.pdf:application/pdf}
}

@article{chen_large-scale_2016,
  title    = {Large-scale training to increase speech intelligibility for
              hearing-impaired listeners in novel noises},
  volume   = {139},
  issn     = {0001-4966},
  url      = {http://asa.scitation.org/doi/10.1121/1.4948445},
  doi      = {10.1121/1.4948445},
  abstract = {Supervised speech segregation has been recently shown to improve
              human speech intelligibility in noise, when trained and tested on
              similar noises. However, a major challenge involves the ability to
              generalize to entirely novel noises. Such generalization would
              enable hearing aid and cochlear implant users to improve speech
              intelligibility in unknown noisy environments. This challenge is
              addressed in the current study through large-scale training.
              Specifically, a deep neural network (DNN) was trained on 10 000
              noises to estimate the ideal ratio mask, and then employed to
              separate sentences from completely new noises (cafeteria and
              babble) at several signal-to-noise ratios (SNRs). Although the DNN
              was trained at the fixed SNR of −−{\textless}math display="inline"
              overflow="scroll" altimg="eq-00001.gif"{\textgreater}{\textless}mo{
              \textgreater}−{\textless}/mo{\textgreater}{\textless}/math{
              \textgreater} 2 dB, testing using hearing-impaired listeners
              demonstrated that speech intelligibility increased substantially
              following speech segregation using the novel noises and unmatched
              SNR conditions of 0 dB and 5 dB. Sentence intelligibility benefit
              was also observed for normal-hearing listeners in most noisy
              conditions. The results indicate that DNN-based supervised speech
              segregation with large-scale training is a very promising approach
              for generalization to new acoustic environments.},
  number   = {5},
  urldate  = {2017-03-07},
  journal  = {The Journal of the Acoustical Society of America},
  author   = {Chen, Jitong and Yuxuan, Wang and Yoho, Sarah and Wang, DeLiang and
              Healy, Eric},
  month    = may,
  year     = {2016},
  pages    = {2604--2612},
  file     = {Chen et al. - 2016 - Large-scale training to increase speech
              intelligib.pdf:/Users/apodusenko/Zotero/storage/4LMU6KVZ/Chen et al. -
              2016 - Large-scale training to increase speech
              intelligib.pdf:application/pdf;Chen et al. - 2016 - Large-scale
              training to increase speech
              intelligib.pdf:/Users/apodusenko/Zotero/storage/X9VZW8RA/Chen et al. -
              2016 - Large-scale training to increase speech
              intelligib.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/RYD9G96D/1.html:text/html
              }
}

@article{campbell_universal_2016,
  title    = {Universal {Darwinism} {As} a {Process} of {Bayesian} {Inference}},
  volume   = {10},
  issn     = {1662-5137},
  url      = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4894882/},
  doi      = {10.3389/fnsys.2016.00049},
  abstract = {Many of the mathematical frameworks describing natural selection
              are equivalent to Bayes' Theorem, also known as Bayesian updating.
              By definition, a process of Bayesian Inference is one which
              involves a Bayesian update, so we may conclude that these
              frameworks describe natural selection as a process of Bayesian
              inference. Thus, natural selection serves as a counter example to a
              widely-held interpretation that restricts Bayesian Inference to
              human mental processes (including the endeavors of statisticians).
              As Bayesian inference can always be cast in terms of (variational)
              free energy minimization, natural selection can be viewed as
              comprising two components: a generative model of an “experiment” in
              the external world environment, and the results of that
              “experiment” or the “surprise” entailed by predicted and actual
              outcomes of the “experiment.” Minimization of free energy implies
              that the implicit measure of “surprise” experienced serves to
              update the generative model in a Bayesian manner. This description
              closely accords with the mechanisms of generalized Darwinian
              process proposed both by Dawkins, in terms of replicators and
              vehicles, and Campbell, in terms of inferential systems. Bayesian
              inference is an algorithm for the accumulation of evidence-based
              knowledge. This algorithm is now seen to operate over a wide range
              of evolutionary processes, including natural selection, the
              evolution of mental models and cultural evolutionary processes,
              notably including science itself. The variational principle of free
              energy minimization may thus serve as a unifying mathematical
              framework for universal Darwinism, the study of evolutionary
              processes operating throughout nature.},
  urldate  = {2016-12-25},
  journal  = {Frontiers in Systems Neuroscience},
  author   = {Campbell, John O.},
  month    = jun,
  year     = {2016},
  pmid     = {27375438},
  pmcid    = {PMC4894882},
  keywords = {Bayesian inference, free energy, information, natural selection,
              Universal Darwinism},
  file     = {Campbell - 2016 - Universal Darwinism As a Process of Bayesian
              Infer.pdf:/Users/apodusenko/Zotero/storage/EUTE5ZXM/Campbell - 2016 -
              Universal Darwinism As a Process of Bayesian
              Infer.pdf:application/pdf;Campbell - 2016 - Universal Darwinism As a
              Process of Bayesian
              Infer.pdf:/Users/apodusenko/Zotero/storage/VDGIV2I2/Campbell - 2016 -
              Universal Darwinism As a Process of Bayesian Infer.pdf:application/pdf}
}

@article{bastos_canonical_2012,
  title    = {Canonical {Microcircuits} for {Predictive} {Coding}},
  volume   = {76},
  issn     = {0896-6273},
  url      = {http://www.cell.com/neuron/abstract/S0896-6273(12)00959-2},
  doi      = {10.1016/j.neuron.2012.10.038},
  abstract = {This Perspective considers the influential notion of a canonical
              (cortical) microcircuit in light of recent theories about neuronal
              processing. Specifically, we conciliate quantitative studies of
              microcircuitry and the functional logic of neuronal computations.
              We revisit the established idea that message passing among
              hierarchical cortical areas implements a form of Bayesian
              inference—paying careful attention to the implications for
              intrinsic connections among neuronal populations. By deriving
              canonical forms for these computations, one can associate specific
              neuronal populations with specific computational roles. This
              analysis discloses a remarkable correspondence between the
              microcircuitry of the cortical column and the connectivity implied
              by predictive coding. Furthermore, it provides some intuitive
              insights into the functional asymmetries between feedforward and
              feedback connections and the characteristic frequencies over which
              they operate.},
  number   = {4},
  journal  = {Neuron},
  author   = {Bastos, Andre M. and Usrey, W. Martin and Adams, Rick A. and Mangun,
              George R. and Fries, Pascal and Friston, Karl J.},
  month    = nov,
  year     = {2012},
  pmid     = {23177956},
  pages    = {695--711},
  file     = {Bastos e.a. - 2012 - Canonical Microcircuits for Predictive
              Coding.pdf:/Users/apodusenko/Zotero/storage/K864D68S/Bastos e.a. - 2012
              - Canonical Microcircuits for Predictive
              Coding.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/762KL9RN/S0896-6273(12)00959-2.html:text/html
              }
}

@inproceedings{al-bashabsheh_normal_2011,
  title      = {Normal factor graphs: {A} diagrammatic approach to linear algebra},
  isbn       = {978-1-4577-0596-0},
  shorttitle = {Normal factor graphs},
  url        = {http://ieeexplore.ieee.org/document/6033944/},
  doi        = {10.1109/ISIT.2011.6033944},
  urldate    = {2017-07-28},
  publisher  = {IEEE},
  author     = {Al-Bashabsheh, Ali and Mao, Yongyi and Vontobel, Pascal O.},
  month      = jul,
  year       = {2011},
  pages      = {2178--2182}
}

@article{feldman_attention_2010,
  title    = {Attention, {Uncertainty}, and {Free}-{Energy}},
  volume   = {4},
  issn     = {1662-5161},
  url      = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3001758/},
  doi      = {10.3389/fnhum.2010.00215},
  abstract = {We suggested recently that attention can be understood as
              inferring the level of uncertainty or precision during hierarchical
              perception. In this paper, we try to substantiate this claim using
              neuronal simulations of directed spatial attention and biased
              competition. These simulations assume that neuronal activity
              encodes a probabilistic representation of the world that optimizes
              free-energy in a Bayesian fashion. Because free-energy bounds
              surprise or the (negative) log-evidence for internal models of the
              world, this optimization can be regarded as evidence accumulation
              or (generalized) predictive coding. Crucially, both predictions
              about the state of the world generating sensory data and the
              precision of those data have to be optimized. Here, we show that if
              the precision depends on the states, one can explain many aspects
              of attention. We illustrate this in the context of the Posner
              paradigm, using the simulations to generate both psychophysical and
              electrophysiological responses. These simulated responses are
              consistent with attentional bias or gating, competition for
              attentional resources, attentional capture and associated
              speed-accuracy trade-offs. Furthermore, if we present both attended
              and non-attended stimuli simultaneously, biased competition for
              neuronal representation emerges as a principled and straightforward
              property of Bayes-optimal perception.},
  urldate  = {2017-08-10},
  journal  = {Frontiers in Human Neuroscience},
  author   = {Feldman, Harriet and Friston, Karl J.},
  month    = dec,
  year     = {2010},
  pmid     = {21160551},
  pmcid    = {PMC3001758},
  file     = {Feldman en Friston - 2010 - Attention, Uncertainty, and
              Free-Energy.pdf:/Users/apodusenko/Zotero/storage/KKE6ZR5Q/Feldman en
              Friston - 2010 - Attention, Uncertainty, and
              Free-Energy.pdf:application/pdf}
}

@article{rao_predictive_1999,
  title      = {Predictive coding in the visual cortex: a functional interpretation
                of some extra-classical receptive-field effects},
  volume     = {2},
  copyright  = {© 1999 Nature Publishing Group},
  issn       = {1097-6256},
  shorttitle = {Predictive coding in the visual cortex},
  url        = {
                http://www.nature.com/neuro/journal/v2/n1/abs/nn0199_79.html?foxtrotcallback=true
                },
  doi        = {10.1038/4580},
  abstract   = {We describe a model of visual processing in which feedback
                connections from a higher- to a lower-order visual cortical area
                carry predictions of lower-level neural activities, whereas the
                feedforward connections carry the residual errors between the
                predictions and the actual lower-level activities. When exposed to
                natural images, a hierarchical network of model neurons
                implementing such a model developed simple-cell-like receptive
                fields. A subset of neurons responsible for carrying the residual
                errors showed endstopping and other extra-classical receptive-field
                effects. These results suggest that rather than being exclusively
                feedforward phenomena, nonclassical surround effects in the visual
                cortex may also result from cortico-cortical feedback as a
                consequence of the visual system using an efficient hierarchical
                strategy for encoding natural images.},
  language   = {en},
  number     = {1},
  journal    = {Nature Neuroscience},
  author     = {Rao, Rajesh P. N. and Ballard, Dana H.},
  month      = jan,
  year       = {1999},
  pages      = {79--87},
  file       = {
                Snapshot:/Users/apodusenko/Zotero/storage/VMRGAV9M/nn0199_79.html:text/html
                }
}

@article{friston_functional_2011,
  title      = {Functional and effective connectivity: a review},
  volume     = {1},
  issn       = {2158-0022},
  shorttitle = {Functional and effective connectivity},
  doi        = {10.1089/brain.2011.0008},
  abstract   = {Over the past 20 years, neuroimaging has become a predominant
                technique in systems neuroscience. One might envisage that over the
                next 20 years the neuroimaging of distributed processing and
                connectivity will play a major role in disclosing the brain's
                functional architecture and operational principles. The inception
                of this journal has been foreshadowed by an ever-increasing number
                of publications on functional connectivity, causal modeling,
                connectomics, and multivariate analyses of distributed patterns of
                brain responses. I accepted the invitation to write this review
                with great pleasure and hope to celebrate and critique the
                achievements to date, while addressing the challenges ahead.},
  language   = {eng},
  number     = {1},
  journal    = {Brain Connectivity},
  author     = {Friston, Karl J.},
  year       = {2011},
  pmid       = {22432952},
  keywords   = {Humans, Brain, Animals, Brain Mapping, Nerve Net, Neural Pathways,
                Models, Neurological},
  pages      = {13--36},
  file       = {Friston - 2011 - Functional and effective connectivity a
                review.pdf:/Users/apodusenko/Zotero/storage/Y76EWU3W/Friston - 2011 -
                Functional and effective connectivity a review.pdf:application/pdf}
}

@article{markovic_comparative_2016,
  title    = {Comparative {Analysis} of {Behavioral} {Models} for {Adaptive} {
              Learning} in {Changing} {Environments}},
  volume   = {10},
  issn     = {1662-5188},
  url      = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4837154/},
  doi      = {10.3389/fncom.2016.00033},
  abstract = {Probabilistic models of decision making under various forms of
              uncertainty have been applied in recent years to numerous
              behavioral and model-based fMRI studies. These studies were highly
              successful in enabling a better understanding of behavior and
              delineating the functional properties of brain areas involved in
              decision making under uncertainty. However, as different studies
              considered different models of decision making under uncertainty,
              it is unclear which of these computational models provides the best
              account of the observed behavioral and neuroimaging data. This is
              an important issue, as not performing model comparison may tempt
              researchers to over-interpret results based on a single model. Here
              we describe how in practice one can compare different behavioral
              models and test the accuracy of model comparison and parameter
              estimation of Bayesian and maximum-likelihood based methods. We
              focus our analysis on two well-established hierarchical
              probabilistic models that aim at capturing the evolution of beliefs
              in changing environments: Hierarchical Gaussian Filters and Change
              Point Models. To our knowledge, these two, well-established models
              have never been compared on the same data. We demonstrate, using
              simulated behavioral experiments, that one can accurately
              disambiguate between these two models, and accurately infer free
              model parameters and hidden belief trajectories (e.g., posterior
              expectations, posterior uncertainties, and prediction errors) even
              when using noisy and highly correlated behavioral measurements.
              Importantly, we found several advantages of Bayesian inference and
              Bayesian model comparison compared to often-used Maximum-Likelihood
              schemes combined with the Bayesian Information Criterion. These
              results stress the relevance of Bayesian data analysis for
              model-based neuroimaging studies that investigate human decision
              making under uncertainty.},
  urldate  = {2017-08-07},
  journal  = {Frontiers in Computational Neuroscience},
  author   = {Marković, Dimitrije and Kiebel, Stefan J.},
  month    = apr,
  year     = {2016},
  pmid     = {27148030},
  pmcid    = {PMC4837154},
  file     = {Marković and Kiebel - 2016 - Comparative Analysis of Behavioral Models
              for Adap.pdf:/Users/apodusenko/Zotero/storage/G7LE835T/Marković and
              Kiebel - 2016 - Comparative Analysis of Behavioral Models for
              Adap.pdf:application/pdf}
}

@article{leong_human_2013,
  title   = {Human reinforcement learning processes act on learned
             attentionally-filtered representations of the world},
  url     = {
             http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.673.2873&rep=rep1&type=pdf#page=43
             },
  urldate = {2017-08-07},
  journal = {RLDM 2013},
  author  = {Leong, Yuan Chang and Niv, Yael},
  year    = {2013},
  pages   = {43},
  file    = {Leong en Niv - 2013 - Human reinforcement learning processes act on
             lear.pdf:/Users/apodusenko/Zotero/storage/ISVXJEL8/Leong en Niv - 2013
             - Human reinforcement learning processes act on
             lear.pdf:application/pdf}
}

@article{farashahi_your_2017,
  title     = {Your favorite color makes learning more adaptable and precise},
  copyright = {© 2017, Posted by Cold Spring Harbor Laboratory Press. The
               copyright holder for this pre-print is the author. All rights
               reserved. The material may not be redistributed, re-used or
               adapted without the author's permission.},
  url       = {http://www.biorxiv.org/content/early/2017/01/03/097741},
  doi       = {10.1101/097741},
  abstract  = {Learning from reward feedback is essential for survival but can
               become extremely challenging when choice options have multiple
               features and feature values (curse of dimensionality). Here, we
               propose a general framework for learning reward values in dynamic
               multi-dimensional environments via encoding and updating the
               average value of individual features. We predicted that this
               feature-based learning occurs not just because it can reduce
               dimensionality, but more importantly because it can increase
               adaptability without compromising precision. We experimentally
               tested this novel prediction and found that in dynamic environments
               , human subjects adopted feature-based learning even when this
               approach does not reduce dimensionality. Even in static
               low-dimensional environment, subjects initially tended to adopt
               feature-based learning and switched to learning individual option
               values only when feature values could not accurately predict all
               objects values. Moreover, behaviors of two alternative network
               models demonstrated that hierarchical decision-making and learning
               could account for our experimental results and thus provides a
               plausible mechanism for model adoption during learning in dynamic
               environments. Our results constrain neural mechanisms underlying
               learning in dynamic multi-dimensional environments, and highlight
               the importance of neurons encoding the value of individual features
               in this learning.},
  language  = {en},
  journal   = {bioRxiv},
  author    = {Farashahi, Shiva and Rowe, Katherine and Aslami, Zohra and Lee,
               Daeyeol and Soltani, Alireza},
  month     = jan,
  year      = {2017},
  pages     = {097741},
  file      = {Farashahi e.a. - 2017 - Your favorite color makes learning more
               adaptable .pdf:/Users/apodusenko/Zotero/storage/Y6J6IY93/Farashahi e.a.
               - 2017 - Your favorite color makes learning more adaptable
               .pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/2GPQTESA/097741.html:text/html
               }
}

@article{eldar_effects_2013,
  title    = {The effects of neural gain on attention and learning},
  volume   = {16},
  issn     = {1546-1726},
  doi      = {10.1038/nn.3428},
  abstract = {Attention is commonly thought to be manifest through local
              variations in neural gain. However, what would be the effects of
              brain-wide changes in gain? We hypothesized that global
              fluctuations in gain modulate the breadth of attention and the
              degree to which processing is focused on aspects of the environment
              to which one is predisposed to attend. We found that measures of
              pupil diameter, which are thought to track levels of locus
              coeruleus norepinephrine activity and neural gain, were correlated
              with the degree to which learning was focused on stimulus
              dimensions that individual human participants were more predisposed
              to process. In support of our interpretation of this effect in
              terms of global changes in gain, we found that the measured
              pupillary and behavioral variables were strongly correlated with
              global changes in the strength and clustering of functional
              connectivity, as brain-wide fluctuations of gain would predict.},
  language = {eng},
  number   = {8},
  journal  = {Nature Neuroscience},
  author   = {Eldar, Eran and Cohen, Jonathan D. and Niv, Yael},
  month    = aug,
  year     = {2013},
  pmid     = {23770566},
  pmcid    = {PMC3725201},
  keywords = {Female, Humans, Male, Attention, Adult, Middle Aged, Young Adult,
              Adolescent, Discrimination Learning, Locus Coeruleus, Magnetic
              Resonance Imaging, Muscle Contraction, Neural Networks (Computer),
              Norepinephrine, Pattern Recognition, Visual, Photic Stimulation,
              Psychomotor Performance, Pupil, Reward, Semantics},
  pages    = {1146--1153},
  file     = {Eldar e.a. - 2013 - The effects of neural gain on attention and
              learni.pdf:/Users/apodusenko/Zotero/storage/WD54VLF4/Eldar e.a. - 2013
              - The effects of neural gain on attention and
              learni.pdf:application/pdf}
}

@inproceedings{joshi_personalizing_2017,
  title     = {Personalizing {Gesture} {Recognition} {Using} {Hierarchical} {
               Bayesian} {Neural} {Networks}},
  url       = {
               https://pdfs.semanticscholar.org/36e0/ba5f46d0eb0f75ed17b8776fa90c5ba83d43.pdf
               },
  urldate   = {2017-08-07},
  booktitle = {Proc. {IEEE} {Conf}. on {Computer} {Vision} and {Pattern} {
               Recognition} ({CVPR})},
  author    = {Joshi, Ajjen and Ghosh, Soumya and Betke, Margrit and Sclaroff, Stan
               and Pfister, Hanspeter},
  year      = {2017},
  file      = {Joshi e.a. - 2017 - Personalizing Gesture Recognition Using
               Hierarchic.pdf:/Users/apodusenko/Zotero/storage/CB4H5MW6/Joshi e.a. -
               2017 - Personalizing Gesture Recognition Using
               Hierarchic.pdf:application/pdf}
}

@article{chen_deep_2016,
  title    = {Deep attractor network for single-microphone speaker separation},
  url      = {http://arxiv.org/abs/1611.08930},
  abstract = {Despite the overwhelming success of deep learning in various
              speech processing tasks, the problem of separating simultaneous
              speakers in a mixture remains challenging. Two major difficulties
              in such systems are the arbitrary source permutation and unknown
              number of sources in the mixture. We propose a novel deep learning
              framework for single channel speech separation by creating
              attractor points in high dimensional embedding space of the
              acoustic signals which pull together the time-frequency bins
              corresponding to each source. Attractor points in this study are
              created by finding the centroids of the sources in the embedding
              space, which are subsequently used to determine the similarity of
              each bin in the mixture to each source. The network is then trained
              to minimize the reconstruction error of each source by optimizing
              the embeddings. The proposed model is different from prior works in
              that it implements an end-to-end training, and it does not depend
              on the number of sources in the mixture. Two strategies are
              explored in the test time, K-means and fixed attractor points,
              where the latter requires no post-processing and can be implemented
              in real-time. We evaluated our system on Wall Street Journal
              dataset and show 5.49{\textbackslash}\% improvement over the
              previous state-of-the-art methods.},
  urldate  = {2017-08-07},
  journal  = {arXiv:1611.08930 [cs]},
  author   = {Chen, Zhuo and Luo, Yi and Mesgarani, Nima},
  month    = nov,
  year     = {2016},
  note     = {arXiv: 1611.08930},
  keywords = {Computer Science - Learning, Computer Science - Sound},
  file     = {arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/X5MMI6UY/1611.html:text/html;Chen
              e.a. - 2016 - Deep attractor network for single-microphone
              speak.pdf:/Users/apodusenko/Zotero/storage/GMPAIYUM/Chen e.a. - 2016 -
              Deep attractor network for single-microphone speak.pdf:application/pdf}
}

@article{baltieri_active_2017,
  title    = {An active inference implementation of phototaxis},
  url      = {http://arxiv.org/abs/1707.01806},
  abstract = {Active inference is emerging as a possible unifying theory of
              perception and action in cognitive and computational neuroscience.
              On this theory, perception is a process of inferring the causes of
              sensory data by minimising the error between actual sensations and
              those predicted by an inner {\textbackslash}emph\{generative\}
              (probabilistic) model. Action on the other hand is drawn as a
              process that modifies the world such that the consequent sensory
              input meets expectations encoded in the same internal model. These
              two processes, inferring properties of the world and inferring
              actions needed to meet expectations, close the sensory/motor loop
              and suggest a deep symmetry between action and perception. In this
              work we present a simple agent-based model inspired by this new
              theory that offers insights on some of its central ideas. Previous
              implementations of active inference have typically examined a "
              perception-oriented" view of this theory, assuming that agents are
              endowed with a detailed generative model of their surrounding
              environment. In contrast, we present an "action-oriented" solution
              showing how adaptive behaviour can emerge even when agents operate
              with a simple model which bears little resemblance to their
              environment. We examine how various parameters of this formulation
              allow phototaxis and present an example of a different, "
              pathological" behaviour.},
  urldate  = {2017-08-06},
  journal  = {arXiv:1707.01806 [q-bio]},
  author   = {Baltieri, Manuel and Buckley, Christopher L.},
  month    = jul,
  year     = {2017},
  note     = {arXiv: 1707.01806},
  keywords = {Quantitative Biology - Neurons and Cognition},
  file     = {arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/AZFLTXVS/1707.html:text/html;Baltieri
              and Buckley - 2017 - An active inference implementation of
              phototaxis.pdf:/Users/apodusenko/Zotero/storage/WCAKZV7X/Baltieri and
              Buckley - 2017 - An active inference implementation of
              phototaxis.pdf:application/pdf}
}

@article{pitkow_inference_2017,
  title      = {Inference in the {Brain}: {Statistics} {Flowing} in {Redundant} {
                Population} {Codes}},
  volume     = {94},
  issn       = {0896-6273},
  shorttitle = {Inference in the {Brain}},
  url        = {http://www.sciencedirect.com/science/article/pii/S089662731730466X},
  doi        = {10.1016/j.neuron.2017.05.028},
  abstract   = {It is widely believed that the brain performs approximate
                probabilistic inference to estimate causal variables in the world
                from ambiguous sensory data. To understand these computations, we
                need to analyze how information is represented and transformed by
                the actions of nonlinear recurrent neural networks. We propose that
                these probabilistic computations function by a message-passing
                algorithm operating at the level of redundant neural populations.
                To explain this framework, we review its underlying concepts,
                including graphical models, sufficient statistics, and
                message-passing, and then describe how these concepts could be
                implemented by recurrently connected probabilistic population
                codes. The relevant information flow in these networks will be most
                interpretable at the population level, particularly for redundant
                neural codes. We therefore outline a general approach to identify
                the essential features of a neural message-passing algorithm.
                Finally, we argue that to reveal the most important aspects of
                these neural computations, we must study large-scale activity
                patterns during moderately complex, naturalistic behaviors.},
  number     = {5},
  urldate    = {2017-08-05},
  journal    = {Neuron},
  author     = {Pitkow, Xaq and Angelaki, Dora E.},
  month      = jun,
  year       = {2017},
  keywords   = {Nonlinear, Inference, coding, message-passing, nuisance,
                population code, redundant, theory},
  pages      = {943--953},
  file       = {ScienceDirect
                Snapshot:/Users/apodusenko/Zotero/storage/53B5UWT5/S089662731730466X.html:text/html
                }
}

@article{zhang_unifying_2017,
  title    = {Unifying {Message} {Passing} {Algorithms} {Under} the {Framework} of
              {Constrained} {Bethe} {Free} {Energy} {Minimization}},
  url      = {http://arxiv.org/abs/1703.10932},
  abstract = {Variational message passing (VMP), belief propagation (BP),
              expectation propagation (EP) and more recent generalized
              approximate message passing (GAMP) have found their wide uses in
              complex statistical inference problems. In addition to view them as
              a class of algorithms operating on graphical models, this paper
              unifies them under an optimization framework, namely, Bethe free
              energy minimization with differently and appropriately imposed
              constraints. This new perspective in terms of constraint
              manipulation can offer additional insights on the connection
              between message passing algorithms and it is valid for a generic
              statistical model, e.g., without requiring a fully separable
              a-priori density or likelihood function. Furthermore, it also
              founds a theoretical framework to systematically derive hybrid
              message passing for achieving a better comprise between inference
              performance and complexity.},
  urldate  = {2017-08-05},
  journal  = {arXiv:1703.10932 [cs, math]},
  author   = {Zhang, Dan and Wang, Wenjin and Fettweis, Gerhard and Gao, Xiqi},
  month    = mar,
  year     = {2017},
  note     = {arXiv: 1703.10932},
  keywords = {Computer Science - Information Theory},
  file     = {arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/TG3BJ2W8/1703.html:text/html;Zhang
              et al. - 2017 - Unifying Message Passing Algorithms Under the
              Fram.pdf:/Users/apodusenko/Zotero/storage/DTXHRW9G/Zhang et al. - 2017
              - Unifying Message Passing Algorithms Under the
              Fram.pdf:application/pdf}
}

@article{zhang_low_2017,
  title    = {Low {Complexity} {Sparse} {Bayesian} {Learning} {Using} {Combined} {
              BP} and {MF} with a {Stretched} {Factor} {Graph}},
  volume   = {131},
  issn     = {01651684},
  url      = {http://arxiv.org/abs/1602.07762},
  doi      = {10.1016/j.sigpro.2016.08.027},
  abstract = {This paper concerns message passing based approaches to sparse
              Bayesian learning (SBL) with a linear model corrupted by additive
              white Gaussian noise with unknown variance. With the conventional
              factor graph, mean field (MF) message passing based algorithms have
              been proposed in the literature. In this work, instead of using the
              conventional factor graph, we modify the factor graph by adding
              some extra hard constraints (the graph looks like being
              `stretched'), which enables the use of combined belief propagation
              (BP) and MF message passing. We then propose a low complexity BP-MF
              SBL algorithm based on which an approximate BP-MF SBL algorithm is
              also developed to further reduce the complexity. Thanks to the use
              of BP, the BP-MF SBL algorithms show their merits compared with
              state-of-the-art MF SBL algorithms: they deliver even better
              performance with much lower complexity compared with the
              vector-form MF SBL algorithm and they significantly outperform the
              scalar-form MF SBL algorithm with similar complexity.},
  urldate  = {2017-08-05},
  journal  = {Signal Processing},
  author   = {Zhang, Chuanzong and Yuan, Zhengdao and Wang, Zhongyong and Guo,
              Qinghua},
  month    = feb,
  year     = {2017},
  note     = {arXiv: 1602.07762},
  keywords = {Computer Science - Information Theory},
  pages    = {344--349},
  file     = {arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/T9K4CQVZ/1602.html:text/html;Zhang
              et al. - 2017 - Low Complexity Sparse Bayesian Learning Using
              Comb.pdf:/Users/apodusenko/Zotero/storage/Z76DJQIS/Zhang et al. - 2017
              - Low Complexity Sparse Bayesian Learning Using
              Comb.pdf:application/pdf}
}

@article{nolan_accurate_2017,
  title      = {Accurate logistic variational message passing: algebraic and
                numerical details},
  volume     = {6},
  issn       = {2049-1573},
  shorttitle = {Accurate logistic variational message passing},
  url        = {http://onlinelibrary.wiley.com/doi/10.1002/sta4.139/abstract},
  doi        = {10.1002/sta4.139},
  abstract   = {We provide full algebraic and numerical details required for
                fitting accurate logistic likelihood regression-type models via
                variational message passing with factor graph fragments. Existing
                methodology of this type involves the Jaakkola–Jordan device, which
                is prone to poor accuracy. We examine two alternatives: the
                Saul–Jordan tilted bound device and conjugacy enforcement via
                multivariate normal prespecification of a key message. Both of
                these approaches appear in related literature. Our contributions
                facilitate immediate implementation within variational message
                passing schemes. Copyright © 2017 John Wiley \& Sons, Ltd.},
  language   = {en},
  number     = {1},
  urldate    = {2017-08-05},
  journal    = {Stat},
  author     = {Nolan, Tui H. and Wand, Matt P.},
  month      = jan,
  year       = {2017},
  keywords   = {Factor graph, approximate Bayesian inference, generalized additive
                models, generalized linear mixed models, mean field variational
                Bayes},
  pages      = {102--112},
  file       = {Nolan and Wand - 2017 - Accurate logistic variational message passing
                alg.pdf:/Users/apodusenko/Zotero/storage/ZKQ2KE73/Nolan and Wand - 2017
                - Accurate logistic variational message passing
                alg.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/4QDUXIAP/abstract.html:text/html
                }
}

@article{engstrom_great_2017,
  title      = {Great expectations: a predictive processing account of automobile
                driving},
  volume     = {0},
  issn       = {1463-922X},
  shorttitle = {Great expectations},
  url        = {http://dx.doi.org/10.1080/1463922X.2017.1306148},
  doi        = {10.1080/1463922X.2017.1306148},
  abstract   = {Predictive processing has been proposed as a unifying framework
                for understanding brain function, suggesting that cognition and
                behaviour can be fundamentally understood based on the single
                principle of prediction error minimisation. According to predictive
                processing, the brain is a statistical organ that continuously
                attempts get a grip on states in the world by predicting how these
                states cause sensory input and minimising the deviations between
                the predicted and actual input. While these ideas have had a strong
                influence in neuroscience and cognitive science, they have so far
                not been adopted in applied human factors research. The present
                paper represents a first attempt to do so, exploring how predictive
                processing concepts can be used to understand automobile driving.
                It is shown how a framework based on predictive processing may
                provide a novel perspective on a range of driving phenomena and
                offer a unifying framework for traditionally disparate human
                factors models.},
  number     = {0},
  urldate    = {2017-08-05},
  journal    = {Theoretical Issues in Ergonomics Science},
  author     = {Engström, Johan and Bärgman, Jonas and Nilsson, Daniel and Seppelt,
                Bobbie and Markkula, Gustav and Piccinini, Giulio Bianchi and Victor,
                Trent},
  month      = apr,
  year       = {2017},
  keywords   = {Action, Perception, driver behaviour, driving, expectancy,
                Predictive processing},
  pages      = {1--39},
  file       = {Engström e.a. - 2017 - Great expectations a predictive processing
                accoun.pdf:/Users/apodusenko/Zotero/storage/CM6UI2WE/Engström e.a. -
                2017 - Great expectations a predictive processing
                accoun.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/BJLEVXVY/1463922X.2017.html:text/html
                }
}

@article{vaidya_neural_2015,
  title     = {Neural {Mechanisms} for {Undoing} the “{Curse} of {Dimensionality}”},
  volume    = {35},
  copyright = {Copyright © 2015 the authors 0270-6474/15/3512083-02\$15.00/0},
  issn      = {0270-6474, 1529-2401},
  url       = {http://www.jneurosci.org/content/35/35/12083},
  doi       = {10.1523/JNEUROSCI.2428-15.2015},
  abstract  = {Human behavior is marked by a sophisticated ability to attribute
               outcomes and events to choices and experiences with surprising
               nuance. Understanding the mechanisms that govern this ability is a
               major focus for cognitive neuroscience. Reinforcement learning (RL)
               theory has provided a tractable},
  language  = {en},
  number    = {35},
  journal   = {Journal of Neuroscience},
  author    = {Vaidya, Avinash R.},
  month     = sep,
  year      = {2015},
  pmid      = {26338319},
  pages     = {12083--12084},
  file      = {
               Snapshot:/Users/apodusenko/Zotero/storage/4Z53WV4P/12083.html:text/html;Vaidya
               - 2015 - Neural Mechanisms for Undoing the “Curse of
               Dimens.pdf:/Users/apodusenko/Zotero/storage/NKRG6D5Q/Vaidya - 2015 -
               Neural Mechanisms for Undoing the “Curse of Dimens.pdf:application/pdf}
}

@article{rusch_two-way_2017,
  title    = {A {Two}-{Way} {Street} between {Attention} and {Learning}},
  volume   = {93},
  issn     = {0896-6273},
  url      = {http://www.sciencedirect.com/science/article/pii/S0896627317300065},
  doi      = {10.1016/j.neuron.2017.01.005},
  abstract = {In an elegant model-based fMRI study, Leong et al. (2017)
              demonstrate how attention and learning interact to facilitate
              value-based decision-making. They combine computational modeling
              with empirical measures of attentional selection derived from
              eye-tracking data and multivariate pattern analyses.},
  number   = {2},
  urldate  = {2017-08-05},
  journal  = {Neuron},
  author   = {Rusch, Tessa and Korn, Christoph W. and Gläscher, Jan},
  month    = jan,
  year     = {2017},
  pages    = {256--258},
  file     = {Rusch e.a. - 2017 - A Two-Way Street between Attention and
              Learning.pdf:/Users/apodusenko/Zotero/storage/HHY3HXC4/Rusch e.a. -
              2017 - A Two-Way Street between Attention and
              Learning.pdf:application/pdf;ScienceDirect
              Snapshot:/Users/apodusenko/Zotero/storage/RIK37FPT/S0896627317300065.html:text/html
              }
}

@article{niv_reinforcement_2015,
  title     = {Reinforcement {Learning} in {Multidimensional} {Environments} {Relies
               } on {Attention} {Mechanisms}},
  volume    = {35},
  copyright = {Copyright © 2015 the authors 0270-6474/15/358145-13\$15.00/0},
  issn      = {0270-6474, 1529-2401},
  url       = {http://www.jneurosci.org/content/35/21/8145},
  doi       = {10.1523/JNEUROSCI.2978-14.2015},
  abstract  = {In recent years, ideas from the computational field of
               reinforcement learning have revolutionized the study of learning in
               the brain, famously providing new, precise theories of how dopamine
               affects learning in the basal ganglia. However, reinforcement
               learning algorithms are notorious for not scaling well to
               multidimensional environments, as is required for real-world
               learning. We hypothesized that the brain naturally reduces the
               dimensionality of real-world problems to only those dimensions that
               are relevant to predicting reward, and conducted an experiment to
               assess by what algorithms and with what neural mechanisms this
               “representation learning” process is realized in humans. Our
               results suggest that a bilateral attentional control network
               comprising the intraparietal sulcus, precuneus, and dorsolateral
               prefrontal cortex is involved in selecting what dimensions are
               relevant to the task at hand, effectively updating the task
               representation through trial and error. In this way, cortical
               attention mechanisms interact with learning in the basal ganglia to
               solve the “curse of dimensionality” in reinforcement learning.},
  language  = {en},
  number    = {21},
  journal   = {Journal of Neuroscience},
  author    = {Niv, Yael and Daniel, Reka and Geana, Andra and Gershman, Samuel J.
               and Leong, Yuan Chang and Radulescu, Angela and Wilson, Robert C.},
  month     = may,
  year      = {2015},
  pmid      = {26019331},
  keywords  = {reinforcement learning, Attention, fMRI, frontoparietal network,
               model comparison, representation learning},
  pages     = {8145--8157},
  file      = {Niv et al. - 2015 - Reinforcement Learning in Multidimensional
               Environ.pdf:/Users/apodusenko/Zotero/storage/D3WI3VFZ/Niv et al. - 2015
               - Reinforcement Learning in Multidimensional
               Environ.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/UCR65PSF/8145.html:text/html
               }
}

@article{leong_dynamic_2017,
  title    = {Dynamic {Interaction} between {Reinforcement} {Learning} and {
              Attention} in {Multidimensional} {Environments}},
  volume   = {93},
  issn     = {1097-4199},
  doi      = {10.1016/j.neuron.2016.12.040},
  abstract = {Little is known about the relationship between attention and
              learning during decision making. Using eye tracking and
              multivariate pattern analysis of fMRI data, we measured
              participants' dimensional attention as they performed a
              trial-and-error learning task in which only one of three stimulus
              dimensions was relevant for reward at any given time. Analysis of
              participants' choices revealed that attention biased both value
              computation during choice and value update during learning. Value
              signals in the ventromedial prefrontal cortex and prediction errors
              in the striatum were similarly biased by attention. In turn,
              participants' focus of attention was dynamically modulated by
              ongoing learning. Attentional switches across dimensions correlated
              with activity in a frontoparietal attention network, which showed
              enhanced connectivity with the ventromedial prefrontal cortex
              between switches. Our results suggest a bidirectional interaction
              between attention and learning: attention constrains learning to
              relevant dimensions of the environment, while we learn what to
              attend to via trial and error.},
  language = {eng},
  number   = {2},
  journal  = {Neuron},
  author   = {Leong, Yuan Chang and Radulescu, Angela and Daniel, Reka and
              DeWoskin, Vivian and Niv, Yael},
  month    = jan,
  year     = {2017},
  pmid     = {28103483},
  pmcid    = {PMC5287409},
  keywords = {Female, Humans, Male, decision making, reinforcement learning,
              Learning, Attention, Brain Mapping, Prediction error, Adult, Young
              Adult, Adolescent, computational modeling, Magnetic Resonance
              Imaging, Reward, fMRI, Choice Behavior, Environment, Eye Movement
              Measurements, Functional Neuroimaging, MVPA, Neostriatum,
              Prefrontal Cortex, Reinforcement (Psychology), striatum, Task
              Performance and Analysis, value, vmPFC},
  pages    = {451--463},
  file     = {Leong e.a. - 2017 - Dynamic Interaction between Reinforcement
              Learning.pdf:/Users/apodusenko/Zotero/storage/YM7Z6R8J/Leong e.a. -
              2017 - Dynamic Interaction between Reinforcement
              Learning.pdf:application/pdf}
}

@article{markovic_modeling_2015,
  title    = {Modeling the {Evolution} of {Beliefs} {Using} an {Attentional} {Focus
              } {Mechanism}},
  volume   = {11},
  issn     = {1553-7358},
  url      = {
              http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1004558
              },
  doi      = {10.1371/journal.pcbi.1004558},
  abstract = {Author Summary When making decisions in our everyday life (e.g.
              where to eat) we first have to identify a set of environmental
              features that are relevant for the decision (e.g. the distance to
              the place, current time or the price). Although we are able to make
              such inferences almost effortlessly, this type of problems is
              computationally challenging, as we live in a complex environment
              that constantly changes and contains an immense number of features.
              Here we investigated the question of how the human brain solves
              this computational challenge. In particular, we designed a new
              experimental paradigm and derived novel behavioral models to test
              the hypothesis that attention modulates the formation of beliefs
              about the relevance of several environmental features. As each
              behavioral model accounted for a different hypothesis about the
              underlying computational mechanism we compared them in their
              ability to explain the measured behavior of human subjects
              performing the experimental task. The model comparison indicates
              that an attentional-focus mechanism is a key feature of behavioral
              models that accurately replicate subjects’ behavior. These findings
              suggest that the evolution of beliefs is modulated by a competitive
              attractor dynamics that forms prior expectation about future
              outcomes. Hence, the findings provide interesting and novel
              insights into the computational mechanisms underlying human
              behavior when making decisions in complex environments.},
  number   = {10},
  journal  = {PLOS Computational Biology},
  author   = {Marković, Dimitrije and Gläscher, Jan and Bossaerts, Peter and
              O’Doherty, John and Kiebel, Stefan J.},
  month    = oct,
  year     = {2015},
  keywords = {Attention, Probability distribution, Covariance, Behavior,
              Experimental design, Sensory perception, Vision},
  pages    = {e1004558},
  file     = {Marković et al. - 2015 - Modeling the Evolution of Beliefs Using an
              Attenti.pdf:/Users/apodusenko/Zotero/storage/CMXKIES8/Marković et al. -
              2015 - Modeling the Evolution of Beliefs Using an
              Attenti.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/J9HDN35T/article.html:text/html
              }
}

@article{zion_golumbic_temporal_2012,
  title      = {Temporal {Context} in {Speech} {Processing} and {Attentional} {Stream
                } {Selection}: {A} {Behavioral} and {Neural} perspective},
  volume     = {122},
  issn       = {0093-934X},
  shorttitle = {Temporal {Context} in {Speech} {Processing} and {Attentional} {
                Stream} {Selection}},
  url        = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3340429/},
  doi        = {10.1016/j.bandl.2011.12.010},
  abstract   = {The human capacity for processing speech is remarkable, especially
                given that information in speech unfolds over multiple time scales
                concurrently. Similarly notable is our ability to filter out of
                extraneous sounds and focus our attention on one conversation,
                epitomized by the ‘Cocktail Party’ effect. Yet, the neural
                mechanisms underlying on-line speech decoding and attentional
                stream selection are not well understood. We review findings from
                behavioral and neurophysiological investigations that underscore
                the importance of the temporal structure of speech for achieving
                these perceptual feats. We discuss the hypothesis that entrainment
                of ambient neuronal oscillations to speech’s temporal structure,
                across multiple time-scales, serves to facilitate its decoding and
                underlies the selection of an attended speech stream over other
                competing input. In this regard, speech decoding and attentional
                stream selection are examples of ‘active sensing’, emphasizing an
                interaction between proactive and predictive top-down modulation of
                neuronal dynamics and bottom-up sensory input.},
  number     = {3},
  urldate    = {2017-08-02},
  journal    = {Brain and language},
  author     = {Zion Golumbic, Elana M. and Poeppel, David and Schroeder, Charles E.
                },
  month      = sep,
  year       = {2012},
  pmid       = {22285024},
  pmcid      = {PMC3340429},
  pages      = {151--161},
  file       = {Zion Golumbic et al. - 2012 - Temporal Context in Speech Processing
                and Attentio.pdf:/Users/apodusenko/Zotero/storage/N47WFNBH/Zion
                Golumbic et al. - 2012 - Temporal Context in Speech Processing and
                Attentio.pdf:application/pdf}
}

@inproceedings{levchuk_application_2017,
  title    = {Application of free energy minimization to the design of adaptive
              multi-agent teams},
  volume   = {10206},
  url      = {http://dx.doi.org/10.1117/12.2263542},
  doi      = {10.1117/12.2263542},
  abstract = {Many novel DoD missions, from disaster relief to cyber
              reconnaissance, require teams of humans and machines with diverse
              capabilities. Current solutions do not account for heterogeneity of
              agent capabilities, uncertainty of team knowledge, and dynamics of
              and dependencies between tasks and agent roles, resulting in
              brittle teams. Most importantly, the state-of-the-art team design
              solutions are either centralized, imposing role and relation
              assignment onto agents, or completely distributed, suitable for
              only homogeneous organizations such as swarms. Centralized design
              models can’t provide insights for team’s self-organization, i.e.
              adapting team structure over time in distributed collaborative
              manner by team members with diverse expertise and responsibilities.
              In this paper we present an information-theoretic formalization of
              team composition and structure adaptation using a minimization of
              variational free energy. The structure adaptation is obtained in an
              iterative distributed and collaborative manner without the need for
              centralized control. We show that our model is lightweight,
              predictive, and produces team structures that theoretically
              approximate an optimal policy for team adaptation. Our model also
              provides a unique coupling between the structure and action policy,
              and captures three essential processes of learning, perception, and
              control.},
  urldate  = {2017-08-01},
  author   = {Levchuk, Georgiy and Pattipati, Krishna and Fouse, Adam and Serfaty,
              Daniel},
  year     = {2017},
  pages    = {102060E--102060E--17}
}

@inproceedings{harding_auditory_2007,
  series     = {Lecture {Notes} in {Computer} {Science}},
  title      = {Auditory {Gist} {Perception}: {An} {Alternative} to {Attentional} {
                Selection} of {Auditory} {Streams}?},
  isbn       = {978-3-540-77342-9 978-3-540-77343-6},
  shorttitle = {Auditory {Gist} {Perception}},
  url        = {https://link.springer.com/chapter/10.1007/978-3-540-77343-6_26},
  doi        = {10.1007/978-3-540-77343-6_26},
  abstract   = {The idea that the gist of a visual scene is perceived before
                attention is focused on the details of a particular object is
                becoming increasingly popular. In the auditory system, on the other
                hand, it is typically assumed that the sensory signal is first
                broken down into streams and then attention is applied to select
                one of the streams. We consider evidence for an alternative: that,
                in close analogy with the visual system, the gist of an auditory
                scene is perceived and only afterwards attention is paid to
                relevant constituents. We find that much experimental evidence is
                consistent with such a proposal, and we suggest some possibilities
                for gist representations.},
  language   = {en},
  booktitle  = {Attention in {Cognitive} {Systems}. {Theories} and {Systems} from
                an {Interdisciplinary} {Viewpoint}},
  publisher  = {Springer, Berlin, Heidelberg},
  author     = {Harding, Sue and Cooke, Martin and König, Peter},
  month      = jan,
  year       = {2007},
  pages      = {399--416},
  file       = {Harding e.a. - 2007 - Auditory Gist Perception An Alternative to
                Attent.pdf:/Users/apodusenko/Zotero/storage/FKBTW5GC/Harding e.a. -
                2007 - Auditory Gist Perception An Alternative to
                Attent.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/P4D7FUBA/978-3-540-77343-6_26.html:text/html
                }
}

@article{snyder_recent_2017,
  title    = {Recent advances in exploring the neural underpinnings of auditory
              scene perception},
  volume   = {1396},
  issn     = {1749-6632},
  url      = {http://onlinelibrary.wiley.com/doi/10.1111/nyas.13317/abstract},
  doi      = {10.1111/nyas.13317},
  abstract = {Studies of auditory scene analysis have traditionally relied on
              paradigms using artificial sounds—and conventional behavioral
              techniques—to elucidate how we perceptually segregate auditory
              objects or streams from each other. In the past few decades,
              however, there has been growing interest in uncovering the neural
              underpinnings of auditory segregation using human and animal
              neuroscience techniques, as well as computational modeling. This
              largely reflects the growth in the fields of cognitive neuroscience
              and computational neuroscience and has led to new theories of how
              the auditory system segregates sounds in complex arrays. The
              current review focuses on neural and computational studies of
              auditory scene perception published in the last few years.
              Following the progress that has been made in these studies, we
              describe (1) theoretical advances in our understanding of the most
              well-studied aspects of auditory scene perception, namely
              segregation of sequential patterns of sounds and concurrently
              presented sounds; (2) the diversification of topics and paradigms
              that have been investigated; and (3) how new neuroscience
              techniques (including invasive neurophysiology in awake humans,
              genotyping, and brain stimulation) have been used in this field.},
  language = {en},
  number   = {1},
  urldate  = {2017-08-01},
  journal  = {Annals of the New York Academy of Sciences},
  author   = {Snyder, Joel S. and Elhilali, Mounya},
  month    = may,
  year     = {2017},
  keywords = {auditory scene analysis, auditory stream segregation, change
              deafness, concurrent sound segregation, informational masking},
  pages    = {39--55},
  file     = {
              Snapshot:/Users/apodusenko/Zotero/storage/QGDAZUG8/abstract.html:text/html;Snyder
              en Elhilali - 2017 - Recent advances in exploring the neural
              underpinni.pdf:/Users/apodusenko/Zotero/storage/S2MS2ZFQ/Snyder en
              Elhilali - 2017 - Recent advances in exploring the neural
              underpinni.pdf:application/pdf}
}

@incollection{elhilali_modeling_2017,
  series    = {Springer {Handbook} of {Auditory} {Research}},
  title     = {Modeling the {Cocktail} {Party} {Problem}},
  isbn      = {978-3-319-51660-8 978-3-319-51662-2},
  url       = {https://link.springer.com/chapter/10.1007/978-3-319-51662-2_5},
  abstract  = {Modeling the cocktail party problem entails developing a
               computational framework able to describe what the auditory system
               does when faced with a complex auditory scene. While completely
               intuitive and omnipresent in humans and animals alike, translating
               this remarkable ability into a quantitative model remains a
               challenge. This chapter touches on difficulties facing the field in
               terms of defining the theoretical principles that govern auditory
               scene analysis, as well as reconciling current knowledge about
               perceptual and physiological data with their formulation into
               computational models. The chapter reviews some of the computational
               theories, algorithmic strategies, and neural infrastructure
               proposed in the literature for developing information systems
               capable of processing multisource sound inputs. Because of
               divergent interests from various disciplines in the cocktail party
               problem, the body of literature modeling this effect is equally
               diverse and multifaceted. The chapter touches on the various
               approaches used in modeling auditory scene analysis from biomimetic
               models to strictly engineering systems.},
  language  = {en},
  booktitle = {The {Auditory} {System} at the {Cocktail} {Party}},
  publisher = {Springer, Cham},
  author    = {Elhilali, Mounya},
  year      = {2017},
  doi       = {10.1007/978-3-319-51662-2_5},
  pages     = {111--135},
  file      = {
               Snapshot:/Users/apodusenko/Zotero/storage/UDJXU8CL/978-3-319-51662-2_5.html:text/html
               }
}

@article{hawkins_why_2017,
  title     = {Why {Does} the {Neocortex} {Have} {Layers} and {Columns}, {A} {Theory
               } of {Learning} the {3D} {Structure} of the {World}},
  copyright = {© 2017, Posted by Cold Spring Harbor Laboratory. This pre-print
               is available under a Creative Commons License
               (Attribution-NonCommercial 4.0 International), CC BY-NC 4.0, as
               described at http://creativecommons.org/licenses/by-nc/4.0/},
  url       = {http://www.biorxiv.org/content/early/2017/07/12/162263},
  doi       = {10.1101/162263},
  abstract  = {Neocortical regions are organized into columns and layers.
               Connections between layers run mostly perpendicular to the surface
               suggesting a columnar functional organization. Some layers have
               long-range lateral connections suggesting interactions between
               columns. Similar patterns of connectivity exist in all regions but
               their exact role remains a mystery. In this paper, we propose a
               network model composed of columns and layers that performs robust
               object learning and recognition. Columns integrate their changing
               inputs over time to learn complete models of observed objects.
               Lateral connections across columns allow the network to more
               rapidly infer objects based on the partial knowledge of adjacent
               columns. Because columns integrate input over time and space, the
               network learns models of complex objects that extend well beyond
               the receptive field of individual cells. Our network model
               introduces a new feature to cortical columns. We propose that a
               representation of location is calculated within the sub-granular
               layers of each column. The representation of location is relative
               to the object being sensed. Pairing sensory features with locations
               is a requirement for modeling objects and therefore must occur
               somewhere in the neocortex. We propose it occurs in every column in
               every region. Our network model contains two layers and one or more
               columns. Simulations show that small single-column networks can
               learn to recognize hundreds of complex multi-dimensional objects.
               Given the ubiquity of columnar and laminar connectivity patterns
               throughout the neocortex, we propose that columns and regions have
               more powerful recognition and modeling capabilities than previously
               assumed.},
  language  = {en},
  journal   = {bioRxiv},
  author    = {Hawkins, Jeff and Ahmad, Subutai and Cui, Yuwei},
  month     = jul,
  year      = {2017},
  pages     = {162263},
  file      = {Hawkins e.a. - 2017 - Why Does the Neocortex Have Layers and Columns,
               A .pdf:/Users/apodusenko/Zotero/storage/HA9JULFA/Hawkins e.a. - 2017 -
               Why Does the Neocortex Have Layers and Columns, A
               .pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/WT9SSVXC/162263.html:text/html
               }
}

@inproceedings{engel_neural_2017,
  title     = {Neural {Audio} {Synthesis} of {Musical} {Notes} with {WaveNet} {
               Autoencoders}},
  url       = {http://proceedings.mlr.press/v70/engel17a.html},
  abstract  = {Generative models in vision have seen rapid progress due to
               algorithmic improvements and the availability of high-quality image
               datasets. In this paper, we offer contributions in both these areas
               t...},
  language  = {en},
  booktitle = {{PMLR}},
  author    = {Engel, Jesse and Resnick, Cinjon and Roberts, Adam and Dieleman,
               Sander and Norouzi, Mohammad and Eck, Douglas and Simonyan, Karen},
  month     = jul,
  year      = {2017},
  pages     = {1068--1077},
  file      = {Full Text PDF:/Users/apodusenko/Zotero/storage/626QRMWK/Engel e.a. -
               2017 - Neural Audio Synthesis of Musical Notes with
               WaveN.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/8WFXHDMN/engel17a.html:text/html
               }
}

@inproceedings{chen_learning_2017,
  title     = {Learning to {Learn} without {Gradient} {Descent} by {Gradient} {
               Descent}},
  url       = {http://proceedings.mlr.press/v70/chen17e.html},
  abstract  = {We learn recurrent neural network optimizers trained on simple
               synthetic functions by gradient descent. We show that these learned
               optimizers exhibit a remarkable degree of transfer in that they
               ca...},
  language  = {en},
  booktitle = {{PMLR}},
  author    = {Chen, Yutian and Hoffman, Matthew W. and Colmenarejo, Sergio Gómez
               and Denil, Misha and Lillicrap, Timothy P. and Botvinick, Matt and
               Freitas, Nando},
  month     = jul,
  year      = {2017},
  pages     = {748--756},
  file      = {Chen et al. - 2017 - Learning to Learn without Gradient Descent by
               Grad.pdf:/Users/apodusenko/Zotero/storage/XYISMKE8/Chen et al. - 2017 -
               Learning to Learn without Gradient Descent by
               Grad.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/JSD3IIXN/chen17e.html:text/html
               }
}

@inproceedings{asadi_alternative_2017,
  title     = {An {Alternative} {Softmax} {Operator} for {Reinforcement} {Learning}},
  url       = {http://proceedings.mlr.press/v70/asadi17a.html},
  abstract  = {A softmax operator applied to a set of values acts somewhat like
               the maximization function and somewhat like an average. In
               sequential decision making, softmax is often used in settings where
               it is...},
  language  = {en},
  booktitle = {{PMLR}},
  author    = {Asadi, Kavosh and Littman, Michael L.},
  month     = jul,
  year      = {2017},
  pages     = {243--252},
  file      = {Full Text PDF:/Users/apodusenko/Zotero/storage/7FC6MXAP/Asadi en
               Littman - 2017 - An Alternative Softmax Operator for Reinforcement
               .pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/RAWBPJGG/asadi17a.html:text/html
               }
}

@inproceedings{maystre_just_2017,
  title     = {Just {Sort} {It}! {A} {Simple} and {Effective} {Approach} to {Active}
               {Preference} {Learning}},
  url       = {http://proceedings.mlr.press/v70/maystre17a.html},
  abstract  = {We address the problem of learning a ranking by using adaptively
               chosen pairwise comparisons. Our goal is to recover the ranking
               accurately but to sample the comparisons sparingly. If all
               compariso...},
  language  = {en},
  booktitle = {{PMLR}},
  author    = {Maystre, Lucas and Grossglauser, Matthias},
  month     = jul,
  year      = {2017},
  pages     = {2344--2353},
  file      = {Maystre en Grossglauser - 2017 - Just Sort It! A Simple and Effective
               Approach to A.pdf:/Users/apodusenko/Zotero/storage/G9B2M6TG/Maystre en
               Grossglauser - 2017 - Just Sort It! A Simple and Effective Approach to
               A.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/4PNEZMNM/maystre17a.html:text/html
               }
}

@article{fitzgerald_sequential_2017,
  title    = {Sequential inference as a mode of cognition and its correlates in
              fronto-parietal and hippocampal brain regions},
  volume   = {13},
  issn     = {1553-7358},
  url      = {
              http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1005418
              },
  doi      = {10.1371/journal.pcbi.1005418},
  abstract = {Author summary When studying human cognition, it is often assumed
              that agents form and update beliefs only about the current state of
              the world, an approach known as Bayesian filtering. However, in
              many situations there are advantages to making inferences about the
              most likely sequence of states that have occurred, which involves
              simultaneously updating beliefs about the present and the past,
              based on incoming information. Currently, very little is known
              about whether humans adopt such sequential inference strategies,
              and if they do, about the neuronal mechanisms involved. We
              addressed this by applying computational modelling to data
              collected during a probabilistic reversal task. At a group level,
              subjects’ behaviour showed clear evidence of sequential inference,
              and between-subject differences in the strategies adopted were
              reflected in variations in brain structure in the prefrontal and
              parietal cortices, as well as the hippocampus. Our results provide
              new insight into the strategies employed in human cognition, as
              well as the neuronal substrates of sequential inference.},
  number   = {5},
  journal  = {PLOS Computational Biology},
  author   = {FitzGerald, Thomas H. B. and Hämmerer, Dorothea and Friston, Karl J.
              and Li, Shu-Chen and Dolan, Raymond J.},
  month    = may,
  year     = {2017},
  keywords = {decision making, Hippocampus, Prefrontal Cortex, Behavior, Central
              nervous system, Elderly, Parietal lobe, Voxel-based morphometry},
  pages    = {e1005418},
  file     = {FitzGerald e.a. - 2017 - Sequential inference as a mode of cognition
              and it.pdf:/Users/apodusenko/Zotero/storage/HL486Q4E/FitzGerald e.a. -
              2017 - Sequential inference as a mode of cognition and
              it.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/K4PC5CEN/article.html:text/html
              }
}

@book{noauthor_analogdeviceslyriclabs/dimple_nodate,
  title    = {{AnalogDevicesLyricLabs}/dimple},
  url      = {https://github.com/AnalogDevicesLyricLabs/dimple},
  abstract = {dimple - Dimple: Java and Matlab libraries for probabilistic
              inference},
  urldate  = {2014-08-05},
  file     = {
              DimpleDeveloperDocumentation.pdf:/Users/apodusenko/Zotero/storage/WTWWHTRJ/DimpleDeveloperDocumentation.pdf:application/pdf;DimpleUserManual.pdf:/Users/apodusenko/Zotero/storage/T8LNQ5QM/DimpleUserManual.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/3AJHKVA5/dimple.html:text/html
              }
}

@book{wiegley_git_2009,
  title    = {Git from the bottom up},
  url      = {http://newartisans.com/2008/04/git-from-the-bottom-up/},
  abstract = {In my pursuit to understand Git, it's been helpful for me to
              understand it from the bottom up – rather than look at it only in
              terms of its high-level commands. And since Git is so beautifully
              sim...},
  urldate  = {2014-04-12},
  author   = {Wiegley, John},
  year     = {2009},
  file     = {
              Snapshot:/Users/apodusenko/Zotero/storage/THGR8EXI/git-from-the-bottom-up.html:text/html;Wiegley
              - 2009 - Git from the bottom
              up.pdf:/Users/apodusenko/Zotero/storage/UJ5ZAL5X/Wiegley - 2009 - Git
              from the bottom up.pdf:application/pdf}
}

@misc{wang_deep_2016,
  title    = {Deep {Learning} {Reinvents} the {Hearing} {Aid}},
  url      = {
              http://spectrum.ieee.org/consumer-electronics/audiovideo/deep-learning-reinvents-the-hearing-aid
              },
  abstract = {Finally, wearers of hearing aids can pick out a voice in a crowded
              room},
  urldate  = {2017-03-10},
  journal  = {IEEE Spectrum: Technology, Engineering, and Science News},
  author   = {Wang, DeLiang},
  month    = jun,
  year     = {2016},
  file     = {Wang - 2016 - Deep Learning Reinvents the Hearing
              Aid.html:/Users/apodusenko/Zotero/storage/53KSBTI8/Wang - 2016 - Deep
              Learning Reinvents the Hearing Aid.html:text/html}
}

@misc{maren_derivation_2017,
  title     = {Derivation of the {Variational} {Bayes} {Equations}},
  publisher = {Themasis Technical Note TN-2017-01},
  author    = {Maren, Alianna J.},
  month     = jul,
  year      = {2017},
  file      = {Maren - 2017 - Derivation of the Variational Bayes
               Equations.pdf:/Users/apodusenko/Zotero/storage/H25KCQTA/Maren - 2017 -
               Derivation of the Variational Bayes Equations.pdf:application/pdf}
}

@misc{friston_et_al._spm12_2014,
  title  = {{SPM12} toolbox, http://www.fil.ion.ucl.ac.uk/spm/software/},
  author = {Friston et al., Karl J.},
  year   = {2014}
}

@inproceedings{ng_algorithms_2000,
  title     = {Algorithms for {Inverse} {Reinforcement} {Learning}},
  abstract  = {This paper addresses the problem of inverse reinforcement learning
               (IRL) in Markov decision processes, that is, the problem of
               extracting a reward function given observed, optimal behaviour. IRL
               may be useful for apprenticeship learning to acquire skilled
               behaviour, and for ascertaining the reward function being optimized
               by a natural system. We rst characterize the set of all reward
               functions for which a given policy is optimal. We then derive three
               algorithms for IRL. The rst two deal with the case where the entire
               policy is known; we handle tabulated reward functions on a nite
               state space and linear functional approximation of the reward
               function over a potentially in- nite state space. The third
               algorithm deals with the more realistic case in which the policy is
               known only through a nite set of observed trajectories. In all
               cases, a key issue is degeneracy{\textbar}the existence of a large
               set of reward functions for which the observed policy is optimal.
               To remove...},
  booktitle = {in {Proc}. 17th {International} {Conf}. on {Machine} {Learning}},
  publisher = {Morgan Kaufmann},
  author    = {Ng, Andrew Y. and Russell, Stuart},
  year      = {2000},
  pages     = {663--670},
  file      = {Citeseer -
               Snapshot:/Users/apodusenko/Zotero/storage/ECISVR2G/summary.html:text/html;Ng
               and Russell - 2000 - Algorithms for Inverse Reinforcement
               Learning.pdf:/Users/apodusenko/Zotero/storage/YZA7RB5P/Ng and Russell -
               2000 - Algorithms for Inverse Reinforcement
               Learning.pdf:application/pdf}
}

@misc{taylor_pyflux:_2016,
  title  = {{PyFlux}: {An} open source time series library for {Python},
            http://www.pyflux.com},
  url    = {http://www.pyflux.com},
  author = {Taylor, Ross},
  year   = {2016}
}

@misc{minka_infer.net_2014,
  title     = {Infer.{NET} 2.6, http://research.microsoft.com/infernet},
  url       = {http://research.microsoft.com/infernet},
  publisher = {Microsoft Research Cambridge},
  author    = {Minka, T. and Winn, J.M. and Guiver, J.P. and Webster, S. and Zaykov
               , Y. and Yangel, B. and Spengler, A. and Bronskill, J.},
  year      = {2014}
}

@article{mlynarski_learning_2017,
  title    = {Learning {Mid}-{Level} {Auditory} {Codes} from {Natural} {Sound} {
              Statistics}},
  url      = {http://arxiv.org/abs/1701.07138},
  abstract = {Interaction with the world requires an organism to transform
              sensory signals into representations in which behaviorally
              meaningful properties of the environment are made explicit. These
              representations are derived through cascades of neuronal processing
              stages in which neurons at each stage recode the output of
              preceding stages. Explanations of sensory coding may thus involve
              understanding how low-level patterns are combined into more complex
              structures. Although models exist in the visual domain to explain
              how mid-level features such as junctions and curves might be
              derived from oriented filters in early visual cortex, little is
              known about analogous grouping principles for mid-level auditory
              representations. We propose a hierarchical generative model of
              natural sounds that learns combinations of spectrotemporal features
              from natural stimulus statistics. In the first layer the model
              forms a sparse convolutional code of spectrograms using a
              dictionary of learned spectrotemporal kernels. To generalize from
              specific kernel activation patterns, the second layer encodes
              patterns of time-varying magnitude of multiple first layer
              coefficients. Because second-layer features are sensitive to
              combinations of spectrotemporal features, the representation they
              support encodes more complex acoustic patterns than the first
              layer. When trained on corpora of speech and environmental sounds,
              some second-layer units learned to group spectrotemporal features
              that occur together in natural sounds. Others instantiate opponency
              between dissimilar sets of spectrotemporal features. Such groupings
              might be instantiated by neurons in the auditory cortex, providing
              a hypothesis for mid-level neuronal computation.},
  urldate  = {2017-07-26},
  journal  = {arXiv:1701.07138 [cs, q-bio]},
  author   = {Młynarski, Wiktor and McDermott, Josh H.},
  month    = jan,
  year     = {2017},
  note     = {arXiv: 1701.07138},
  keywords = {Computer Science - Sound, Quantitative Biology - Neurons and
              Cognition},
  file     = {arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/EAGATGMG/1701.html:text/html;Młynarski
              and McDermott - 2017 - Learning Mid-Level Auditory Codes from Natural
              Sou.pdf:/Users/apodusenko/Zotero/storage/KDMYWK4C/Młynarski and
              McDermott - 2017 - Learning Mid-Level Auditory Codes from Natural
              Sou.pdf:application/pdf}
}

@article{pitkow_how_2017,
  title      = {How the brain might work: statistics flowing in redundant population
                codes},
  shorttitle = {How the brain might work},
  url        = {http://arxiv.org/abs/1702.03492},
  abstract   = {It is widely believed that the brain performs approximate
                probabilistic inference to estimate causal variables in the world
                from ambiguous sensory data. To understand these computations, we
                need to analyze how information is represented and transformed by
                the actions of nonlinear recurrent neural networks. We propose that
                these probabilistic computations function by a message-passing
                algorithm operating at the level of redundant neural populations.
                To explain this framework, we review its underlying concepts,
                including graphical models, sufficient statistics, and
                message-passing, and then describe how these concepts could be
                implemented by recurrently connected probabilistic population
                codes. The relevant information flow in these networks will be most
                interpretable at the population level, particularly for redundant
                neural codes. We therefore outline a general approach to identify
                the essential features of a neural message-passing algorithm.
                Finally, we argue that to reveal the most important aspects of
                these neural computations, we must study large-scale activity
                patterns during moderately complex, naturalistic behaviors.},
  urldate    = {2017-07-26},
  journal    = {arXiv:1702.03492 [q-bio]},
  author     = {Pitkow, Xaq and Angelaki, Dora},
  month      = feb,
  year       = {2017},
  note       = {arXiv: 1702.03492},
  keywords   = {Quantitative Biology - Neurons and Cognition},
  file       = {arXiv.org
                Snapshot:/Users/apodusenko/Zotero/storage/Z2P2WRCT/1702.html:text/html;Pitkow
                and Angelaki - 2017 - How the brain might work statistics flowing in
                re.pdf:/Users/apodusenko/Zotero/storage/Y5QQLCRQ/Pitkow and Angelaki -
                2017 - How the brain might work statistics flowing in
                re.pdf:application/pdf}
}

@inproceedings{cournapeau_using_2010,
  title     = {Using online model comparison in the {Variational} {Bayes} framework
               for online unsupervised {Voice} {Activity} {Detection}},
  doi       = {10.1109/ICASSP.2010.5495610},
  abstract  = {This paper presents the use of online Variational Bayes method for
               online Voice Activity Detection (VAD) in an unsupervised context.
               In conventional VAD, the final step often relies on state machines
               whose parameters are heuristically tuned. The goal of this study is
               to propose a solid statistical scheme for VAD using online model
               comparison which is provided from the Variational Bayes framework.
               In this scheme, two models are estimated online in parallel: one
               for the noise-only situation, and the other for the
               noise-plus-signal situation The VAD decision is done automatically
               depending on the selected model. An experimental evaluation on the
               CENSREC-1-C database shows a significant improvement by the
               proposed method compared to conventional statistical VAD methods.},
  booktitle = {2010 {IEEE} {International} {Conference} on {Acoustics} {Speech}
               and {Signal} {Processing} ({ICASSP})},
  author    = {Cournapeau, D. and Watanabe, S. and Nakamura, A. and Kawahara, T.},
  month     = mar,
  year      = {2010},
  keywords  = {Variational Bayes, variational Bayes, Bayes methods, Gaussian
               distribution, Random variables, speech recognition, Working
               environment noise, SE, Noise robustness, Switches, State estimation
               , sequential estimation, Automatic speech recognition, CENSREC-1-C
               database, Informatics, Internet, online model comparison, online
               unsupervised voice activity detection, Robustness, Training data,
               unsupervised context, unsupervised learning, VAD, variational Bayes
               framework, Voice Activity Detection},
  pages     = {4462--4465},
  file      = {Cournapeau et al. - 2010 - Using online model comparison in the
               Variational B.pdf:/Users/apodusenko/Zotero/storage/ZKN7D9A5/Cournapeau
               et al. - 2010 - Using online model comparison in the Variational
               B.pdf:application/pdf;IEEE Xplore Abstract
               Record:/Users/apodusenko/Zotero/storage/ZM9M3STU/login.html:text/html}
}

@article{dickey_weighted_1971,
  title    = {The {Weighted} {Likelihood} {Ratio}, {Linear} {Hypotheses} on {Normal
              } {Location} {Parameters}},
  volume   = {42},
  issn     = {0003-4851},
  doi      = {10.1214/aoms/1177693507},
  language = {en},
  number   = {1},
  urldate  = {2015-06-11},
  journal  = {The Annals of Mathematical Statistics},
  author   = {Dickey, James M.},
  month    = feb,
  year     = {1971},
  pages    = {204--223},
  file     = {
              Snapshot:/Users/apodusenko/Zotero/storage/6JFD465C/1177693507.html:text/html
              }
}

@misc{skilling_this_2008,
  title  = {This {Physicist}'s view of {Gelman}'s {Bayes}},
  url    = {http://www.mrao.cam.ac.uk/~steve/maxent2009/images/rant.pdf},
  author = {Skilling, John},
  year   = {2008},
  file   = {Skilling - 2008 - This Physicist's view of Gelman's
            Bayes.pdf:/Users/apodusenko/Zotero/storage/N4DZZ9IW/Skilling - 2008 -
            This Physicist's view of Gelman's Bayes.pdf:application/pdf}
}

@article{dalton_impact_2003,
  title   = {The impact of hearing loss on quality of life in older adults},
  volume  = {43},
  number  = {5},
  journal = {The Gerontologist},
  author  = {Dalton, Dayna S. and Cruickshanks, Karen J. and Klein, Barbara EK
             and Klein, Ronald and Wiley, Terry L. and Nondahl, David M.},
  year    = {2003},
  pages   = {661--668}
}

@article{moore_choice_2008,
  title      = {The {Choice} of {Compression} {Speed} in {Hearing} {Aids}: {
                Theoretical} and {Practical} {Considerations} and the {Role} of {
                Individual} {Differences}},
  volume     = {12},
  issn       = {1084-7138},
  shorttitle = {The {Choice} of {Compression} {Speed} in {Hearing} {Aids}},
  url        = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4111434/},
  doi        = {10.1177/1084713808317819},
  abstract   = {Compression is used in hearing aids to compensate for the effects
                of loudness recruitment. This article describes the distinction
                between, and relative merits of, slow and fast compression systems.
                A study of Gatehouse and coworkers leads to the following
                conclusions: (a) The benefit from compression is greatest among
                individuals who experience a wide range of sound levels within
                short periods of time, (b) slow compression generally leads to
                higher listening comfort than fast compression, (c) the benefit
                from fast compression varies across individuals, and those with
                high cognitive ability are able to benefit from fast compression to
                take advantage of temporal dips in a background sound. It is argued
                that listening in the dips depends on the ability to process the
                temporal fine structure of sounds. It is proposed that a test of
                the ability to process temporal fine structure might be useful for
                selecting compression speed for an individual.},
  number     = {2},
  urldate    = {2016-06-18},
  journal    = {Trends in Amplification},
  author     = {Moore, Brian C. J.},
  year       = {2008},
  pmid       = {18567591},
  pmcid      = {PMC4111434},
  pages      = {103--112},
  file       = {Moore - 2008 - The Choice of Compression Speed in Hearing Aids
                T.pdf:/Users/apodusenko/Zotero/storage/WIX3VQZ7/Moore - 2008 - The
                Choice of Compression Speed in Hearing Aids
                T.pdf:application/pdf;PubMed Central Full Text
                PDF:/Users/apodusenko/Zotero/storage/4A7JPBGB/Moore - 2008 - The Choice
                of Compression Speed in Hearing Aids T.pdf:application/pdf}
}

@article{killion_3_1993,
  title   = {The 3 types of sensorineural hearing loss: {Loudness} and
             intelligibility considerations},
  volume  = {46},
  journal = {Hearing journal},
  author  = {Killion, MEAD C. and Fikret-Pasa, SELDA},
  year    = {1993},
  pages   = {31--31},
  file    = {Killion and Fikret-Pasa - 1993 - The 3 Types of Sensorineural Hearing
             Loss Loudnes.pdf:/Users/apodusenko/Zotero/storage/CH2X5945/Killion and
             Fikret-Pasa - 1993 - The 3 Types of Sensorineural Hearing Loss
             Loudnes.pdf:application/pdf}
}

@article{swingler_simple_1993,
  title    = {Simple approximations to the {Cramer}-{Rao} lower bound on direction
              of arrival for closely spaced sources},
  volume   = {41},
  issn     = {1053-587X},
  doi      = {10.1109/78.212739},
  abstract = {It is demonstrated that the Cramer-Rao lower bound (CRLB) on the
              directions of arrival (DOAs) of two closely spaced uncorrelated
              plane wave sources impinging on a uniform line array can be
              approximated by simple algebraic formulas which provide insight
              into the behavior of the bound with respect to signal-to-noise
              ratios (SNRs), source separation, etc. These formulas are
              sufficiently accurate for general use with the number of sensors
              between about 6 and 60, for source separations down to about 0.1 of
              a classical beamwidth, and for SNRs from -30 to +30 dB},
  number   = {4},
  journal  = {IEEE Transactions on Signal Processing},
  author   = {Swingler, D. N.},
  month    = apr,
  year     = {1993},
  keywords = {matrices, Signal to noise ratio, Covariance matrix, Narrowband,
              SNR, Gaussian noise, Array signal processing, Cramer-Rao lower
              bound, sensor arrays, source separation, Direction of arrival,
              algebraic formulas, approximations, closely spaced sources, closely
              spaced uncorrelated plane wave sources, CRLB, Direction of arrival
              estimation, matrix algebra, Power engineering and energy, uniform
              line array, Wideband},
  pages    = {1668--1672},
  file     = {IEEE Xplore Abstract
              Record:/Users/apodusenko/Zotero/storage/IAXBRMM5/212739.html:text/html;Swingler
              - 1993 - Simple approximations to the Cramer-Rao lower
              boun.pdf:/Users/apodusenko/Zotero/storage/QZCM9AHM/Swingler - 1993 -
              Simple approximations to the Cramer-Rao lower boun.pdf:application/pdf}
}

@article{sato_online_2001,
  title    = {Online {Model} {Selection} {Based} on the {Variational} {Bayes}},
  volume   = {13},
  issn     = {0899-7667},
  url      = {http://dx.doi.org/10.1162/089976601750265045},
  doi      = {10.1162/089976601750265045},
  abstract = {The Bayesian framework provides a principled way of model
              selection. This framework estimates a probability distribution over
              an ensemble of models, and the prediction is done by averaging over
              the ensemble of models. Accordingly, the uncertainty of the models
              is taken into account, and complex models with more degrees of
              freedom are penalized. However, integration over model parameters
              is often intractable, and some approximation scheme is needed.},
  number   = {7},
  urldate  = {2015-09-08},
  journal  = {Neural Computation},
  author   = {Sato, Masa-aki},
  month    = jul,
  year     = {2001},
  pages    = {1649--1681},
  file     = {
              089976601750265045.pdf:/Users/apodusenko/Zotero/storage/UIK87KP8/089976601750265045.pdf:application/pdf;Neural
              Computation
              Snapshot:/Users/apodusenko/Zotero/storage/TV889UVS/089976601750265045.html:text/html;Sato
              - 2001 - Online Model Selection Based on the Variational
              Ba.pdf:/Users/apodusenko/Zotero/storage/EU3SXJEJ/Sato - 2001 - Online
              Model Selection Based on the Variational Ba.pdf:application/pdf}
}

@inproceedings{lake_one-shot_2014,
  title   = {One-shot learning of generative speech concepts},
  url     = {http://www.bibsonomy.org/bibtex/26aa05340224719776c4d3017aa6ed882/dblp},
  urldate = {2017-03-07},
  author  = {Lake, Brenden M. and Lee, Chia-ying and Glass, James and Tenenbaum,
             Joshua B.},
  year    = {2014},
  file    = {Lake et al. - 2014 - One-shot learning of generative speech
             concepts.pdf:/Users/apodusenko/Zotero/storage/95UDER32/Lake et al. -
             2014 - One-shot learning of generative speech
             concepts.pdf:application/pdf}
}

@article{lomakina_inversion_2015,
  title    = {Inversion of hierarchical {Bayesian} models using {Gaussian}
              processes},
  volume   = {118},
  issn     = {1053-8119},
  url      = {http://www.sciencedirect.com/science/article/pii/S1053811915004747},
  doi      = {10.1016/j.neuroimage.2015.05.084},
  abstract = {Over the past decade, computational approaches to neuroimaging
              have increasingly made use of hierarchical Bayesian models (HBMs),
              either for inferring on physiological mechanisms underlying fMRI
              data (e.g., dynamic causal modelling, DCM) or for deriving
              computational trajectories (from behavioural data) which serve as
              regressors in general linear models. However, an unresolved problem
              is that standard methods for inverting the hierarchical Bayesian
              model are either very slow, e.g. Markov Chain Monte Carlo Methods
              (MCMC), or are vulnerable to local minima in non-convex
              optimisation problems, such as variational Bayes (VB). This article
              considers Gaussian process optimisation (GPO) as an alternative
              approach for global optimisation of sufficiently smooth and
              efficiently evaluable objective functions. GPO avoids being trapped
              in local extrema and can be computationally much more efficient
              than MCMC. Here, we examine the benefits of GPO for inverting HBMs
              commonly used in neuroimaging, including DCM for fMRI and the
              Hierarchical Gaussian Filter (HGF). Importantly, to achieve
              computational efficiency despite high-dimensional optimisation
              problems, we introduce a novel combination of GPO and local
              gradient-based search methods. The utility of this GPO
              implementation for DCM and HGF is evaluated against MCMC and VB,
              using both synthetic data from simulations and empirical data. Our
              results demonstrate that GPO provides parameter estimates with
              equivalent or better accuracy than the other techniques, but at a
              fraction of the computational cost required for MCMC. We anticipate
              that GPO will prove useful for robust and efficient inversion of
              high-dimensional and nonlinear models of neuroimaging data.},
  urldate  = {2015-09-03},
  journal  = {NeuroImage},
  author   = {Lomakina, Ekaterina I. and Paliwal, Saee and Diaconescu, Andreea O.
              and Brodersen, Kay H. and Aponte, Eduardo A. and Buhmann, Joachim M.
              and Stephan, Klaas E.},
  month    = sep,
  year     = {2015},
  keywords = {Bayesian inference, Gaussian processes, Dynamic causal modelling,
              Global optimisation, Hierarchical models, MCMC},
  pages    = {133--145},
  file     = {Lomakina et al. - 2015 - Inversion of hierarchical Bayesian models
              using Ga.pdf:/Users/apodusenko/Zotero/storage/7V7HVZGD/Lomakina et al.
              - 2015 - Inversion of hierarchical Bayesian models using
              Ga.pdf:application/pdf;Lomakina et al. - 2015 - Inversion of
              hierarchical Bayesian models using
              Ga.pdf:/Users/apodusenko/Zotero/storage/IQXGMNAB/Lomakina et al. - 2015
              - Inversion of hierarchical Bayesian models using
              Ga.pdf:application/pdf;ScienceDirect
              Snapshot:/Users/apodusenko/Zotero/storage/FZ7M2NPC/S1053811915004747.html:text/html;ScienceDirect
              Snapshot:/Users/apodusenko/Zotero/storage/AQ34E4FQ/S1053811915004747.html:text/html
              }
}

@inproceedings{kristjansson_high_2003,
  title     = {High resolution signal reconstruction},
  url       = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1318456},
  urldate   = {2015-12-29},
  booktitle = {Automatic {Speech} {Recognition} and {Understanding}, 2003. {ASRU
               }'03. 2003 {IEEE} {Workshop} on},
  publisher = {IEEE},
  author    = {Kristjansson, Trausti and Hershey, John},
  year      = {2003},
  pages     = {291--296},
  file      = {Kristjansson and Hershey - 2003 - High resolution signal
               reconstruction.pdf:/Users/apodusenko/Zotero/storage/BAH5BGRD/Kristjansson
               and Hershey - 2003 - High resolution signal
               reconstruction.pdf:application/pdf;Kristjansson and Hershey - 2003 -
               High resolution signal
               reconstruction.pdf:/Users/apodusenko/Zotero/storage/4QJSDQSP/Kristjansson
               and Hershey - 2003 - High resolution signal
               reconstruction.pdf:application/pdf;Kristjansson and Hershey - 2003 -
               High resolution signal
               reconstruction.pdf:/Users/apodusenko/Zotero/storage/ND2H3S6T/Kristjansson
               and Hershey - 2003 - High resolution signal
               reconstruction.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/N4BVERJF/login.html:text/html;Snapshot:/Users/apodusenko/Zotero/storage/HAA8UICX/abs_all.html:text/html;Snapshot:/Users/apodusenko/Zotero/storage/22QXI8ZT/abs_all.html:text/html
               }
}

@article{deisenroth_expectation_2012,
  title      = {Expectation {Propagation} in {Gaussian} {Process} {Dynamical} {
                Systems}: {Extended} {Version}},
  shorttitle = {Expectation {Propagation} in {Gaussian} {Process} {Dynamical} {
                Systems}},
  url        = {http://arxiv.org/abs/1207.2940},
  abstract   = {Rich and complex time-series data, such as those generated from
                engineering systems, financial markets, videos or neural recordings
                , are now a common feature of modern data analysis. Explaining the
                phenomena underlying these diverse data sets requires flexible and
                accurate models. In this paper, we promote Gaussian process
                dynamical systems (GPDS) as a rich model class that is appropriate
                for such analysis. In particular, we present a message passing
                algorithm for approximate inference in GPDSs based on expectation
                propagation. By posing inference as a general message passing
                problem, we iterate forward-backward smoothing. Thus, we obtain
                more accurate posterior distributions over latent structures,
                resulting in improved predictive performance compared to
                state-of-the-art GPDS smoothers, which are special cases of our
                general message passing algorithm. Hence, we provide a unifying
                approach within which to contextualize message passing in GPDSs.},
  urldate    = {2014-04-10},
  journal    = {arXiv:1207.2940 [cs, stat]},
  author     = {Deisenroth, Marc Peter and Mohamed, Shakir},
  month      = jul,
  year       = {2012},
  file       = {arXiv.org
                Snapshot:/Users/apodusenko/Zotero/storage/48FA2577/1207.html:text/html;Deisenroth
                and Mohamed - 2012 - Expectation Propagation in Gaussian Process
                Dynami.pdf:/Users/apodusenko/Zotero/storage/QB36SNZC/Deisenroth and
                Mohamed - 2012 - Expectation Propagation in Gaussian Process
                Dynami.pdf:application/pdf}
}

@article{beaulieu_optimal_2004,
  title    = {An optimal lognormal approximation to lognormal sum distributions},
  volume   = {53},
  url      = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1275712},
  number   = {2},
  urldate  = {2015-11-03},
  journal  = {Vehicular Technology, IEEE Transactions on},
  author   = {Beaulieu, Norman C. and Xie, Qiong},
  year     = {2004},
  keywords = {lognormal},
  pages    = {479--489},
  file     = {[PDF] from webs.com:/Users/apodusenko/Zotero/storage/6H2JM3JC/Beaulieu
              and Xie - 2004 - An optimal lognormal approximation to lognormal
              su.pdf:application/pdf;Beaulieu and Xie - 2004 - An optimal lognormal
              approximation to lognormal
              su.pdf:/Users/apodusenko/Zotero/storage/3XRK3V7T/Beaulieu and Xie -
              2004 - An optimal lognormal approximation to lognormal
              su.pdf:application/pdf;Beaulieu and Xie - 2004 - An optimal lognormal
              approximation to lognormal
              su.pdf:/Users/apodusenko/Zotero/storage/4SIPPEFJ/Beaulieu and Xie -
              2004 - An optimal lognormal approximation to lognormal
              su.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/SKT5ZVI8/login.html:text/html;Snapshot:/Users/apodusenko/Zotero/storage/MVA788M9/abs_all.html:text/html;Snapshot:/Users/apodusenko/Zotero/storage/MRQAI7H5/login.html:text/html
              }
}

@incollection{chapelle_empirical_2011,
  title     = {An {Empirical} {Evaluation} of {Thompson} {Sampling}},
  url       = {
               http://papers.nips.cc/paper/4321-an-empirical-evaluation-of-thompson-sampling.pdf
               },
  urldate   = {2015-03-03},
  booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 24},
  publisher = {Curran Associates, Inc.},
  author    = {Chapelle, Olivier and Li, Lihong},
  editor    = {Shawe-Taylor, J. and Zemel, R. S. and Bartlett, P. L. and Pereira,
               F. and Weinberger, K. Q.},
  year      = {2011},
  pages     = {2249--2257},
  file      = {Chapelle and Li - 2011 - An Empirical Evaluation of Thompson
               Sampling.pdf:/Users/apodusenko/Zotero/storage/P7FKDJWA/Chapelle and Li
               - 2011 - An Empirical Evaluation of Thompson
               Sampling.pdf:application/pdf;NIPS
               Snapshort:/Users/apodusenko/Zotero/storage/SHFDVFVQ/4321-an-empirical-evaluation-of-thompson-sampling.html:text/html
               }
}

@article{deliang_wang_computational_2008,
  title  = {A {Computational} {Auditory} {Scene} {Analysis}},
  url    = {http://web.cse.ohio-state.edu/~wang.77/papers/SSJW.csl10.pdf},
  author = {DeLiang Wang, Yang Shao},
  month  = mar,
  year   = {2008},
  file   = {AuditorySceneAnalysis,
            Brown.pdf:/Users/apodusenko/Zotero/storage/MAN3WA79/SSJW.csl10.pdf:application/pdf
            }
}

@inproceedings{droppo_comparison_2003,
  title     = {A comparison of three non-linear observation models for noisy speech
               features.},
  url       = {http://research.microsoft.com/pubs/56956/2003-jasha-eurospeech.pdf},
  urldate   = {2015-12-30},
  booktitle = {{INTERSPEECH}},
  author    = {Droppo, Jasha and Deng, Li and Acero, Alex},
  year      = {2003},
  file      = {Droppo et al. - 2003 - A comparison of three non-linear observation
               model.pdf:/Users/apodusenko/Zotero/storage/UIJTQGMD/Droppo et al. -
               2003 - A comparison of three non-linear observation
               model.pdf:application/pdf;Droppo et al. - 2003 - A comparison of three
               non-linear observation
               model.pdf:/Users/apodusenko/Zotero/storage/9AAEWWTP/Droppo et al. -
               2003 - A comparison of three non-linear observation
               model.pdf:application/pdf}
}

@inproceedings{reller_glue_2012,
  title     = {Glue factors, likelihood computation, and filtering in state space
               models},
  doi       = {10.1109/Allerton.2012.6483284},
  abstract  = {Factor graphs of statistical models can be augmented by a glue
               factor that expresses some additional (initial, final, or otherwise
               “local”) condition. That applies, in particular, to (otherwise
               time-invariant) linear Gaussian state space models, which are thus
               generalized to pulse-like models that are localized anywhere in
               time. The model likelihood can then be computed by
               (forward-backward or forward-only) sum-product message passing,
               which leads to the concept of a likelihood filter. We propose to
               build (forward-only) likelihood filters from a bank of second-order
               linear systems. We also observe that such likelihood filters can be
               cascaded into a new sort of neural network that works naturally
               with multichannel time signals at multiple time scales.},
  booktitle = {2012 50th {Annual} {Allerton} {Conference} on {Communication}, {
               Control}, and {Computing} ({Allerton})},
  author    = {Reller, C. and Devarakonda, M.V.R.S. and Loeliger, H.-A.},
  month     = oct,
  year      = {2012},
  keywords  = {Message passing, factor graphs, graph theory, Probability, neural
               network, neural nets, filtering theory, Gaussian processes, hidden
               Markov models, Switches, filtering, computational modeling, glue
               factors, likelihood computation, likelihood filter, linear Gaussian
               state space models, multichannel time signals, pulse-like models,
               statistical analysis, statistical models},
  pages     = {686--689},
  file      = {IEEE Xplore Abstract
               Record:/Users/apodusenko/Zotero/storage/3FHQ3XGA/abs_all.html:text/html;Reller
               et al. - 2012 - Glue factors, likelihood computation, and
               filterin.pdf:/Users/apodusenko/Zotero/storage/L965JDIF/Reller et al. -
               2012 - Glue factors, likelihood computation, and
               filterin.pdf:application/pdf}
}

@article{friston_what_2012,
  title    = {What is value-accumulated reward or evidence?},
  volume   = {6},
  issn     = {1662-5218},
  doi      = {10.3389/fnbot.2012.00011},
  abstract = {Why are you reading this abstract? In some sense, your answer will
              cast the exercise as valuable-but what is value? In what follows,
              we suggest that value is evidence or, more exactly, log Bayesian
              evidence. This implies that a sufficient explanation for valuable
              behavior is the accumulation of evidence for internal models of our
              world. This contrasts with normative models of optimal control and
              reinforcement learning, which assume the existence of a value
              function that explains behavior, where (somewhat tautologically)
              behavior maximizes value. In this paper, we consider an alternative
              formulation-active inference-that replaces policies in normative
              models with prior beliefs about the (future) states agents should
              occupy. This enables optimal behavior to be cast purely in terms of
              inference: where agents sample their sensorium to maximize the
              evidence for their generative model of hidden states in the world,
              and minimize their uncertainty about those states. Crucially, this
              formulation resolves the tautology inherent in normative models and
              allows one to consider how prior beliefs are themselves optimized
              in a hierarchical setting. We illustrate these points by showing
              that any optimal policy can be specified with prior beliefs in the
              context of Bayesian inference. We then show how these prior beliefs
              are themselves prescribed by an imperative to minimize uncertainty.
              This formulation explains the saccadic eye movements required to
              read this text and defines the value of the visual sensations you
              are soliciting.},
  language = {eng},
  journal  = {Frontiers in Neurorobotics},
  author   = {Friston, Karl and Adams, Rick and Montague, Read},
  year     = {2012},
  pmid     = {23133414},
  pmcid    = {PMC3487150},
  keywords = {Active inference, Bayesian, Free energy, surprise,
              self-organization, evidence, value, selection},
  pages    = {11},
  file     = {Friston et al. - 2012 - What is value-accumulated reward or
              evidence.pdf:/Users/apodusenko/Zotero/storage/FMDESS4V/Friston et al. -
              2012 - What is value-accumulated reward or evidence.pdf:application/pdf
              }
}

@article{mirza_scene_2016,
  title    = {Scene {Construction}, {Visual} {Foraging}, and {Active} {Inference}},
  url      = {http://journal.frontiersin.org/article/10.3389/fncom.2016.00056/full},
  doi      = {10.3389/fncom.2016.00056},
  abstract = {This paper describes an active inference scheme for visual
              searches and the perceptual synthesis entailed by scene
              construction. Active inference assumes that perception and action
              minimize variational free energy, where actions are selected to
              minimize the free energy expected in the future. This assumption
              generalizes risk-sensitive control and expected utility theory to
              include epistemic value; namely, the value (or salience) of
              information inherent in resolving uncertainty about the causes of
              ambiguous cues or outcomes. Here, we apply active inference to
              saccadic searches of a visual scene. We consider the (difficult)
              problem of categorizing a scene, based on the spatial relationship
              among visual objects where, crucially, visual cues are sampled
              myopically through a sequence of saccadic eye movements. This means
              that evidence for competing hypotheses about the scene has to be
              accumulated sequentially, calling upon both prediction (planning)
              and postdiction (memory). Our aim is to highlight some simple but
              fundamental aspects of the requisite functional anatomy; namely,
              the link between approximate Bayesian inference under mean field
              assumptions and functional segregation in the visual cortex. This
              link rests upon the (neurobiologically plausible) process theory
              that accompanies the normative formulation of active inference for
              Markov decision processes. In future work, we hope to use this
              scheme to model empirical saccadic searches and identify the prior
              beliefs that underwrite intersubject variability in the way people
              forage for information in visual scenes (e.g., in schizophrenia).},
  urldate  = {2016-08-03},
  journal  = {Frontiers in Computational Neuroscience},
  author   = {Mirza, M. Berk and Adams, Rick A. and Mathys, Christoph D. and
              Friston, Karl J.},
  year     = {2016},
  keywords = {Bayesian inference, Active inference, Free energy, Epistemic value
              , Information gain, salience, scene construction, visual search},
  pages    = {56},
  file     = {Mirza et al. - 2016 - Scene Construction, Visual Foraging, and Active
              In.pdf:/Users/apodusenko/Zotero/storage/HCGASFYG/Mirza et al. - 2016 -
              Scene Construction, Visual Foraging, and Active In.pdf:application/pdf}
}

@article{ruckert_learned_2013,
  title    = {Learned graphical models for probabilistic planning provide a new
              class of movement primitives},
  volume   = {6},
  url      = {
              http://journal.frontiersin.org/article/10.3389/fncom.2012.00097/abstract
              },
  doi      = {10.3389/fncom.2012.00097},
  abstract = {Biological movement generation combines three interesting aspects:
              its modular organization in movement primitives (MPs), its
              characteristics of stochastic optimality under perturbations, and
              its efficiency in terms of learning. A common approach to motor
              skill learning is to endow the primitives with dynamical systems.
              Here, the parameters of the primitive indirectly define the shape
              of a reference trajectory. We propose an alternative MP
              representation based on probabilistic inference in learned
              graphical models with new and interesting properties that complies
              with salient features of biological movement control. Instead of
              endowing the primitives with dynamical systems, we propose to endow
              MPs with an intrinsic probabilistic planning system, integrating
              the power of stochastic optimal control (SOC) methods within a MP.
              The parameterization of the primitive is a graphical model that
              represents the dynamics and intrinsic cost function such that
              inference in this graphical model yields the control policy. We
              parameterize the intrinsic cost function using task-relevant
              features, such as the importance of passing through certain
              via-points. The system dynamics as well as intrinsic cost function
              parameters are learned in a reinforcement learning (RL) setting. We
              evaluate our approach on a complex 4-link balancing task. Our
              experiments show that our movement representation facilitates
              learning significantly and leads to better generalization to new
              task settings without re-learning.},
  urldate  = {2015-04-01},
  journal  = {Frontiers in Computational Neuroscience},
  author   = {Rückert, Elmar A. and Neumann, Gerhard and Toussaint, Marc and Maass
              , Wolfgang},
  year     = {2013},
  keywords = {Graphical models, reinforcement learning, optimal control, motor
              planning, movement primitives},
  pages    = {97},
  file     = {Rückert et al. - 2013 - Learned graphical models for probabilistic
              plannin.pdf:/Users/apodusenko/Zotero/storage/QNK6GYDX/Rückert et al. -
              2013 - Learned graphical models for probabilistic
              plannin.pdf:application/pdf}
}

@article{fitzgerald_dopamine_2015,
  title    = {Dopamine, reward learning, and active inference},
  url      = {
              http://journal.frontiersin.org/article/10.3389/fncom.2015.00136/abstract
              },
  doi      = {10.3389/fncom.2015.00136},
  abstract = {Temporal difference learning models propose phasic dopamine
              signaling encodes reward prediction errors that drive learning.
              This is supported by studies where optogenetic stimulation of
              dopamine neurons can stand in lieu of actual reward. Nevertheless,
              a large body of data also shows that dopamine is not necessary for
              learning, and that dopamine depletion primarily affects task
              performance. We offer a resolution to this paradox based on an
              hypothesis that dopamine encodes the precision of beliefs about
              alternative actions, and thus controls the outcome-sensitivity of
              behavior. We extend an active inference scheme for solving Markov
              decision processes to include learning, and show that simulated
              dopamine dynamics strongly resemble those actually observed during
              instrumental conditioning. Furthermore, simulated dopamine
              depletion impairs performance but spares learning, while simulated
              excitation of dopamine neurons drives reward learning, through
              aberrant inference about outcome states. Our formal approach
              provides a novel and parsimonious reconciliation of apparently
              divergent experimental findings.},
  urldate  = {2015-11-06},
  journal  = {Frontiers in Computational Neuroscience},
  author   = {FitzGerald, Thomas H. B. and Dolan, Raymond J. and Friston, Karl},
  year     = {2015},
  keywords = {Active inference, dopamine, Reward, incentive salience,
              instrumental conditioning, reward learning, variational inference},
  pages    = {136},
  file     = {FitzGerald et al. - 2015 - Dopamine, reward learning, and active
              inference.pdf:/Users/apodusenko/Zotero/storage/GUUFCR4X/FitzGerald et
              al. - 2015 - Dopamine, reward learning, and active
              inference.pdf:application/pdf}
}

@article{penny_forward_2013,
  title    = {Forward and {Backward} {Inference} in {Spatial} {Cognition}},
  volume   = {9},
  issn     = {1553-7358},
  url      = {
              http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1003383
              },
  doi      = {10.1371/journal.pcbi.1003383},
  abstract = {Author Summary The ability of mammals to navigate is well studied,
              both behaviourally and in terms on the underlying neurophysiology.
              Navigation is a well studied topic in computational fields such as
              machine learning and signal processing. However, studies in
              computational neuroscience, which draw together these findings,
              have mainly focused on specific navigation tasks such as spatial
              localisation. In this paper, we propose a single probabilistic
              model which can support multiple tasks, from working out which
              environment you are in, to computing a sequence of motor commands
              that will take you to a sensory goal, such as being warm or viewing
              a particular object. We describe how these tasks can be implemented
              using a common set of lower level algorithms that implement
              ‘forward and backward inference over time’. We relate these
              algorithms to recent findings in animal electrophysiology, where
              sequences of hippocampal cell activations are observed before,
              during or after a navigation task, and these sequences are played
              either forwards or backwards. Additionally, one function of the
              hippocampus that is preserved across mammals is that it integrates
              spatial and non-spatial information, and we propose how the forward
              and backward inference algorithms naturally map onto this
              architecture.},
  number   = {12},
  urldate  = {2016-07-19},
  journal  = {PLOS Comput Biol},
  author   = {Penny, Will D. and Zeidman, Peter and Burgess, Neil},
  month    = dec,
  year     = {2013},
  keywords = {Machine learning algorithms, decision making, Covariance,
              Hippocampus, Agent-based modeling, Memory, Sensory cues},
  pages    = {e1003383},
  file     = {Penny et al. - 2013 - Forward and Backward Inference in Spatial
              Cognitio.pdf:/Users/apodusenko/Zotero/storage/3D6MURN6/Penny et al. -
              2013 - Forward and Backward Inference in Spatial
              Cognitio.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/T8M2VKU6/article.html:text/html
              }
}

@article{schenkendorf_online_2013,
  title    = {Online model selection approach based on {Unscented} {Kalman} {
              Filtering}},
  volume   = {23},
  issn     = {0959-1524},
  url      = {http://www.sciencedirect.com/science/article/pii/S0959152412002387},
  doi      = {10.1016/j.jprocont.2012.10.009},
  abstract = {Highly predictive mathematical models are of inestimable value in
              systems biology. Their application ranges from investigations of
              basic processes in living organisms up to model based drug design
              in the field of pharmacology. For the development of reliable
              models suitable model candidates and related model parameters have
              to be identified by minimising the difference between the model
              outcome and available measurement data. Due to the complexity of
              the analysed processes mathematical models capture only the
              essential features of interest. This approximate representation,
              which is usually combined with a vague knowledge of basic processes
              , leads in many cases to a variety of potential model candidates
              describing the real process almost equally well. To determine the
              most plausible model candidate is the objective of model selection
              or model discrimination methods. If under given operation
              conditions no sufficient discrimination can be achieved, Optimal
              Experimental Designs (OED) comes into play. OED searches for
              operation conditions which facilitate the overall selection
              process. In this work an online model selection framework is
              presented. Here, the Unscented Kalman Filter (UKF) provides
              statistical information which is used to assign probability values
              to every model candidate. These probability values are immediately
              updated as soon as new measurement data become available. In
              addition, during the experimental run the process is steered in a
              fashion which maximises the differences in these candidates. To
              overcome limitations caused by parameter uncertainties the most
              sensitive model parameters are simultaneously estimated in the
              course of the model selection framework. The combined application
              of the online framework and the joint estimation of sensitive model
              parameters provides a very efficient usage of measurement data
              reducing the overall number of experiments. The method is
              demonstrated for a well known motif in signalling pathways, the
              mitogen-activated protein (MAP) kinase.},
  number   = {1},
  urldate  = {2015-10-05},
  journal  = {Journal of Process Control},
  author   = {Schenkendorf, René and Mangold, Michael},
  month    = jan,
  year     = {2013},
  keywords = {Kalman filter, Model selection, Online design, Optimal
              Experimental Design},
  pages    = {44--57},
  file     = {ScienceDirect
              Snapshot:/Users/apodusenko/Zotero/storage/9QM46ZK8/S0959152412002387.html:text/html
              }
}

@article{heng_nature_2014,
  title    = {The {Nature} of {Scientific} {Proof} in the {Age} of {Simulations}},
  url      = {http://arxiv.org/abs/1404.6248},
  abstract = {Is numerical mimicry a third way of establishing truth?},
  urldate  = {2015-05-11},
  journal  = {arXiv:1404.6248 [astro-ph]},
  author   = {Heng, Kevin},
  month    = apr,
  year     = {2014},
  note     = {arXiv: 1404.6248},
  keywords = {Astrophysics - Instrumentation and Methods for Astrophysics,
              Astrophysics - Earth and Planetary Astrophysics},
  file     = {arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/XAXDZ5N6/1404.html:text/html;Heng
              - 2014 - The Nature of Scientific Proof in the Age of
              Simul.pdf:/Users/apodusenko/Zotero/storage/WPZBD9XA/Heng - 2014 - The
              Nature of Scientific Proof in the Age of Simul.pdf:application/pdf}
}

@book{karny_optimized_2006,
  address   = {London},
  series    = {Advanced {Information} and {Knowledge} {Processing}},
  title     = {Optimized {Bayesian} {Dynamic} {Advising}},
  isbn      = {1-85233-928-4},
  url       = {http://link.springer.com/10.1007/1-84628-254-3},
  language  = {en},
  urldate   = {2015-12-17},
  publisher = {Springer-Verlag},
  author    = {Karny, MIroslav},
  year      = {2006},
  file      = {Karny - 2006 - Optimized Bayesian Dynamic
               Advising.pdf:/Users/apodusenko/Zotero/storage/QIEZWWL7/Karny - 2006 -
               Optimized Bayesian Dynamic Advising.pdf:application/pdf}
}

@incollection{rombouts_neurally_2012,
  title     = {Neurally {Plausible} {Reinforcement} {Learning} of {Working} {Memory}
               {Tasks}},
  url       = {
               http://papers.nips.cc/paper/4813-neurally-plausible-reinforcement-learning-of-working-memory-tasks.pdf
               },
  urldate   = {2015-09-17},
  booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 25},
  publisher = {Curran Associates, Inc.},
  author    = {Rombouts, Jaldert and Roelfsema, Pieter and Bohte, Sander M.},
  editor    = {Pereira, F. and Burges, C. J. C. and Bottou, L. and Weinberger, K.
               Q.},
  year      = {2012},
  pages     = {1871--1879},
  file      = {NIPS
               Snapshort:/Users/apodusenko/Zotero/storage/6MTAL7H9/4813-neurally-plausible-reinforcement-learning-of-working-memory-tasks.html:text/html;Rombouts
               et al. - 2012 - Neurally Plausible Reinforcement Learning of
               Worki.pdf:/Users/apodusenko/Zotero/storage/VISZWV2A/Rombouts et al. -
               2012 - Neurally Plausible Reinforcement Learning of
               Worki.pdf:application/pdf}
}

@article{blundell_weight_2015,
  title    = {Weight {Uncertainty} in {Neural} {Networks}},
  url      = {http://arxiv.org/abs/1505.05424},
  abstract = {We introduce a new, efficient, principled and
              backpropagation-compatible algorithm for learning a probability
              distribution on the weights of a neural network, called Bayes by
              Backprop. It regularises the weights by minimising a compression
              cost, known as the variational free energy or the expected lower
              bound on the marginal likelihood. We show that this principled kind
              of regularisation yields comparable performance to dropout on MNIST
              classification. We then demonstrate how the learnt uncertainty in
              the weights can be used to improve generalisation in non-linear
              regression problems, and how this weight uncertainty can be used to
              drive the exploration-exploitation trade-off in reinforcement
              learning.},
  urldate  = {2015-09-17},
  journal  = {arXiv:1505.05424 [cs, stat]},
  author   = {Blundell, Charles and Cornebise, Julien and Kavukcuoglu, Koray and
              Wierstra, Daan},
  month    = may,
  year     = {2015},
  note     = {arXiv: 1505.05424},
  keywords = {Computer Science - Learning, Statistics - Machine Learning},
  file     = {arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/BDEFVE8J/1505.html:text/html;Blundell
              et al. - 2015 - Weight Uncertainty in Neural
              Networks.pdf:/Users/apodusenko/Zotero/storage/YAV2DTGY/Blundell et al.
              - 2015 - Weight Uncertainty in Neural Networks.pdf:application/pdf}
}

@misc{kulkarni_variational_2016,
  title  = {Variational {Particle} {Approximations}},
  author = {Kulkarni, Tejas D. and Saeedi, Ardavan and Gershman, Samuel J.},
  year   = {2016},
  file   = {Kulkarni et al. - 2016 - Variational Particle
            Approximations.pdf:/Users/apodusenko/Zotero/storage/23A6FRMP/Kulkarni
            et al. - 2016 - Variational Particle Approximations.pdf:application/pdf
            }
}

@book{tantau_tikz_2013,
  title  = {The {TikZ} and {PGF} {Packages}: {Manual} for version 3.0.0},
  url    = {http://sourceforge.net/projects/pgf/},
  author = {Tantau, Till},
  month  = dec,
  year   = {2013},
  file   = {Tantau - 2013 - The TikZ and PGF Packages Manual for version
            3.0..pdf:/Users/apodusenko/Zotero/storage/TFCRN3PI/Tantau - 2013 - The
            TikZ and PGF Packages Manual for version 3.0..pdf:application/pdf}
}

@article{pezzulo_principles_2014,
  title      = {The principles of goal-directed decision-making: from neural
                mechanisms to computation and robotics},
  volume     = {369},
  copyright  = {© 2014 The Author(s) Published by the Royal Society. All rights
                reserved.},
  issn       = {0962-8436, 1471-2970},
  shorttitle = {The principles of goal-directed decision-making},
  url        = {http://rstb.royalsocietypublishing.org/content/369/1655/20130470},
  doi        = {10.1098/rstb.2013.0470},
  language   = {en},
  number     = {1655},
  urldate    = {2015-09-17},
  journal    = {Philosophical Transactions of the Royal Society of London B:
                Biological Sciences},
  author     = {Pezzulo, Giovanni and Verschure, Paul F. M. J. and Balkenius,
                Christian and Pennartz, Cyriel M. A.},
  month      = nov,
  year       = {2014},
  pmid       = {25267813},
  pages      = {20130470},
  file       = {Pezzulo et al. - 2014 - The principles of goal-directed
                decision-making f.pdf:/Users/apodusenko/Zotero/storage/9DSD3X7W/Pezzulo
                et al. - 2014 - The principles of goal-directed decision-making
                f.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/XMJALI4B/20130470.html:text/html
                }
}

@article{thompson_likelihood_1933,
  title   = {On the {Likelihood} that {One} {Unknown} {Probability} {Exceeds} {
             Another} in {View} of the {Evidence} of {Two} {Samples}},
  volume  = {25},
  issn    = {0006-3444},
  url     = {http://www.jstor.org/stable/2332286},
  doi     = {10.2307/2332286},
  number  = {3/4},
  urldate = {2015-11-16},
  journal = {Biometrika},
  author  = {Thompson, William R.},
  year    = {1933},
  pages   = {285--294}
}

@article{lindley_measure_1956,
  title    = {On a {Measure} of the {Information} {Provided} by an {Experiment}},
  volume   = {27},
  issn     = {0003-4851, 2168-8990},
  url      = {http://projecteuclid.org/euclid.aoms/1177728069},
  doi      = {10.1214/aoms/1177728069},
  abstract = {A measure is introduced of the information provided by an
              experiment. The measure is derived from the work of Shannon [10]
              and involves the knowledge prior to performing the experiment,
              expressed through a prior probability distribution over the
              parameter space. The measure is used to compare some pairs of
              experiments without reference to prior distributions; this method
              of comparison is contrasted with the methods discussed by
              Blackwell. Finally, the measure is applied to provide a solution to
              some problems of experimental design, where the object of
              experimentation is not to reach decisions but rather to gain
              knowledge about the world.},
  language = {EN},
  number   = {4},
  urldate  = {2015-03-31},
  journal  = {The Annals of Mathematical Statistics},
  author   = {Lindley, D. V.},
  month    = dec,
  year     = {1956},
  mrnumber = {MR83936},
  zmnumber = {0073.14103},
  pages    = {986--1005},
  file     = {Lindley - 1956 - On a Measure of the Information Provided by an
              Exp.pdf:/Users/apodusenko/Zotero/storage/724N6VJB/Lindley - 1956 - On a
              Measure of the Information Provided by an
              Exp.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/8ACYSDFA/1177728069.html:text/html
              }
}

@book{bart_mesman_intelligent_2015,
  title  = {Intelligent {Architectures}},
  author = {{Bart Mesman}},
  year   = {2015},
  file   = {Bart Mesman - 2015 - Intelligent
            Architectures.pdf:/Users/apodusenko/Zotero/storage/3DSP5JMG/Bart Mesman
            - 2015 - Intelligent Architectures.pdf:application/pdf}
}

@article{solway_goal-directed_2012,
  title      = {Goal-directed decision making as probabilistic inference: {A}
                computational framework and potential neural correlates},
  volume     = {119},
  issn       = {0033-295X},
  shorttitle = {Goal-directed decision making as probabilistic inference},
  url        = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3767755/},
  doi        = {10.1037/a0026435},
  abstract   = {Recent work has given rise to the view that reward-based decision
                making is governed by two key controllers: a habit system, which
                stores stimulus-response associations shaped by past reward, and a
                goal-oriented system that selects actions based on their
                anticipated outcomes. The current literature provides a rich body
                of computational theory addressing habit formation, centering on
                temporal-difference learning mechanisms. Less progress has been
                made toward formalizing the processes involved in goal-directed
                decision making. We draw on recent work in cognitive neuroscience,
                animal conditioning, cognitive and developmental psychology and
                machine learning, to outline a new theory of goal-directed decision
                making. Our basic proposal is that the brain, within an
                identifiable network of cortical and subcortical structures,
                implements a probabilistic generative model of reward, and that
                goal-directed decision making is effected through Bayesian
                inversion of this model. We present a set of simulations
                implementing the account, which address benchmark behavioral and
                neuroscientific findings, and which give rise to a set of testable
                predictions. We also discuss the relationship between the proposed
                framework and other models of decision making, including recent
                models of perceptual choice, to which our theory bears a direct
                connection.},
  number     = {1},
  urldate    = {2016-03-03},
  journal    = {Psychological review},
  author     = {Solway, A. and Botvinick, M.},
  month      = jan,
  year       = {2012},
  pmid       = {22229491},
  pmcid      = {PMC3767755},
  pages      = {120--154},
  file       = {Solway and Botvinick - 2012 - Goal-directed decision making as
                probabilistic inf.pdf:/Users/apodusenko/Zotero/storage/C7URVB72/Solway
                and Botvinick - 2012 - Goal-directed decision making as probabilistic
                inf.pdf:application/pdf}
}

@patent{baskent_genetic_2013,
  title       = {Genetic algorithms with subjective input for hearing assistance
                 devices},
  url         = {http://www.google.com/patents/US8559662},
  abstract    = {Disclosed herein, among other things, is an apparatus for fitting
                 a hearing assistance device using a genetic algorithm. The
                 apparatus includes a first population of a plurality of parent sets
                 representing at least one device parameter. A first pair from the
                 parent sets is presented with assistance of the hearing assistance
                 device, the first pair comprising a first and second set. A user
                 selects a preference between the first and second sets. A child set
                 is determined by operating on at least one set of the plurality of
                 parent sets. The child set can include a crossover of the at least
                 one parent set, where the crossover includes an arithmetic or
                 geometrical operation to parameter values of the parent set, or a
                 mutation of the at least one parent set, where the mutation
                 includes replacing a lowest ranked parameter value in the parent
                 set with a randomly generated parameter value.},
  nationality = {United States},
  assignee    = {Starkey Laboratories, Inc.},
  number      = {US8559662 B2},
  urldate     = {2015-02-24},
  author      = {Baskent, Deniz},
  month       = oct,
  year        = {2013},
  note        = {U.S. Classification 381/314, 381/323, 705/315, 381/312, 381/313,
                 705/14.1, 381/23.1, 381/60; International Classification H04R25/00;
                 Cooperative Classification H04R25/70, H04R25/505},
  file        = {Google Patents PDF:/Users/apodusenko/Zotero/storage/SI7RAL8A/Baskent -
                 2013 - Genetic algorithms with subjective input for
                 heari.pdf:application/pdf}
}

@article{cooray_dynamic_2016,
  title    = {Dynamic causal modelling of electrographic seizure activity using {
              Bayesian} belief updating},
  volume   = {125},
  issn     = {1053-8119},
  url      = {http://www.sciencedirect.com/science/article/pii/S1053811915006837},
  doi      = {10.1016/j.neuroimage.2015.07.063},
  abstract = {Seizure activity in EEG recordings can persist for hours with
              seizure dynamics changing rapidly over time and space. To
              characterise the spatiotemporal evolution of seizure activity,
              large data sets often need to be analysed. Dynamic causal modelling
              (DCM) can be used to estimate the synaptic drivers of cortical
              dynamics during a seizure; however, the requisite (Bayesian)
              inversion procedure is computationally expensive. In this note, we
              describe a straightforward procedure, within the DCM framework,
              that provides efficient inversion of seizure activity measured with
              non-invasive and invasive physiological recordings; namely,
              EEG/ECoG. We describe the theoretical background behind a Bayesian
              belief updating scheme for DCM. The scheme is tested on simulated
              and empirical seizure activity (recorded both invasively and
              non-invasively) and compared with standard Bayesian inversion. We
              show that the Bayesian belief updating scheme provides similar
              estimates of time-varying synaptic parameters, compared to standard
              schemes, indicating no significant qualitative change in accuracy.
              The difference in variance explained was small (less than 5\%). The
              updating method was substantially more efficient, taking
              approximately 5–10 min compared to approximately 1–2 h. Moreover,
              the setup of the model under the updating scheme allows for a clear
              specification of how neuronal variables fluctuate over separable
              timescales. This method now allows us to investigate the effect of
              fast (neuronal) activity on slow fluctuations in (synaptic)
              parameters, paving a way forward to understand how seizure activity
              is generated.},
  urldate  = {2016-08-03},
  journal  = {NeuroImage},
  author   = {Cooray, Gerald K. and Sengupta, Biswa and Douglas, Pamela K. and
              Friston, Karl},
  month    = jan,
  year     = {2016},
  keywords = {Bayesian belief updating, Dynamic causal modelling (DCM), EEG/ECoG
              , Epilepsy, Seizure activity},
  pages    = {1142--1154},
  file     = {Cooray et al. - 2016 - Dynamic causal modelling of electrographic
              seizure.pdf:/Users/apodusenko/Zotero/storage/JEY7CYCM/Cooray et al. -
              2016 - Dynamic causal modelling of electrographic
              seizure.pdf:application/pdf;ScienceDirect
              Snapshot:/Users/apodusenko/Zotero/storage/SMKXUA56/S1053811915006837.html:text/html
              }
}

@article{maisto_divide_2015,
  title      = {Divide et impera: subgoaling reduces the complexity of probabilistic
                inference and problem solving},
  volume     = {12},
  copyright  = {© 2015 The Author(s) Published by the Royal Society. All rights
                reserved.},
  issn       = {1742-5689, 1742-5662},
  shorttitle = {Divide et impera},
  url        = {http://rsif.royalsocietypublishing.org/content/12/104/20141335},
  doi        = {10.1098/rsif.2014.1335},
  abstract   = {It has long been recognized that humans (and possibly other
                animals) usually break problems down into smaller and more
                manageable problems using subgoals. Despite a general consensus
                that subgoaling helps problem solving, it is still unclear what the
                mechanisms guiding online subgoal selection are during the solution
                of novel problems for which predefined solutions are not available.
                Under which conditions does subgoaling lead to optimal behaviour?
                When is subgoaling better than solving a problem from start to
                finish? Which is the best number and sequence of subgoals to solve
                a given problem? How are these subgoals selected during online
                inference? Here, we present a computational account of subgoaling
                in problem solving. Following Occam's razor, we propose that good
                subgoals are those that permit planning solutions and controlling
                behaviour using less information resources, thus yielding parsimony
                in inference and control. We implement this principle using
                approximate probabilistic inference: subgoals are selected using a
                sampling method that considers the descriptive complexity of the
                resulting sub-problems. We validate the proposed method using a
                standard reinforcement learning benchmark (four-rooms scenario) and
                show that the proposed method requires less inferential steps and
                permits selecting more compact control programs compared to an
                equivalent procedure without subgoaling. Furthermore, we show that
                the proposed method offers a mechanistic explanation of the
                neuronal dynamics found in the prefrontal cortex of monkeys that
                solve planning problems. Our computational framework provides a
                novel integrative perspective on subgoaling and its adaptive
                advantages for planning, control and learning, such as for example
                lowering cognitive effort and working memory load.},
  language   = {en},
  number     = {104},
  urldate    = {2015-03-29},
  journal    = {Journal of The Royal Society Interface},
  author     = {Maisto, Domenico and Donnarumma, Francesco and Pezzulo, Giovanni},
  month      = mar,
  year       = {2015},
  pmid       = {25652466},
  pages      = {20141335},
  file       = {Maisto et al. - 2015 - Divide et impera subgoaling reduces the
                complexit.pdf:/Users/apodusenko/Zotero/storage/8XJQKLKS/Maisto et al. -
                2015 - Divide et impera subgoaling reduces the
                complexit.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/UM4LEFSB/20141335.html:text/html
                }
}

@article{jankowiak_pathwise_2018,
  title    = {Pathwise {Derivatives} {Beyond} the {Reparameterization} {Trick}},
  url      = {http://arxiv.org/abs/1806.01851},
  abstract = {We observe that gradients computed via the reparameterization
              trick are in direct correspondence with solutions of the transport
              equation in the formalism of optimal transport. We use this
              perspective to compute (approximate) pathwise gradients for
              probability distributions not directly amenable to the
              reparameterization trick: Gamma, Beta, and Dirichlet. We further
              observe that when the reparameterization trick is applied to the
              Cholesky-factorized multivariate Normal distribution, the resulting
              gradients are suboptimal in the sense of optimal transport. We
              derive the optimal gradients and show that they have reduced
              variance in a Gaussian Process regression task. We demonstrate with
              a variety of synthetic experiments and stochastic variational
              inference tasks that our pathwise gradients are competitive with
              other methods.},
  urldate  = {2018-11-29},
  journal  = {arXiv:1806.01851 [cs, stat]},
  author   = {Jankowiak, Martin and Obermeyer, Fritz},
  month    = jun,
  year     = {2018},
  note     = {arXiv: 1806.01851},
  keywords = {Statistics - Machine Learning, Computer Science - Machine Learning
              },
  file     = {arXiv\:1806.01851
              PDF:/Users/apodusenko/Zotero/storage/WKZV92HJ/Jankowiak and Obermeyer -
              2018 - Pathwise Derivatives Beyond the
              Reparameterization.pdf:application/pdf;arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/PQ2RNXKM/1806.html:text/html}
}

@article{salimbeni_natural_2018,
  title      = {Natural {Gradients} in {Practice}: {Non}-{Conjugate} {Variational} {
                Inference} in {Gaussian} {Process} {Models}},
  shorttitle = {Natural {Gradients} in {Practice}},
  url        = {http://arxiv.org/abs/1803.09151},
  abstract   = {The natural gradient method has been used effectively in conjugate
                Gaussian process models, but the non-conjugate case has been
                largely unexplored. We examine how natural gradients can be used in
                non-conjugate stochastic settings, together with hyperparameter
                learning. We conclude that the natural gradient can significantly
                improve performance in terms of wall-clock time. For
                ill-conditioned posteriors the benefit of the natural gradient
                method is especially pronounced, and we demonstrate a practical
                setting where ordinary gradients are unusable. We show how natural
                gradients can be computed efficiently and automatically in any
                parameterization, using automatic differentiation. Our code is
                integrated into the GPflow package.},
  urldate    = {2018-11-29},
  journal    = {arXiv:1803.09151 [cs, stat]},
  author     = {Salimbeni, Hugh and Eleftheriadis, Stefanos and Hensman, James},
  month      = mar,
  year       = {2018},
  note       = {arXiv: 1803.09151},
  keywords   = {Statistics - Machine Learning, Computer Science - Machine Learning
                },
  file       = {arXiv\:1803.09151
                PDF:/Users/apodusenko/Zotero/storage/4SQBTNIU/Salimbeni et al. - 2018 -
                Natural Gradients in Practice Non-Conjugate
                Varia.pdf:application/pdf;arXiv.org
                Snapshot:/Users/apodusenko/Zotero/storage/37MQW5FX/1803.html:text/html}
}

@book{amari_natural_nodate,
  title    = {Natural {Gradient} {Works} {Efficiently} in {Learning}},
  abstract = {When a parameter space has a certain underlying structure, the
              ordinary gradient of a function does not represent its steepest
              direction, but the natural gradient does. Information geometry is
              used for calculating the natural gradients in the parameter space
              of perceptrons, the space of ma-trices (for blind source
              separation), and the space of linear dynamical systems (for blind
              source deconvolution). The dynamical behavior of natural gradient
              online learning is analyzed and is proved to be Fisher efficient,
              implying that it has asymptotically the same performance as the
              optimal batch estimation of parameters. This suggests that the
              plateau phenomenon, which appears in the backpropagation learning
              algorithm of multilayer perceptrons, might disappear or might not
              be so serious when the natural gradient is used. An adaptive method
              of updating the learning rate is proposed and analyzed.},
  author   = {Amari, Shun-ichi},
  file     = {Amari - Natural Gradient Works Efficiently in
              Learning.pdf:/Users/apodusenko/Zotero/storage/YK4K6FDB/Amari - Natural
              Gradient Works Efficiently in Learning.pdf:application/pdf;Citeseer -
              Snapshot:/Users/apodusenko/Zotero/storage/G4CI8KQB/summary.html:text/html
              }
}

@article{ikeda_stochastic_2004,
  title    = {Stochastic {Reasoning}, {Free} {Energy}, and {Information} {Geometry}
              },
  volume   = {16},
  issn     = {0899-7667, 1530-888X},
  url      = {http://www.mitpressjournals.org/doi/10.1162/0899766041336477},
  doi      = {10.1162/0899766041336477},
  language = {en},
  number   = {9},
  urldate  = {2018-11-29},
  journal  = {Neural Computation},
  author   = {Ikeda, Shiro and Tanaka, Toshiyuki and Amari, Shun-ichi},
  month    = sep,
  year     = {2004},
  pages    = {1779--1810},
  file     = {Ikeda et al. - 2004 - Stochastic Reasoning, Free Energy, and
              Information.pdf:/Users/apodusenko/Zotero/storage/XP2FGTT3/Ikeda et al.
              - 2004 - Stochastic Reasoning, Free Energy, and
              Information.pdf:application/pdf}
}

@article{honkela_approximate_nodate,
  title    = {Approximate {Riemannian} {Conjugate} {Gradient} {Learning} for {Fixed
              }-{Form} {Variational} {Bayes}},
  abstract = {Variational Bayesian (VB) methods are typically only applied to
              models in the conjugate-exponential family using the variational
              Bayesian expectation maximisation (VB EM) algorithm or one of its
              variants. In this paper we present an efﬁcient algorithm for
              applying VB to more general models. The method is based on
              specifying the functional form of the approximation, such as
              multivariate Gaussian. The parameters of the approximation are
              optimised using a conjugate gradient algorithm that utilises the
              Riemannian geometry of the space of the approximations. This leads
              to a very efﬁcient algorithm for suitably structured
              approximations. It is shown empirically that the proposed method is
              comparable or superior in efﬁciency to the VB EM in a case where
              both are applicable. We also apply the algorithm to learning a
              nonlinear state-space model and a nonlinear factor analysis model
              for which the VB EM is not applicable. For these models, the
              proposed algorithm outperforms alternative gradient-based methods
              by a signiﬁcant margin.},
  language = {en},
  author   = {Honkela, Antti and Raiko, Tapani and Kuusela, Mikael and Tornio,
              Matti and Karhunen, Juha},
  pages    = {34},
  file     = {Honkela et al. - Approximate Riemannian Conjugate Gradient
              Learning.pdf:/Users/apodusenko/Zotero/storage/BM7S3KZJ/Honkela et al. -
              Approximate Riemannian Conjugate Gradient Learning.pdf:application/pdf}
}

@incollection{ishikawa_natural_2008,
  address   = {Berlin, Heidelberg},
  title     = {Natural {Conjugate} {Gradient} in {Variational} {Inference}},
  volume    = {4985},
  isbn      = {978-3-540-69159-4 978-3-540-69162-4},
  url       = {http://link.springer.com/10.1007/978-3-540-69162-4_32},
  abstract  = {Variational methods for approximate inference in machine learning
               often adapt a parametric probability distribution to optimize a
               given objective function. This view is especially useful when
               applying variational Bayes (VB) to models outside the
               conjugate-exponential family. For them, variational Bayesian
               expectation maximization (VB EM) algorithms are not easily
               available, and gradient-based methods are often used as
               alternatives. Traditional natural gradient methods use the
               Riemannian structure (or geometry) of the predictive distribution
               to speed up maximum likelihood estimation. We propose using the
               geometry of the variational approximating distribution instead to
               speed up a conjugate gradient method for variational learning and
               inference. The computational overhead is small due to the
               simplicity of the approximating distribution. Experiments with
               real-world speech data show signiﬁcant speedups over alternative
               learning algorithms.},
  language  = {en},
  urldate   = {2018-11-29},
  booktitle = {Neural {Information} {Processing}},
  publisher = {Springer Berlin Heidelberg},
  author    = {Honkela, Antti and Tornio, Matti and Raiko, Tapani and Karhunen,
               Juha},
  editor    = {Ishikawa, Masumi and Doya, Kenji and Miyamoto, Hiroyuki and Yamakawa
               , Takeshi},
  year      = {2008},
  doi       = {10.1007/978-3-540-69162-4_32},
  pages     = {305--314},
  file      = {Honkela et al. - 2008 - Natural Conjugate Gradient in Variational
               Inferenc.pdf:/Users/apodusenko/Zotero/storage/CJKLVNZG/Honkela et al. -
               2008 - Natural Conjugate Gradient in Variational
               Inferenc.pdf:application/pdf}
}

@article{tran_copula_2015,
  title    = {Copula variational inference},
  url      = {http://arxiv.org/abs/1506.03159},
  abstract = {We develop a general variational inference method that preserves
              dependency among the latent variables. Our method uses copulas to
              augment the families of distributions used in mean-field and
              structured approximations. Copulas model the dependency that is not
              captured by the original variational distribution, and thus the
              augmented variational family guarantees better approximations to
              the posterior. With stochastic optimization, inference on the
              augmented distribution is scalable. Furthermore, our strategy is
              generic: it can be applied to any inference procedure that
              currently uses the mean-field or structured approach. Copula
              variational inference has many advantages: it reduces bias; it is
              less sensitive to local optima; it is less sensitive to
              hyperparameters; and it helps characterize and interpret the
              dependency among the latent variables.},
  urldate  = {2018-11-29},
  journal  = {arXiv:1506.03159 [cs, stat]},
  author   = {Tran, Dustin and Blei, David M. and Airoldi, Edoardo M.},
  month    = jun,
  year     = {2015},
  note     = {arXiv: 1506.03159},
  keywords = {Statistics - Machine Learning, Statistics - Computation,
              Statistics - Methodology, Computer Science - Machine Learning},
  file     = {arXiv\:1506.03159 PDF:/Users/apodusenko/Zotero/storage/SA2AUTXT/Tran
              et al. - 2015 - Copula variational
              inference.pdf:application/pdf;arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/S99Q7SN3/1506.html:text/html}
}

@inproceedings{parr_frontoparietal_2018,
  address   = {Thessaloniki, Greece},
  title     = {Frontoparietal connections and active inference},
  booktitle = {Conference on {Complex} {Systems} ({CCS})},
  author    = {Parr, Thomas},
  month     = sep,
  year      = {2018}
}

@article{tran_copula_2018,
  title    = {Copula {Variational} {Bayes} inference via information geometry},
  url      = {http://arxiv.org/abs/1803.10998},
  abstract = {Variational Bayes (VB), also known as independent mean-field
              approximation, has become a popular method for Bayesian network
              inference in recent years. Its application is vast, e.g. in neural
              network, compressed sensing, clustering, etc. to name just a few.
              In this paper, the independence constraint in VB will be relaxed to
              a conditional constraint class, called copula in statistics. Since
              a joint probability distribution always belongs to a copula class,
              the novel copula VB (CVB) approximation is a generalized form of
              VB. Via information geometry, we will see that CVB algorithm
              iteratively projects the original joint distribution to a copula
              constraint space until it reaches a local minimum Kullback-Leibler
              (KL) divergence. By this way, all mean-field approximations, e.g.
              iterative VB, Expectation-Maximization (EM), Iterated Conditional
              Mode (ICM) and k-means algorithms, are special cases of CVB
              approximation. For a generic Bayesian network, an augmented
              hierarchy form of CVB will also be designed. While mean-field
              algorithms can only return a locally optimal approximation for a
              correlated network, the augmented CVB network, which is an
              optimally weighted average of a mixture of simpler network
              structures, can potentially achieve the globally optimal
              approximation for the first time. Via simulations of Gaussian
              mixture clustering, the classification's accuracy of CVB will be
              shown to be far superior to that of state-of-the-art VB, EM and
              k-means algorithms.},
  urldate  = {2018-11-29},
  journal  = {arXiv:1803.10998 [cs, math, stat]},
  author   = {Tran, Viet Hung},
  month    = mar,
  year     = {2018},
  note     = {arXiv: 1803.10998},
  keywords = {Statistics - Machine Learning, Computer Science - Information
              Theory},
  file     = {arXiv\:1803.10998 PDF:/Users/apodusenko/Zotero/storage/EEBTHGVM/Tran -
              2018 - Copula Variational Bayes inference via
              information.pdf:application/pdf;arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/MXWYR7KF/1803.html:text/html}
}

@article{figurnov_implicit_2018,
  title    = {Implicit {Reparameterization} {Gradients}},
  url      = {http://arxiv.org/abs/1805.08498},
  abstract = {By providing a simple and efficient way of computing low-variance
              gradients of continuous random variables, the reparameterization
              trick has become the technique of choice for training a variety of
              latent variable models. However, it is not applicable to a number
              of important continuous distributions. We introduce an alternative
              approach to computing reparameterization gradients based on
              implicit differentiation and demonstrate its broader applicability
              by applying it to Gamma, Beta, Dirichlet, and von Mises
              distributions, which cannot be used with the classic
              reparameterization trick. Our experiments show that the proposed
              approach is faster and more accurate than the existing gradient
              estimators for these distributions.},
  urldate  = {2018-11-29},
  journal  = {arXiv:1805.08498 [cs, stat]},
  author   = {Figurnov, Michael and Mohamed, Shakir and Mnih, Andriy},
  month    = may,
  year     = {2018},
  note     = {arXiv: 1805.08498},
  keywords = {Statistics - Machine Learning, Computer Science - Machine Learning
              },
  file     = {arXiv\:1805.08498
              PDF:/Users/apodusenko/Zotero/storage/9NV2SJQ9/Figurnov et al. - 2018 -
              Implicit Reparameterization Gradients.pdf:application/pdf;arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/7UVLJZKA/1805.html:text/html}
}

@article{mandt_stochastic_nodate,
  title    = {Stochastic {Gradient} {Descent} as {Approximate} {Bayesian} {
              Inference}},
  abstract = {Stochastic Gradient Descent with a constant learning rate
              (constant SGD) simulates a Markov chain with a stationary
              distribution. With this perspective, we derive several new results.
              (1) We show that constant SGD can be used as an approximate
              Bayesian posterior inference algorithm. Specifically, we show how
              to adjust the tuning parameters of constant SGD to best match the
              stationary distribution to a posterior, minimizing the
              Kullback-Leibler divergence between these two distributions. (2) We
              demonstrate that constant SGD gives rise to a new variational EM
              algorithm that optimizes hyperparameters in complex probabilistic
              models. (3) We also show how to tune SGD with momentum for
              approximate sampling. (4) We analyze stochastic-gradient MCMC
              algorithms. For Stochastic-Gradient Langevin Dynamics and
              Stochastic-Gradient Fisher Scoring, we quantify the approximation
              errors due to ﬁnite learning rates. Finally (5), we use the
              stochastic process perspective to give a short proof of why Polyak
              averaging is optimal. Based on this idea, we propose a scalable
              approximate MCMC algorithm, the Averaged Stochastic Gradient
              Sampler.},
  language = {en},
  author   = {Mandt, Stephan and Hoffman, Matthew D},
  pages    = {35},
  file     = {Mandt and Hoffman - Stochastic Gradient Descent as Approximate
              Bayesia.pdf:/Users/apodusenko/Zotero/storage/F9YAV7MF/Mandt and Hoffman
              - Stochastic Gradient Descent as Approximate
              Bayesia.pdf:application/pdf}
}

@article{ruiz_generalized_2016,
  title    = {The {Generalized} {Reparameterization} {Gradient}},
  url      = {http://arxiv.org/abs/1610.02287},
  abstract = {The reparameterization gradient has become a widely used method to
              obtain Monte Carlo gradients to optimize the variational objective.
              However, this technique does not easily apply to commonly used
              distributions such as beta or gamma without further approximations,
              and most practical applications of the reparameterization gradient
              fit Gaussian distributions. In this paper, we introduce the
              generalized reparameterization gradient, a method that extends the
              reparameterization gradient to a wider class of variational
              distributions. Generalized reparameterizations use invertible
              transformations of the latent variables which lead to transformed
              distributions that weakly depend on the variational parameters.
              This results in new Monte Carlo gradients that combine
              reparameterization gradients and score function gradients. We
              demonstrate our approach on variational inference for two complex
              probabilistic models. The generalized reparameterization is
              effective: even a single sample from the variational distribution
              is enough to obtain a low-variance gradient.},
  urldate  = {2018-11-29},
  journal  = {arXiv:1610.02287 [stat]},
  author   = {Ruiz, Francisco J. R. and Titsias, Michalis K. and Blei, David M.},
  month    = oct,
  year     = {2016},
  note     = {arXiv: 1610.02287},
  keywords = {Statistics - Machine Learning},
  file     = {arXiv\:1610.02287 PDF:/Users/apodusenko/Zotero/storage/SIEDKSQA/Ruiz
              et al. - 2016 - The Generalized Reparameterization
              Gradient.pdf:application/pdf;arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/EWQVA2AF/1610.html:text/html}
}

@article{ranganath_black_nodate,
  title    = {Black {Box} {Variational} {Inference} ({Extra} {Materials})},
  language = {en},
  author   = {Ranganath, Rajesh and Gerrish, Sean and Blei, David M},
  pages    = {3},
  file     = {Ranganath et al. - Black Box Variational Inference (Extra
              Materials).pdf:/Users/apodusenko/Zotero/storage/BI5JD8VU/Ranganath et
              al. - Black Box Variational Inference (Extra
              Materials).pdf:application/pdf}
}

@article{zhang_advances_2017,
  title    = {Advances in {Variational} {Inference}},
  url      = {http://arxiv.org/abs/1711.05597},
  abstract = {Many modern unsupervised or semi-supervised machine learning
              algorithms rely on Bayesian probabilistic models. These models are
              usually intractable and thus require approximate inference.
              Variational inference (VI) lets us approximate a high-dimensional
              Bayesian posterior with a simpler variational distribution by
              solving an optimization problem. This approach has been
              successfully used in various models and large-scale applications.
              In this review, we give an overview of recent trends in variational
              inference. We first introduce standard mean field variational
              inference, then review recent advances focusing on the following
              aspects: (a) scalable VI, which includes stochastic approximations,
              (b) generic VI, which extends the applicability of VI to a large
              class of otherwise intractable models, such as non-conjugate models
              , (c) accurate VI, which includes variational models beyond the
              mean field approximation or with atypical divergences, and (d)
              amortized VI, which implements the inference over local latent
              variables with inference networks. Finally, we provide a summary of
              promising future research directions.},
  urldate  = {2018-11-29},
  journal  = {arXiv:1711.05597 [cs, stat]},
  author   = {Zhang, Cheng and Butepage, Judith and Kjellstrom, Hedvig and Mandt,
              Stephan},
  month    = nov,
  year     = {2017},
  note     = {arXiv: 1711.05597},
  keywords = {Statistics - Machine Learning, Computer Science - Machine Learning
              },
  file     = {arXiv\:1711.05597 PDF:/Users/apodusenko/Zotero/storage/7PA8IDM8/Zhang
              et al. - 2017 - Advances in Variational
              Inference.pdf:application/pdf;arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/KV8IWXN9/1711.html:text/html}
}

@article{yoon_inference_2018,
  title    = {Inference in {Probabilistic} {Graphical} {Models} by {Graph} {Neural}
              {Networks}},
  url      = {http://arxiv.org/abs/1803.07710},
  abstract = {A fundamental computation for statistical inference and accurate
              decision-making is to compute the marginal probabilities or most
              probable states of task-relevant variables. Probabilistic graphical
              models can efficiently represent the structure of such complex data
              , but performing these inferences is generally difficult.
              Message-passing algorithms, such as belief propagation, are a
              natural way to disseminate evidence amongst correlated variables
              while exploiting the graph structure, but these algorithms can
              struggle when the conditional dependency graphs contain loops. Here
              we use Graph Neural Networks (GNNs) to learn a message-passing
              algorithm that solves these inference tasks. We first show that the
              architecture of GNNs is well-matched to inference tasks. We then
              demonstrate the efficacy of this inference approach by training
              GNNs on a collection of graphical models and showing that they
              substantially outperform belief propagation on loopy graphs. Our
              message-passing algorithms generalize out of the training set to
              larger graphs and graphs with different structure.},
  urldate  = {2018-11-29},
  journal  = {arXiv:1803.07710 [cs, stat]},
  author   = {Yoon, KiJung and Liao, Renjie and Xiong, Yuwen and Zhang, Lisa and
              Fetaya, Ethan and Urtasun, Raquel and Zemel, Richard and Pitkow, Xaq},
  month    = mar,
  year     = {2018},
  note     = {arXiv: 1803.07710},
  keywords = {Statistics - Machine Learning, Computer Science - Artificial
              Intelligence, Computer Science - Machine Learning},
  file     = {arXiv\:1803.07710 PDF:/Users/apodusenko/Zotero/storage/BE4NKUXH/Yoon
              et al. - 2018 - Inference in Probabilistic Graphical Models by
              Gra.pdf:application/pdf;arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/IHV3K4JV/1803.html:text/html}
}

@article{parr_working_2017,
  title   = {Working memory, attention, and salience in active inference},
  volume  = {7},
  number  = {1},
  journal = {Scientific reports},
  author  = {Parr, Thomas and Friston, Karl J.},
  year    = {2017},
  pages   = {14678},
  file    = {Full
             Text:/Users/apodusenko/Zotero/storage/SV58F35I/s41598-017-15249-0.html:text/html
             }
}

@article{dayan_helmholtz_1995,
  title   = {The helmholtz machine},
  volume  = {7},
  number  = {5},
  journal = {Neural computation},
  author  = {Dayan, Peter and Hinton, Geoffrey E. and Neal, Radford M. and Zemel,
             Richard S.},
  year    = {1995},
  pages   = {889--904},
  file    = {Dayan e.a. - 1995 - The helmholtz
             machine.pdf:/Users/apodusenko/Zotero/storage/WQKR7PRU/Dayan e.a. - 1995
             - The helmholtz
             machine.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/BFPI3VZ9/neco.1995.7.5.html:text/html
             }
}

@article{hohwy_predictive_2008,
  title      = {Predictive coding explains binocular rivalry: {An} epistemological
                review},
  volume     = {108},
  shorttitle = {Predictive coding explains binocular rivalry},
  number     = {3},
  journal    = {Cognition},
  author     = {Hohwy, Jakob and Roepstorff, Andreas and Friston, Karl},
  year       = {2008},
  pages      = {687--701},
  file       = {Hohwy e.a. - 2008 - Predictive coding explains binocular rivalry An
                e.pdf:/Users/apodusenko/Zotero/storage/IV6EG5AE/Hohwy e.a. - 2008 -
                Predictive coding explains binocular rivalry An
                e.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/EH7I3ANG/S0010027708001327.html:text/html
                }
}

@article{deutsch_auditory_1974,
  title   = {An auditory illusion},
  volume  = {251},
  number  = {5473},
  journal = {Nature},
  author  = {Deutsch, Diana},
  year    = {1974},
  pages   = {307},
  file    = {Deutsch - 1974 - An auditory
             illusion.pdf:/Users/apodusenko/Zotero/storage/DF99PZN8/Deutsch - 1974 -
             An auditory
             illusion.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/EAXAX6R8/251307a0.html:text/html
             }
}

@article{blake_neural_2004,
  title   = {Neural synergy between kinetic vision and touch},
  volume  = {15},
  number  = {6},
  journal = {Psychological science},
  author  = {Blake, Randolph and Sobel, Kenith V. and James, Thomas W.},
  year    = {2004},
  pages   = {397--402},
  file    = {Blake e.a. - 2004 - Neural synergy between kinetic vision and
             touch.pdf:/Users/apodusenko/Zotero/storage/MCL7E3GB/Blake e.a. - 2004 -
             Neural synergy between kinetic vision and
             touch.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/CLYU6PAY/j.0956-7976.2004.00691.html:text/html
             }
}

@article{mcgurk_hearing_1976,
  title   = {Hearing lips and seeing voices},
  volume  = {264},
  number  = {5588},
  journal = {Nature},
  author  = {McGurk, Harry and MacDonald, John},
  year    = {1976},
  pages   = {746},
  file    = {McGurk en MacDonald - 1976 - Hearing lips and seeing
             voices.pdf:/Users/apodusenko/Zotero/storage/5URJFLTH/McGurk en
             MacDonald - 1976 - Hearing lips and seeing
             voices.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/MXPUSWE9/264746a0.html:text/html
             }
}

@article{lupyan_cognitive_2015,
  title      = {Cognitive penetrability of perception in the age of prediction: {
                Predictive} systems are penetrable systems},
  volume     = {6},
  shorttitle = {Cognitive penetrability of perception in the age of prediction},
  number     = {4},
  journal    = {Review of philosophy and psychology},
  author     = {Lupyan, Gary},
  year       = {2015},
  pages      = {547--569},
  file       = {Full
                Text:/Users/apodusenko/Zotero/storage/6NFXCNL6/s13164-015-0253-4.html:text/html
                }
}

@article{clark_whatever_2013,
  title      = {Whatever next? {Predictive} brains, situated agents, and the future
                of cognitive science},
  volume     = {36},
  shorttitle = {Whatever next?},
  number     = {3},
  journal    = {Behavioral and brain sciences},
  author     = {Clark, Andy},
  year       = {2013},
  pages      = {181--204},
  file       = {Clark - 2013 - Whatever next Predictive brains, situated agents,
                .pdf:/Users/apodusenko/Zotero/storage/AJXDMZNT/Clark - 2013 - Whatever
                next Predictive brains, situated agents,
                .pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/A4SP2CRR/33542C736E17E3D1D44E8D03BE5F4CD9.html:text/html
                }
}

@article{silverstein_schizophrenia-related_2013,
  title    = {Schizophrenia-related phenomena that challenge prediction error as
              the basis of cognitive functioning},
  volume   = {36},
  issn     = {0140-525X, 1469-1825},
  url      = {http://www.journals.cambridge.org/abstract_S0140525X12002221},
  doi      = {10.1017/S0140525X12002221},
  abstract = {Brains, it has recently been argued, are essentially prediction
              machines. They are bundles of cells that support perception and
              action by constantly attempting to match incoming sensory inputs
              with top-down expectations or predictions. This is achieved using a
              hierarchical generative model that aims to minimize prediction
              error within a bidirectional cascade of cortical processing. Such
              accounts offer a unifying model of perception and action,
              illuminate the functional role of attention, and may neatly capture
              the special contribution of cortical processing to adaptive
              success. This target article critically examines this “hierarchical
              prediction machine” approach, concluding that it offers the best
              clue yet to the shape of a uniﬁed science of mind and action.
              Sections 1 and 2 lay out the key elements and implications of the
              approach. Section 3 explores a variety of pitfalls and challenges,
              spanning the evidential, the methodological, and the more properly
              conceptual. The paper ends (sections 4 and 5) by asking how such
              approaches might impact our more general vision of mind, experience
              , and agency.},
  language = {en},
  number   = {03},
  urldate  = {2018-11-29},
  journal  = {Behavioral and Brain Sciences},
  author   = {Silverstein, Steven M.},
  month    = jun,
  year     = {2013},
  pages    = {229--230},
  file     = {Silverstein - 2013 - Schizophrenia-related phenomena that challenge
              pre.pdf:/Users/apodusenko/Zotero/storage/MVXDUTCC/Silverstein - 2013 -
              Schizophrenia-related phenomena that challenge pre.pdf:application/pdf}
}

@book{hohwy_predictive_2013,
  title     = {The predictive mind},
  publisher = {Oxford University Press},
  author    = {Hohwy, Jakob},
  year      = {2013},
  file      = {
               Snapshot:/Users/apodusenko/Zotero/storage/EPGNFRZ3/books.html:text/html
               }
}

@article{bezanson_julia:_2012,
  title      = {Julia: {A} {Fast} {Dynamic} {Language} for {Technical} {Computing}},
  shorttitle = {Julia},
  url        = {http://arxiv.org/abs/1209.5145},
  abstract   = {Dynamic languages have become popular for scientific computing.
                They are generally considered highly productive, but lacking in
                performance. This paper presents Julia, a new dynamic language for
                technical computing, designed for performance from the beginning by
                adapting and extending modern programming language techniques. A
                design based on generic functions and a rich type system
                simultaneously enables an expressive programming model and
                successful type inference, leading to good performance for a wide
                range of programs. This makes it possible for much of the Julia
                library to be written in Julia itself, while also incorporating
                best-of-breed C and Fortran libraries.},
  urldate    = {2018-11-27},
  journal    = {arXiv:1209.5145 [cs]},
  author     = {Bezanson, Jeff and Karpinski, Stefan and Shah, Viral B. and Edelman,
                Alan},
  month      = sep,
  year       = {2012},
  note       = {arXiv: 1209.5145},
  keywords   = {Computer Science - Programming Languages, Computer Science -
                Computational Engineering, Finance, and Science, D.3.2},
  file       = {arXiv.org
                Snapshot:/Users/apodusenko/Zotero/storage/V6P4IU54/1209.html:text/html;Bezanson
                et al. - 2012 - Julia A Fast Dynamic Language for Technical
                Compu.pdf:/Users/apodusenko/Zotero/storage/4Q9THC8V/Bezanson et al. -
                2012 - Julia A Fast Dynamic Language for Technical
                Compu.pdf:application/pdf}
}

@article{tukey_future_1962,
  title   = {The future of data analysis},
  volume  = {33},
  number  = {1},
  journal = {The annals of mathematical statistics},
  author  = {Tukey, John W.},
  year    = {1962},
  pages   = {1--67},
  file    = {
             Snapshot:/Users/apodusenko/Zotero/storage/UQSM9IDW/2237638.html:text/html;Tukey
             - 1962 - The future of data
             analysis.pdf:/Users/apodusenko/Zotero/storage/B4KD6TMU/Tukey - 1962 -
             The future of data analysis.pdf:application/pdf}
}

@book{gallager_stochastic_2013,
  title     = {Stochastic {Process} {Theory} for {Applications}},
  publisher = {Cambridge University Press},
  author    = {Gallager, Robert G},
  year      = {2013}
}

@article{paulus_roadmap_2016,
  series   = {Model-{Based} and {Machine}-{Learning} {Approaches} in {Applied} {
              Computational} {Psychiatry}},
  title    = {A {Roadmap} for the {Development} of {Applied} {Computational} {
              Psychiatry}},
  volume   = {1},
  issn     = {2451-9022},
  url      = {http://www.sciencedirect.com/science/article/pii/S2451902216300350},
  doi      = {10.1016/j.bpsc.2016.05.001},
  abstract = {Computational psychiatry (CP) is a burgeoning field that uses
              mathematical approaches to investigate psychiatric disorders,
              derive quantitative predictions, and integrate data across multiple
              levels of description. CP has already led to many new insights into
              the neurobehavioral mechanisms that underlie several psychiatric
              disorders, but its usefulness from a clinical standpoint is only
              now starting to be considered. Examples of CP are highlighted, and
              a phase-based pipeline for the development of clinical
              computational–psychiatry applications is proposed, similar to the
              phase-based pipeline used in drug development. We propose that each
              phase has unique endpoints and deliverables that will be important
              milestones to move tasks, procedures, computational models, and
              algorithms from the laboratory to clinical practice. Application of
              computational approaches should be tested on healthy volunteers in
              phase I, transitioned to target populations in phases IB and IIA,
              and thoroughly evaluated using randomized clinical trials in phases
              IIB and III. Successful completion of these phases should be the
              basis of determining whether computational models are useful tools
              for prognosis, diagnosis, or treatment of psychiatric patients. A
              new type of infrastructure will be necessary to implement the
              proposed pipeline. This infrastructure should consist of groups of
              investigators with diverse backgrounds collaborating to make CP
              relevant for the clinic.},
  number   = {5},
  urldate  = {2018-11-26},
  journal  = {Biological Psychiatry: Cognitive Neuroscience and Neuroimaging},
  author   = {Paulus, Martin P. and Huys, Quentin J. M. and Maia, Tiago V.},
  month    = sep,
  year     = {2016},
  keywords = {Machine learning, Biomarkers, Computational psychiatry,
              Development pipelines, Prediction, Translational psychiatry},
  pages    = {386--392},
  file     = {Paulus et al. - 2016 - A Roadmap for the Development of Applied
              Computati.pdf:/Users/apodusenko/Zotero/storage/VXLWMP2P/Paulus et al. -
              2016 - A Roadmap for the Development of Applied
              Computati.pdf:application/pdf;ScienceDirect
              Snapshot:/Users/apodusenko/Zotero/storage/JATPVP7H/S2451902216300350.html:text/html
              }
}

@article{vossel_cortical_2015,
  title     = {Cortical {Coupling} {Reflects} {Bayesian} {Belief} {Updating} in the
               {Deployment} of {Spatial} {Attention}},
  volume    = {35},
  copyright = {Copyright © 2015 Vossel et al.. This article is freely available
               online through the J Neurosci Author Open Choice option.},
  issn      = {0270-6474, 1529-2401},
  url       = {http://www.jneurosci.org/content/35/33/11532},
  doi       = {10.1523/JNEUROSCI.1382-15.2015},
  abstract  = {The deployment of visuospatial attention and the programming of
               saccades are governed by the inferred likelihood of events. In the
               present study, we combined computational modeling of psychophysical
               data with fMRI to characterize the computational and neural
               mechanisms underlying this flexible attentional control. Sixteen
               healthy human subjects performed a modified version of Posner's
               location-cueing paradigm in which the percentage of cue validity
               varied in time and the targets required saccadic responses.
               Trialwise estimates of the certainty (precision) of the prediction
               that the target would appear at the cued location were derived from
               a hierarchical Bayesian model fitted to individual trialwise
               saccadic response speeds. Trial-specific model parameters then
               entered analyses of fMRI data as parametric regressors. Moreover,
               dynamic causal modeling (DCM) was performed to identify the most
               likely functional architecture of the attentional reorienting
               network and its modulation by (Bayes-optimal) precision-dependent
               attention. While the frontal eye fields (FEFs), intraparietal
               sulcus, and temporoparietal junction (TPJ) of both hemispheres
               showed higher activity on invalid relative to valid trials,
               reorienting responses in right FEF, TPJ, and the putamen were
               significantly modulated by precision-dependent attention. Our DCM
               results suggested that the precision of predictability underlies
               the attentional modulation of the coupling of TPJ with FEF and the
               putamen. Our results shed new light on the computational
               architecture and neuronal network dynamics underlying the
               context-sensitive deployment of visuospatial attention.
               SIGNIFICANCE STATEMENT Spatial attention and its neural correlates
               in the human brain have been studied extensively with the help of
               fMRI and cueing paradigms in which the location of targets is
               pre-cued on a trial-by-trial basis. One aspect that has so far been
               neglected concerns the question of how the brain forms attentional
               expectancies when no a priori probability information is available
               but needs to be inferred from observations. This study elucidates
               the computational and neural mechanisms under which probabilistic
               inference governs attentional deployment. Our results show that
               Bayesian belief updating explains changes in cortical connectivity;
               in that directional influences from the temporoparietal junction on
               the frontal eye fields and the putamen were modulated by
               (Bayes-optimal) updates.},
  language  = {en},
  number    = {33},
  urldate   = {2018-11-26},
  journal   = {Journal of Neuroscience},
  author    = {Vossel, Simone and Mathys, Christoph and Stephan, Klaas E. and
               Friston, Karl J.},
  month     = aug,
  year      = {2015},
  pmid      = {26290231},
  keywords  = {Bayesian inference, fMRI, attentional networks, saccades, spatial
               cueing},
  pages     = {11532--11542},
  file      = {
               Snapshot:/Users/apodusenko/Zotero/storage/5T5GP2ZT/11532.html:text/html;Vossel
               et al. - 2015 - Cortical Coupling Reflects Bayesian Belief
               Updatin.pdf:/Users/apodusenko/Zotero/storage/6ILN78SV/Vossel et al. -
               2015 - Cortical Coupling Reflects Bayesian Belief
               Updatin.pdf:application/pdf}
}

@misc{van_de_cruys_affective_nodate,
  title        = {Affective {Value} in the {Predictive} {Mind}},
  url          = {https://lirias.kuleuven.be/retrieve/443336},
  abstract     = {Although affective value is fundamental in explanations of
                  behavior, it is still a somewhat alien concept in cognitive
                  science. It implies a normativity or directionality that mere
                  information processing models cannot seem to provide. In this paper
                  we trace how affective value can emerge from information processing
                  in the brain, as described by predictive processing. We explain the
                  grounding of predictive processing in homeostasis, and articulate
                  the implications this has for the concept of reward and motivation.
                  However, at first sight, this new conceptualization creates a
                  strong tension with conventional ideas on reward and affective
                  experience. We propose this tension can be resolved by realizing
                  that valence, a core component of all emotions, might be the
                  reflection of a specific aspect of predictive information
                  processing, namely the dynamics in prediction errors across time
                  and the expectations we, in turn, form about these dynamics.
                  Specifically, positive affect seems to be caused by positive rates
                  of prediction error reduction, while negative affect is induced by
                  a shift in a state with lower prediction errors to one with higher
                  prediction errors (i.e., a negative rate of error reduction). We
                  also consider how intense emotional episodes might be related to
                  unexpected changes in prediction errors, suggesting that we also
                  build (meta)predictions on error reduction rates. Hence in this
                  account emotions appear as the continuous non-conceptual feedback
                  on evolving âincreasing or decreasingâuncertainties relative to
                  our predictions. The upshot of this view is that the various
                  emotions, from âbasicâ ones to the non-typical ones such as
                  humor, curiosity and aesthetic affects, can be shown to follow a
                  single underlying logic. Our analysis takes several cues from
                  existing emotion theories but deviates from them in revealing ways.
                  The account on offer does not just specify the interactions between
                  emotion and cognition, rather it entails a deep integration of the
                  two.},
  language     = {eng},
  urldate      = {2018-11-26},
  publisher    = {MIND Group; Frankfurt am Main},
  author       = {Van de Cruys, Sander},
  collaborator = {Metzinger, Thomas K. and Wiese, W.},
  keywords     = {affect, dark room, emotion, prediction error, predictive
                  processing, reward, uncertainty, valence}
}

@article{leike_scalable_2018,
  title      = {Scalable agent alignment via reward modeling: a research direction},
  shorttitle = {Scalable agent alignment via reward modeling},
  url        = {http://arxiv.org/abs/1811.07871},
  abstract   = {One obstacle to applying reinforcement learning algorithms to
                real-world problems is the lack of suitable reward functions.
                Designing such reward functions is difficult in part because the
                user only has an implicit understanding of the task objective. This
                gives rise to the agent alignment problem: how do we create agents
                that behave in accordance with the user's intentions? We outline a
                high-level research direction to solve the agent alignment problem
                centered around reward modeling: learning a reward function from
                interaction with the user and optimizing the learned reward
                function with reinforcement learning. We discuss the key challenges
                we expect to face when scaling reward modeling to complex and
                general domains, concrete approaches to mitigate these challenges,
                and ways to establish trust in the resulting agents.},
  urldate    = {2018-11-26},
  journal    = {arXiv:1811.07871 [cs, stat]},
  author     = {Leike, Jan and Krueger, David and Everitt, Tom and Martic, Miljan
                and Maini, Vishal and Legg, Shane},
  month      = nov,
  year       = {2018},
  note       = {arXiv: 1811.07871},
  keywords   = {Statistics - Machine Learning, Computer Science - Artificial
                Intelligence, Computer Science - Neural and Evolutionary Computing,
                Computer Science - Machine Learning},
  file       = {arXiv\:1811.07871 PDF:/Users/apodusenko/Zotero/storage/2TKQY4P7/Leike
                et al. - 2018 - Scalable agent alignment via reward modeling a
                re.pdf:application/pdf;arXiv.org
                Snapshot:/Users/apodusenko/Zotero/storage/4QPJ43W9/1811.html:text/html}
}

@article{hadfield-menell_cooperative_2016,
  title    = {Cooperative {Inverse} {Reinforcement} {Learning}},
  url      = {http://arxiv.org/abs/1606.03137},
  abstract = {For an autonomous system to be helpful to humans and to pose no
              unwarranted risks, it needs to align its values with those of the
              humans in its environment in such a way that its actions contribute
              to the maximization of value for the humans. We propose a formal
              deﬁnition of the value alignment problem as cooperative inverse
              reinforcement learning (CIRL). A CIRL problem is a cooperative,
              partialinformation game with two agents, human and robot; both are
              rewarded according to the human’s reward function, but the robot
              does not initially know what this is. In contrast to classical IRL,
              where the human is assumed to act optimally in isolation, optimal
              CIRL solutions produce behaviors such as active teaching, active
              learning, and communicative actions that are more effective in
              achieving value alignment. We show that computing optimal joint
              policies in CIRL games can be reduced to solving a POMDP, prove
              that optimality in isolation is suboptimal in CIRL, and derive an
              approximate CIRL algorithm.},
  language = {en},
  urldate  = {2018-11-26},
  journal  = {arXiv:1606.03137 [cs]},
  author   = {Hadfield-Menell, Dylan and Dragan, Anca and Abbeel, Pieter and
              Russell, Stuart},
  month    = jun,
  year     = {2016},
  note     = {arXiv: 1606.03137},
  keywords = {Computer Science - Artificial Intelligence},
  file     = {Hadfield-Menell et al. - 2016 - Cooperative Inverse Reinforcement
              Learning.pdf:/Users/apodusenko/Zotero/storage/T7V7IDIX/Hadfield-Menell
              et al. - 2016 - Cooperative Inverse Reinforcement
              Learning.pdf:application/pdf}
}

@incollection{graves_practical_2011,
  title     = {Practical {Variational} {Inference} for {Neural} {Networks}},
  url       = {
               http://papers.nips.cc/paper/4329-practical-variational-inference-for-neural-networks.pdf
               },
  urldate   = {2018-11-22},
  booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 24},
  publisher = {Curran Associates, Inc.},
  author    = {Graves, Alex},
  editor    = {Shawe-Taylor, J. and Zemel, R. S. and Bartlett, P. L. and Pereira,
               F. and Weinberger, K. Q.},
  year      = {2011},
  pages     = {2348--2356},
  file      = {NIPS Full Text PDF:/Users/apodusenko/Zotero/storage/TAHZ26D5/Graves -
               2011 - Practical Variational Inference for Neural
               Network.pdf:application/pdf;NIPS
               Snapshot:/Users/apodusenko/Zotero/storage/SKTKGYFQ/4329-practical-variational-inference-for-neural-networks.html:text/html
               }
}

@article{friston_predictive_2009,
  title   = {Predictive coding under the free-energy principle},
  volume  = {364},
  number  = {1521},
  journal = {Philosophical Transactions of the Royal Society of London B:
             Biological Sciences},
  author  = {Friston, Karl and Kiebel, Stefan},
  year    = {2009},
  pages   = {1211--1221},
  file    = {Friston en Kiebel - 2009 - Predictive coding under the free-energy
             principle.html:/Users/apodusenko/Zotero/storage/9KKPV6MP/Friston en
             Kiebel - 2009 - Predictive coding under the free-energy
             principle.html:text/html}
}

@book{camacho_model_2013,
  title     = {Model predictive control},
  publisher = {Springer Science \& Business Media},
  author    = {Camacho, Eduardo F. and Alba, Carlos Bordons},
  year      = {2013},
  file      = {Camacho en Alba - 2013 - Model predictive
               control.pdf:/Users/apodusenko/Zotero/storage/JJ5Y6PB5/Camacho en Alba -
               2013 - Model predictive
               control.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/ISXMETKZ/books.html:text/html
               }
}

@article{chodrow_divergence_2017,
  title      = {Divergence, {Entropy}, {Information}: {An} {Opinionated} {
                Introduction} to {Information} {Theory}},
  shorttitle = {Divergence, {Entropy}, {Information}},
  url        = {http://arxiv.org/abs/1708.07459},
  abstract   = {Information theory is a mathematical theory of learning with deep
                connections with topics as diverse as artificial intelligence,
                statistical physics, and biological evolution. Many primers on the
                topic paint a broad picture with relatively little mathematical
                sophistication, while many others develop specific application
                areas in detail. In contrast, these informal notes aim to outline
                some elements of the information-theoretic "way of thinking," by
                cutting a rapid and interesting path through some of the theory's
                foundational concepts and theorems. We take the Kullback-Leibler
                divergence as our foundational concept, and then proceed to develop
                the entropy and mutual information. We discuss some of the main
                foundational results, including the Chernoff bounds as a
                characterization of the divergence; Gibbs' Theorem; and the Data
                Processing Inequality. A recurring theme is that the definitions of
                information theory support natural theorems that sound "obvious"
                when translated into English. More pithily, "information theory
                makes common sense precise." Since the focus of the notes is not
                primarily on technical details, proofs are provided only where the
                relevant techniques are illustrative of broader themes. Otherwise,
                proofs and intriguing tangents are referenced in
                liberally-sprinkled footnotes. The notes close with a highly
                nonexhaustive list of references to resources and other
                perspectives on the field.},
  urldate    = {2018-11-20},
  journal    = {arXiv:1708.07459 [physics, stat]},
  author     = {Chodrow, Philip},
  month      = aug,
  year       = {2017},
  note       = {arXiv: 1708.07459},
  keywords   = {Computer Science - Information Theory, Mathematics - Statistics
                Theory, Physics - Data Analysis, Statistics and Probability},
  file       = {arXiv\:1708.07459
                PDF:/Users/apodusenko/Zotero/storage/R3FG587K/Chodrow - 2017 -
                Divergence, Entropy, Information An Opinionated
                I.pdf:application/pdf;arXiv.org
                Snapshot:/Users/apodusenko/Zotero/storage/EJ27UMJK/1708.html:text/html}
}

@book{von_helmholtz_handbuch_1867,
  title     = {Handbuch der physiologischen {Optik}},
  volume    = {9},
  publisher = {Voss},
  author    = {Von Helmholtz, Hermann},
  year      = {1867},
  file      = {Full
               Text:/Users/apodusenko/Zotero/storage/7K69EHZ6/books.html:text/html}
}

@article{beer_dynamical_1995,
  title    = {A dynamical systems perspective on agent-environment interaction},
  volume   = {72},
  issn     = {0004-3702},
  url      = {http://www.sciencedirect.com/science/article/pii/000437029400005L},
  doi      = {10.1016/0004-3702(94)00005-L},
  abstract = {Using the language of dynamical systems theory, a general
              theoretical framework for the synthesis and analysis of autonomous
              agents is sketched. In this framework, an agent and its environment
              are modeled as two coupled dynamical systems whose mutual
              interaction is in general jointly responsible for the agent's
              behavior. In addition, the adaptive fit between an agent and its
              environment is characterized in terms of the satisfaction of a
              given constraint on the trajectories of the coupled
              agent-environment system. The utility of this framework is
              demonstrated by using it to first synthesize and then analyze a
              walking behavior for a legged agent.},
  number   = {1},
  urldate  = {2018-11-20},
  journal  = {Artificial Intelligence},
  author   = {Beer, Randall D.},
  month    = jan,
  year     = {1995},
  pages    = {173--215},
  file     = {Beer - 1995 - A dynamical systems perspective on
              agent-environme.pdf:/Users/apodusenko/Zotero/storage/7LF4Q4RH/Beer -
              1995 - A dynamical systems perspective on
              agent-environme.pdf:application/pdf;ScienceDirect
              Snapshot:/Users/apodusenko/Zotero/storage/ZQKL4F37/000437029400005L.html:text/html
              }
}

@article{furnkranz_preference-based_2012,
  title      = {Preference-based reinforcement learning: a formal framework and a
                policy iteration algorithm},
  volume     = {89},
  issn       = {1573-0565},
  shorttitle = {Preference-based reinforcement learning},
  url        = {https://doi.org/10.1007/s10994-012-5313-8},
  doi        = {10.1007/s10994-012-5313-8},
  abstract   = {This paper makes a first step toward the integration of two
                subfields of machine learning, namely preference learning and
                reinforcement learning (RL). An important motivation for a
                preference-based approach to reinforcement learning is the
                observation that in many real-world domains, numerical feedback
                signals are not readily available, or are defined arbitrarily in
                order to satisfy the needs of conventional RL algorithms. Instead,
                we propose an alternative framework for reinforcement learning, in
                which qualitative reward signals can be directly used by the
                learner. The framework may be viewed as a generalization of the
                conventional RL framework in which only a partial order between
                policies is required instead of the total order induced by their
                respective expected long-term reward.Therefore, building on novel
                methods for preference learning, our general goal is to equip the
                RL agent with qualitative policy models, such as ranking functions
                that allow for sorting its available actions from most to least
                promising, as well as algorithms for learning such models from
                qualitative feedback. As a proof of concept, we realize a first
                simple instantiation of this framework that defines preferences
                based on utilities observed for trajectories. To that end, we build
                on an existing method for approximate policy iteration based on
                roll-outs. While this approach is based on the use of
                classification methods for generalization and policy learning, we
                make use of a specific type of preference learning method called
                label ranking. Advantages of preference-based approximate policy
                iteration are illustrated by means of two case studies.},
  language   = {en},
  number     = {1},
  urldate    = {2018-11-20},
  journal    = {Machine Learning},
  author     = {Fürnkranz, Johannes and Hüllermeier, Eyke and Cheng, Weiwei and Park
                , Sang-Hyeun},
  month      = oct,
  year       = {2012},
  keywords   = {Preference learning, Reinforcement learning},
  pages      = {123--156},
  file       = {Fürnkranz e.a. - 2012 - Preference-based reinforcement learning a
                formal .pdf:/Users/apodusenko/Zotero/storage/V2ZJ6C4V/Fürnkranz e.a. -
                2012 - Preference-based reinforcement learning a formal
                .pdf:application/pdf}
}

@inproceedings{kuss_gaussian_2004,
  title     = {Gaussian processes in reinforcement learning},
  booktitle = {Advances in neural information processing systems},
  author    = {Kuss, Malte and Rasmussen, Carl E.},
  year      = {2004},
  pages     = {751--758},
  file      = {Kuss en Rasmussen - 2004 - Gaussian processes in reinforcement
               learning.pdf:/Users/apodusenko/Zotero/storage/NRIYYZM3/Kuss en
               Rasmussen - 2004 - Gaussian processes in reinforcement
               learning.pdf:application/pdf}
}

@inproceedings{das_application_2016,
  address   = {Kolkata, India},
  title     = {Application of the tuned {Kalman} filter in speech enhancement},
  isbn      = {978-1-4799-1769-3},
  url       = {http://ieeexplore.ieee.org/document/7413711/},
  doi       = {10.1109/CMI.2016.7413711},
  abstract  = {The Kalman ﬁlter has a wide range of applications, noise removal
               from corrupted speech being one of them. The ﬁlter performance is
               subject to the accurate tuning of its parameters, namely the
               process noise covariance, Q, and the measurement noise covariance,
               R. In this paper, the Kalman ﬁlter has been tuned to get a suitable
               value of Q by deﬁning the robustness and sensitivity metrics, and
               then applied on noisy speech signals. The Kalman gain is another
               factor that greatly affects ﬁlter performance. The speech signal
               has been frame-wise decomposed into silent and voiced zones, and
               the Kalman gain has been adjusted according to this distinction to
               get best overall ﬁlter performance. Finally, the algorithm has been
               applied to clean a noise corrupted known signal from the NOIZEUS
               database. It is observed that signiﬁcant noise removal has been
               achieved, both audibly and from the spectrograms of noisy and
               processed signals.},
  language  = {en},
  urldate   = {2018-11-20},
  booktitle = {2016 {IEEE} {First} {International} {Conference} on {Control}, {
               Measurement} and {Instrumentation} ({CMI})},
  publisher = {IEEE},
  author    = {Das, Orchisama and Goswami, Bhaswati and Ghosh, Ratna},
  month     = jan,
  year      = {2016},
  pages     = {62--66},
  file      = {Das et al. - 2016 - Application of the tuned Kalman filter in speech
               e.pdf:/Users/apodusenko/Zotero/storage/B5LWBHPY/Das et al. - 2016 -
               Application of the tuned Kalman filter in speech e.pdf:application/pdf}
}

@article{friston_action_2010,
  title      = {Action and behavior: a free-energy formulation},
  volume     = {102},
  shorttitle = {Action and behavior},
  number     = {3},
  journal    = {Biological cybernetics},
  author     = {Friston, Karl J. and Daunizeau, Jean and Kilner, James and Kiebel,
                Stefan J.},
  year       = {2010},
  pages      = {227--260},
  file       = {Friston e.a. - 2010 - Action and behavior a free-energy
                formulation.pdf:/Users/apodusenko/Zotero/storage/7ADF6I9G/Friston e.a.
                - 2010 - Action and behavior a free-energy
                formulation.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/RXVG26N5/s00422-010-0364-z.html:text/html
                }
}

@book{sutton_reinforcement_2018,
  title      = {Reinforcement learning: {An} introduction},
  shorttitle = {Reinforcement learning},
  publisher  = {MIT press},
  author     = {Sutton, Richard S. and Barto, Andrew G.},
  year       = {2018},
  file       = {Sutton en Barto - 2018 - Reinforcement learning An
                introduction.pdf:/Users/apodusenko/Zotero/storage/KUD7V6QL/Sutton en
                Barto - 2018 - Reinforcement learning An
                introduction.pdf:application/pdf}
}

@inproceedings{van_de_laar_variational_2017,
  title     = {Variational stabilized linear forgetting in state-space models},
  booktitle = {Signal {Processing} {Conference} ({EUSIPCO}), 2017 25th {European
               }},
  publisher = {IEEE},
  author    = {van de Laar, Thijs and Cox, Marco and van Diepen, Anouk and de Vries
               , Bert},
  year      = {2017},
  pages     = {818--822},
  file      = {
               Snapshot:/Users/apodusenko/Zotero/storage/A77F9T5T/8081321.html:text/html;supplement.pdf:/Users/apodusenko/Zotero/storage/8V37C6MI/supplement.pdf:application/pdf;van
               de Laar et al. - 2017 - Variational stabilized linear forgetting in
               state-.pdf:/Users/apodusenko/Zotero/storage/KAWNZNJN/van de Laar et al.
               - 2017 - Variational stabilized linear forgetting in
               state-.pdf:application/pdf;Variational Stabilized Linear Forgetting in
               State-Space
               Models.pdf:/Users/apodusenko/Zotero/storage/MAESRPKN/Variational
               Stabilized Linear Forgetting in State-Space Models.pdf:application/pdf}
}

@inproceedings{cox_forneylab.jl:_2018,
  address   = {Boston, MA},
  title     = {{ForneyLab}.jl: {Fast} and flexible automated inference through
               message passing in {Julia}},
  booktitle = {International {Conference} on {Probabilistic} {Programming}},
  author    = {Cox, Marco and van de Laar, Thijs and de Vries, Bert},
  month     = oct,
  year      = {2018},
  file      = {Cox et al. - 2018 - ForneyLab.jl Fast and flexible automated
               inferenc.pdf:/Users/apodusenko/Zotero/storage/QQ5TMX2X/Cox et al. -
               2018 - ForneyLab.jl Fast and flexible automated
               inferenc.pdf:application/pdf;poster_probprog.pdf:/Users/apodusenko/Zotero/storage/C9X9YV5J/poster_probprog.pdf:application/pdf
               }
}

@inproceedings{van_de_laar_forneylab.jl:_2018,
  address   = {London, UK},
  title     = {{ForneyLab}.jl: a {Julia} {Toolbox} for {Factor} {Graph}-based {
               Probabilistic} {Programming}},
  booktitle = {{JuliaCon}},
  author    = {van de Laar, Thijs and Cox, Marco and de Vries, Bert},
  month     = aug,
  year      = {2018},
  file      = {van de Laar e.a. - 2018 - ForneyLab.jl a Julia Toolbox for Factor
               Graph-bas.pdf:/Users/apodusenko/Zotero/storage/5PFDM5MB/van de Laar
               e.a. - 2018 - ForneyLab.jl a Julia Toolbox for Factor
               Graph-bas.pdf:application/pdf}
}

@inproceedings{van_de_laar_forneylab:_2018,
  address   = {Thessaloniki, Greece},
  title     = {{ForneyLab}: {A} {Toolbox} for {Biologically} {Plausible} {Free} {
               Energy} {Minimization} in {Dynamic} {Neural} {Models}},
  booktitle = {Conference on {Complex} {Systems} ({CCS})},
  author    = {van de Laar, Thijs and Cox, Marco and Senoz, Ismail and Bocharov,
               Ivan and de Vries, Bert},
  month     = sep,
  year      = {2018},
  file      = {
               poster_ccs.pdf:/Users/apodusenko/Zotero/storage/5Z97GCPB/poster_ccs.pdf:application/pdf;van
               de Laar e.a. - 2018 - ForneyLab A Toolbox for Biologically Plausible
               Fr.pdf:/Users/apodusenko/Zotero/storage/NG8TGY5M/van de Laar e.a. -
               2018 - ForneyLab A Toolbox for Biologically Plausible
               Fr.pdf:application/pdf}
}

@misc{noauthor_worldwide_nodate,
  title    = {Worldwide {Wearables} {Market} to {Nearly} {Double} by 2021, {
              According} to {IDC}},
  url      = {https://www.idc.com/getdoc.jsp?containerId=prUS42818517},
  abstract = {IDC examines consumer markets by devices, applications, networks,
              and services to provide complete solutions for succeeding in these
              expanding markets.},
  urldate  = {2018-11-16},
  journal  = {IDC: The premier global market intelligence company},
  file     = {
              Snapshot:/Users/apodusenko/Zotero/storage/FL6VV9RV/getdoc.html:text/html
              }
}

@misc{noauthor_wearables_nodate,
  title    = {Wearables shipments worldwide by product 2014-2021},
  url      = {
              https://www.statista.com/statistics/437879/wearables-worldwide-shipments-by-product-category/
              },
  abstract = {The statistic shows the projected unit shipments of wearable
              devices worldwide from 2014 to 2021, broken down by product
              category. In 2018, shipments of wearable wristwear are forecast to
              amount to around 115 million units.},
  language = {en},
  urldate  = {2018-11-16},
  journal  = {Statista},
  file     = {
              Snapshot:/Users/apodusenko/Zotero/storage/XSWWAAB7/wearables-worldwide-shipments-by-product-category.html:text/html
              }
}

@misc{ghandi_future_nodate,
  title   = {The future of biosensing wearables},
  url     = {https://rockhealth.com/reports/the-future-of-biosensing-wearables/},
  urldate = {2018-11-16},
  author  = {Ghandi, M. and Wang, Teresa}
}

@article{tehrani_wearable_2014,
  title      = {Wearable technology and wearable devices: {Everything} you need to
                know},
  shorttitle = {Wearable technology and wearable devices},
  journal    = {Wearable Devices Magazine},
  author     = {Tehrani, Kiana and Michael, Andrew},
  year       = {2014}
}

@article{raskovic_medical_2004,
  title    = {Medical {Monitoring} {Applications} for {Wearable} {Computing}},
  volume   = {47},
  issn     = {0010-4620},
  url      = {https://academic.oup.com/comjnl/article/47/4/495/368561},
  doi      = {10.1093/comjnl/47.4.495},
  abstract = {Abstract. Medical monitors have benefited from technological
              advances in the field of wireless communication, processing and
              power sources. These advances have},
  language = {en},
  number   = {4},
  urldate  = {2018-11-16},
  journal  = {The Computer Journal},
  author   = {Raskovic, Dejan and Martin, Thomas and Jovanov, Emil},
  month    = jan,
  year     = {2004},
  pages    = {495--504},
  file     = {Raskovic e.a. - 2004 - Medical Monitoring Applications for Wearable
              Computing.pdf:/Users/apodusenko/Zotero/storage/2CIW4HCL/Raskovic e.a. -
              2004 - Medical Monitoring Applications for Wearable
              Computing.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/Q7AUAGY6/368561.html:text/html
              }
}

@article{yang_user_2016,
  title      = {User acceptance of wearable devices: {An} extended perspective of
                perceived value},
  volume     = {33},
  shorttitle = {User acceptance of wearable devices},
  number     = {2},
  journal    = {Telematics and Informatics},
  author     = {Yang, Heetae and Yu, Jieun and Zo, Hangjung and Choi, Munkee},
  year       = {2016},
  pages      = {256--269},
  file       = {
                Snapshot:/Users/apodusenko/Zotero/storage/UKHQL6TB/S0736585315001069.html:text/html
                }
}

@article{patel_wearable_2015,
  title   = {Wearable devices as facilitators, not drivers, of health behavior
             change},
  volume  = {313},
  number  = {5},
  journal = {Jama},
  author  = {Patel, Mitesh S. and Asch, David A. and Volpp, Kevin G.},
  year    = {2015},
  pages   = {459--460},
  file    = {Patel e.a. - 2015 - Wearable devices as facilitators, not drivers, of
             .pdf:/Users/apodusenko/Zotero/storage/F7JSU9DB/Patel e.a. - 2015 -
             Wearable devices as facilitators, not drivers, of
             .pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/UYJ8HV3R/2089651.html:text/html
             }
}

@article{kiebel_perception_2009,
  title    = {Perception and {Hierarchical} {Dynamics}},
  volume   = {3},
  issn     = {1662-5196},
  url      = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2718783/},
  doi      = {10.3389/neuro.11.020.2009},
  abstract = {In this paper, we suggest that perception could be modeled by
              assuming that sensory input is generated by a hierarchy of
              attractors in a dynamic system. We describe a mathematical model
              which exploits the temporal structure of rapid sensory dynamics to
              track the slower trajectories of their underlying causes. This
              model establishes a proof of concept that slowly changing neuronal
              states can encode the trajectories of faster sensory signals. We
              link this hierarchical account to recent developments in the
              perception of human action; in particular artificial speech
              recognition. We argue that these hierarchical models of dynamical
              systems are a plausible starting point to develop robust
              recognition schemes, because they capture critical temporal
              dependencies induced by deep hierarchical structure. We conclude by
              suggesting that a fruitful computational neuroscience approach may
              emerge from modeling perception as non-autonomous recognition
              dynamics enslaved by autonomous hierarchical dynamics in the
              sensorium.},
  urldate  = {2018-11-15},
  journal  = {Frontiers in Neuroinformatics},
  author   = {Kiebel, Stefan J. and Daunizeau, Jean and Friston, Karl J.},
  month    = jul,
  year     = {2009},
  pmid     = {19649171},
  pmcid    = {PMC2718783},
  file     = {Kiebel e.a. - 2009 - Perception and Hierarchical
              Dynamics.pdf:/Users/apodusenko/Zotero/storage/ZDE4SHQH/Kiebel e.a. -
              2009 - Perception and Hierarchical Dynamics.pdf:application/pdf}
}

@article{ramachandran_bayesian_nodate,
  title    = {Bayesian {Inverse} {Reinforcement} {Learning}},
  abstract = {Inverse Reinforcement Learning (IRL) is the problem of learning
              the reward function underlying a Markov Decision Process given the
              dynamics of the system and the behaviour of an expert. IRL is
              motivated by situations where knowledge of the rewards is a goal by
              itself (as in preference elicitation) and by the task of
              apprenticeship learning (learning policies from an expert). In this
              paper we show how to combine prior knowledge and evidence from the
              expert’s actions to derive a probability distribution over the
              space of reward functions. We present efﬁcient algorithms that ﬁnd
              solutions for the reward learning and apprenticeship learning tasks
              that generalize well over these distributions. Experimental results
              show strong improvement for our methods over previous
              heuristic-based approaches.},
  language = {en},
  author   = {Ramachandran, Deepak},
  pages    = {6},
  file     = {Ramachandran - Bayesian Inverse Reinforcement
              Learning.pdf:/Users/apodusenko/Zotero/storage/42LAHV7G/Ramachandran -
              Bayesian Inverse Reinforcement Learning.pdf:application/pdf}
}

@incollection{buntine_active_2009,
  address   = {Berlin, Heidelberg},
  title     = {Active {Learning} for {Reward} {Estimation} in {Inverse} {
               Reinforcement} {Learning}},
  volume    = {5782},
  isbn      = {978-3-642-04173-0 978-3-642-04174-7},
  url       = {http://link.springer.com/10.1007/978-3-642-04174-7_3},
  abstract  = {Inverse reinforcement learning addresses the general problem of
               recovering a reward function from samples of a policy provided by
               an expert/demonstrator. In this paper, we introduce active learning
               for inverse reinforcement learning. We propose an algorithm that
               allows the agent to query the demonstrator for samples at speciﬁc
               states, instead of relying only on samples provided at “arbitrary”
               states. The purpose of our algorithm is to estimate the reward
               function with similar accuracy as other methods from the literature
               while reducing the amount of policy samples required from the
               expert. We also discuss the use of our algorithm in higher
               dimensional problems, using both Monte Carlo and gradient methods.
               We present illustrative results of our algorithm in several
               simulated examples of diﬀerent complexities.},
  language  = {en},
  urldate   = {2018-11-15},
  booktitle = {Machine {Learning} and {Knowledge} {Discovery} in {Databases}},
  publisher = {Springer Berlin Heidelberg},
  author    = {Lopes, Manuel and Melo, Francisco and Montesano, Luis},
  editor    = {Buntine, Wray and Grobelnik, Marko and Mladenić, Dunja and
               Shawe-Taylor, John},
  year      = {2009},
  doi       = {10.1007/978-3-642-04174-7_3},
  pages     = {31--46},
  file      = {Lopes et al. - 2009 - Active Learning for Reward Estimation in Inverse
               R.pdf:/Users/apodusenko/Zotero/storage/D3846537/Lopes et al. - 2009 -
               Active Learning for Reward Estimation in Inverse R.pdf:application/pdf}
}

@article{averbeck_theory_2015,
  title    = {Theory of {Choice} in {Bandit}, {Information} {Sampling} and {
              Foraging} {Tasks}},
  volume   = {11},
  issn     = {1553-7358},
  url      = {http://dx.plos.org/10.1371/journal.pcbi.1004164},
  doi      = {10.1371/journal.pcbi.1004164},
  language = {en},
  number   = {3},
  urldate  = {2018-11-14},
  journal  = {PLOS Computational Biology},
  author   = {Averbeck, Bruno B.},
  editor   = {Schrater, Paul},
  month    = mar,
  year     = {2015},
  pages    = {e1004164},
  file     = {Averbeck - 2015 - Theory of Choice in Bandit, Information Sampling
              a.pdf:/Users/apodusenko/Zotero/storage/M6GAJL7Q/Averbeck - 2015 -
              Theory of Choice in Bandit, Information Sampling a.pdf:application/pdf}
}

@article{aroian_probability_1947,
  title    = {The {Probability} {Function} of the {Product} of {Two} {Normally} {
              Distributed} {Variables}},
  volume   = {18},
  issn     = {0003-4851, 2168-8990},
  url      = {https://projecteuclid.org/euclid.aoms/1177730442},
  doi      = {10.1214/aoms/1177730442},
  abstract = {Let xxx and yyy follow a normal bivariate probability function
              with means X¯,Y¯X¯,Y¯{\textbackslash}bar X, {\textbackslash}bar Y,
              standard deviations σ1,σ2σ1,σ2{\textbackslash}sigma\_1, {
              \textbackslash}sigma\_2, respectively, rrr the coefficient of
              correlation, and ρ1=X¯/σ1,ρ2=Y¯/σ2ρ1=X¯/σ1,ρ2=Y¯/σ2{\textbackslash}
              rho\_1 = {\textbackslash}bar X/{\textbackslash}sigma\_1, {
              \textbackslash}rho\_2 = {\textbackslash}bar Y/{\textbackslash}sigma
              \_2. Professor C. C. Craig [1] has found the probability function
              of z=xy/σ1σ2z=xy/σ1σ2z = xy/{\textbackslash}sigma\_1{\textbackslash
              }sigma\_2 in closed form as the difference of two integrals. For
              purposes of numerical computation he has expanded this result in an
              infinite series involving powers of z,ρ1,ρ2z,ρ1,ρ2z, {
              \textbackslash}rho\_1, {\textbackslash}rho\_2, and Bessel functions
              of a certain type; in addition, he has determined the moments,
              semin-variants, and the moment generating function of zzz. However,
              for ρ1ρ1{\textbackslash}rho\_1 and ρ2ρ2{\textbackslash}rho\_2 large
              , as Craig points out, the series expansion converges very slowly.
              Even for ρ1ρ1{\textbackslash}rho\_1 and ρ2ρ2{\textbackslash}rho\_2
              as small as 2, the expansion is unwieldy. We shall show that as
              ρ1ρ1{\textbackslash}rho\_1 and ρ2→∞ρ2→∞{\textbackslash}rho\_2 {
              \textbackslash}rightarrow {\textbackslash}infty, the probability
              function of zzz approaches a normal curve and in case r=0r=0r = 0
              the Type III function and the Gram-Charlier Type A series are
              excellent approximations to the zzz distribution in the proper
              region. Numerical integration provides a substitute for the
              infinite series wherever the exact values of the probability
              function of zzz are needed. Some extensions of the main theorem are
              given in section 5 and a practical problem involving the
              probability function of zzz is solved.},
  language = {EN},
  number   = {2},
  urldate  = {2018-11-14},
  journal  = {The Annals of Mathematical Statistics},
  author   = {Aroian, Leo A.},
  month    = jun,
  year     = {1947},
  mrnumber = {MR21284},
  zmnumber = {0041.45004},
  pages    = {265--271},
  file     = {Aroian - 1947 - The Probability Function of the Product of Two
              Nor.pdf:/Users/apodusenko/Zotero/storage/ZZTE5E4K/Aroian - 1947 - The
              Probability Function of the Product of Two
              Nor.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/SSUQ637F/1177730442.html:text/html
              }
}

@inproceedings{attias_variational_1999,
  title     = {A {Variational} {Bayesian} {Framework} for {Graphical} {Models}},
  volume    = {12},
  url       = {
               http://papers.nips.cc/paper/1726-a-variational-baysian-framework-for-graphical-models.pdf
               },
  booktitle = {{NIPS}},
  author    = {Attias, Hagai},
  year      = {1999}
}

@article{chiel_brain_1997,
  title      = {The brain has a body: adaptive behavior emerges from interactions of
                nervous system, body and environment},
  volume     = {20},
  issn       = {01662236},
  shorttitle = {The brain has a body},
  url        = {http://linkinghub.elsevier.com/retrieve/pii/S0166223697011491},
  doi        = {10.1016/S0166-2236(97)01149-1},
  language   = {en},
  number     = {12},
  urldate    = {2018-11-13},
  journal    = {Trends in Neurosciences},
  author     = {Chiel, Hillel J. and Beer, Randall D.},
  month      = dec,
  year       = {1997},
  pages      = {553--557},
  file       = {Chiel and Beer - 1997 - The brain has a body adaptive behavior emerges
                fr.pdf:/Users/apodusenko/Zotero/storage/IYUJ64MM/Chiel and Beer - 1997
                - The brain has a body adaptive behavior emerges fr.pdf:application/pdf
                }
}

@article{seth_active_2016,
  title    = {Active interoceptive inference and the emotional brain},
  volume   = {371},
  issn     = {0962-8436, 1471-2970},
  url      = {
              http://rstb.royalsocietypublishing.org/lookup/doi/10.1098/rstb.2016.0007
              },
  doi      = {10.1098/rstb.2016.0007},
  language = {en},
  number   = {1708},
  urldate  = {2018-11-13},
  journal  = {Philosophical Transactions of the Royal Society B: Biological
              Sciences},
  author   = {Seth, Anil K. and Friston, Karl J.},
  month    = nov,
  year     = {2016},
  pages    = {20160007},
  file     = {Seth and Friston - 2016 - Active interoceptive inference and the
              emotional b.pdf:/Users/apodusenko/Zotero/storage/8S6ECVGN/Seth and
              Friston - 2016 - Active interoceptive inference and the emotional
              b.pdf:application/pdf}
}

@article{friston_active_2016,
  title    = {Active inference and learning},
  issn     = {0149-7634},
  url      = {http://www.sciencedirect.com/science/article/pii/S0149763416301336},
  doi      = {10.1016/j.neubiorev.2016.06.022},
  abstract = {This paper offers an active inference account of choice behaviour
              and learning. It focuses on the distinction between goal-directed
              and habitual behaviour and how they contextualise each other. We
              show that habits emerge naturally (and autodidactically) from
              sequential policy optimisation when agents are equipped with
              state-action policies. In active inference, behaviour has
              explorative (epistemic) and exploitative (pragmatic) aspects that
              are sensitive to ambiguity and risk respectively, where epistemic
              (ambiguity-resolving) behaviour enables pragmatic (reward-seeking)
              behaviour and the subsequent emergence of habits. Although
              goal-directed and habitual policies are usually associated with
              model-based and model-free schemes, we find the more important
              distinction is between belief-free and belief-based schemes. The
              underlying (variational) belief updating provides a comprehensive
              (if metaphorical) process theory for several phenomena, including
              the transfer of dopamine responses, reversal learning, habit
              formation and devaluation. Finally, we show that active inference
              reduces to a classical (Bellman) scheme, in the absence of
              ambiguity.},
  urldate  = {2016-07-05},
  journal  = {Neuroscience \& Biobehavioral Reviews},
  author   = {Friston, Karl and FitzGerald, Thomas and Rigoli, Francesco and
              Schwartenbeck, Philipp and O’Doherty, John and Pezzulo, Giovanni},
  year     = {2016},
  keywords = {Bayesian inference, Active inference, Free energy, Epistemic value
              , Information gain, Bayesian surprise, Exploitation, Exploration,
              Goal-directed, Habit learning},
  file     = {Friston e.a. - Active inference and
              learning.pdf:/Users/apodusenko/Zotero/storage/WSIHAHX9/Friston e.a. -
              Active inference and learning.pdf:application/pdf;ScienceDirect
              Snapshot:/Users/apodusenko/Zotero/storage/8EUQWN8N/S0149763416301336.html:text/html
              }
}

@misc{noauthor_dynamic_nodate,
  title    = {Dynamic {Interaction} between {Reinforcement} {Learning} and {
              Attention} in {Multidimensional} {Environments}},
  url      = {
              https://reader.elsevier.com/reader/sd/pii/S089662731631039X?token=0844DE23D8AA0C2CE116D7FC4254102CFF4BAB3D6B4EE2630567B71711FEBAB03BD40DE2F89658F8682D130C2F6149A6
              },
  language = {en},
  urldate  = {2018-11-13},
  doi      = {10.1016/j.neuron.2016.12.040},
  file     = {Accepted Version:/Users/apodusenko/Zotero/storage/5WSTQ7UM/Dynamic
              Interaction between Reinforcement
              Learning.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/DWWRJH38/S089662731631039X.html:text/html
              }
}

@article{friston_dem:_2008,
  title      = {{DEM}: a variational treatment of dynamic systems},
  volume     = {41},
  shorttitle = {{DEM}},
  number     = {3},
  journal    = {Neuroimage},
  author     = {Friston, Karl J. and Trujillo-Barreto, N. and Daunizeau, Jean},
  year       = {2008},
  pages      = {849--885},
  file       = {Friston e.a. - 2008 - DEM a variational treatment of dynamic
                systems.pdf:/Users/apodusenko/Zotero/storage/JLHTYVRW/Friston e.a. -
                2008 - DEM a variational treatment of dynamic
                systems.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/XL6R8CU4/S1053811908001894.html:text/html
                }
}

@article{daunizeau_observing_2010,
  title      = {Observing the {Observer} ({I}): {Meta}-{Bayesian} {Models} of {
                Learning} and {Decision}-{Making}},
  volume     = {5},
  issn       = {1932-6203},
  shorttitle = {Observing the {Observer} ({I})},
  url        = {https://dx.plos.org/10.1371/journal.pone.0015554},
  doi        = {10.1371/journal.pone.0015554},
  abstract   = {In this paper, we present a generic approach that can be used to
                infer how subjects make optimal decisions under uncertainty. This
                approach induces a distinction between a subject’s perceptual model
                , which underlies the representation of a hidden ‘‘state of
                affairs’’ and a response model, which predicts the ensuing
                behavioural (or neurophysiological) responses to those inputs. We
                start with the premise that subjects continuously update a
                probabilistic representation of the causes of their sensory inputs
                to optimise their behaviour. In addition, subjects have preferences
                or goals that guide decisions about actions given the above
                uncertain representation of these hidden causes or state of
                affairs. From a Bayesian decision theoretic perspective, uncertain
                representations are so-called ‘‘posterior’’ beliefs, which are
                influenced by subjective ‘‘prior’’ beliefs. Preferences and goals
                are encoded through a ‘‘loss’’ (or ‘‘utility’’) function, which
                measures the cost incurred by making any admissible decision for
                any given (hidden) state of affair. By assuming that subjects make
                optimal decisions on the basis of updated (posterior) beliefs and
                utility (loss) functions, one can evaluate the likelihood of
                observed behaviour. Critically, this enables one to ‘‘observe the
                observer’’, i.e. identify (context- or subject-dependent) prior
                beliefs and utility-functions using psychophysical or
                neurophysiological measures. In this paper, we describe the main
                theoretical components of this meta-Bayesian approach (i.e. a
                Bayesian treatment of Bayesian decision theoretic predictions). In
                a companion paper (‘Observing the observer (II): deciding when to
                decide’), we describe a concrete implementation of it and
                demonstrate its utility by applying it to simulated and real
                reaction time data from an associative learning task.},
  language   = {en},
  number     = {12},
  urldate    = {2018-11-08},
  journal    = {PLoS ONE},
  author     = {Daunizeau, Jean and den Ouden, Hanneke E. M. and Pessiglione,
                Matthias and Kiebel, Stefan J. and Stephan, Klaas E. and Friston,
                Karl J.},
  editor     = {Sporns, Olaf},
  month      = dec,
  year       = {2010},
  pages      = {e15554},
  file       = {Daunizeau et al. - 2010 - Observing the Observer (I) Meta-Bayesian
                Models o.pdf:/Users/apodusenko/Zotero/storage/53IRHY4J/Daunizeau et al.
                - 2010 - Observing the Observer (I) Meta-Bayesian Models
                o.pdf:application/pdf}
}

@article{daunizeau_observing_2010-1,
  title      = {Observing the {Observer} ({II}): {Deciding} {When} to {Decide}},
  volume     = {5},
  issn       = {1932-6203},
  shorttitle = {Observing the {Observer} ({II})},
  url        = {https://dx.plos.org/10.1371/journal.pone.0015555},
  doi        = {10.1371/journal.pone.0015555},
  abstract   = {In a companion paper [1], we have presented a generic approach for
                inferring how subjects make optimal decisions under uncertainty.
                From a Bayesian decision theoretic perspective, uncertain
                representations correspond to ‘‘posterior’’ beliefs, which result
                from integrating (sensory) information with subjective ‘‘prior’’
                beliefs. Preferences and goals are encoded through a ‘‘loss’’ (or
                ‘‘utility’’) function, which measures the cost incurred by making
                any admissible decision for any given (hidden or unknown) state of
                the world. By assuming that subjects make optimal decisions on the
                basis of updated (posterior) beliefs and utility (loss) functions,
                one can evaluate the likelihood of observed behaviour. In this
                paper, we describe a concrete implementation of this meta-Bayesian
                approach (i.e. a Bayesian treatment of Bayesian decision theoretic
                predictions) and demonstrate its utility by applying it to both
                simulated and empirical reaction time data from an associative
                learning task. Here, inter-trial variability in reaction times is
                modelled as reflecting the dynamics of the subjects’ internal
                recognition process, i.e. the updating of representations
                (posterior densities) of hidden states over trials while subjects
                learn probabilistic audio-visual associations. We use this paradigm
                to demonstrate that our meta-Bayesian framework allows for (i)
                probabilistic inference on the dynamics of the subject’s
                representation of environmental states, and for (ii) model
                selection to disambiguate between alternative preferences (loss
                functions) human subjects could employ when dealing with trade-offs
                , such as between speed and accuracy. Finally, we illustrate how
                our approach can be used to quantify subjective beliefs and
                preferences that underlie inter-individual differences in
                behaviour.},
  language   = {en},
  number     = {12},
  urldate    = {2018-11-08},
  journal    = {PLoS ONE},
  author     = {Daunizeau, Jean and den Ouden, Hanneke E. M. and Pessiglione,
                Matthias and Kiebel, Stefan J. and Friston, Karl J. and Stephan,
                Klaas E.},
  editor     = {Sporns, Olaf},
  month      = dec,
  year       = {2010},
  pages      = {e15555},
  file       = {Daunizeau et al. - 2010 - Observing the Observer (II) Deciding When to
                Deci.pdf:/Users/apodusenko/Zotero/storage/UBNUXNBI/Daunizeau et al. -
                2010 - Observing the Observer (II) Deciding When to
                Deci.pdf:application/pdf}
}

@article{rezende_variational_2015,
  title    = {Variational {Inference} with {Normalizing} {Flows}},
  url      = {http://arxiv.org/abs/1505.05770},
  abstract = {The choice of approximate posterior distribution is one of the
              core problems in variational inference. Most applications of
              variational inference employ simple families of posterior
              approximations in order to allow for efficient inference, focusing
              on mean-field or other simple structured approximations. This
              restriction has a significant impact on the quality of inferences
              made using variational methods. We introduce a new approach for
              specifying flexible, arbitrarily complex and scalable approximate
              posterior distributions. Our approximations are distributions
              constructed through a normalizing flow, whereby a simple initial
              density is transformed into a more complex one by applying a
              sequence of invertible transformations until a desired level of
              complexity is attained. We use this view of normalizing flows to
              develop categories of finite and infinitesimal flows and provide a
              unified view of approaches for constructing rich posterior
              approximations. We demonstrate that the theoretical advantages of
              having posteriors that better match the true posterior, combined
              with the scalability of amortized variational approaches, provides
              a clear improvement in performance and applicability of variational
              inference.},
  urldate  = {2018-10-30},
  journal  = {arXiv:1505.05770 [cs, stat]},
  author   = {Rezende, Danilo Jimenez and Mohamed, Shakir},
  month    = may,
  year     = {2015},
  note     = {arXiv: 1505.05770},
  keywords = {Statistics - Machine Learning, Computer Science - Artificial
              Intelligence, Statistics - Computation, Statistics - Methodology,
              Computer Science - Machine Learning},
  file     = {arXiv\:1505.05770
              PDF:/Users/apodusenko/Zotero/storage/GTS9JNEG/Rezende and Mohamed -
              2015 - Variational Inference with Normalizing
              Flows.pdf:application/pdf;arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/TVDBG2PE/1505.html:text/html}
}

@article{bijl_online_2015,
  title      = {Online sparse {Gaussian} process regression using {FITC} and {PITC}
                approximations**{This} research is supported by the {Dutch} {
                Technology} {Foundation} {STW}, which is part of the {Netherlands} {
                Organisation} for {Scientific} {Research} ({NWO}), and which is partly
                funded by the {Ministry} of {Economic} {Affairs}. {The} work was also
                supported by the {Swedish} research {Council} ({VR}) via the project {
                Probabilistic} modeling of dynamical systems ({Contract} number:
                621-2013-5524).},
  volume     = {48},
  issn       = {24058963},
  shorttitle = {Online sparse {Gaussian} process regression using {FITC} and {
                PITC} approximations**{This} research is supported by the {Dutch}
                {Technology} {Foundation} {STW}, which is part of the {
                Netherlands} {Organisation} for {Scientific} {Research} ({NWO}),
                and which is partly funded by the {Ministry} of {Economic} {
                Affairs}. {The} work was also supported by the {Swedish} research
                {Council} ({VR}) via the project {Probabilistic} modeling of
                dynamical systems ({Contract} number},
  url        = {https://linkinghub.elsevier.com/retrieve/pii/S2405896315028360},
  doi        = {10.1016/j.ifacol.2015.12.212},
  language   = {en},
  number     = {28},
  urldate    = {2018-11-01},
  journal    = {IFAC-PapersOnLine},
  author     = {Bijl, Hildo and van Wingerden, Jan-Willem and B. Schön, Thomas and
                Verhaegen, Michel},
  year       = {2015},
  pages      = {703--708},
  file       = {Bijl et al. - 2015 - Online sparse Gaussian process regression using
                FI.pdf:/Users/apodusenko/Zotero/storage/A9KKB8VK/Bijl et al. - 2015 -
                Online sparse Gaussian process regression using FI.pdf:application/pdf}
}

@article{brochu_tutorial_2010,
  title    = {A {Tutorial} on {Bayesian} {Optimization} of {Expensive} {Cost} {
              Functions}, with {Application} to {Active} {User} {Modeling} and {
              Hierarchical} {Reinforcement} {Learning}},
  url      = {http://arxiv.org/abs/1012.2599},
  abstract = {We present a tutorial on Bayesian optimization, a method of
              finding the maximum of expensive cost functions. Bayesian
              optimization employs the Bayesian technique of setting a prior over
              the objective function and combining it with evidence to get a
              posterior function. This permits a utility-based selection of the
              next observation to make on the objective function, which must take
              into account both exploration (sampling from areas of high
              uncertainty) and exploitation (sampling areas likely to offer
              improvement over the current best observation). We also present two
              detailed extensions of Bayesian optimization, with
              experiments---active user modelling with preferences, and
              hierarchical reinforcement learning---and a discussion of the pros
              and cons of Bayesian optimization based on our experiences.},
  urldate  = {2018-10-30},
  journal  = {arXiv:1012.2599 [cs]},
  author   = {Brochu, Eric and Cora, Vlad M. and de Freitas, Nando},
  month    = dec,
  year     = {2010},
  note     = {arXiv: 1012.2599},
  keywords = {G.3, I.2.6, Computer Science - Machine Learning, G.1.6},
  file     = {arXiv\:1012.2599 PDF:/Users/apodusenko/Zotero/storage/JL32RCWN/Brochu
              et al. - 2010 - A Tutorial on Bayesian Optimization of Expensive
              C.pdf:application/pdf;arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/ZR6937K6/1012.html:text/html}
}

@article{jin_inverse_2015,
  title    = {Inverse {Reinforcement} {Learning} via {Deep} {Gaussian} {Process}},
  url      = {http://arxiv.org/abs/1512.08065},
  abstract = {We propose a new approach to inverse reinforcement learning (IRL)
              based on the deep Gaussian process (deep GP) model, which is
              capable of learning complicated reward structures with few
              demonstrations. Our model stacks multiple latent GP layers to learn
              abstract representations of the state feature space, which is
              linked to the demonstrations through the Maximum Entropy learning
              framework. Incorporating the IRL engine into the nonlinear latent
              structure renders existing deep GP inference approaches
              intractable. To tackle this, we develop a non-standard variational
              approximation framework which extends previous inference schemes.
              This allows for approximate Bayesian treatment of the feature space
              and guards against overfitting. Carrying out representation and
              inverse reinforcement learning simultaneously within our model
              outperforms state-of-the-art approaches, as we demonstrate with
              experiments on standard benchmarks ("object world","highway driving
              ") and a new benchmark ("binary world").},
  urldate  = {2018-10-30},
  journal  = {arXiv:1512.08065 [cs, stat]},
  author   = {Jin, Ming and Damianou, Andreas and Abbeel, Pieter and Spanos,
              Costas},
  month    = dec,
  year     = {2015},
  note     = {arXiv: 1512.08065},
  keywords = {Statistics - Machine Learning, Computer Science - Robotics,
              Computer Science - Machine Learning},
  file     = {arXiv\:1512.08065 PDF:/Users/apodusenko/Zotero/storage/PY4TW2HF/Jin et
              al. - 2015 - Inverse Reinforcement Learning via Deep Gaussian
              P.pdf:application/pdf;arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/IW7TLFY2/1512.html:text/html}
}

@article{gregor_temporal_2018,
  title    = {Temporal {Difference} {Variational} {Auto}-{Encoder}},
  url      = {http://arxiv.org/abs/1806.03107},
  abstract = {One motivation for learning generative models of environments is
              to use them as simulators for model-based reinforcement learning.
              Yet, it is intuitively clear that when time horizons are long,
              rolling out single step transitions is inefﬁcient and often
              prohibitive. In this paper, we propose a generative model that
              learns state representations containing explicit beliefs about
              states several time steps in the future and that can be rolled out
              directly in these states without executing single step transitions.
              The model is trained on pairs of temporally separated time points,
              using an analogue of temporal difference learning used in
              reinforcement learning, taking the belief about possible futures at
              one time point as a bootstrap for training the belief at an earlier
              time. While we focus purely on the study of the model rather than
              its use in reinforcement learning, the model architecture we design
              respects agents’ constraints as it builds the representation
              online.},
  language = {en},
  urldate  = {2018-10-29},
  journal  = {arXiv:1806.03107 [cs, stat]},
  author   = {Gregor, Karol and Besse, Frederic},
  month    = jun,
  year     = {2018},
  note     = {arXiv: 1806.03107},
  keywords = {Statistics - Machine Learning, Computer Science - Machine Learning
              },
  file     = {Gregor and Besse - 2018 - Temporal Difference Variational
              Auto-Encoder.pdf:/Users/apodusenko/Zotero/storage/V7AUMGHR/Gregor and
              Besse - 2018 - Temporal Difference Variational
              Auto-Encoder.pdf:application/pdf}
}

@article{hernandez_variational_2018,
  title    = {Variational {Encoding} of {Complex} {Dynamics}},
  volume   = {97},
  issn     = {2470-0045, 2470-0053},
  url      = {http://arxiv.org/abs/1711.08576},
  doi      = {10.1103/PhysRevE.97.062412},
  abstract = {Often the analysis of time-dependent chemical and biophysical
              systems produces high-dimensional time-series data for which it can
              be difficult to interpret which individual features are most
              salient. While recent work from our group and others has
              demonstrated the utility of time-lagged co-variate models to study
              such systems, linearity assumptions can limit the compression of
              inherently nonlinear dynamics into just a few characteristic
              components. Recent work in the field of deep learning has led to
              the development of variational autoencoders (VAE), which are able
              to compress complex datasets into simpler manifolds. We present the
              use of a time-lagged VAE, or variational dynamics encoder (VDE), to
              reduce complex, nonlinear processes to a single embedding with high
              fidelity to the underlying dynamics. We demonstrate how the VDE is
              able to capture nontrivial dynamics in a variety of examples,
              including Brownian dynamics and atomistic protein folding.
              Additionally, we demonstrate a method for analyzing the VDE model,
              inspired by saliency mapping, to determine what features are
              selected by the VDE model to describe dynamics. The VDE presents an
              important step in applying techniques from deep learning to more
              accurately model and interpret complex biophysics.},
  number   = {6},
  urldate  = {2018-10-29},
  journal  = {Physical Review E},
  author   = {Hernández, Carlos X. and Wayment-Steele, Hannah K. and Sultan,
              Mohammad M. and Husic, Brooke E. and Pande, Vijay S.},
  month    = jun,
  year     = {2018},
  note     = {arXiv: 1711.08576},
  keywords = {Statistics - Machine Learning, Physics - Computational Physics,
              Physics - Biological Physics, Physics - Chemical Physics,
              Quantitative Biology - Biomolecules},
  file     = {arXiv\:1711.08576
              PDF:/Users/apodusenko/Zotero/storage/6EN9P37J/Hernández et al. - 2018 -
              Variational Encoding of Complex Dynamics.pdf:application/pdf;arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/LZNBXRSY/1711.html:text/html}
}

@article{safa_real_2017,
  title    = {The {Real} {Time} {Implementation} on {DSP} of {Speech} {Enhancement}
              {Based} on {Kalman} {Filter} and {Wavelet} {Thresholding}},
  volume   = {10},
  issn     = {0974 -5645},
  url      = {http://www.indjst.org/index.php/indjst/article/view/115044},
  doi      = {10.17485/ijst/2017/v10i24/115044},
  abstract = {Background/Objectives: The speech signal is an attractive research
              issue that solves signal noises. For this reason, the paper main
              goal is to effectively ensure speech quality. An accurate Optimized
              Speech Enhancement Algorithm (OSEA) has been proposed and
              implemented to improve speech intelligibility and quality. The
              proposed approach was developed and implemented through a Discrete
              Wavelet Transform (DWT) and kalman filter methods. To assess speech
              quality, various noisy types for each SNR level were used. The
              obtained results were compared with a wavelet based speech
              enhancement using objective assessments like SNR, NRMSE and PESQ.
              Indeed, the obtained results demonstrated a better speech quality
              .All prepared tests have been implemented on a TMS320C6416 fixed
              –point digital signal processor which appears to be suitable
              platform for real-time requirements.},
  language = {en},
  number   = {24},
  urldate  = {2018-11-07},
  journal  = {Indian Journal of Science and Technology},
  author   = {Safa, Saoud and Mouhamed, Bennasr and Adnen, Cherif},
  month    = jun,
  year     = {2017},
  keywords = {Digital Signal Processor, Discrete Wavelet Transform, Kalman
              Filter, Real-Time, Speech Enhancement},
  file     = {Safa et al. - 2017 - The Real Time Implementation on DSP of Speech
              Enha.html:/Users/apodusenko/Zotero/storage/ZNIJKCJM/Safa et al. - 2017
              - The Real Time Implementation on DSP of Speech
              Enha.html:text/html;Safa et al. - 2017 - The Real Time Implementation
              on DSP of Speech
              Enha.pdf:/Users/apodusenko/Zotero/storage/RYF86EWV/Safa et al. - 2017 -
              The Real Time Implementation on DSP of Speech Enha.pdf:application/pdf}
}

@article{moens_variational_2017,
  title     = {Variational {Treatment} of {Trial}-by-{Trial} {Drift}-{Diffusion} {
               Models} of {Behaviour}},
  copyright = {© 2017, Posted by Cold Spring Harbor Laboratory. This pre-print
               is available under a Creative Commons License
               (Attribution-NonCommercial-NoDerivs 4.0 International), CC
               BY-NC-ND 4.0, as described at
               http://creativecommons.org/licenses/by-nc-nd/4.0/},
  url       = {https://www.biorxiv.org/content/early/2017/11/16/220517},
  doi       = {10.1101/220517},
  abstract  = {Drift-Diffusion Models (DDM) are widely used to model behaviour.
               Accounting for trial-to-trial variations of the parameters in DDM
               leads to better fits and allows researchers to investigate
               variations of parameters over time. However, parameter estimation
               with conventional integration techniques is slow and cannot be
               applied to large problems. Here we propose to rely on Variational
               Bayesian inference to estimate the posterior probability of the
               parameters at the local (trial) and global (subject and population)
               levels. We present a data-driven model that uses Inference Networks
               to estimate an approximate distribution for each of the local
               parameters given an arbitrary set of regressors. We apply the
               method to simulated and real datasets and show that it converges
               within reasonable time frame, achieves good fitting performance and
               highlights complex auto-correlation structures between trials.
               Using simulations, we show that our approach outperforms the
               gold-standard Linear-Regression Model, estimated both using
               Variational Inference and Markov-Chain Monte-Carlo method.
               Therefore, the present Bayesian approach, relying on variational
               inference networks, is a promising method to solve trial-by-trial
               DDM.},
  language  = {en},
  urldate   = {2018-03-27},
  journal   = {bioRxiv},
  author    = {Moens, Vincent and Zenon, Alexandre},
  month     = nov,
  year      = {2017},
  pages     = {220517},
  file      = {Moens en Zenon - 2017 - Variational Treatment of Trial-by-Trial
               Drift-Diff.pdf:/Users/apodusenko/Zotero/storage/JKR66AL9/Moens en Zenon
               - 2017 - Variational Treatment of Trial-by-Trial
               Drift-Diff.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/7T5RKD99/220517.html:text/html
               }
}

@incollection{bitzer_brain_2015,
  title     = {The {Brain} {Uses} {Reliability} of {Stimulus} {Information} when {
               Making} {Perceptual} {Decisions}},
  url       = {
               http://papers.nips.cc/paper/5789-the-brain-uses-reliability-of-stimulus-information-when-making-perceptual-decisions.pdf
               },
  urldate   = {2018-11-07},
  booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 28},
  publisher = {Curran Associates, Inc.},
  author    = {Bitzer, Sebastian and Kiebel, Stefan},
  editor    = {Cortes, C. and Lawrence, N. D. and Lee, D. D. and Sugiyama, M. and
               Garnett, R.},
  year      = {2015},
  pages     = {1045--1053},
  file      = {Bitzer and Kiebel - 2015 - The Brain Uses Reliability of Stimulus
               Information.pdf:/Users/apodusenko/Zotero/storage/WSHQFLJK/Bitzer and
               Kiebel - 2015 - The Brain Uses Reliability of Stimulus
               Information.pdf:application/pdf;NIPS
               Snapshot:/Users/apodusenko/Zotero/storage/5BJITMIU/5789-the-brain-uses-reliability-of-stimulus-information-when-making-perceptual-decisions.html:text/html
               }
}

@article{fard_bayesian_2017,
  title    = {A {Bayesian} {Reformulation} of the {Extended} {Drift}-{Diffusion} {
              Model} in {Perceptual} {Decision} {Making}},
  volume   = {11},
  issn     = {1662-5188},
  url      = {https://www.frontiersin.org/articles/10.3389/fncom.2017.00029/full},
  doi      = {10.3389/fncom.2017.00029},
  abstract = {Perceptual decision making can be described as a process of
              accumulating evidence to a bound which has been formalized within
              drift-diffusion models. Recently, an equivalent Bayesian model has
              been proposed. In contrast to standard drift-diffusion models, this
              Bayesian model directly links information in the stimulus to the
              decision process. Here, we extend this Bayesian model further and
              allow inter-trial variability of two parameters following the
              extended version of the drift-diffusion model. We derive parameter
              distributions for the Bayesian model and show that they lead to
              predictions that are qualitatively equivalent to those made by the
              extended drift-diffusion model. Further, we demonstrate the
              usefulness of the extended Bayesian model for the analysis of
              concrete behavioral data. Specifically, using Bayesian model
              selection, we find evidence that including additional inter-trial
              parameter variability provides for a better model, when the model
              is constrained by trial-wise stimulus features. This result is
              remarkable because it was derived using just 200 trials per
              condition, which is typically thought to be insufficient for
              identifying variability parameters in drift-diffusion models. In
              sum, we present a Bayesian analysis, which provides for a novel and
              promising analysis of perceptual decision making experiments.},
  language = {English},
  urldate  = {2018-11-07},
  journal  = {Frontiers in Computational Neuroscience},
  author   = {Fard, Pouyan R. and Park, Hame and Warkentin, Andrej and Kiebel,
              Stefan J. and Bitzer, Sebastian},
  year     = {2017},
  keywords = {Bayesian Models, drift-diffusion model, exact input modeling,
              Model Comparison, parameter fitting, perceptual decision making,
              single-trial models},
  file     = {Fard et al. - 2017 - A Bayesian Reformulation of the Extended
              Drift-Dif.pdf:/Users/apodusenko/Zotero/storage/HDYD6U48/Fard et al. -
              2017 - A Bayesian Reformulation of the Extended
              Drift-Dif.pdf:application/pdf}
}

@article{tervo_direction_2015,
  title    = {Direction of {Arrival} {Estimation} of {Reflections} from {Room} {
              Impulse} {Responses} {Using} a {Spherical} {Microphone} {Array}},
  volume   = {23},
  issn     = {2329-9290},
  doi      = {10.1109/TASLP.2015.2439573},
  abstract = {This paper studies the direction of arrival estimation of
              reflections in short time windows of room impulse responses
              measured with a spherical microphone array. Spectral-based methods,
              such as multiple signal classification (MUSIC) and beamforming, are
              commonly used in the analysis of spatial room impulse responses.
              However, the room acoustic reflections are highly correlated or
              even coherent in a single analysis window and this imposes
              limitations on the use of spectral-based methods. Here, we apply
              maximum likelihood (ML) methods, which are suitable for direction
              of arrival estimation of coherent reflections. These methods have
              been earlier developed in the linear space domain and here we
              present the ML methods in the context of spherical microphone array
              processing and room impulse responses. Experiments are conducted
              with simulated and real data using the em32 Eigenmike. The results
              show that direction estimation with ML methods is more robust
              against noise and less biased than MUSIC or beamforming.},
  number   = {10},
  journal  = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  author   = {Tervo, S. and Politis, A.},
  month    = oct,
  year     = {2015},
  keywords = {Estimation, Acoustics, Arrays, Covariance matrices,
              Direction-of-arrival estimation, array signal processing,
              beamforming, Direction of arrival (DOA), direction-of-arrival
              estimation, em32 Eigenmike, linear space domain, maximum likelihood
              estimation, maximum likelihood method, microphone arrays,
              Microphones, multiple signal classification, MUSIC, reflection
              direction of arrival estimation, room acoustics, room impulse
              response, signal classification, Smoothing methods, spatial room
              impulse response, spectral-based method, spherical microphone array
              , spherical microphone arrays, transient response},
  pages    = {1539--1551},
  file     = {Tervo and Politis - 2015 - Direction of Arrival Estimation of
              Reflections fro.html:/Users/apodusenko/Zotero/storage/M6XFE75T/Tervo
              and Politis - 2015 - Direction of Arrival Estimation of Reflections
              fro.html:text/html;Tervo and Politis - 2015 - Direction of Arrival
              Estimation of Reflections
              fro.pdf:/Users/apodusenko/Zotero/storage/KMVXABXU/Tervo and Politis -
              2015 - Direction of Arrival Estimation of Reflections
              fro.pdf:application/pdf}
}

@article{wrigley_tensor_nodate,
  title    = {Tensor {Belief} {Propagation}},
  abstract = {We propose a new approximate inference algorithm for graphical
              models, tensor belief propagation, based on approximating the
              messages passed in the junction tree algorithm. Our algorithm
              represents the potential functions of the graphical model and all
              messages on the junction tree compactly as mixtures of rank-1
              tensors. Using this representation, we show how to perform the
              operations required for inference on the junction tree efﬁciently:
              marginalisation can be computed quickly due to the factored form of
              rank-1 tensors while multiplication can be approximated using
              sampling. Our analysis gives sufﬁcient conditions for the algorithm
              to perform well, including for the case of high-treewidth graphs,
              for which exact inference is intractable. We compare our algorithm
              experimentally with several approximate inference algorithms and
              show that it performs well.},
  language = {en},
  author   = {Wrigley, Andrew and Lee, Wee Sun and Ye, Nan},
  pages    = {9},
  file     = {Wrigley et al. - Tensor Belief
              Propagation.pdf:/Users/apodusenko/Zotero/storage/5N2MZ6C4/Wrigley et
              al. - Tensor Belief Propagation.pdf:application/pdf}
}

@incollection{kellermann_beamforming_2008,
  address   = {New York, NY},
  title     = {Beamforming for {Speech} and {Audio} {Signals}},
  isbn      = {978-0-387-30441-0},
  url       = {https://doi.org/10.1007/978-0-387-30441-0_35},
  abstract  = {If microphone arrays instead of a single microphone are employed
               for sampling acoustic wavefields, signal processing of the sensor
               data can exploit the spatial diversity to better detect or extract
               desired source signals and to suppress unwanted interference.
               Beamforming represents a class of such multichannel signal
               processing algorithms and suggests a spatial filtering which points
               a beam of increased sensitivity to desired source locations while
               suppressing signals originating from all other locations. While
               beamforming techniques are also extensively used in other areas,
               e.g. in underwater acoustics, ultrasound diagnostics, and radio
               communications [1–3], the treatment is concentrating here on
               wideband acoustic signals in the audio frequency range.},
  language  = {en},
  urldate   = {2018-10-29},
  booktitle = {Handbook of {Signal} {Processing} in {Acoustics}},
  publisher = {Springer New York},
  author    = {Kellermann, Walter},
  editor    = {Havelock, David and Kuwano, Sonoko and Vorländer, Michael},
  year      = {2008},
  doi       = {10.1007/978-0-387-30441-0_35},
  keywords  = {Adaptive Beamforming, Array Response, Audio Signal, Blind Source
               Separation, Microphone Array},
  pages     = {691--702},
  file      = {Kellermann - 2008 - Beamforming for Speech and Audio
               Signals.pdf:/Users/apodusenko/Zotero/storage/BLFHKE87/Kellermann - 2008
               - Beamforming for Speech and Audio Signals.pdf:application/pdf}
}

@article{qian_deep_2018,
  title    = {Deep {Learning} {Based} {Speech} {Beamforming}},
  url      = {http://arxiv.org/abs/1802.05383},
  abstract = {Multi-channel speech enhancement with ad-hoc sensors has been a
              challenging task. Speech model guided beamforming algorithms are
              able to recover natural sounding speech, but the speech models tend
              to be oversimplified or the inference would otherwise be too
              complicated. On the other hand, deep learning based enhancement
              approaches are able to learn complicated speech distributions and
              perform efficient inference, but they are unable to deal with
              variable number of input channels. Also, deep learning approaches
              introduce a lot of errors, particularly in the presence of unseen
              noise types and settings. We have therefore proposed an enhancement
              framework called DEEPBEAM, which combines the two complementary
              classes of algorithms. DEEPBEAM introduces a beamforming filter to
              produce natural sounding speech, but the filter coefficients are
              determined with the help of a monaural speech enhancement neural
              network. Experiments on synthetic and real-world data show that
              DEEPBEAM is able to produce clean, dry and natural sounding speech,
              and is robust against unseen noise.},
  urldate  = {2018-10-29},
  journal  = {arXiv:1802.05383 [cs, eess]},
  author   = {Qian, Kaizhi and Zhang, Yang and Chang, Shiyu and Yang, Xuesong and
              Florencio, Dinei and Hasegawa-Johnson, Mark},
  month    = feb,
  year     = {2018},
  note     = {arXiv: 1802.05383},
  keywords = {Computer Science - Sound, Computer Science - Artificial
              Intelligence, Computer Science - Computation and Language,
              Electrical Engineering and Systems Science - Audio and Speech
              Processing, Electrical Engineering and Systems Science - Signal
              Processing},
  file     = {Qian et al. - 2018 - Deep Learning Based Speech
              Beamforming.html:/Users/apodusenko/Zotero/storage/GSWTYUVQ/Qian et al.
              - 2018 - Deep Learning Based Speech Beamforming.html:text/html;Qian et
              al. - 2018 - Deep Learning Based Speech
              Beamforming.pdf:/Users/apodusenko/Zotero/storage/9B9YPPAG/Qian et al. -
              2018 - Deep Learning Based Speech Beamforming.pdf:application/pdf}
}

@inproceedings{rakesh_performance_2017,
  title     = {Performance evaluation of beamforming techniques for speech
               enhancement},
  doi       = {10.1109/ICSCN.2017.8085647},
  abstract  = {This paper presents speech enhancement using fixed and adaptive
               beamforming techniques to improve quality of speech signal in noisy
               environment. Microphone arrays provide a means of enhancing a
               desired signal in the presence of corrupting noise sources using
               spatial filtering. In this paper, Delay and Sum Beamformer (DSB)
               and Generalized Side lobe Canceller (GSC) beamformer are
               implemented using microphone array and their performance are
               evaluated by considering various noises under different SNR levels.
               Enhanced speech characteristics are represented in time domain and
               frequency domain. The interference is nullified and the beamformer
               is steered towards desired direction using spatial response. An
               enhanced speech with improved SNR is achieved by generalized side
               lobe canceller with Least Mean Square (LMS) algorithm than Delay
               and Sum beamformer.},
  booktitle = {2017 {Fourth} {International} {Conference} on {Signal} {
               Processing}, {Communication} and {Networking} ({ICSCN})},
  author    = {Rakesh, P. and Priyanka, S. S. and Kumar, T. K.},
  month     = mar,
  year      = {2017},
  keywords  = {Speech enhancement, least mean squares methods, Signal to noise
               ratio, Speech, adaptive filters, Array signal processing, array
               signal processing, adaptive beamforming techniques, Delay and Sum
               beamformer, Delay and Sum Beamformer, Delays, fixed beamforming
               techniques, Generalized Side lobe Canceller, Generalized Side lobe
               Canceller beamformer, Least Mean Square algorithm, microphone array
               , Microphone arrays, noisy environment, Signal to Noise Ratio,
               spatial filtering, spatial filters, Spatial Response, speech
               enhancement, speech signal quality},
  pages     = {1--5},
  file      = {Rakesh et al. - 2017 - Performance evaluation of beamforming
               techniques f.html:/Users/apodusenko/Zotero/storage/FPTQPB3I/Rakesh et
               al. - 2017 - Performance evaluation of beamforming techniques
               f.html:text/html;Rakesh et al. - 2017 - Performance evaluation of
               beamforming techniques
               f.pdf:/Users/apodusenko/Zotero/storage/SX2V9XHL/Rakesh et al. - 2017 -
               Performance evaluation of beamforming techniques f.pdf:application/pdf}
}

@inproceedings{gozcu_manifold_2013,
  address   = {St. Martin, France},
  title     = {Manifold sparse beamforming},
  isbn      = {978-1-4673-3146-3 978-1-4673-3144-9},
  url       = {http://ieeexplore.ieee.org/document/6714020/},
  doi       = {10.1109/CAMSAP.2013.6714020},
  abstract  = {We consider the minimum variance distortionless response (MVDR)
               beamforming problems where the array covariance matrix is rank
               deﬁcient. The conventional approach handles such rank-deﬁciencies
               via diagonal loading on the covariance matrix. In this setting, we
               show that the array weights for optimal signal estimation can admit
               a sparse representation on the array manifold. To exploit this
               structure, we propose a convex regularizer in a grid-free fashion,
               which requires semi-deﬁnite programming. We then provide numerical
               evidence showing that the new formulation can signiﬁcantly
               outperform diagonal loading when the regularization parameters are
               correctly tuned.},
  language  = {en},
  urldate   = {2018-10-29},
  booktitle = {2013 5th {IEEE} {International} {Workshop} on {Computational} {
               Advances} in {Multi}-{Sensor} {Adaptive} {Processing} ({CAMSAP})},
  publisher = {IEEE},
  author    = {Gozcu, Baran and Asaei, Afsaneh and Cevher, Volkan},
  month     = dec,
  year      = {2013},
  pages     = {113--116},
  file      = {Gozcu et al. - 2013 - Manifold sparse
               beamforming.pdf:/Users/apodusenko/Zotero/storage/BJI93V45/Gozcu et al.
               - 2013 - Manifold sparse beamforming.pdf:application/pdf}
}

@article{cherkassky_new_2016,
  title      = {New {Insights} into the {Kalman} {Filter} {Beamformer}: {Applications
                } to {Speech} and {Robustness}},
  volume     = {23},
  issn       = {1070-9908},
  shorttitle = {New {Insights} into the {Kalman} {Filter} {Beamformer}},
  doi        = {10.1109/LSP.2016.2519859},
  abstract   = {Statistically optimal spatial processors (also referred to as
                data-dependent beamformers) are widely-used spatial focusing
                techniques for desired source extraction. The Kalman filter-based
                beamformer (KFB) [1] is a recursive Bayesian method for
                implementing the beamformer. This letter provides new insights into
                the KFB. Specifically, we adopt the KFB framework to the task of
                speech extraction. We formalize the KFB with a set of linear
                constraints and present its equivalence to the linearly constrained
                minimum power (LCMP) beamformer. We further show that the optimal
                output power, required for implementing the KFB, is merely
                controlling the white noise gain (WNG) of the beamformer. We also
                show, that in static scenarios, the adaptation rule of the KFB
                reduces to the simpler affine projection algorithm (APA). The
                analytically derived results are verified and exemplified by a
                simulation study.},
  number     = {3},
  journal    = {IEEE Signal Processing Letters},
  author     = {Cherkassky, D. and Gannot, S.},
  month      = mar,
  year       = {2016},
  keywords   = {Kalman filters, Kalman filter, Speech enhancement, Mathematical
                model, feature extraction, Speech processing, array signal
                processing, microphone arrays, Microphone arrays, speech
                enhancement, adaptive beamformer, Adaptive beamformer, affine
                projection algorithm, affine transforms, APA, data-dependent
                beamformers, Kalman filter-based beamformer, KFB, LCMP beamformer,
                linear constraints, linearly constrained minimum power beamformer,
                Power generation, recursive Bayesian method, source extraction,
                spatial focusing techniques, spatial processors, speech extraction,
                speech processing, white noise gain, WNG},
  pages      = {376--380},
  file       = {Cherkassky and Gannot - 2016 - New Insights into the Kalman Filter
                Beamformer Ap.pdf:/Users/apodusenko/Zotero/storage/F832Y9NX/Cherkassky
                and Gannot - 2016 - New Insights into the Kalman Filter Beamformer
                Ap.pdf:application/pdf}
}

@article{el-keyi_robust_2005,
  title    = {Robust adaptive beamforming based on the {Kalman} filter},
  volume   = {53},
  issn     = {1053-587X},
  doi      = {10.1109/TSP.2005.851108},
  abstract = {In this paper, we present a novel approach to implement the robust
              minimum variance distortionless response (MVDR) beamformer. This
              beamformer is based on worst-case performance optimization and has
              been shown to provide an excellent robustness against arbitrary but
              norm-bounded mismatches in the desired signal steering vector.
              However, the existing algorithms to solve this problem do not have
              direct computationally efficient online implementations. In this
              paper, we develop a new algorithm for the robust MVDR beamformer,
              which is based on the constrained Kalman filter and can be
              implemented online with a low computational cost. Our algorithm is
              shown to have a similar performance to that of the original
              second-order cone programming (SOCP)-based implementation of the
              robust MVDR beamformer. We also present two improved modifications
              of the proposed algorithm to additionally account for nonstationary
              environments. These modifications are based on model switching and
              hypothesis merging techniques that further improve the robustness
              of the beamformer against rapid (abrupt) environmental changes.},
  number   = {8},
  journal  = {IEEE Transactions on Signal Processing},
  author   = {El-Keyi, A. and Kirubarajan, T. and Gershman, A. B.},
  month    = aug,
  year     = {2005},
  keywords = {Kalman filters, Optimization, Array signal processing, Robustness,
              array signal processing, adaptive signal processing, Computational
              efficiency, constrained Kalman filter, Constrained Kalman filter,
              Distortion, hypothesis merging technique, interacting multiple
              model estimation, Interference, mathematical programming, Merging,
              minimum variance distortionless response beamformer, model
              switching technique, Programming profession, robust adaptive
              beamforming, robust MVDR beamforming, second-order cone programming
              , Signal processing, signal steering vector, worst-case performance
              optimization},
  pages    = {3032--3041},
  file     = {El-Keyi et al. - 2005 - Robust adaptive beamforming based on the
              Kalman fi.html:/Users/apodusenko/Zotero/storage/JNBYYTBH/El-Keyi et al.
              - 2005 - Robust adaptive beamforming based on the Kalman
              fi.html:text/html;El-Keyi et al. - 2005 - Robust adaptive beamforming
              based on the Kalman
              fi.pdf:/Users/apodusenko/Zotero/storage/X8JEZGUX/El-Keyi et al. - 2005
              - Robust adaptive beamforming based on the Kalman
              fi.pdf:application/pdf}
}

@inproceedings{hadad_comparison_2017,
  title     = {Comparison of two binaural beamforming approaches for hearing aids},
  doi       = {10.1109/ICASSP.2017.7952153},
  abstract  = {Beamforming algorithms in binaural hearing aids are crucial to
               improve speech understanding in background noise for hearing
               impaired persons. In this study, we compare and evaluate the
               performance of two recently proposed minimum variance (MV)
               beamforming approaches for binaural hearing aids. The binaural
               linearly constrained MV (BLCMV) beamformer applies linear
               constraints to maintain the target source and mitigate the
               interfering sources, taking into account the reverberant nature of
               sound propagation. The inequality constrained MV (ICMV) beamformer
               applies inequality constraints to maintain the target source and
               mitigate the interfering sources, utilizing estimates of the
               direction of arrivals (DOAs) of the target and interfering sources.
               The similarities and differences between these two approaches is
               discussed and the performance of both algorithms is evaluated using
               simulated data and using real-world recordings, particularly
               focusing on the robustness to estimation errors of the relative
               transfer functions (RTFs) and DOAs. The BLCMV achieves a good
               performance if the RTFs are accurately estimated while the ICMV
               shows a good robustness to DOA estimation errors.},
  booktitle = {2017 {IEEE} {International} {Conference} on {Acoustics}, {Speech}
               and {Signal} {Processing} ({ICASSP})},
  author    = {Hadad, E. and Marquardt, D. and Pu, W. and Gannot, S. and Doclo, S.
               and Luo, Z. and Merks, I. and Zhang, T.},
  month     = mar,
  year      = {2017},
  keywords  = {hearing aids, Hearing aids, Noise measurement, Acoustics, noise
               reduction, Array signal processing, Direction-of-arrival estimation
               , DOA estimation, Robustness, array signal processing,
               direction-of-arrival estimation, Microphones, speech processing,
               acoustic beamforming, background noise, binaural beamforming
               approach, binaural hearing aids, binaural linearly constrained MV
               beamformer, Binaural signal processing, BLCMV beamformer, direction
               of arrival estimation, ICMV beamformer, inequality constrained MV
               beamformer, interfering source mitigation, LCMV, minimum variance
               beamforming approaches, MV beamforming approaches, relative
               transfer function, RTF, sound propagation reverberant nature,
               speech understanding improvement, transfer functions},
  pages     = {236--240},
  file      = {Hadad et al. - 2017 - Comparison of two binaural beamforming
               approaches .html:/Users/apodusenko/Zotero/storage/3ZHXTCSU/Hadad et al.
               - 2017 - Comparison of two binaural beamforming approaches
               .html:text/html;Hadad et al. - 2017 - Comparison of two binaural
               beamforming approaches
               .pdf:/Users/apodusenko/Zotero/storage/MLD4ZMC2/Hadad et al. - 2017 -
               Comparison of two binaural beamforming approaches .pdf:application/pdf}
}

@article{lorenz_1_nodate,
  title    = {1 {Robust} {Minimum} {Variance} {Beamforming}},
  language = {en},
  author   = {Lorenz, R G and Boyd, S P},
  pages    = {49},
  file     = {Lorenz and Boyd - 1 Robust Minimum Variance
              Beamforming.pdf:/Users/apodusenko/Zotero/storage/9ZWDYU82/Lorenz and
              Boyd - 1 Robust Minimum Variance Beamforming.pdf:application/pdf}
}

@article{sohn_belief_2011,
  title    = {Belief {Propagation} for {Distributed} {Downlink} {Beamforming} in {
              Cooperative} {MIMO} {Cellular} {Networks}},
  volume   = {10},
  issn     = {1536-1276},
  doi      = {10.1109/TWC.2011.101210.101698},
  abstract = {We propose a new graphical model approach to cooperative
              multiple-input multiple-output (MIMO) cellular networks. The
              objective is to optimize downlink transmit beamforming at each BS
              in order to maximize the sum throughput over the entire network.
              While ideal centralized beamforming requires full channel state
              information (CSI) sharing among all BSs in the network and huge
              computational complexity for combinatorial optimization, the
              proposed graphical model enables distributed beamforming which
              requires only local CSI sharing between neighboring BSs and
              efficiently solves the optimization problem in a distributed
              manner. As distributed solvers for this problem, we derive
              message-passing algorithms which can be implemented with
              polynomial-time computational complexity. Furthermore, we make a
              slight approximation on the objective function to derive a simpler
              graphical model, providing further complexity saving. Simulation
              results indicate that the proposed distributed downlink beamforming
              achieves average cell throughput typically within just 2\% of ideal
              centralized beamforming.},
  number   = {12},
  journal  = {IEEE Transactions on Wireless Communications},
  author   = {Sohn, I. and Lee, S. H. and Andrews, J. G.},
  month    = dec,
  year     = {2011},
  keywords = {Graphical models, computational complexity, message passing,
              cooperative communication, Array signal processing, array signal
              processing, Interference, base station cooperation, belief
              propagation, belief-propagation, broadcast channels, cellular
              network, cellular radio, channel state information, cooperative
              MIMO cellular networks, distributed downlink beamforming, Downlink,
              intercell interference, message-passing algorithms, MIMO, MIMO
              communication, Multiple-input multiple-output, polynomial
              approximation, polynomial-time computational complexity, sum
              throughput, Throughput, transmit beamforming},
  pages    = {4140--4149},
  file     = {IEEE Xplore Abstract
              Record:/Users/apodusenko/Zotero/storage/WARH5MTE/6059454.html:text/html
              }
}

@article{veitch_empirical_2018,
  title    = {Empirical {Risk} {Minimization} and {Stochastic} {Gradient} {Descent}
              for {Relational} {Data}},
  url      = {http://arxiv.org/abs/1806.10701},
  abstract = {Empirical risk minimization is the principal tool for prediction
              problems, but its extension to relational data remains unsolved. We
              solve this problem using recent advances in graph sampling theory.
              We (i) define an empirical risk for relational data and (ii) obtain
              stochastic gradients for this risk that are automatically unbiased.
              The key ingredient is to consider the method by which data is
              sampled from a graph as an explicit component of model design.
              Theoretical results establish that the choice of sampling scheme is
              critical. By integrating fast implementations of graph sampling
              schemes with standard automatic differentiation tools, we are able
              to solve the risk minimization in a plug-and-play fashion even on
              large datasets. We demonstrate empirically that relational ERM
              models achieve state-of-the-art results on semi-supervised node
              classification tasks. The experiments also confirm the importance
              of the choice of sampling scheme.},
  urldate  = {2018-10-24},
  journal  = {arXiv:1806.10701 [cs, stat]},
  author   = {Veitch, Victor and Austern, Morgane and Zhou, Wenda and Blei, David
              M. and Orbanz, Peter},
  month    = jun,
  year     = {2018},
  note     = {arXiv: 1806.10701},
  keywords = {Statistics - Machine Learning, Computer Science - Social and
              Information Networks, Computer Science - Machine Learning},
  file     = {arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/ZQ8KNYD5/1806.html:text/html;Veitch
              et al. - 2018 - Empirical Risk Minimization and Stochastic
              Gradien.pdf:/Users/apodusenko/Zotero/storage/G8PFDEVR/Veitch et al. -
              2018 - Empirical Risk Minimization and Stochastic
              Gradien.pdf:application/pdf}
}

@article{newton_appendix_nodate,
  title    = {Appendix {D} {Matrix} calculus},
  language = {en},
  author   = {Newton, Isaac},
  pages    = {27},
  file     = {Newton - Appendix D Matrix
              calculus.pdf:/Users/apodusenko/Zotero/storage/LMHQVJYX/Newton -
              Appendix D Matrix calculus.pdf:application/pdf}
}

@article{bebiano_matrix_2004,
  title    = {Matrix inequalities in statistical mechanics},
  volume   = {376},
  issn     = {00243795},
  url      = {http://linkinghub.elsevier.com/retrieve/pii/S0024379503006888},
  doi      = {10.1016/j.laa.2003.07.004},
  abstract = {Some matrix inequalities used in statistical mechanics are
              presented. A straightforward proof of the Thermodynamic Inequality
              is given and its equivalence to the Peierls–Bogoliubov inequality
              is shown.},
  language = {en},
  urldate  = {2018-10-23},
  journal  = {Linear Algebra and its Applications},
  author   = {Bebiano, N. and da Providência, J. and Lemos, R.},
  month    = jan,
  year     = {2004},
  pages    = {265--273},
  file     = {Bebiano et al. - 2004 - Matrix inequalities in statistical
              mechanics.pdf:/Users/apodusenko/Zotero/storage/BQ9C82JI/Bebiano et al.
              - 2004 - Matrix inequalities in statistical
              mechanics.pdf:application/pdf}
}

@article{malik_bayesian_2014,
  title    = {A {Bayesian} {Framework} for {Blind} {Adaptive} {Beamforming}},
  volume   = {62},
  issn     = {1053-587X},
  doi      = {10.1109/TSP.2014.2310432},
  abstract = {In this work, the problem of blind adaptive beamforming in the
              presence of steering-vector uncertainty is addressed within a
              Bayesian estimation framework. We express the single-input
              multiple-output (SIMO) observation model in the
              short-time-Fourier-transform (STFT) domain and employ a variational
              formulation to obtain iterative closed-form learning rules for
              inferring approximate posteriors on the steering vector and the
              target signal. By varying the a priori belief in the top-level
              statistical model, i.e., modeling a quantity as a random process or
              an unknown deterministic entity, it is shown that the considered
              framework yields a variety of beamforming algorithms including the
              celebrated minimum variance distortionless response (MVDR)
              beamformer. We highlight these interconnections and show by means
              of simulation results that the Bayesian approach alleviates signal
              distortion in noisy and uncertain environments as compared to the
              conventional MVDR beamformer by adaptively learning and
              incorporating uncertainty pertaining to the steering vector.},
  number   = {9},
  journal  = {IEEE Transactions on Signal Processing},
  author   = {Malik, S. and Benesty, J. and Chen, J.},
  month    = may,
  year     = {2014},
  keywords = {Noise, belief networks, inference mechanisms, Fourier transforms,
              Bayes methods, Uncertainty, Adaptation models, learning (artificial
              intelligence), Vectors, Bayesian estimation, Array signal
              processing, Robustness, array signal processing, minimum variance
              distortionless response beamformer, a priori belief, Adaptive
              beamforming, approximate posterior, Bayesian learning, blind
              adaptive beamforming, iterative closed form learning rules, short
              time Fourier transform, SIMO, single-input multiple-output
              observation, steering vector uncertainty, steering-vector
              uncertainty, variational calculus, variational formulation},
  pages    = {2370--2384},
  file     = {IEEE Xplore Abstract
              Record:/Users/apodusenko/Zotero/storage/UQB5K2KY/6763092.html:text/html;Malik
              et al. - 2014 - A Bayesian Framework for Blind Adaptive
              Beamformin.pdf:/Users/apodusenko/Zotero/storage/QG3WZEFH/Malik et al. -
              2014 - A Bayesian Framework for Blind Adaptive
              Beamformin.pdf:application/pdf}
}

@article{kim_recognition_2017,
  title    = {Recognition {Dynamics} in the {Brain} under the {Free} {Energy} {
              Principle}},
  url      = {http://arxiv.org/abs/1710.09118},
  abstract = {We formulate the computational processes of perception in the
              framework of the principle of least action by postulating the
              theoretical action as a time integral of the free energy in the
              brain sciences. The free energy principle is accordingly rephrased
              as that for autopoietic grounds all viable organisms attempt to
              minimize the sensory uncertainty about the unpredictable
              environment over a temporal horizon. By varying the informational
              action, we derive the brain's recognition dynamics (RD) which
              conducts Bayesian filtering of the external causes from noisy
              sensory inputs. Consequently, we effectively cast the
              gradient-descent scheme of minimizing the free energy into
              Hamiltonian mechanics by addressing only positions and momenta of
              the organisms' representations of the causal environment. To
              manifest the utility of our theory, we show how the RD may be
              implemented in a neuronally based biophysical model at a
              single-cell level and subsequently in a coarse-grained,
              hierarchical architecture of the brain. We also present formal
              solutions to the RD for a model brain in linear regime and analyze
              the perceptual trajectories around attractors in neural state
              space.},
  urldate  = {2018-10-18},
  journal  = {arXiv:1710.09118 [q-bio]},
  author   = {Kim, Chang Sub},
  month    = oct,
  year     = {2017},
  note     = {arXiv: 1710.09118},
  keywords = {Quantitative Biology - Neurons and Cognition},
  file     = {arXiv\:1710.09118 PDF:/Users/apodusenko/Zotero/storage/59U9RRF3/Kim -
              2017 - Recognition Dynamics in the Brain under the Free
              E.pdf:application/pdf;arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/UFD2NVH6/1710.html:text/html}
}

@article{besson_robust_2013,
  series   = {Special {Issue} on {Advances} in {Sensor} {Array} {Processing} in {
              Memory} of {Alex} {B}. {Gershman}},
  title    = {Robust adaptive beamforming using a {Bayesian} steering vector error
              model},
  volume   = {93},
  issn     = {0165-1684},
  url      = {http://www.sciencedirect.com/science/article/pii/S0165168412004604},
  doi      = {10.1016/j.sigpro.2012.12.018},
  abstract = {We propose a Bayesian approach to robust adaptive beamforming
              which entails considering the steering vector of interest as a
              random variable with some prior distribution. The latter can be
              tuned in a simple way to reflect how far is the actual steering
              vector from its presumed value. Two different priors are proposed,
              namely a Bingham prior distribution and a distribution that
              directly reveals and depends upon the angle between the true and
              presumed steering vector. Accordingly, a non-informative prior is
              assigned to the interference plus noise covariance matrix R, which
              can be viewed as a means to introduce diagonal loading in a
              Bayesian framework. The minimum mean square distance estimate of
              the steering vector as well as the minimum mean square error
              estimate of R are derived and implemented using a Gibbs sampling
              strategy. Numerical simulations show that the new beamformers
              possess a very good rate of convergence even in the presence of
              steering vector errors.},
  number   = {12},
  urldate  = {2018-10-16},
  journal  = {Signal Processing},
  author   = {Besson, Olivier and Bidon, Stéphanie},
  month    = dec,
  year     = {2013},
  keywords = {Gibbs sampling, Bayesian estimation, Bingham distribution, Robust
              adaptive beamforming},
  pages    = {3290--3299},
  file     = {ScienceDirect Full Text
              PDF:/Users/apodusenko/Zotero/storage/DBRPMNEA/Besson and Bidon - 2013 -
              Robust adaptive beamforming using a Bayesian
              steer.pdf:application/pdf;ScienceDirect
              Snapshot:/Users/apodusenko/Zotero/storage/G5GN7E7V/S0165168412004604.html:text/html
              }
}

@article{wang_iterative_2016,
  title    = {Iterative beamforming for identification of multiple broadband sound
              sources},
  volume   = {365},
  issn     = {0022-460X},
  url      = {http://www.sciencedirect.com/science/article/pii/S0022460X15009566},
  doi      = {10.1016/j.jsv.2015.11.036},
  abstract = {The reconstruction of broadband sound sources is an important
              issue in industrial acoustics. In this paper, a model comprising
              multiple incoherent Gaussian random sources is considered. The aim
              is to estimate locations and powers of the sound sources using the
              pressures measured by an array of microphones. Each measured
              pressure is interpreted as a mixture of latent signals emitted by
              different sound sources. Then, an Iterative Beamforming (IB) method
              is developed to estimate the source parameters. This approach is
              based on the Expectation-Maximization (EM) algorithm, a well-known
              iterative procedure for solving maximum likelihood parameter
              estimation. More specifically, IB iteratively estimates the source
              contributions and performs beamforming on these estimates. In this
              work, experiments on real data illustrate the advantage of IB with
              respect to classical beamforming and Near-field Acoustical
              Holography (NAH). In particular, the proposed method is shown to
              work over a wider range of frequencies, to better estimate the
              source locations, and is able to quantify the powers of the
              sources. Furthermore, experiments illustrate that IB can not only
              localize the sources on a given surface, but also accurately
              estimate their 3D locations.},
  urldate  = {2018-10-16},
  journal  = {Journal of Sound and Vibration},
  author   = {Wang, Xun and Quost, Benjamin and Chazot, Jean-Daniel and Antoni,
              Jérôme},
  month    = mar,
  year     = {2016},
  pages    = {260--275},
  file     = {Wang et al. - 2016 - Iterative beamforming for identification of
              multip.html:/Users/apodusenko/Zotero/storage/IVNUBDMI/Wang et al. -
              2016 - Iterative beamforming for identification of
              multip.html:text/html;Wang et al. - 2016 - Iterative beamforming for
              identification of
              multip.pdf:/Users/apodusenko/Zotero/storage/WLYGE26Q/Wang et al. - 2016
              - Iterative beamforming for identification of
              multip.pdf:application/pdf}
}

@article{lam_bayesian_2006,
  title      = {Bayesian {Beamforming} for {DOA} {Uncertainty}: {Theory} and {
                Implementation}},
  volume     = {54},
  issn       = {1053-587X},
  shorttitle = {Bayesian {Beamforming} for {DOA} {Uncertainty}},
  doi        = {10.1109/TSP.2006.880257},
  abstract   = {A Bayesian approach to adaptive narrowband beamforming for
                uncertain source direction-of-arrival (DOA) is presented. The DOA
                is modeled as a random variable with prior statistics that describe
                its level of uncertainty. The Bayesian beamformer is the
                corresponding minimum mean-square error (MMSE) estimator, which can
                be viewed as a mixture of directional beamformers combined
                according to the posterior distribution of the DOA given the data.
                Under a deterministic DOA, the mean-square error (MSE) of the
                Bayesian beamformer becomes as low as that of the directional
                beamformer equipped with the DOA candidate in the prior set that is
                the closest to the true DOA at exponential rate, where closeness is
                defined in the Kullback-Leibler sense. Two efficient algorithms
                using a uniform linear array (ULA) are presented. The first method
                utilizes the efficiency of the fast Fourier transform (FFT) to
                compute the posterior distribution on a large number of DOA
                candidates. The second method approximates the posterior
                distribution by a Gaussian distribution, which leads to a
                directional beamformer incorporated with a particular spreading
                matrix and an adjusted DOA. Numerical simulations show that the
                proposed beamformer outperforms other related blind or robust
                beamforming algorithms over a wide range of signal-to-noise ratios
                (SNRs)},
  number     = {11},
  journal    = {IEEE Transactions on Signal Processing},
  author     = {Lam, C. J. and Singer, A. C.},
  month      = nov,
  year       = {2006},
  keywords   = {Bayes methods, Bayesian methods, Gaussian distribution, Random
                variables, Statistical distributions, Uncertainty, least mean
                squares methods, Narrowband, signal-to-noise ratios, SNR, Array
                signal processing, Direction of arrival estimation, array signal
                processing, direction-of-arrival estimation, Adaptive beamforming,
                Bayesian beamforming, Bayesian model, direction-of-arrival (DOA)
                uncertainty, Distributed computing, DOA uncertainty, fast Fourier
                transform, fast Fourier transforms, Fast Fourier transforms, FFT,
                Kullback-Leibler sense, minimum mean-square error, minimum
                mean-square error (MMSE) estimation, MMSE, uncertain source
                direction-of-arrival, uniform linear array},
  pages      = {4435--4445},
  file       = {IEEE Xplore Abstract
                Record:/Users/apodusenko/Zotero/storage/ECVK2C6Q/1710387.html:text/html;Lam
                and Singer - 2006 - Bayesian Beamforming for DOA Uncertainty Theory
                a.pdf:/Users/apodusenko/Zotero/storage/J76P6XPR/Lam and Singer - 2006 -
                Bayesian Beamforming for DOA Uncertainty Theory a.pdf:application/pdf}
}

@misc{noauthor_robust_nodate,
  title   = {Robust {Beamforming} via {Worst}-{Case} {SINR} {Maximization} - {IEEE
             } {Journals} \& {Magazine}},
  url     = {https://ieeexplore.ieee.org/document/4471885},
  urldate = {2018-10-16},
  file    = {Robust Beamforming via Worst-Case SINR
             Maximizatio.html:/Users/apodusenko/Zotero/storage/Y95Q2U3S/Robust
             Beamforming via Worst-Case SINR Maximizatio.html:text/html}
}

@article{capon_high-resolution_1969,
  title    = {High-resolution frequency-wavenumber spectrum analysis},
  volume   = {57},
  issn     = {0018-9219},
  doi      = {10.1109/PROC.1969.7278},
  abstract = {The output of an array of sansors is considered to be a
              homogeneous random field. In this case there is a spectral
              representation for this field, similar to that for stationary
              random processes, which consists of a superposition of traveling
              waves. The frequency-wavenumber power spectral density provides the
              mean-square value for the amplitudes of these waves and is of
              considerable importance in the analysis of propagating waves by
              means of an array of sensors. The conventional method of
              frequency-wavenumber power spectral density estimation uses a
              fixed-wavenumber window and its resolution is determined
              essentially by the beam pattern of the array of sensors. A
              high-resolution method of estimation is introduced which employs a
              wavenumber window whose shape changes and is a function of the
              wavenumber at which an estimate is obtained. It is shown that the
              wavenumber resolution of this method is considerably better than
              that of the conventional method. Application of these results is
              given to seismic data obtained from the large aperture seismic
              array located in eastern Montana. In addition, the application of
              the high-resolution method to other areas, such as radar, sonar,
              and radio astronomy, is indicated.},
  number   = {8},
  journal  = {Proceedings of the IEEE},
  author   = {Capon, J.},
  month    = aug,
  year     = {1969},
  keywords = {Frequency estimation, Phased arrays, Signal resolution, Antenna
              arrays, Radar antennas, Radar applications, Radio astronomy, Random
              processes, Sensor arrays, Shape},
  pages    = {1408--1418},
  file     = {Capon - 1969 - High-resolution frequency-wavenumber spectrum
              anal.html:/Users/apodusenko/Zotero/storage/VVKYSFE8/Capon - 1969 -
              High-resolution frequency-wavenumber spectrum anal.html:text/html;Capon
              - 1969 - High-resolution frequency-wavenumber spectrum
              anal.pdf:/Users/apodusenko/Zotero/storage/5ARUK2H6/Capon - 1969 -
              High-resolution frequency-wavenumber spectrum anal.pdf:application/pdf}
}

@article{lorenz_robust_2005,
  title    = {Robust minimum variance beamforming},
  volume   = {53},
  issn     = {1053-587X},
  url      = {http://ieeexplore.ieee.org/document/1420809/},
  doi      = {10.1109/TSP.2005.845436},
  abstract = {This paper introduces an extension of minimum variance beamforming
              that explicitly takes into account variation or uncertainty in the
              array response. Sources of this uncertainty include imprecise
              knowledge of the angle of arrival and uncertainty in the array
              manifold.},
  language = {en},
  number   = {5},
  urldate  = {2018-10-16},
  journal  = {IEEE Transactions on Signal Processing},
  author   = {Lorenz, R.G. and Boyd, S.P.},
  month    = may,
  year     = {2005},
  pages    = {1684--1696},
  file     = {Lorenz and Boyd - 2005 - Robust minimum variance
              beamforming.pdf:/Users/apodusenko/Zotero/storage/N6K26MD7/Lorenz and
              Boyd - 2005 - Robust minimum variance beamforming.pdf:application/pdf}
}

@inproceedings{liao_effective_2015,
  title     = {An effective low complexity binaural beamforming algorithm for
               hearing aids},
  doi       = {10.1109/WASPAA.2015.7336916},
  abstract  = {In this paper, we propose a low complexity beamforming algorithm
               for binaural hearing aids that achieves a high degree of noise
               suppression without causing significant speech distortion.
               Specifically, we incorporate a priori approximate spatial
               information in the formulation and develop several effective
               techniques to reduce the communication overhead. For the resulting
               quadratically constrained beamforming problem, we propose a low
               complexity algorithm based on the Alternating Direction Method of
               Multipliers (ADMM) method. This new approach is shown to be
               computationally much more efficient than the previously proposed
               binaural beamforming algorithms. Through experiments on the
               real-world recording, the effectiveness of the binaural hearing aid
               design and the corresponding low complexity implementation are
               validated.},
  booktitle = {2015 {IEEE} {Workshop} on {Applications} of {Signal} {Processing}
               to {Audio} and {Acoustics} ({WASPAA})},
  author    = {Liao, W. and Luo, Z. and Merks, I. and Zhang, T.},
  month     = oct,
  year      = {2015},
  keywords  = {Auditory system, hearing aids, Hearing aids, Signal processing
               algorithms, noise suppression, Speech, Complexity theory, array
               signal processing, binaural hearing aids, Binaural signal
               processing, Acoustic distortion, ADMM method, Alternating direction
               method of multipliers, alternating direction method of multipliers
               method, effective low complexity binaural beamforming algorithm,
               priori approximate spatial information, spatial information, speech
               distortion},
  pages     = {1--5},
  file      = {IEEE Xplore Abstract
               Record:/Users/apodusenko/Zotero/storage/FYXE4RB4/7336916.html:text/html;Liao
               et al. - 2015 - An effective low complexity binaural beamforming
               a.pdf:/Users/apodusenko/Zotero/storage/HLI6MSQT/Liao et al. - 2015 - An
               effective low complexity binaural beamforming a.pdf:application/pdf}
}

@misc{noauthor_comparison_nodate,
  title   = {Comparison of two binaural beamforming approaches for hearing aids -
             {IEEE} {Conference} {Publication}},
  url     = {https://ieeexplore.ieee.org/document/7952153},
  urldate = {2018-10-16},
  file    = {Comparison of two binaural beamforming approaches for hearing aids -
             IEEE Conference
             Publication:/Users/apodusenko/Zotero/storage/3933WM9T/7952153.html:text/html
             }
}

@article{veen_beamforming:_1988,
  title      = {Beamforming: a versatile approach to spatial filtering},
  volume     = {5},
  issn       = {0740-7467},
  shorttitle = {Beamforming},
  doi        = {10.1109/53.665},
  abstract   = {An overview of beamforming from a signal-processing perspective is
                provided, with an emphasis on recent research. Data-independent,
                statistically optimum, adaptive, and partially adaptive beamforming
                are discussed. Basic notation, terminology, and concepts are
                included. Several beamformer implementations are briefly described.
                {\textless}{\textless}ETX{\textgreater}{\textgreater}},
  number     = {2},
  journal    = {IEEE ASSP Magazine},
  author     = {Veen, B. D. Van and Buckley, K. M.},
  month      = apr,
  year       = {1988},
  keywords   = {Sampling methods, Array signal processing, spatial filtering,
                Interference, Sensor arrays, adaptive beamforming, Apertures,
                beamformer implementations, data independent beamforming, Feeds,
                Filtering, filtering and prediction theory, Frequency, Microwave
                filters, notation, overview, partially adaptive beamforming,
                reviews, signal processing, signal-processing, Spatial filters,
                statistically optimum beamforming, terminology},
  pages      = {4--24},
  file       = {Veen and Buckley - 1988 - Beamforming a versatile approach to spatial
                filte.html:/Users/apodusenko/Zotero/storage/UIRV6F6F/Veen and Buckley -
                1988 - Beamforming a versatile approach to spatial
                filte.html:text/html;Veen and Buckley - 1988 - Beamforming a versatile
                approach to spatial
                filte.pdf:/Users/apodusenko/Zotero/storage/ZJW389LH/Veen and Buckley -
                1988 - Beamforming a versatile approach to spatial
                filte.pdf:application/pdf}
}

@inproceedings{kavalekalam_kalman_2016,
  title     = {Kalman filter for speech enhancement in cocktail party scenarios
               using a codebook-based approach},
  doi       = {10.1109/ICASSP.2016.7471663},
  abstract  = {Enhancement of speech in non-stationary background noise is a
               challenging task, and conventional single channel speech
               enhancement algorithms have not been able to improve the speech
               intelligibility in such scenarios. The work proposed in this paper
               investigates a single channel Kalman filter based speech
               enhancement algorithm, whose parameters are estimated using a
               codebook based approach. The results indicate that the enhancement
               algorithm is able to improve the speech intelligibility and quality
               according to objective measures. Moreover, we investigate the
               effects of utilizing a speaker specific trained codebook over a
               generic speech codebook in relation to the performance of the
               speech enhancement system.},
  booktitle = {2016 {IEEE} {International} {Conference} on {Acoustics}, {Speech}
               and {Signal} {Processing} ({ICASSP})},
  author    = {Kavalekalam, M. S. and Christensen, M. G. and Gran, F. and Boldt, J.
               B.},
  month     = mar,
  year      = {2016},
  keywords  = {Estimation, Kalman filters, Approximation algorithms, Speech
               enhancement, Noise measurement, Speech, speech enhancement,
               autoregressive models, cocktail party scenarios, codebook based
               approach, kalman filter, nonstationary background noise, single
               channel Kalman filter, single channel speech enhancement algorithms
               , speech codebook, Speech coding},
  pages     = {191--195},
  file      = {Kavalekalam et al. - 2016 - Kalman filter for speech enhancement in
               cocktail p.html:/Users/apodusenko/Zotero/storage/EJMCD5JK/Kavalekalam
               et al. - 2016 - Kalman filter for speech enhancement in cocktail
               p.html:text/html;Kavalekalam et al. - 2016 - Kalman filter for speech
               enhancement in cocktail
               p.pdf:/Users/apodusenko/Zotero/storage/BRH5T6LT/Kavalekalam et al. -
               2016 - Kalman filter for speech enhancement in cocktail
               p.pdf:application/pdf}
}

@misc{noauthor_speech_nodate-1,
  title   = {Speech enhancement using combination of digital audio effects with {
             Kalman} filter - {IEEE} {Conference} {Publication}},
  url     = {https://ieeexplore.ieee.org/document/7955633},
  urldate = {2018-10-16},
  file    = {Speech enhancement using combination of digital
             au.html:/Users/apodusenko/Zotero/storage/TQ6VC5IS/Speech enhancement
             using combination of digital au.html:text/html}
}

@article{ruskai_convexity_1990,
  title    = {Convexity inequalities for estimating free energy and relative
              entropy},
  volume   = {23},
  issn     = {0305-4470, 1361-6447},
  url      = {
              http://stacks.iop.org/0305-4470/23/i=12/a=023?key=crossref.0f5aa30be494c5bc36289cc74dfffa2f
              },
  doi      = {10.1088/0305-4470/23/12/023},
  language = {en},
  number   = {12},
  urldate  = {2018-10-09},
  journal  = {Journal of Physics A: Mathematical and General},
  author   = {Ruskai, M B and Stillinger, F H},
  month    = jun,
  year     = {1990},
  pages    = {2421--2437},
  file     = {Ruskai and Stillinger - 1990 - Convexity inequalities for estimating
              free energy .pdf:/Users/apodusenko/Zotero/storage/C8WWKM25/Ruskai and
              Stillinger - 1990 - Convexity inequalities for estimating free energy
              .pdf:application/pdf}
}

@article{rezende_taming_2018,
  title    = {Taming {VAEs}},
  url      = {http://arxiv.org/abs/1810.00597},
  abstract = {In spite of remarkable progress in deep latent variable generative
              modeling, training still remains a challenge due to a combination
              of optimization and generalization issues. In practice, a
              combination of heuristic algorithms (such as hand-crafted annealing
              of KL-terms) is often used in order to achieve the desired results,
              but such solutions are not robust to changes in model architecture
              or dataset. The best settings can often vary dramatically from one
              problem to another, which requires doing expensive parameter sweeps
              for each new case. Here we develop on the idea of training VAEs
              with additional constraints as a way to control their behaviour. We
              first present a detailed theoretical analysis of constrained VAEs,
              expanding our understanding of how these models work. We then
              introduce and analyze a practical algorithm termed Generalized ELBO
              with Constrained Optimization, GECO. The main advantage of GECO for
              the machine learning practitioner is a more intuitive, yet
              principled, process of tuning the loss. This involves defining of a
              set of constraints, which typically have an explicit relation to
              the desired model performance, in contrast to tweaking abstract
              hyper-parameters which implicitly affect the model behavior.
              Encouraging experimental results in several standard datasets
              indicate that GECO is a very robust and effective tool to balance
              reconstruction and compression constraints.},
  urldate  = {2018-10-08},
  journal  = {arXiv:1810.00597 [cs, stat]},
  author   = {Rezende, Danilo Jimenez and Viola, Fabio},
  month    = oct,
  year     = {2018},
  note     = {arXiv: 1810.00597},
  keywords = {Statistics - Machine Learning, Computer Science - Machine Learning
              },
  file     = {arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/D9AL4E9G/1810.html:text/html;Rezende
              and Viola - 2018 - Taming
              VAEs.pdf:/Users/apodusenko/Zotero/storage/FUSSWCFQ/Rezende and Viola -
              2018 - Taming VAEs.pdf:application/pdf}
}

@article{liu_manifold_nodate,
  title    = {Manifold of probability distribution},
  language = {en},
  author   = {Liu, Yunshu},
  pages    = {36},
  file     = {Liu - Manifold of probability
              distribution.pdf:/Users/apodusenko/Zotero/storage/VUL5QBR8/Liu -
              Manifold of probability distribution.pdf:application/pdf}
}

@article{yates_dc_2008,
  title   = {{DC} {Blocker} {Algorithms} [{DSP} {Tips} \& {Tricks}]},
  volume  = {25},
  issn    = {1053-5888},
  url     = {http://ieeexplore.ieee.org/document/4472252/},
  doi     = {10.1109/MSP.2007.914713},
  number  = {2},
  urldate = {2017-07-28},
  journal = {IEEE Signal Processing Magazine},
  author  = {Yates, R. and Lyons, R.},
  month   = mar,
  year    = {2008},
  pages   = {132--134},
  file    = {Yates and Lyons - 2008 - DC Blocker Algorithms [DSP Tips &
             Tricks].pdf:/Users/apodusenko/Zotero/storage/A9RXTSBV/Yates and Lyons -
             2008 - DC Blocker Algorithms [DSP Tips & Tricks].pdf:application/pdf}
}

@book{manolakis_statistical_2005,
  address    = {Boston},
  series     = {Artech {House} signal processing library},
  title      = {Statistical and adaptive signal processing: spectral estimation,
                signal modeling, adaptive filtering, and array processing},
  isbn       = {978-1-58053-610-3},
  shorttitle = {Statistical and adaptive signal processing},
  publisher  = {Artech House},
  author     = {Manolakis, Dimitris G. and Ingle, Vinay K. and Kogon, Stephen M.},
  year       = {2005},
  keywords   = {Signal processing, Adaptive signal processing, Statistical methods
                },
  file       = {Statistical and Adaptive Signal
                Processing.pdf:/Users/apodusenko/Zotero/storage/XI93RDG9/Statistical
                and Adaptive Signal Processing.pdf:application/pdf}
}

@article{cox_probability_1946,
  title   = {Probability, frequency and reasonable expectation},
  volume  = {14},
  number  = {1},
  journal = {American journal of physics},
  author  = {Cox, Richard T.},
  year    = {1946},
  pages   = {1--13},
  file    = {Fulltext:/Users/apodusenko/Zotero/storage/KJD2WG98/Cox - 1946 -
             Probability, frequency and reasonable
             expectation.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/AVVGSPD4/1.html:text/html
             }
}

@inproceedings{titsias_doubly_2014,
  address   = {Beijing, China},
  series    = {{ICML}'14},
  title     = {Doubly {Stochastic} {Variational} {Bayes} for {Non}-conjugate {
               Inference}},
  url       = {http://dl.acm.org/citation.cfm?id=3044805.3045112},
  abstract  = {We propose a simple and effective variational inference algorithm
               based on stochastic optimisation that can be widely applied for
               Bayesian non-conjugate inference in continuous parameter spaces.
               This algorithm is based on stochastic approximation and allows for
               efficient use of gradient information from the model joint density.
               We demonstrate these properties using illustrative examples as well
               as in challenging and diverse Bayesian inference problems such as
               variable selection in logistic regression and fully Bayesian
               inference over kernel hyperparameters in Gaussian process
               regression.},
  urldate   = {2018-09-24},
  booktitle = {Proceedings of the 31st {International} {Conference} on {
               International} {Conference} on {Machine} {Learning} - {Volume} 32},
  publisher = {JMLR.org},
  author    = {Titsias, Michalis K. and Lázaro-Gredilla, Miguel},
  year      = {2014},
  pages     = {II--1971--II--1980},
  file      = {Titsias and Lázaro-Gredilla - 2014 - Doubly Stochastic Variational
               Bayes for
               Non-conjug.pdf:/Users/apodusenko/Zotero/storage/C4RKA7RH/Titsias and
               Lázaro-Gredilla - 2014 - Doubly Stochastic Variational Bayes for
               Non-conjug.pdf:application/pdf}
}

@article{van_de_laar_probabilistic_2016,
  title    = {A {Probabilistic} {Modeling} {Approach} to {Hearing} {Loss} {
              Compensation}},
  volume   = {24},
  issn     = {2329-9290},
  doi      = {10.1109/TASLP.2016.2599275},
  abstract = {Hearing Aid (HA) algorithms need to be tuned (“fitted”) to match
              the impairment of each specific patient. The lack of a fundamental
              HA fitting theory is a strong contributing factor to an
              unsatisfying sound experience for about 20\% of HA patients. This
              paper proposes a probabilistic modeling approach to the design of
              HA algorithms. The proposed method relies on a generative
              probabilistic model for the hearing loss problem and provides for
              automated inference of the corresponding (1) signal processing
              algorithm, (2) the fitting solution as well as (3) a principled
              performance evaluation metric. All three tasks are realized as
              message passing algorithms in a factor graph representation of the
              generative model, which in principle allows for fast implementation
              on HA or mobile device hardware. The methods are theoretically
              worked out and simulated with a custom-built factor graph toolbox
              for a specific hearing loss model.},
  number   = {11},
  journal  = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  author   = {van de Laar, Thijs and de Vries, Bert},
  month    = nov,
  year     = {2016},
  keywords = {Auditory system, Message passing, machine learning, factor graphs,
              inference mechanisms, Signal processing algorithms, medical signal
              processing, hearing, physiological models, ear, Probabilistic logic
              , statistical analysis, automated inference, factor graph
              representation, fundamental HA fitting theory, Gain, hearing aid
              algorithms, hearing loss compensation, message passing algorithms,
              principled performance evaluation metric, probabilistic modeling,
              probabilistic modeling approach, signal processing algorithm,
              Tuning},
  pages    = {2200--2213},
  file     = {IEEE Xplore Abstract
              Record:/Users/apodusenko/Zotero/storage/RVNXED58/7539610.html:text/html;van
              de Laar and de Vries - 2016 - A Probabilistic Modeling Approach to
              Hearing Loss .pdf:/Users/apodusenko/Zotero/storage/RUL8ZA68/van de Laar
              and de Vries - 2016 - A Probabilistic Modeling Approach to Hearing Loss
              .pdf:application/pdf}
}

@inproceedings{rahimi_random_2007,
  title     = {Random {Features} for {Large}-{Scale} {Kernel} {Machines}.},
  volume    = {3},
  url       = {
               https://papers.nips.cc/paper/3182-random-features-for-large-scale-kernel-machines.pdf
               },
  urldate   = {2017-05-01},
  booktitle = {{NIPS}},
  author    = {Rahimi, Ali and Recht, Benjamin and {others}},
  year      = {2007},
  pages     = {5},
  file      = {
               07.rah.rec.nips.pdf.pdf:/Users/apodusenko/Zotero/storage/FN9PKSA3/07.rah.rec.nips.pdf.pdf:application/pdf
               }
}

@article{iglesias_hierarchical_2013,
  title    = {Hierarchical {Prediction} {Errors} in {Midbrain} and {Basal} {
              Forebrain} during {Sensory} {Learning}},
  volume   = {80},
  issn     = {0896-6273},
  url      = {https://www.cell.com/neuron/abstract/S0896-6273(13)00807-6},
  doi      = {10.1016/j.neuron.2013.09.009},
  language = {English},
  number   = {2},
  urldate  = {2018-05-07},
  journal  = {Neuron},
  author   = {Iglesias, Sandra and Mathys, Christoph and Brodersen, Kay H. and
              Kasper, Lars and Piccirelli, Marco and den Ouden, Hanneke E. M. and
              Stephan, Klaas E.},
  month    = oct,
  year     = {2013},
  pmid     = {24139048},
  pages    = {519--530},
  file     = {Iglesias et al. - 2013 - Hierarchical Prediction Errors in Midbrain
              and Bas.pdf:/Users/apodusenko/Zotero/storage/V65UQZNL/Iglesias et al. -
              2013 - Hierarchical Prediction Errors in Midbrain and
              Bas.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/7MU3H2XN/S0896-6273(13)00807-6.html:text/html
              }
}

@article{buckley_free_2017,
  title      = {The free energy principle for action and perception: {A} mathematical
                review},
  issn       = {0022-2496},
  shorttitle = {The free energy principle for action and perception},
  url        = {http://www.sciencedirect.com/science/article/pii/S0022249617300962},
  doi        = {10.1016/j.jmp.2017.09.004},
  abstract   = {The ‘free energy principle’ (FEP) has been suggested to provide a
                unified theory of the brain, integrating data and theory relating
                to action, perception, and learning. The theory and implementation
                of the FEP combines insights from Helmholtzian ‘perception as
                inference’, machine learning theory, and statistical
                thermodynamics. Here, we provide a detailed mathematical evaluation
                of a suggested biologically plausible implementation of the FEP
                that has been widely used to develop the theory. Our objectives are
                (i) to describe within a single article the mathematical structure
                of this implementation of the FEP; (ii) provide a simple but
                complete agent-based model utilising the FEP and (iii) to disclose
                the assumption structure of this implementation of the FEP to help
                elucidate its significance for the brain sciences.},
  urldate    = {2017-11-03},
  journal    = {Journal of Mathematical Psychology},
  author     = {Buckley, Christopher L. and Kim, Chang Sub and McGregor, Simon and
                Seth, Anil K.},
  month      = oct,
  year       = {2017},
  keywords   = {Action, Inference, Perception, Bayesian brain, Agent-based model,
                Free energy principle},
  file       = {Buckley et al. - 2017 - The free energy principle for action and
                perceptio.pdf:/Users/apodusenko/Zotero/storage/GSMTXZW9/Buckley et al.
                - 2017 - The free energy principle for action and
                perceptio.pdf:application/pdf;ScienceDirect
                Snapshot:/Users/apodusenko/Zotero/storage/F4P85LKN/S0022249617300962.html:text/html
                }
}

@book{lanczos_variational_2012,
  title     = {The variational principles of mechanics},
  publisher = {Courier Corporation},
  author    = {Lanczos, Cornelius},
  year      = {2012},
  file      = {Fulltext:/Users/apodusenko/Zotero/storage/86RT62U5/Lanczos - 2012 -
               The variational principles of
               mechanics.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/R9V3E8TH/books.html:text/html
               }
}

@article{still_information-theoretic_2009,
  title   = {Information-theoretic approach to interactive learning},
  volume  = {85},
  number  = {2},
  journal = {EPL (Europhysics Letters)},
  author  = {Still, Susanne},
  year    = {2009},
  pages   = {28005},
  file    = {Fulltext:/Users/apodusenko/Zotero/storage/UETBIVUA/Still - 2009 -
             Information-theoretic approach to interactive
             lear.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/TKESJ55J/meta.html:text/html
             }
}

@misc{noauthor_modeling_nodate,
  title   = {Modeling {Agents} with {Probabilistic} {Programs}},
  url     = {https://agentmodels.org/},
  urldate = {2018-08-17},
  file    = {Modeling Agents with Probabilistic
             Programs:/Users/apodusenko/Zotero/storage/CD8RFQTX/agentmodels.org.html:text/html
             }
}

@misc{noauthor_probabilistic_nodate,
  title   = {Probabilistic {Models} of {Cognition} - 2nd {Edition}},
  url     = {https://probmods.org/},
  urldate = {2018-08-17},
  file    = {Probabilistic Models of Cognition - 2nd
             Edition:/Users/apodusenko/Zotero/storage/VMTFZGV3/probmods.org.html:text/html
             }
}

@inproceedings{dauwels_steepest_2005,
  address   = {Awaji Island, Japan},
  title     = {Steepest descent as message passing},
  doi       = {10.1109/ITW.2005.1531853},
  abstract  = {It is shown how steepest descent (or steepest ascent) may be
               viewed as a message passing algorithm with "local" message update
               rules. For example, the well-known backpropagation algorithm for
               the training of feedforward neural networks may be viewed as
               message passing on a factor graph. The factor graph approach with
               its emphasis on "local" computations makes it easy to combine
               steepest descent with other message passing algorithms such as the
               sum/max-product algorithms, expectation maximization, Kalman
               filtering/smoothing, and particle filters. As an example, parameter
               estimation in a state space model is considered. For this example,
               it is shown how steepest descent can be used for the maximization
               step in expectation maximization.},
  booktitle = {{IEEE} {Information} {Theory} {Workshop}, 2005.},
  author    = {Dauwels, J. and Korl, S. and Loeliger, H. A.},
  month     = aug,
  year      = {2005},
  keywords  = {Kalman filters, Message passing, graph theory, expectation
               maximization, expectation-maximisation algorithm, Filtering
               algorithms, Kalman filtering, Neural networks, backpropagation
               algorithm, Backpropagation algorithms, factor graph, gradient
               methods, Kalman smoothing, max-product algorithms, state space
               model, steepest descent, sum product algorithm},
  file      = {Dauwels et al. - 2005 - Steepest Descent as Message
               Passing.pdf:/Users/apodusenko/Zotero/storage/Z3EX4446/Dauwels et al. -
               2005 - Steepest Descent as Message Passing.pdf:application/pdf;IEEE
               Xplore Abstract
               Record:/Users/apodusenko/Zotero/storage/9FR3RP6D/1531853.html:text/html
               }
}

@article{parr_uncertainty_2017,
  title     = {Uncertainty, epistemics and active inference},
  volume    = {14},
  copyright = {© 2017 The Authors.. Published by the Royal Society under the
               terms of the Creative Commons Attribution License
               http://creativecommons.org/licenses/by/4.0/, which permits
               unrestricted use, provided the original author and source are
               credited.},
  issn      = {1742-5689, 1742-5662},
  url       = {http://rsif.royalsocietypublishing.org/content/14/136/20170376},
  doi       = {10.1098/rsif.2017.0376},
  abstract  = {Biological systems—like ourselves—are constantly faced with
               uncertainty. Despite noisy sensory data, and volatile environments,
               creatures appear to actively maintain their integrity. To account
               for this remarkable ability to make optimal decisions in the face
               of a capricious world, we propose a generative model that
               represents the beliefs an agent might possess about their own
               uncertainty. By simulating a noisy and volatile environment, we
               demonstrate how uncertainty influences optimal epistemic (visual)
               foraging. In our simulations, saccades were deployed less
               frequently to regions with a lower sensory precision, while a
               greater volatility led to a shorter inhibition of return. These
               simulations illustrate a principled explanation for some cardinal
               aspects of visual foraging—and allow us to propose a correspondence
               between the representation of uncertainty and ascending
               neuromodulatory systems, complementing that suggested by Yu \&
               Dayan (Yu \& Dayan 2005 Neuron 46, 681–692.
               (doi:10.1016/j.neuron.2005.04.026)).},
  language  = {en},
  number    = {136},
  urldate   = {2017-11-29},
  journal   = {Journal of The Royal Society Interface},
  author    = {Parr, Thomas and Friston, Karl J.},
  month     = nov,
  year      = {2017},
  pmid      = {29167370},
  pages     = {20170376},
  file      = {Parr and Friston - 2017 - Uncertainty, epistemics and active
               inference.pdf:/Users/apodusenko/Zotero/storage/JKMFGF26/Parr and
               Friston - 2017 - Uncertainty, epistemics and active
               inference.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/BS67CFHG/20170376.html:text/html
               }
}

@book{metz_practical_2018,
  address    = {Boston, MA},
  edition    = {2 edition},
  title      = {Practical {Object}-{Oriented} {Design}: {An} {Agile} {Primer} {Using}
                {Ruby}},
  isbn       = {978-0-13-445647-8},
  shorttitle = {Practical {Object}-{Oriented} {Design}},
  abstract   = {The Complete Guide to Writing Maintainable, Manageable, Pleasing,
                and Powerful Object-Oriented Applications Object-oriented
                programming languages exist to help you create beautiful,
                straightforward applications that are easy to change and simple to
                extend. Unfortunately, the world is awash with object-oriented (OO)
                applications that are difficult to understand and expensive to
                change. Practical Object-Oriented Design, Second Edition, immerses
                you in an OO mindset and teaches you powerful, real-world,
                object-oriented design techniques with simple and practical
                examples. Sandi Metz demonstrates how to build new applications
                that can “survive success” and repair existing applications that
                have become impossible to change. Each technique is illustrated
                with extended examples in the easy-to-understand Ruby programming
                language, all downloadable from the companion website, poodr.com.
                Fully updated for Ruby 2.5, this guide shows how to Decide what
                belongs in a single class Avoid entangling objects that should be
                kept separate Define flexible interfaces among objects Reduce
                programming overhead costs with duck typing Successfully apply
                inheritance Build objects via composition Whatever your previous
                object-oriented experience, this concise guide will help you
                achieve the superior outcomes you’re looking for. Register your
                book for convenient access to downloads, updates, and/or
                corrections as they become available. See inside book for details.},
  language   = {English},
  publisher  = {Addison-Wesley Professional},
  author     = {Metz, Sandi},
  month      = aug,
  year       = {2018}
}

@article{hohwy_self-evidencing_2016,
  title     = {The {Self}-{Evidencing} {Brain}},
  volume    = {50},
  copyright = {© 2014 Wiley Periodicals, Inc.},
  issn      = {1468-0068},
  url       = {https://onlinelibrary.wiley.com/doi/abs/10.1111/nous.12062},
  doi       = {10.1111/nous.12062},
  abstract  = {An exciting theory in neuroscience is that the brain is an organ
               for prediction error minimization (PEM). This theory is rapidly
               gaining influence and is set to dominate the science of mind and
               brain in the years to come. PEM has extreme explanatory ambition,
               and profound philosophical implications. Here, I assume the theory,
               briefly explain it, and then I argue that PEM implies that the
               brain is essentially self-evidencing. This means it is imperative
               to identify an evidentiary boundary between the brain and its
               environment. This boundary defines the mind-world relation, opens
               the door to skepticism, and makes the mind transpire as more
               inferentially secluded and neurocentrically skull-bound than many
               would nowadays think. Therefore, PEM somewhat deflates contemporary
               hypotheses that cognition is extended, embodied and enactive;
               however, it can nevertheless accommodate the kinds of cases that
               fuel these hypotheses.},
  language  = {en},
  number    = {2},
  urldate   = {2018-08-16},
  journal   = {Noûs},
  author    = {Hohwy, Jakob},
  month     = jun,
  year      = {2016},
  pages     = {259--285},
  file      = {Snapshot:/Users/apodusenko/Zotero/storage/TI6SE6MC/nous.html:text/html
               }
}

@article{chertkov_loop_2006,
  title    = {Loop {Calculus} in {Statistical} {Physics} and {Information} {Science
              }},
  volume   = {73},
  issn     = {1539-3755, 1550-2376},
  url      = {http://arxiv.org/abs/cond-mat/0601487},
  doi      = {10.1103/PhysRevE.73.065102},
  abstract = {Considering a discrete and finite statistical model of a general
              position we introduce an exact expression for the partition
              function in terms of a finite series. The leading term in the
              series is the Bethe-Peierls (Belief Propagation)-BP contribution,
              the rest are expressed as loop-contributions on the factor graph
              and calculated directly using the BP solution. The series unveils a
              small parameter that often makes the BP approximation so
              successful. Applications of the loop calculus in statistical
              physics and information science are discussed.},
  number   = {6},
  urldate  = {2018-08-13},
  journal  = {Physical Review E},
  author   = {Chertkov, Michael and Chernyak, Vladimir Y.},
  month    = jun,
  year     = {2006},
  note     = {arXiv: cond-mat/0601487},
  keywords = {Computer Science - Information Theory, Condensed Matter -
              Statistical Mechanics, Condensed Matter - Disordered Systems and
              Neural Networks},
  file     = {arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/EPZAY9ZJ/0601487.html:text/html;Chertkov
              and Chernyak - 2006 - Loop Calculus in Statistical Physics and
              Informati.pdf:/Users/apodusenko/Zotero/storage/G8GBG45Z/Chertkov and
              Chernyak - 2006 - Loop Calculus in Statistical Physics and
              Informati.pdf:application/pdf}
}

@article{al-bashabsheh_normal_2011-1,
  title    = {Normal {Factor} {Graphs} and {Holographic} {Transformations}},
  volume   = {57},
  issn     = {0018-9448},
  doi      = {10.1109/TIT.2010.2094870},
  abstract = {This paper stands at the intersection of two distinct lines of
              research. One line is “holographic algorithms,” a powerful approach
              introduced by Valiant for solving various counting problems in
              computer science; the other is “normal factor graphs,” an elegant
              framework proposed by Forney for representing codes defined on
              graphs. We introduce the notion of holographic transformations for
              normal factor graphs, and establish a very general theorem, called
              the generalized Holant theorem, which relates a normal factor graph
              to its holographic transformation. We show that the generalized
              Holant theorem on the one hand underlies the principle of
              holographic algorithms, and on the other hand reduces to a general
              duality theorem for normal factor graphs, a special case of which
              was first proved by Forney. In the course of our development, we
              formalize a new semantics for normal factor graphs, which
              highlights various linear algebraic properties that potentially
              enable the use of normal factor graphs as a linear algebraic tool.},
  number   = {2},
  journal  = {IEEE Transactions on Information Theory},
  author   = {Al-Bashabsheh, A. and Mao, Y.},
  month    = feb,
  year     = {2011},
  keywords = {encoding, Vectors, Semantics, factor graph, Codes on graphs,
              duality, duality (mathematics), duality theorem, general normal
              factor graph duality theorem, generalized Holant theorem, Holant
              theorem, holographic algorithm, holographic transformation,
              holography, Information theory, Joining processes, linear algebra,
              normal factor graph, Special issues and sections, Tensile stress},
  pages    = {752--763},
  file     = {IEEE Xplore Abstract
              Record:/Users/apodusenko/Zotero/storage/4A9W7GI5/5695119.html:text/html
              }
}

@article{aji_generalized_2000,
  title    = {The {Generalized} {Distributive} {Law}},
  volume   = {46},
  abstract = {In this semitutorial paper we discuss a general message passing
              algorithm, which we call the generalized distributive law (GDL).
              The GDL is a synthesis of the work of many authors in the
              information theory, digital communications, signal processing,
              statistics, and artificial intelligence communities. It includes as
              special cases the Baum–Welch algorithm, the fast Fourier transform
              (FFT) on any finite Abelian group, the Gallager–Tanner–Wiberg
              decoding algorithm, Viterbi’s algorithm, the BCJR algorithm,
              Pearl’s “belief propagation” algorithm, the Shafer–Shenoy
              probability propagation algorithm, and the turbo decoding
              algorithm. Although this algorithm is guaranteed to give exact
              answers only in certain cases (the “junction tree” condition),
              unfortunately not including the cases of GTW with cycles or turbo
              decoding, there is much experimental evidence, and a few theorems,
              suggesting that it often works approximately even when it is not
              supposed to.},
  language = {en},
  number   = {2},
  journal  = {IEEE TRANSACTIONS ON INFORMATION THEORY},
  author   = {Aji, Srinivas M and McEliece, Robert J},
  year     = {2000},
  pages    = {19},
  file     = {Aji and McEliece - 2000 - The Generalized Distributive
              Law.pdf:/Users/apodusenko/Zotero/storage/FXH9E5GL/Aji and McEliece -
              2000 - The Generalized Distributive Law.pdf:application/pdf}
}

@book{nelson_user-friendly_2015,
  address   = {Providence, Rhode Island},
  series    = {The {Student} {Mathematical} {Library}},
  title     = {A {User}-{Friendly} {Introduction} to {Lebesgue} {Measure} and {
               Integration}},
  volume    = {78},
  isbn      = {978-1-4704-2199-1 978-1-4704-2737-5},
  url       = {http://www.ams.org/stml/078},
  language  = {en},
  urldate   = {2018-08-07},
  publisher = {American Mathematical Society},
  author    = {Nelson, Gail},
  month     = nov,
  year      = {2015},
  doi       = {10.1090/stml/078},
  file      = {Nelson - 2015 - A User-Friendly Introduction to Lebesgue Measure
               a.pdf:/Users/apodusenko/Zotero/storage/8IY3ZY9L/Nelson - 2015 - A
               User-Friendly Introduction to Lebesgue Measure a.pdf:application/pdf}
}

@article{power_message-passing_nodate,
  title    = {Message-{Passing} {Monte} {Carlo}},
  language = {en},
  author   = {Power, Samuel},
  pages    = {1},
  file     = {Power - Message-Passing Monte
              Carlo.pdf:/Users/apodusenko/Zotero/storage/CRW7BBG9/Power -
              Message-Passing Monte Carlo.pdf:application/pdf}
}

@misc{byrne_automatic_2017,
  title    = {Automatic {Differentiation} for the {Greeks}},
  url      = {https://www.wilmott.com/automatic-for-the-greeks/},
  abstract = {The sensitivities of the value of an option to the model
              parameters, a.k.a. “the Greeks,” are crucial to understanding the
              risk of an option position, a},
  language = {en-GB},
  urldate  = {2018-07-22},
  journal  = {Wilmott},
  author   = {Byrne, Simon and Greenwell, Andrew},
  month    = apr,
  year     = {2017},
  file     = {
              Snapshot:/Users/apodusenko/Zotero/storage/69UT85AB/automatic-for-the-greeks.html:text/html
              }
}

@article{revels_forward-mode_2016,
  title    = {Forward-{Mode} {Automatic} {Differentiation} in {Julia}},
  url      = {http://arxiv.org/abs/1607.07892},
  abstract = {We present ForwardDiff, a Julia package for forward-mode automatic
              differentiation (AD) featuring performance competitive with
              low-level languages like C++. Unlike recently developed AD tools in
              other popular high-level languages such as Python and MATLAB,
              ForwardDiff takes advantage of just-in-time (JIT) compilation to
              transparently recompile AD-unaware user code, enabling efficient
              support for higher-order differentiation and differentiation using
              custom number types (including complex numbers). For gradient and
              Jacobian calculations, ForwardDiff provides a variant of
              vector-forward mode that avoids expensive heap allocation and makes
              better use of memory bandwidth than traditional vector mode. In our
              numerical experiments, we demonstrate that for nontrivially large
              dimensions, ForwardDiff's gradient computations can be faster than
              a reverse-mode implementation from the Python-based autograd
              package. We also illustrate how ForwardDiff is used effectively
              within JuMP, a modeling language for optimization. According to our
              usage statistics, 41 unique repositories on GitHub depend on
              ForwardDiff, with users from diverse fields such as astronomy,
              optimization, finite element analysis, and statistics. This
              document is an extended abstract that has been accepted for
              presentation at the AD2016 7th International Conference on
              Algorithmic Differentiation.},
  urldate  = {2018-07-22},
  journal  = {arXiv:1607.07892 [cs]},
  author   = {Revels, Jarrett and Lubin, Miles and Papamarkou, Theodore},
  month    = jul,
  year     = {2016},
  note     = {arXiv: 1607.07892},
  keywords = {Computer Science - Mathematical Software},
  file     = {arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/VU44UU6E/1607.html:text/html;Revels
              et al. - 2016 - Forward-Mode Automatic Differentiation in
              Julia.pdf:/Users/apodusenko/Zotero/storage/5NLMWUVP/Revels et al. -
              2016 - Forward-Mode Automatic Differentiation in
              Julia.pdf:application/pdf}
}

@article{parr_discrete_2018,
  title      = {The {Discrete} and {Continuous} {Brain}: {From} {Decisions} to {
                Movement}—and {Back} {Again}},
  issn       = {0899-7667},
  shorttitle = {The {Discrete} and {Continuous} {Brain}},
  url        = {https://doi.org/10.1162/neco_a_01102},
  doi        = {10.1162/neco_a_01102},
  abstract   = {To act upon the world, creatures must change continuous variables
                such as muscle length or chemical concentration. In contrast,
                decision making is an inherently discrete process, involving the
                selection among alternative courses of action. In this article, we
                consider the interface between the discrete and continuous
                processes that translate our decisions into movement in a Newtonian
                world—and how movement informs our decisions. We do so by appealing
                to active inference, with a special focus on the oculomotor system.
                Within this exemplar system, we argue that the superior colliculus
                is well placed to act as a discrete-continuous interface.
                Interestingly, when the neuronal computations within the superior
                colliculus are formulated in terms of active inference, we find
                that many aspects of its neuroanatomy emerge from the computations
                it must perform in this role.},
  urldate    = {2018-06-16},
  journal    = {Neural Computation},
  author     = {Parr, Thomas and Friston, Karl J.},
  month      = jun,
  year       = {2018},
  pages      = {1--29},
  file       = {Parr en Friston - 2018 - The Discrete and Continuous Brain From
                Decisions .pdf:/Users/apodusenko/Zotero/storage/72J7RBJJ/Parr en
                Friston - 2018 - The Discrete and Continuous Brain From Decisions
                .pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/KA6WNRVW/neco_a_01102.html:text/html
                }
}

@book{hipel_time_1994,
  title     = {Time series modelling of water resources and environmental systems},
  isbn      = {0-08-087036-8},
  publisher = {Elsevier},
  author    = {Hipel, Keith W. and McLeod, A. Ian},
  year      = {1994}
}

@article{murphy_bayes_2001,
  title   = {The bayes net toolbox for matlab},
  volume  = {33},
  url     = {http://interfacesymposia.org/I01/I2001Proceedings/KMurphy/KMurphy.pdf},
  number  = {2},
  journal = {Computing science and statistics},
  author  = {Murphy, Kevin},
  year    = {2001},
  pages   = {1024--1034}
}

@misc{minka_infer.net_2018,
  title  = {Infer.{NET} 2.7},
  url    = {http://research.microsoft.com/infernet},
  author = {Minka, T. and Winn, J.M. and Guiver, J.P. and Zaykov, Y. and Fabian,
            D. and Bronskill, J.},
  year   = {2018},
  note   = {Microsoft Research Cambridge}
}

@article{yedidia_constructing_2005,
  title    = {Constructing free-energy approximations and generalized belief
              propagation algorithms},
  volume   = {51},
  issn     = {0018-9448},
  url      = {http://ieeexplore.ieee.org/abstract/document/1459044},
  doi      = {10.1109/TIT.2005.850085},
  abstract = {Important inference problems in statistical physics, computer
              vision, error-correcting coding theory, and artificial intelligence
              can all be reformulated as the computation of marginal
              probabilities on factor graphs. The belief propagation (BP)
              algorithm is an efficient way to solve these problems that is exact
              when the factor graph is a tree, but only approximate when the
              factor graph has cycles. We show that BP fixed points correspond to
              the stationary points of the Bethe approximation of the free energy
              for a factor graph. We explain how to obtain region-based free
              energy approximations that improve the Bethe approximation, and
              corresponding generalized belief propagation (GBP) algorithms. We
              emphasize the conditions a free energy approximation must satisfy
              in order to be a "valid" or "maxent-normal" approximation. We
              describe the relationship between four different methods that can
              be used to generate valid approximations: the "Bethe method", the "
              junction graph method", the "cluster variation method", and the "
              region graph method". Finally, we explain how to tell whether a
              region-based approximation, and its corresponding GBP algorithm, is
              likely to be accurate, and describe empirical results showing that
              GBP can significantly outperform BP.},
  number   = {7},
  journal  = {IEEE Transactions on Information Theory},
  author   = {Yedidia, Jonathan S. and Freeman, W.T. and Weiss, Y.},
  month    = jul,
  year     = {2005},
  keywords = {Message passing, Belief propagation, Approximation algorithms,
              Artificial intelligence, backpropagation, belief networks, Belief
              propagation (BP), Bethe approximation, Bethe free energy, cluster
              variation method, Clustering algorithms, Codes, Computer errors,
              Computer vision, factor graphs, free energy approximation, GBP
              algorithm, generalized belief propagation, generalized belief
              propagation (GBP), graph theory, Inference algorithms, inference
              mechanisms, inference problem, junction graph method, Kikuchi free
              energy, Physics computing, Probability, region graph method,
              sum-product algorithm, sum–product algorithm},
  pages    = {2282--2312},
  file     = {IEEE Xplore Abstract
              Record:/Users/apodusenko/Zotero/storage/CSY5GAI2/login.html:text/html;Yedidia
              et al. - 2005 - Constructing free-energy approximations and
              genera.pdf:/Users/apodusenko/Zotero/storage/LM4JS687/Yedidia et al. -
              2005 - Constructing free-energy approximations and
              genera.pdf:application/pdf}
}

@inproceedings{bishop_vibes:_2003,
  title      = {{VIBES}: {A} variational inference engine for {Bayesian} networks},
  shorttitle = {{VIBES}},
  url        = {
                http://papers.nips.cc/paper/2172-vibes-a-variational-inference-engine-for-bayesian-networks.pdf
                },
  booktitle  = {Advances in neural information processing systems},
  author     = {Bishop, Christopher M. and Spiegelhalter, David and Winn, John},
  year       = {2003},
  pages      = {793--800},
  file       = {Fulltext:/Users/apodusenko/Zotero/storage/RG24ZA3G/Bishop e.a. - 2003
                - VIBES A variational inference engine for
                Bayesian.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/B6Q4ZCQW/Bishop
                e.a. - 2003 - VIBES A variational inference engine for
                Bayesian.pdf:application/pdf}
}

@article{harva_bayes_2012,
  title      = {Bayes {Blocks}: {An} implementation of the variational {Bayesian}
                building blocks framework},
  shorttitle = {Bayes {Blocks}},
  url        = {https://arxiv.org/pdf/1207.1380},
  journal    = {arXiv preprint arXiv:1207.1380},
  author     = {Harva, Markus and Raiko, Tapani and Honkela, Antti and Valpola,
                Harri and Karhunen, Juha},
  year       = {2012},
  file       = {Snapshot:/Users/apodusenko/Zotero/storage/ESTTGSWP/1207.html:text/html
                }
}

@inproceedings{bergstra_theano:_2010,
  title      = {Theano: {A} {CPU} and {GPU} math compiler in {Python}},
  volume     = {1},
  shorttitle = {Theano},
  url        = {http://conference.scipy.org/proceedings/scipy2010/pdfs/bergstra.pdf},
  booktitle  = {Proc. 9th {Python} in {Science} {Conf}},
  author     = {Bergstra, James and Breuleux, Olivier and Bastien, Frédéric and
                Lamblin, Pascal and Pascanu, Razvan and Desjardins, Guillaume and
                Turian, Joseph and Warde-Farley, David and Bengio, Yoshua},
  year       = {2010},
  file       = {Fulltext:/Users/apodusenko/Zotero/storage/5P647VJK/Bergstra e.a. -
                2010 - Theano A CPU and GPU math compiler in
                Python.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/6YTM2QRX/Bergstra
                e.a. - 2010 - Theano A CPU and GPU math compiler in
                Python.pdf:application/pdf}
}

@inproceedings{abadi_tensorflow:_2016,
  title      = {{TensorFlow}: a system for large-scale machine learning},
  shorttitle = {{TensorFlow}},
  url        = {https://www.usenix.org/system/files/conference/osdi16/osdi16-abadi.pdf},
  booktitle  = {Proceedings of the 12th {USENIX} conference on {Operating} {
                Systems} {Design} and {Implementation}},
  publisher  = {USENIX Association},
  author     = {Abadi, Martín and Barham, Paul and Chen, Jianmin and Chen, Zhifeng
                and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat,
                Sanjay and Irving, Geoffrey and Isard, Michael},
  year       = {2016},
  pages      = {265--283},
  file       = {
                Snapshot:/Users/apodusenko/Zotero/storage/T3L7GJSJ/citation.html:text/html
                }
}

@inproceedings{stuhlmuller_learning_2013,
  title     = {Learning stochastic inverses},
  url       = {http://papers.nips.cc/paper/4966-learning-stochastic-inverses.pdf},
  booktitle = {Advances in neural information processing systems},
  author    = {Stuhlmüller, Andreas and Taylor, Jacob and Goodman, Noah},
  year      = {2013},
  pages     = {3048--3056},
  file      = {Fulltext:/Users/apodusenko/Zotero/storage/C2QVG9UJ/Stuhlmüller e.a. -
               2013 - Learning stochastic
               inverses.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/SVIBWUF6/4966-learning-stochastic-inverses.html:text/html
               }
}

@article{kuss_assessing_2005,
  title   = {Assessing approximate inference for binary {Gaussian} process
             classification},
  volume  = {6},
  url     = {http://www.jmlr.org/papers/volume6/kuss05a/kuss05a.pdf},
  urldate = {2014-04-10},
  journal = {The Journal of Machine Learning Research},
  author  = {Kuss, Malte and Rasmussen, Carl Edward},
  year    = {2005},
  pages   = {1679--1704},
  file    = {Kuss - 2005 - Assessing Approximate Inference for Binary Gaussian
             Process
             Classification.pdf:/Users/apodusenko/Zotero/storage/N7VDXWND/Kuss -
             2005 - Assessing Approximate Inference for Binary Gaussian Process
             Classification.pdf:application/pdf}
}

@inproceedings{tucker_rebar:_2017,
  title      = {{REBAR}: {Low}-variance, unbiased gradient estimates for discrete
                latent variable models},
  shorttitle = {{REBAR}},
  url        = {
                http://papers.nips.cc/paper/6856-rebar-low-variance-unbiased-gradient-estimates-for-discrete-latent-variable-models.pdf
                },
  booktitle  = {Advances in {Neural} {Information} {Processing} {Systems}},
  author     = {Tucker, George and Mnih, Andriy and Maddison, Chris J. and Lawson,
                John and Sohl-Dickstein, Jascha},
  year       = {2017},
  pages      = {2624--2633},
  file       = {Fulltext:/Users/apodusenko/Zotero/storage/HJD7HYAY/Tucker e.a. - 2017
                - REBAR Low-variance, unbiased gradient estimates
                f.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/FAKNHULW/6856-rebar-low-variance-unbiased-gradient-estimates-for-discrete-latent-variable-models.html:text/html
                }
}

@article{carpenter_stan:_2017,
  title      = {Stan: {A} {Probabilistic} {Programming} {Language}},
  volume     = {76},
  issn       = {1548-7660},
  shorttitle = {\textit{{Stan}}},
  url        = {http://www.jstatsoft.org/v76/i01},
  doi        = {10.18637/jss.v076.i01},
  language   = {en},
  number     = {1},
  urldate    = {2017-06-07},
  journal    = {Journal of Statistical Software},
  author     = {Carpenter, Bob and Gelman, Andrew and Hoffman, Matthew D. and Lee,
                Daniel and Goodrich, Ben and Betancourt, Michael and Brubaker, Marcus
                and Guo, Jiqiang and Li, Peter and Riddell, Allen},
  year       = {2017},
  file       = {Carpenter et al. - 2017 - Stan A Probabilistic Programming
                Language.pdf:/Users/apodusenko/Zotero/storage/BMKKW9GZ/Carpenter et al.
                - 2017 - Stan A Probabilistic Programming Language.pdf:application/pdf}
}

@inproceedings{lienart_expectation_2015,
  title     = {Expectation particle belief propagation},
  url       = {
               http://papers.nips.cc/paper/5674-expectation-particle-belief-propagation
               },
  booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
  author    = {Lienart, Thibaut and Teh, Yee Whye and Doucet, Arnaud},
  year      = {2015},
  pages     = {3609--3617},
  file      = {Lienart e.a. - 2015 - Expectation particle belief
               propagation.pdf:/Users/apodusenko/Zotero/storage/NNDNR43P/Lienart e.a.
               - 2015 - Expectation particle belief
               propagation.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/N37M7KX2/5674-expectation-particle-belief-propagation.html:text/html
               }
}

@article{moens_hierarchical_2018,
  title    = {The {Hierarchical} {Adaptive} {Forgetting} {Variational} {Filter}},
  url      = {http://arxiv.org/abs/1805.05703},
  abstract = {A common problem in Machine Learning and statistics consists in
              detecting whether the current sample in a stream of data belongs to
              the same distribution as previous ones, is an isolated outlier or
              inaugurates a new distribution of data. We present a hierarchical
              Bayesian algorithm that aims at learning a time-specific
              approximate posterior distribution of the parameters describing the
              distribution of the data observed. We derive the update equations
              of the variational parameters of the approximate posterior at each
              time step for models from the exponential family, and show that
              these updates find interesting correspondents in Reinforcement
              Learning (RL). In this perspective, our model can be seen as a
              hierarchical RL algorithm that learns a posterior distribution
              according to a certain stability confidence that is, in turn,
              learned according to its own stability confidence. Finally, we show
              some applications of our generic model, first in a RL context, next
              with an adaptive Bayesian Autoregressive model, and finally in the
              context of Stochastic Gradient Descent optimization.},
  urldate  = {2018-05-23},
  journal  = {arXiv:1805.05703 [cs, stat]},
  author   = {Moens, Vincent},
  month    = may,
  year     = {2018},
  note     = {arXiv: 1805.05703},
  keywords = {Computer Science - Learning, Statistics - Machine Learning},
  file     = {arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/7ENQDG9I/1805.html:text/html;Moens
              - 2018 - The Hierarchical Adaptive Forgetting Variational
              F.pdf:/Users/apodusenko/Zotero/storage/LU7NYHJJ/Moens - 2018 - The
              Hierarchical Adaptive Forgetting Variational F.pdf:application/pdf}
}

@article{tran_comment_2017,
  title   = {Comment},
  volume  = {112},
  issn    = {0162-1459},
  url     = {https://doi.org/10.1080/01621459.2016.1270044},
  doi     = {10.1080/01621459.2016.1270044},
  number  = {517},
  urldate = {2018-05-22},
  journal = {Journal of the American Statistical Association},
  author  = {Tran, Dustin and Blei, David M.},
  month   = jan,
  year    = {2017},
  pages   = {156--158},
  file    = {Full Text PDF:/Users/apodusenko/Zotero/storage/HW3DU78G/Tran en Blei -
             2017 -
             Comment.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/KIKN2DSX/01621459.2016.html:text/html
             }
}

@phdthesis{mathys_hierarchical_2012,
  title    = {Hierarchical {Gaussian} filtering},
  url      = {http://e-collection.library.ethz.ch/view/eth:6419},
  urldate  = {2014-04-10},
  school   = {Diss., Eidgenoessische Technische Hochschule ETH Zuerich, Nr. 20909},
  author   = {Mathys, Christoph D.},
  year     = {2012},
  keywords = {neuroscience, variational Bayes},
  file     = {FW Some questions re HGF and variational
              filtering.txt:/Users/apodusenko/Zotero/storage/5CRDK8IA/FW Some
              questions re HGF and variational filtering.txt:text/plain;Mathys - 2012
              - dissertation - Hierarchical Gaussian
              Filtering.pdf:/Users/apodusenko/Zotero/storage/5UPWAHN5/Mathys - 2012 -
              dissertation - Hierarchical Gaussian Filtering.pdf:application/pdf}
}

@article{mathys_bayesian_nodate,
  title    = {Bayesian modeling of learning and decision-making in uncertain
              environments},
  language = {en},
  author   = {Mathys, Christoph},
  pages    = {51},
  file     = {Mathys - Bayesian modeling of learning and decision-making
              .pdf:/Users/apodusenko/Zotero/storage/8NY5VNBP/Mathys - Bayesian
              modeling of learning and decision-making .pdf:application/pdf}
}

@article{marshall_pharmacological_2016,
  title    = {Pharmacological {Fingerprints} of {Contextual} {Uncertainty}},
  volume   = {14},
  issn     = {1544-9173},
  url      = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5113004/},
  doi      = {10.1371/journal.pbio.1002575},
  abstract = {Successful interaction with the environment requires flexible
              updating of our beliefs about the world. By estimating the
              likelihood of future events, it is possible to prepare appropriate
              actions in advance and execute fast, accurate motor responses.
              According to theoretical proposals, agents track the variability
              arising from changing environments by computing various forms of
              uncertainty. Several neuromodulators have been linked to
              uncertainty signalling, but comprehensive empirical
              characterisation of their relative contributions to perceptual
              belief updating, and to the selection of motor responses, is
              lacking. Here we assess the roles of noradrenaline, acetylcholine,
              and dopamine within a single, unified computational framework of
              uncertainty. Using pharmacological interventions in a sample of 128
              healthy human volunteers and a hierarchical Bayesian learning model
              , we characterise the influences of noradrenergic, cholinergic, and
              dopaminergic receptor antagonism on individual computations of
              uncertainty during a probabilistic serial reaction time task. We
              propose that noradrenaline influences learning of uncertain events
              arising from unexpected changes in the environment. In contrast,
              acetylcholine balances attribution of uncertainty to chance
              fluctuations within an environmental context, defined by a stable
              set of probabilistic associations, or to gross environmental
              violations following a contextual switch. Dopamine supports the use
              of uncertainty representations to engender fast, adaptive
              responses., Pharmacological interventions and hierarchical Bayesian
              modelling pinpoint the roles of noradrenaline, acetylcholine, and
              dopamine in computing different forms of uncertainty and in
              sensitizing actions to our beliefs about uncertainty., Interacting
              with dynamic and ever-changing environments requires frequent
              updating of our beliefs about the world. By learning the
              relationships that link events in the current environmental context
              , it is possible to prepare and execute fast, accurate responses to
              those events that are predictable. However, the world’s complex
              dynamics give rise to uncertainty about the relationships that
              exist between events and uncertainty about how these relationships
              might change over time. Several neuromodulators have been proposed
              to signal these different forms of uncertainty, but their relative
              contributions to updating beliefs and modulating responses have
              remained elusive. Here we combine a probabilistic reaction time
              task, pharmacological interventions, and a hierarchical Bayesian
              learning model to identify the roles of noradrenaline,
              acetylcholine, and dopamine in individual computations of
              uncertainty. We propose that noradrenaline modulates learning about
              the instability of the relationships that link environmental
              events. Acetylcholine balances the attribution of uncertainty to
              unexpected events occurring within an environmental context or to
              gross violations of our expectations following a context change. In
              contrast, dopamine sensitises our actions to our beliefs about
              uncertainty.},
  number   = {11},
  urldate  = {2018-05-07},
  journal  = {PLoS Biology},
  author   = {Marshall, Louise and Mathys, Christoph and Ruge, Diane and de Berker
              , Archy O. and Dayan, Peter and Stephan, Klaas E. and Bestmann, Sven},
  month    = nov,
  year     = {2016},
  pmid     = {27846219},
  pmcid    = {PMC5113004},
  file     = {Marshall et al. - 2016 - Pharmacological Fingerprints of Contextual
              Uncerta.pdf:/Users/apodusenko/Zotero/storage/CG2A577N/Marshall et al. -
              2016 - Pharmacological Fingerprints of Contextual
              Uncerta.pdf:application/pdf}
}

@book{mackay_information_2003,
  title     = {Information theory, inference and learning algorithms},
  publisher = {Cambridge university press},
  author    = {MacKay, David},
  year      = {2003},
  file      = {MacKay - 2003 - Information theory, inference and learning
               algorit.pdf:/Users/apodusenko/Zotero/storage/9AZW69DP/MacKay - 2003 -
               Information theory, inference and learning algorit.pdf:application/pdf}
}

@inproceedings{tarapore_how_2016,
  title     = {How do {Different} {Encodings} {Influence} the {Performance} of the {
               MAP}-{Elites} {Algorithm}?},
  isbn      = {978-1-4503-4206-3},
  url       = {http://dl.acm.org/citation.cfm?doid=2908812.2908875},
  doi       = {10.1145/2908812.2908875},
  abstract  = {Limbo (LIbrary for Model-Based Optimization) is an open-source
               C++11 library for Gaussian Processes and data-efﬁcient optimization
               (e.g., Bayesian optimization, see [12]) that is designed to be both
               highly ﬂexible and very fast. It can be used as a state-of-theart
               optimization library or to experiment with novel algorithms with
               “plugin” components. Limbo is currently mostly used for
               data-efﬁcient policy search in robot learning [8] and online
               adaptation because computation time matters when using the lowpower
               embedded computers of robots. For example, Limbo was the key
               library to develop a new algorithm that allows a legged robot to
               learn a new gait after a mechanical damage in about 1015 trials (2
               minutes) [5], and a 4-DOF manipulator to learn neural networks
               policies for goal reaching in about 5 trials [3].},
  language  = {en},
  urldate   = {2018-05-01},
  publisher = {ACM Press},
  author    = {Tarapore, Danesh and Clune, Jeff and Cully, Antoine and Mouret,
               Jean-Baptiste},
  year      = {2016},
  pages     = {173--180}
}

@article{cully_limbo:_2016,
  title   = {Limbo: {A} {Flexible} {High}-performance {Library} for {Gaussian} {
             Processes} modeling and {Data}-{Efficient} {Optimization}},
  url     = {https://members.loria.fr/JBMouret/pdf/limbo_paper.pdf},
  journal = {Preprint},
  author  = {Cully, A. and Chatzilygeroudis, K. and Allocati, F. and Mouret,
             J.-B.},
  year    = {2016},
  file    = {Cully et al. - 2016 - Limbo A Flexible High-performance Library for
             Gau.pdf:/Users/apodusenko/Zotero/storage/RZE92UWN/Cully et al. - 2016 -
             Limbo A Flexible High-performance Library for Gau.pdf:application/pdf}
}

@misc{noauthor_us_nodate,
  title    = {{US} {Dollar} {Swiss} {Franc} {Exchange} {Rate} ({USD} {CHF}) - {
              Historical} {Chart}},
  url      = {
              http://www.macrotrends.net/2558/us-dollar-swiss-franc-exchange-rate-historical-chart
              },
  abstract = {Interactive historical chart showing the daily U.S. Dollar - Swiss
              Franc (USDCHF) exchange rate back to 1991.},
  urldate  = {2018-04-24},
  file     = {US Dollar Swiss Franc Exchange Rate (USD CHF) -
              Hi.html:/Users/apodusenko/Zotero/storage/YDIQML34/US Dollar Swiss Franc
              Exchange Rate (USD CHF) - Hi.html:text/html}
}

@misc{noauthor_google_nodate,
  title    = {Google},
  url      = {
              https://www.google.com/logos/doodles/2018/earth-day-2018-6526947692642304.2-2xa.gif
              },
  abstract = {Happy Earth Day 2018! Check out Jane Goodall's message to the
              world in this year's \#EarthDay \#GoogleDoodle},
  language = {en-NL},
  urldate  = {2018-04-22},
  file     = {
              Snapshot:/Users/apodusenko/Zotero/storage/PFTXMZZJ/www.google.com.html:text/html
              }
}

@inproceedings{minka_expectation_2001,
  address   = {San Francisco, CA, USA},
  series    = {{UAI}'01},
  title     = {Expectation {Propagation} for {Approximate} {Bayesian} {Inference}},
  isbn      = {978-1-55860-800-9},
  url       = {http://dl.acm.org/citation.cfm?id=2074022.2074067},
  abstract  = {This paper presents a new deterministic approximation technique in
               Bayesian networks. This method, "Expectation Propagation," unifies
               two previous techniques: assumed-density filtering, an extension of
               the Kalman filter, and loopy belief propagation, an extension of
               belief propagation in Bayesian networks. Loopy belief propagation,
               because it propagates exact belief states, is useful for a limited
               class of belief networks, such as those which are purely discrete.
               Expectation Propagation approximates the belief states by only
               retaining expectations, such as mean and varitmce, and iterates
               until these expectations are consistent throughout the network.
               This makes it applicable to hybrid networks with discrete and
               continuous nodes. Experiments with Gaussian mixture models show
               Expectation Propagation to be donvincingly better than methods with
               similar computational cost: Laplace's method, variational Bayes,
               and Monte Carlo. Expectation Propagation also provides an efficient
               algorithm for training Bayes point machine classifiers.},
  urldate   = {2018-04-04},
  booktitle = {Proceedings of the {Seventeenth} {Conference} on {Uncertainty} in
               {Artificial} {Intelligence}},
  publisher = {Morgan Kaufmann Publishers Inc.},
  author    = {Minka, Thomas P.},
  year      = {2001},
  pages     = {362--369},
  file      = {ACM Full Text PDF:/Users/apodusenko/Zotero/storage/LRQI3NUX/Minka -
               2001 - Expectation Propagation for Approximate Bayesian
               I.pdf:application/pdf}
}

@book{breese_uncertainty_2001,
  address    = {San Francisco, Calif},
  title      = {Uncertainty in artificial intelligence: proceedings of the
                seventeenth conference (2001) ; {August} 2 - 5, 2001, {University} of
                {Washington}, {Seattle}, {Washington} ; [the {Seventeenth} {Conference
                } on {Uncertainty} in {Artificial} {Intelligence} ({UAI}-2001)]},
  isbn       = {978-1-55860-800-9},
  shorttitle = {Uncertainty in artificial intelligence},
  language   = {eng},
  publisher  = {Morgan Kaufmann},
  editor     = {Breese, Jack and Conference on Uncertainty in Artificial
                Intelligence and American Association for Artificial Intelligence},
  year       = {2001},
  note       = {OCLC: 248199110}
}

@article{albert_bayesian_1993,
  title    = {Bayesian {Analysis} of {Binary} and {Polychotomous} {Response} {Data}
              },
  volume   = {88},
  issn     = {0162-1459},
  url      = {https://www.tandfonline.com/doi/abs/10.1080/01621459.1993.10476321},
  doi      = {10.1080/01621459.1993.10476321},
  abstract = {A vast literature in statistics, biometrics, and econometrics is
              concerned with the analysis of binary and polychotomous response
              data. The classical approach fits a categorical response regression
              model using maximum likelihood, and inferences about the model are
              based on the associated asymptotic theory. The accuracy of
              classical confidence statements is questionable for small sample
              sizes. In this article, exact Bayesian methods for modeling
              categorical response data are developed using the idea of data
              augmentation. The general approach can be summarized as follows.
              The probit regression model for binary outcomes is seen to have an
              underlying normal regression structure on latent continuous data.
              Values of the latent data can be simulated from suitable truncated
              normal distributions. If the latent data are known, then the
              posterior distribution of the parameters can be computed using
              standard results for normal linear models. Draws from this
              posterior are used to sample new latent data, and the process is
              iterated with Gibbs sampling. This data augmentation approach
              provides a general framework for analyzing binary regression
              models. It leads to the same simplification achieved earlier for
              censored regression models. Under the proposed framework, the class
              of probit regression models can be enlarged by using mixtures of
              normal distributions to model the latent data. In this normal
              mixture class, one can investigate the sensitivity of the parameter
              estimates to the choice of “link function,” which relates the
              linear regression estimate to the fitted probabilities. In addition
              , this approach allows one to easily fit Bayesian hierarchical
              models. One specific model considered here reflects the belief that
              the vector of regression coefficients lies on a smaller dimension
              linear subspace. The methods can also be generalized to multinomial
              response models with J {\textgreater} 2 categories. In the ordered
              multinomial model, the J categories are ordered and a model is
              written linking the cumulative response probabilities with the
              linear regression structure. In the unordered multinomial model,
              the latent variables have a multivariate normal distribution with
              unknown variance-covariance matrix. For both multinomial models,
              the data augmentation method combined with Gibbs sampling is
              outlined. This approach is especially attractive for the
              multivariate probit model, where calculating the likelihood can be
              difficult.},
  number   = {422},
  urldate  = {2018-03-26},
  journal  = {Journal of the American Statistical Association},
  author   = {Albert, James H. and Chib, Siddhartha},
  month    = jun,
  year     = {1993},
  keywords = {Binary probit, Data augmentation, Gibbs sampling, Hierarchical
              Bayes modeling, Latent data, Logit model, Multinomial probit,
              Residual analysis, Student-t link function},
  pages    = {669--679},
  file     = {
              Snapshot:/Users/apodusenko/Zotero/storage/C2XW83NW/01621459.1993.html:text/html
              }
}

@article{forney_partition_2011,
  title    = {Partition {Functions} of {Normal} {Factor} {Graphs}},
  url      = {http://arxiv.org/abs/1102.0316},
  abstract = {One of the most common types of functions in mathematics, physics,
              and engineering is a sum of products, sometimes called a partition
              function. After "normalization," a sum of products has a natural
              graphical representation, called a normal factor graph (NFG), in
              which vertices represent factors, edges represent internal
              variables, and half-edges represent the external variables of the
              partition function. In physics, so-called trace diagrams share
              similar features. We believe that the conceptual framework of
              representing sums of products as partition functions of NFGs is an
              important and intuitive paradigm that, surprisingly, does not seem
              to have been introduced explicitly in the previous factor graph
              literature. Of particular interest are NFG modifications that leave
              the partition function invariant. A simple subclass of such NFG
              modifications offers a unifying view of the Fourier transform,
              tree-based reparameterization, loop calculus, and the Legendre
              transform.},
  urldate  = {2018-03-19},
  journal  = {arXiv:1102.0316 [cs, math]},
  author   = {Forney, Jr and Vontobel, Pascal O.},
  month    = feb,
  year     = {2011},
  note     = {arXiv: 1102.0316},
  keywords = {Computer Science - Information Theory},
  file     = {arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/LSQFJTFR/1102.html:text/html;Forney
              and Vontobel - 2011 - Partition Functions of Normal Factor
              Graphs.pdf:/Users/apodusenko/Zotero/storage/QN7LFZ6A/Forney and
              Vontobel - 2011 - Partition Functions of Normal Factor
              Graphs.pdf:application/pdf}
}

@book{kruschke_doing_2015,
  address    = {Boston},
  edition    = {Edition 2},
  title      = {Doing {Bayesian} data analysis: a tutorial with {R}, {JAGS}, and {
                Stan}},
  isbn       = {978-0-12-405888-0},
  shorttitle = {Doing {Bayesian} data analysis},
  publisher  = {Academic Press},
  author     = {Kruschke, John K.},
  year       = {2015},
  keywords   = {Bayesian statistical decision theory, R (Computer program
                language)},
  file       = {
                DBDA2Eprograms.zip:/Users/apodusenko/Zotero/storage/EEYMTGR6/DBDA2Eprograms.zip:application/x-zip-compressed;Kruschke
                - 2015 - Doing Bayesian data analysis a tutorial with R,
                J.pdf:/Users/apodusenko/Zotero/storage/2PGKJ3NP/Kruschke - 2015 - Doing
                Bayesian data analysis a tutorial with R,
                J.pdf:application/pdf;Kruschke-DBDA2E-ExerciseSolutions.pdf:/Users/apodusenko/Zotero/storage/8JFRNY85/Kruschke-DBDA2E-ExerciseSolutions.pdf:application/pdf
                }
}

@article{koutsourelakis_uncertainty_2008,
  title    = {Uncertainty quantification in complex systems using approximate
              solvers},
  url      = {http://arxiv.org/abs/0808.3416},
  abstract = {This paper proposes a novel uncertainty quantification framework
              for computationally demanding systems characterized by a large
              vector of non-Gaussian uncertainties. It combines state-of-the-art
              techniques in advanced Monte Carlo sampling with Bayesian
              formulations. The key departure from existing works is the use of
              inexpensive, approximate computational models in a rigorous manner.
              Such models can readily be derived by coarsening the discretization
              size in the solution of the governing PDEs, increasing the time
              step when integration of ODEs is performed, using fewer iterations
              if a non-linear solver is employed or making use of lower order
              models. It is shown that even in cases where the inexact models
              provide very poor approximations of the exact response, statistics
              of the latter can be quantified accurately with significant
              reductions in the computational effort. Multiple approximate models
              can be used and rigorous confidence bounds of the estimates
              produced are provided at all stages.},
  urldate  = {2018-03-08},
  journal  = {arXiv:0808.3416 [physics, stat]},
  author   = {Koutsourelakis, Phaedon-Stelios},
  month    = aug,
  year     = {2008},
  note     = {arXiv: 0808.3416},
  keywords = {Statistics - Machine Learning, Mathematics - Numerical Analysis,
              Physics - Computational Physics, Statistics - Computation, 65C05,
              62F15, 62G08},
  file     = {arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/AF6FN4F8/0808.html:text/html;Koutsourelakis
              - 2008 - Uncertainty quantification in complex systems
              usin.pdf:/Users/apodusenko/Zotero/storage/HKMSKPRH/Koutsourelakis -
              2008 - Uncertainty quantification in complex systems
              usin.pdf:application/pdf}
}

@article{bojchevski_netgan:_2018,
  title      = {{NetGAN}: {Generating} {Graphs} via {Random} {Walks}},
  shorttitle = {{NetGAN}},
  url        = {http://arxiv.org/abs/1803.00816},
  abstract   = {We propose NetGAN - the first implicit generative model for graphs
                able to mimic real-world networks. We pose the problem of graph
                generation as learning the distribution of biased random walks over
                the input graph. The proposed model is based on a stochastic neural
                network that generates discrete output samples and is trained using
                the Wasserstein GAN objective. NetGAN is able to produce graphs
                that exhibit the well-known network patterns without explicitly
                specifying them in the model definition. At the same time, our
                model exhibits strong generalization properties, as highlighted by
                its competitive link prediction performance, despite not being
                trained specifically for this task. Being the first approach to
                combine both of these desirable properties, NetGAN opens exciting
                further avenues for research.},
  urldate    = {2018-03-08},
  journal    = {arXiv:1803.00816 [cs, stat]},
  author     = {Bojchevski, Aleksandar and Shchur, Oleksandr and Zügner, Daniel and
                Günnemann, Stephan},
  month      = mar,
  year       = {2018},
  note       = {arXiv: 1803.00816},
  keywords   = {Computer Science - Learning, Statistics - Machine Learning,
                Computer Science - Social and Information Networks},
  file       = {arXiv.org
                Snapshot:/Users/apodusenko/Zotero/storage/B6J6SZ2Z/1803.html:text/html;Bojchevski
                et al. - 2018 - NetGAN Generating Graphs via Random
                Walks.pdf:/Users/apodusenko/Zotero/storage/IUEZFFGE/Bojchevski et al. -
                2018 - NetGAN Generating Graphs via Random Walks.pdf:application/pdf}
}

@article{nacson_convergence_2018,
  title    = {Convergence of {Gradient} {Descent} on {Separable} {Data}},
  url      = {http://arxiv.org/abs/1803.01905},
  abstract = {The implicit bias of gradient descent is not fully understood even
              in simple linear classification tasks (e.g., logistic regression).
              Soudry et al. (2018) studied this bias on separable data, where
              there are multiple solutions that correctly classify the data. It
              was found that, when optimizing monotonically decreasing loss
              functions with exponential tails using gradient descent, the linear
              classifier specified by the gradient descent iterates converge to
              the \$L\_2\$ max margin separator. However, the convergence rate to
              the maximum margin solution with fixed step size was found to be
              extremely slow: \$1/{\textbackslash}log(t)\$. Here we examine how
              the convergence is influenced by using different loss functions and
              by using variable step sizes. First, we calculate the convergence
              rate for loss functions with poly-exponential tails near \${
              \textbackslash}exp(-u{\textasciicircum}\{{\textbackslash}nu\})\$.
              We prove that \${\textbackslash}nu=1\$ yields the optimal
              convergence rate in the range \${\textbackslash}nu{\textgreater}
              0.25\$. Based on further analysis we conjecture that this remains
              the optimal rate for \${\textbackslash}nu {\textbackslash}leq 0.25
              \$, and even for sub-poly-exponential tails --- until loss
              functions with polynomial tails no longer converge to the max
              margin. Second, we prove the convergence rate could be improved to
              \$({\textbackslash}log t) /{\textbackslash}sqrt\{t\}\$ for the
              exponential loss, by using aggressive step sizes which compensate
              for the rapidly vanishing gradients.},
  urldate  = {2018-03-08},
  journal  = {arXiv:1803.01905 [cs, stat]},
  author   = {Nacson, Mor Shpigel and Lee, Jason and Gunasekar, Suriya and Srebro,
              Nathan and Soudry, Daniel},
  month    = mar,
  year     = {2018},
  note     = {arXiv: 1803.01905},
  keywords = {Computer Science - Learning, Statistics - Machine Learning},
  file     = {arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/BZCT7ETY/1803.html:text/html;Nacson
              et al. - 2018 - Convergence of Gradient Descent on Separable
              Data.pdf:/Users/apodusenko/Zotero/storage/W92LRC2Q/Nacson et al. - 2018
              - Convergence of Gradient Descent on Separable Data.pdf:application/pdf
              }
}

@article{hsu_extracting_2018,
  title    = {Extracting {Domain} {Invariant} {Features} by {Unsupervised} {
              Learning} for {Robust} {Automatic} {Speech} {Recognition}},
  url      = {http://arxiv.org/abs/1803.02551},
  abstract = {The performance of automatic speech recognition (ASR) systems can
              be significantly compromised by previously unseen conditions, which
              is typically due to a mismatch between training and testing
              distributions. In this paper, we address robustness by studying
              domain invariant features, such that domain information becomes
              transparent to ASR systems, resolving the mismatch problem.
              Specifically, we investigate a recent model, called the Factorized
              Hierarchical Variational Autoencoder (FHVAE). FHVAEs learn to
              factorize sequence-level and segment-level attributes into
              different latent variables without supervision. We argue that the
              set of latent variables that contain segment-level information is
              our desired domain invariant feature for ASR. Experiments are
              conducted on Aurora-4 and CHiME-4, which demonstrate 41\% and 27\%
              absolute word error rate reductions respectively on mismatched
              domains.},
  urldate  = {2018-03-08},
  journal  = {arXiv:1803.02551 [cs, eess, stat]},
  author   = {Hsu, Wei-Ning and Glass, James},
  month    = mar,
  year     = {2018},
  note     = {arXiv: 1803.02551},
  keywords = {Computer Science - Learning, Computer Science - Sound, Statistics
              - Machine Learning, Computer Science - Computation and Language,
              Electrical Engineering and Systems Science - Audio and Speech
              Processing},
  file     = {arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/GITXCVMJ/1803.html:text/html;Hsu
              and Glass - 2018 - Extracting Domain Invariant Features by
              Unsupervis.pdf:/Users/apodusenko/Zotero/storage/G38DLXXE/Hsu and Glass
              - 2018 - Extracting Domain Invariant Features by
              Unsupervis.pdf:application/pdf}
}

@incollection{wipf_new_2008,
  title     = {A {New} {View} of {Automatic} {Relevance} {Determination}},
  url       = {
               http://papers.nips.cc/paper/3372-a-new-view-of-automatic-relevance-determination.pdf
               },
  urldate   = {2018-03-06},
  booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 20},
  publisher = {Curran Associates, Inc.},
  author    = {Wipf, David P. and Nagarajan, Srikantan S.},
  editor    = {Platt, J. C. and Koller, D. and Singer, Y. and Roweis, S. T.},
  year      = {2008},
  pages     = {1625--1632},
  file      = {NIPS Full Text PDF:/Users/apodusenko/Zotero/storage/JM69MXPS/Wipf and
               Nagarajan - 2008 - A New View of Automatic Relevance
               Determination.pdf:application/pdf;NIPS
               Snapshort:/Users/apodusenko/Zotero/storage/APU8N7G7/3372-a-new-view-of-automatic-relevance-determination.html:text/html
               }
}

@inproceedings{sutton_gain_1992,
  title     = {Gain adaptation beats least squares},
  volume    = {161168},
  booktitle = {Proceedings of the 7th {Yale} workshop on adaptive and learning
               systems},
  author    = {Sutton, Richard S.},
  year      = {1992},
  file      = {Sutton - 1992 - Gain adaptation beats least
               squares.pdf:/Users/apodusenko/Zotero/storage/A24U8TMK/Sutton - 1992 -
               Gain adaptation beats least squares.pdf:application/pdf}
}

@inproceedings{vontobel_analysis_2016,
  title     = {Analysis of double covers of factor graphs},
  booktitle = {Signal {Processing} and {Communications} ({SPCOM}), 2016 {
               International} {Conference} on},
  publisher = {IEEE},
  author    = {Vontobel, Pascal O.},
  year      = {2016},
  pages     = {1--5},
  file      = {Vontobel - 2016 - Analysis of double covers of factor
               graphs.pdf:/Users/apodusenko/Zotero/storage/3I3QMPCZ/Vontobel - 2016 -
               Analysis of double covers of factor graphs.pdf:application/pdf}
}

@article{ahn_gauged_2018,
  title    = {Gauged {Mini}-{Bucket} {Elimination} for {Approximate} {Inference}},
  url      = {http://arxiv.org/abs/1801.01649},
  abstract = {Computing the partition function \$Z\$ of a discrete graphical
              model is a fundamental inference challenge. Since this is
              computationally intractable, variational approximations are often
              used in practice. Recently, so-called gauge transformations were
              used to improve variational lower bounds on \$Z\$. In this paper,
              we propose a new gauge-variational approach, termed WMBE-G, which
              combines gauge transformations with the weighted mini-bucket
              elimination (WMBE) method. WMBE-G can provide both upper and lower
              bounds on \$Z\$, and is easier to optimize than the prior
              gauge-variational algorithm. We show that WMBE-G strictly improves
              the earlier WMBE approximation for symmetric models including Ising
              models with no magnetic field. Our experimental results demonstrate
              the effectiveness of WMBE-G even for generic, nonsymmetric models.},
  urldate  = {2018-02-27},
  journal  = {arXiv:1801.01649 [stat]},
  author   = {Ahn, Sungsoo and Chertkov, Michael and Shin, Jinwoo and Weller,
              Adrian},
  month    = jan,
  year     = {2018},
  note     = {arXiv: 1801.01649},
  keywords = {Statistics - Machine Learning},
  file     = {Ahn et al. - 2018 - Gauged Mini-Bucket Elimination for Approximate
              Inf.pdf:/Users/apodusenko/Zotero/storage/8VQHPJCY/Ahn et al. - 2018 -
              Gauged Mini-Bucket Elimination for Approximate
              Inf.pdf:application/pdf;arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/NZPXJYU4/1801.html:text/html}
}

@article{chien_hardware_2018,
  title    = {Hardware {Implementation} of an {Event}-{Based} {Message} {Passing} {
              Graphical} {Model} {Network}},
  volume   = {PP},
  issn     = {1549-8328},
  doi      = {10.1109/TCSI.2018.2798289},
  abstract = {This paper presents a hardware system that implements a factor
              graph, where messages are sent using an event-based belief
              propagation algorithm. The system, comprising an FPGA and an
              application specific integrated circuit (ASIC) chip, can be used to
              construct a graph with upto 16 output message channels. The ASIC
              chip with 16 channels is fabricated in a 0.35 um 2P4M CMOS process
              and occupies 2.16 x 2.74 mm². Each channel dissipates 46 uW. The
              output analog messages of the channels are encoded through the
              interspike intervals of the output spike streams or events. The
              system can be used to implement graphs with arbitrary variable
              distributions for its inputs and using constraint functions, such
              as “plus' and “equality'. Using Kullback-Leibler divergence, we
              show that the measured distributions from the implemented graphs on
              the hardware show close similarity to the theoretical
              distributions.},
  number   = {99},
  journal  = {IEEE Transactions on Circuits and Systems I: Regular Papers},
  author   = {Chien, C. H. and Longinotti, L. and Steimer, A. and Liu, S. C.},
  year     = {2018},
  keywords = {Message passing, Belief propagation, event-based, Factor graph,
              Field programmable gate arrays, Graphical models, Hardware, hazard
              function, interspike interval, Neurons, random sampling, real-time.
              , renewal theory, spike-based, Stochastic processes},
  pages    = {1--14},
  file     = {Chien e.a. - 2018 - Hardware Implementation of an Event-Based Message
              .pdf:/Users/apodusenko/Zotero/storage/XHK6ICYD/Chien e.a. - 2018 -
              Hardware Implementation of an Event-Based Message
              .pdf:application/pdf;IEEE Xplore Abstract
              Record:/Users/apodusenko/Zotero/storage/PEXQQGNQ/8288861.html:text/html
              }
}

@book{gupta_matrix_1999,
  title     = {Matrix variate distributions},
  volume    = {104},
  isbn      = {1-58488-046-5},
  publisher = {CRC Press},
  author    = {Gupta, Arjun K. and Nagar, Daya K.},
  year      = {1999}
}

@phdthesis{sibel_region-based_nodate,
  title  = {Region-based approximation to solve inference in loopy factor graphs:
            decoding {LDPC} codes by {Generalized} {Belief} {Propagation}},
  author = {Sibel, Jean Cristoph},
  year   = {2013},
  month  = jun,
  school = {University of Cergy-Pontoise},
  file   = {Sibel - Region-based approximation to solve inference in
            l.pdf:/Users/apodusenko/Zotero/storage/DJPJBF65/Sibel - Region-based
            approximation to solve inference in l.pdf:application/pdf}
}

@article{friston_woodlice_2018,
  title   = {Of woodlice and men: {A} {Bayesian} account of cognition, life and
             consciousness. {An} interview with {Kark} {Friston}},
  volume  = {2},
  journal = {ALIUS Bulletin},
  author  = {Friston, Karl J and Fortier, Martin and Friedman, Daniel A.},
  year    = {2018},
  pages   = {17--43},
  file    = {Friston et al. - 2018 - Of woodlice and men A Bayesian account of
             cogniti.pdf:/Users/apodusenko/Zotero/storage/3IZUIPFW/Friston et al. -
             2018 - Of woodlice and men A Bayesian account of
             cogniti.pdf:application/pdf}
}

@inproceedings{tran_deep_2017,
  title   = {Deep {Probabilistic} {Programming}},
  urldate = {2017-01-10},
  author  = {Tran, Dustin and Hoffman, Matt and Saurous, Rif and Brevdo, Eugene
             and Murphy, Kevin P and Blei, David},
  year    = {2017},
  file    = {Tran et al. - 2017 - Deep Probabilistic
             Programming.pdf:/Users/apodusenko/Zotero/storage/JJTH7TH6/Tran et al. -
             2017 - Deep Probabilistic Programming.pdf:application/pdf}
}

@article{maddison_concrete_2016,
  title      = {The {Concrete} {Distribution}: {A} {Continuous} {Relaxation} of {
                Discrete} {Random} {Variables}},
  shorttitle = {The {Concrete} {Distribution}},
  url        = {http://arxiv.org/abs/1611.00712},
  abstract   = {The reparameterization trick enables optimizing large scale
                stochastic computation graphs via gradient descent. The essence of
                the trick is to refactor each stochastic node into a differentiable
                function of its parameters and a random variable with fixed
                distribution. After refactoring, the gradients of the loss
                propagated by the chain rule through the graph are low variance
                unbiased estimators of the gradients of the expected loss. While
                many continuous random variables have such reparameterizations,
                discrete random variables lack useful reparameterizations due to
                the discontinuous nature of discrete states. In this work we
                introduce Concrete random variables---continuous relaxations of
                discrete random variables. The Concrete distribution is a new
                family of distributions with closed form densities and a simple
                reparameterization. Whenever a discrete stochastic node of a
                computation graph can be refactored into a one-hot bit
                representation that is treated continuously, Concrete stochastic
                nodes can be used with automatic differentiation to produce
                low-variance biased gradients of objectives (including objectives
                that depend on the log-probability of latent stochastic nodes) on
                the corresponding discrete graph. We demonstrate the effectiveness
                of Concrete relaxations on density estimation and structured
                prediction tasks using neural networks.},
  urldate    = {2018-02-06},
  journal    = {arXiv:1611.00712 [cs, stat]},
  author     = {Maddison, Chris J. and Mnih, Andriy and Teh, Yee Whye},
  month      = nov,
  year       = {2016},
  note       = {arXiv: 1611.00712},
  keywords   = {Computer Science - Learning, Statistics - Machine Learning},
  file       = {arXiv.org
                Snapshot:/Users/apodusenko/Zotero/storage/MRSRDB28/1611.html:text/html;Maddison
                e.a. - 2016 - The Concrete Distribution A Continuous
                Relaxation.pdf:/Users/apodusenko/Zotero/storage/NH4CHBMK/Maddison e.a.
                - 2016 - The Concrete Distribution A Continuous
                Relaxation.pdf:application/pdf}
}

@techreport{mackay_ensemble_1997,
  title       = {Ensemble learning for hidden {Markov} models},
  institution = {Citeseer},
  author      = {MacKay, David JC},
  year        = {1997},
  file        = {MacKay - Ensemble Learning for Hidden Markov
                 Models.pdf:/Users/apodusenko/Zotero/storage/5A5G5WAM/MacKay - Ensemble
                 Learning for Hidden Markov Models.pdf:application/pdf}
}

@article{pfeffer_structured_2016,
  title      = {Structured {Factored} {Inference}: {A} {Framework} for {Automated} {
                Reasoning} in {Probabilistic} {Programming} {Languages}},
  shorttitle = {Structured {Factored} {Inference}},
  url        = {http://arxiv.org/abs/1606.03298},
  abstract   = {Reasoning on large and complex real-world models is a
                computationally difficult task, yet one that is required for
                effective use of many AI applications. A plethora of inference
                algorithms have been developed that work well on specific models or
                only on parts of general models. Consequently, a system that can
                intelligently apply these inference algorithms to different parts
                of a model for fast reasoning is highly desirable. We introduce a
                new framework called structured factored inference (SFI) that
                provides the foundation for such a system. Using models encoded in
                a probabilistic programming language, SFI provides a sound means to
                decompose a model into sub-models, apply an inference algorithm to
                each sub-model, and combine the resulting information to answer a
                query. Our results show that SFI is nearly as accurate as exact
                inference yet retains the benefits of approximate inference
                methods.},
  urldate    = {2018-01-10},
  journal    = {arXiv:1606.03298 [cs]},
  author     = {Pfeffer, Avi and Ruttenberg, Brian and Kretschmer, William},
  month      = jun,
  year       = {2016},
  note       = {arXiv: 1606.03298},
  keywords   = {Computer Science - Artificial Intelligence},
  file       = {arXiv.org
                Snapshot:/Users/apodusenko/Zotero/storage/H825LPYU/1606.html:text/html;Pfeffer
                et al. - 2016 - Structured Factored Inference A Framework for
                Aut.pdf:/Users/apodusenko/Zotero/storage/PK8VGX8F/Pfeffer et al. - 2016
                - Structured Factored Inference A Framework for Aut.pdf:application/pdf
                }
}

@article{caticha_lectures_2008,
  title    = {Lectures on {Probability}, {Entropy}, and {Statistical} {Physics}},
  url      = {http://arxiv.org/abs/0808.0012},
  abstract = {These lectures deal with the problem of inductive inference, that
              is, the problem of reasoning under conditions of incomplete
              information. Is there a general method for handling uncertainty? Or
              , at least, are there rules that could in principle be followed by
              an ideally rational mind when discussing scientific matters? What
              makes one statement more plausible than another? How much more
              plausible? And then, when new information is acquired how do we
              change our minds? Or, to put it differently, are there rules for
              learning? Are there rules for processing information that are
              objective and consistent? Are they unique? And, come to think of it
              , what, after all, is information? It is clear that data contains
              or conveys information, but what does this precisely mean? Can
              information be conveyed in other ways? Is information physical? Can
              we measure amounts of information? Do we need to? Our goal is to
              develop the main tools for inductive inference--probability and
              entropy--from a thoroughly Bayesian point of view and to illustrate
              their use in physics with examples borrowed from the foundations of
              classical statistical physics.},
  urldate  = {2017-12-17},
  journal  = {arXiv:0808.0012 [cond-mat, physics:physics, stat]},
  author   = {Caticha, Ariel},
  month    = jul,
  year     = {2008},
  note     = {arXiv: 0808.0012},
  keywords = {Computer Science - Information Theory, Condensed Matter -
              Statistical Mechanics, Mathematics - Statistics Theory, Physics -
              General Physics, Physics - Data Analysis, Statistics and
              Probability},
  file     = {arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/M9MTH2VV/0808.html:text/html;Caticha
              - 2008 - Lectures on Probability, Entropy, and Statistical
              .pdf:/Users/apodusenko/Zotero/storage/VWWJZ24I/Caticha - 2008 -
              Lectures on Probability, Entropy, and Statistical .pdf:application/pdf}
}

@article{wiecki_hddm:_2013,
  title      = {{HDDM}: {Hierarchical} {Bayesian} estimation of the {Drift}-{
                Diffusion} {Model} in {Python}},
  volume     = {7},
  issn       = {1662-5196},
  shorttitle = {{HDDM}},
  url        = {
                http://journal.frontiersin.org/article/10.3389/fninf.2013.00014/abstract
                },
  doi        = {10.3389/fninf.2013.00014},
  abstract   = {The diffusion model is a commonly used tool to infer latent
                psychological processes underlying decision making, and to link
                them to neural mechanisms based on reaction times. Although
                efficient open source software has been made available to
                quantitatively fit the model to data, current estimation methods
                require an abundance of reaction time measurements to recover
                meaningful parameters, and only provide point estimates of each
                parameter. In contrast, hierarchical Bayesian parameter estimation
                methods are useful for enhancing statistical power, allowing for
                simultaneous estimation of individual subject parameters and the
                group distribution that they are drawn from, while also providing
                measures of uncertainty in these parameters in the posterior
                distribution. Here, we present a novel Python-based toolbox called
                HDDM (hierarchical drift diffusion model), which allows fast and
                flexible estimation of the the drift-diffusion model and the
                related linear ballistic accumulator model. HDDM requires fewer
                data per subject / condition than non-hierarchical method, allows
                for full Bayesian data analysis, and can handle outliers in the
                data. Finally, HDDM supports the estimation of how trial-by-trial
                measurements (e.g. fMRI) influence decision making parameters. This
                paper will first describe the theoretical background of
                drift-diffusion model and Bayesian inference. We then illustrate
                usage of the toolbox on a real-world data set from our lab. Finally
                , parameter recovery studies show that HDDM beats alternative
                fitting methods like the chi-quantile method as well as maximum
                likelihood estimation. The software and documentation can be
                downloaded at: http://ski.clps.brown.edu/hddm\_docs},
  language   = {English},
  urldate    = {2017-03-26},
  journal    = {Frontiers in Neuroinformatics},
  author     = {Wiecki, Thomas V. and Sofer, Imri and Frank, Michael J.},
  year       = {2013},
  keywords   = {Bayesian modeling, decision making, drift diffusion model, python,
                Software},
  file       = {Wiecki et al. - 2013 - HDDM Hierarchical Bayesian estimation of the
                Drif.pdf:/Users/apodusenko/Zotero/storage/GZT5NE6X/Wiecki et al. - 2013
                - HDDM Hierarchical Bayesian estimation of the Drif.pdf:application/pdf
                }
}

@article{friston_variational_2008,
  title    = {Variational filtering},
  volume   = {41},
  issn     = {1053-8119},
  url      = {http://www.sciencedirect.com/science/article/pii/S1053811908002462},
  doi      = {10.1016/j.neuroimage.2008.03.017},
  abstract = {This note presents a simple Bayesian filtering scheme, using
              variational calculus, for inference on the hidden states of dynamic
              systems. Variational filtering is a stochastic scheme that
              propagates particles over a changing variational energy landscape,
              such that their sample density approximates the conditional density
              of hidden and states and inputs. The key innovation, on which
              variational filtering rests, is a formulation in generalised
              coordinates of motion. This renders the scheme much simpler and
              more versatile than existing approaches, such as those based on
              particle filtering. We demonstrate variational filtering using
              simulated and real data from hemodynamic systems studied in
              neuroimaging and provide comparative evaluations using particle
              filtering and the fixed-form homologue of variational filtering,
              namely dynamic expectation maximisation.},
  number   = {3},
  urldate  = {2017-11-21},
  journal  = {NeuroImage},
  author   = {Friston, K. J.},
  month    = jul,
  year     = {2008},
  keywords = {Action, Bayesian filtering, Dynamic expectation maximisation,
              Dynamical systems, Free-energy, Generalised coordinates, Nonlinear,
              Variational Bayes, Variational filtering},
  pages    = {747--766},
  file     = {Friston - 2008 - Variational
              filtering.pdf:/Users/apodusenko/Zotero/storage/ZC66FR56/Friston - 2008
              - Variational filtering.pdf:application/pdf;ScienceDirect
              Snapshot:/Users/apodusenko/Zotero/storage/S8A38GPN/S1053811908002462.html:text/html
              }
}

@article{kim_recognition_2017-1,
  title    = {Recognition {Dynamics} of the {Brain} in the {Free} {Energy} {
              Principle}},
  url      = {http://arxiv.org/abs/1710.09118},
  abstract = {We formulate the computational processes of perception in the
              framework of the principle of least action by postulating the
              theoretical action being time-integral of the free energy in the
              brain sciences. The free energy principle is accordingly rephrased
              as that for autopoietic reasons all viable organisms attempt to
              minimize the sensory uncertainty about the unpredictable
              environment over a temporal horizon. By varying the informational
              action, we derive the brain's recognition dynamics which conducts
              the adaptive inference of the external causes of sensory data with
              addressing only canonical positions and momenta of the brain's
              representations of the dynamical world. To manifest the utility of
              our theory, we provide how the neural computation may be
              implemented biophysically at a single-cell level and subsequently
              be scaled up to a large-scale functional architecture of the brain.
              We also present formal solutions to the recognition dynamics for a
              model brain in linear regime and analyze the perceptual
              trajectories about fixed points in state space.},
  urldate  = {2017-11-21},
  journal  = {arXiv:1710.09118 [q-bio]},
  author   = {Kim, Chang Sub},
  month    = oct,
  year     = {2017},
  note     = {arXiv: 1710.09118},
  keywords = {Quantitative Biology - Neurons and Cognition},
  file     = {arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/WBTX8M8T/1710.html:text/html;Kim
              - 2017 - Recognition Dynamics of the Brain in the Free
              Ener.pdf:/Users/apodusenko/Zotero/storage/PP5UCAID/Kim - 2017 -
              Recognition Dynamics of the Brain in the Free Ener.pdf:application/pdf}
}

@article{dempster_maximum_1977,
  title    = {Maximum {Likelihood} from {Incomplete} {Data} via the {EM} {Algorithm
              }},
  volume   = {39},
  issn     = {0035-9246},
  url      = {http://www.jstor.org/stable/2984875},
  abstract = {A broadly applicable algorithm for computing maximum likelihood
              estimates from incomplete data is presented at various levels of
              generality. Theory showing the monotone behaviour of the likelihood
              and convergence of the algorithm is derived. Many examples are
              sketched, including missing value situations, applications to
              grouped, censored or truncated data, finite mixture models,
              variance component estimation, hyperparameter estimation,
              iteratively reweighted least squares and factor analysis.},
  number   = {1},
  journal  = {Journal of the Royal Statistical Society. Series B (Methodological)
              },
  author   = {Dempster, A. P. and Laird, N. M. and Rubin, D. B.},
  year     = {1977},
  pages    = {1--38},
  file     = {Dempster et al. - 1977 - Maximum Likelihood from Incomplete Data via
              the EM.pdf:/Users/apodusenko/Zotero/storage/SR5U3ZA2/Dempster et al. -
              1977 - Maximum Likelihood from Incomplete Data via the
              EM.pdf:application/pdf}
}

@article{turner_free_nodate,
  title  = {Free {Energy} in {Statistical} {Physics} and {Inference}},
  author = {Turner, Richard},
  file   = {
            FreeEnergyNotes.pdf:/Users/apodusenko/Zotero/storage/6FEGE2HG/FreeEnergyNotes.pdf:application/pdf
            }
}

@article{jordan_introduction_1999,
  title   = {An introduction to variational methods for graphical models},
  volume  = {37},
  number  = {2},
  journal = {Machine learning},
  author  = {Jordan, Michael I. and Ghahramani, Zoubin and Jaakkola, Tommi S. and
             Saul, Lawrence K.},
  year    = {1999},
  pages   = {183--233},
  file    = {Jordan et al. - 1999 - An introduction to variational methods for
             graphic.pdf:/Users/apodusenko/Zotero/storage/E9PB8HK5/Jordan et al. -
             1999 - An introduction to variational methods for
             graphic.pdf:application/pdf}
}

@article{leike_optimal_2017,
  title     = {Optimal {Belief} {Approximation}},
  volume    = {19},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  url       = {http://www.mdpi.com/1099-4300/19/8/402},
  doi       = {10.3390/e19080402},
  abstract  = {In Bayesian statistics probability distributions express beliefs.
               However, for many problems the beliefs cannot be computed
               analytically and approximations of beliefs are needed. We seek a
               loss function that quantifies how “embarrassing” it is to
               communicate a given approximation. We reproduce and discuss an old
               proof showing that there is only one ranking under the requirements
               that (1) the best ranked approximation is the non-approximated
               belief and (2) that the ranking judges approximations only by their
               predictions for actual outcomes. The loss function that is obtained
               in the derivation is equal to the Kullback-Leibler divergence when
               normalized. This loss function is frequently used in the
               literature. However, there seems to be confusion about the correct
               order in which its functional arguments—the approximated and
               non-approximated beliefs—should be used. The correct order ensures
               that the recipient of a communication is only deprived of the
               minimal amount of information. We hope that the elementary
               derivation settles the apparent confusion. For example when
               approximating beliefs with Gaussian distributions the optimal
               approximation is given by moment matching. This is in contrast to
               many suggested computational schemes.},
  language  = {en},
  number    = {8},
  urldate   = {2017-11-14},
  journal   = {Entropy},
  author    = {Leike, Reimar H. and Enßlin, Torsten A.},
  month     = aug,
  year      = {2017},
  keywords  = {axiomatic derivation, Bayesian inference, information theory, loss
               function, machine learning},
  pages     = {402},
  file      = {Leike en Enßlin - 2017 - Optimal Belief
               Approximation.pdf:/Users/apodusenko/Zotero/storage/FKBCUYMB/Leike en
               Enßlin - 2017 - Optimal Belief
               Approximation.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/TKND8B9H/402.html:text/html
               }
}

@article{phan_audio_2017,
  title    = {Audio {Scene} {Classification} with {Deep} {Recurrent} {Neural} {
              Networks}},
  url      = {http://arxiv.org/abs/1703.04770},
  abstract = {We introduce in this work an efficient approach for audio scene
              classification using deep recurrent neural networks. An audio scene
              is firstly transformed into a sequence of high-level label tree
              embedding feature vectors. The vector sequence is then divided into
              multiple subsequences on which a deep GRU-based recurrent neural
              network is trained for sequence-to-label classification. The global
              predicted label for the entire sequence is finally obtained via
              aggregation of subsequence classification outputs. We will show
              that our approach obtains an F1-score of 97.7\% on the LITIS Rouen
              dataset, which is the largest dataset publicly available for the
              task. Compared to the best previously reported result on the
              dataset, our approach is able to reduce the relative classification
              error by 35.3\%.},
  urldate  = {2017-11-08},
  journal  = {arXiv:1703.04770 [cs]},
  author   = {Phan, Huy and Koch, Philipp and Katzberg, Fabrice and Maass, Marco
              and Mazur, Radoslaw and Mertins, Alfred},
  month    = mar,
  year     = {2017},
  note     = {arXiv: 1703.04770},
  keywords = {Computer Science - Learning, Computer Science - Sound},
  file     = {arXiv\:1703.04770 PDF:/Users/apodusenko/Zotero/storage/K5YVVTKZ/Phan
              et al. - 2017 - Audio Scene Classification with Deep Recurrent
              Neu.pdf:application/pdf;arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/JPRLM7ZN/1703.html:text/html}
}

@article{helske_mixture_2017,
  title      = {Mixture hidden {Markov} models for sequence data: the {seqHMM}
                package in {R}},
  shorttitle = {Mixture hidden {Markov} models for sequence data},
  journal    = {arXiv preprint arXiv:1704.00543},
  author     = {Helske, Satu and Helske, Jouni},
  year       = {2017},
  file       = {
                1704.00543.pdf:/Users/apodusenko/Zotero/storage/33B72PMT/1704.00543.pdf:application/pdf
                }
}

@article{brown_active_2013,
  title    = {Active inference, sensory attenuation and illusions},
  volume   = {14},
  issn     = {1612-4782},
  url      = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3824582/},
  doi      = {10.1007/s10339-013-0571-3},
  abstract = {Active inference provides a simple and neurobiologically plausible
              account of how action and perception are coupled in producing
              (Bayes) optimal behaviour. This can be seen most easily as
              minimising prediction error: we can either change our predictions
              to explain sensory input through perception. Alternatively, we can
              actively change sensory input to fulfil our predictions. In active
              inference, this action is mediated by classical reflex arcs that
              minimise proprioceptive prediction error created by descending
              proprioceptive predictions. However, this creates a conflict
              between action and perception; in that, self-generated movements
              require predictions to override the sensory evidence that one is
              not actually moving. However, ignoring sensory evidence means that
              externally generated sensations will not be perceived. Conversely,
              attending to (proprioceptive and somatosensory) sensations enables
              the detection of externally generated events but precludes
              generation of actions. This conflict can be resolved by attenuating
              the precision of sensory evidence during movement or, equivalently,
              attending away from the consequences of self-made acts. We propose
              that this Bayes optimal withdrawal of precise sensory evidence
              during movement is the cause of psychophysical sensory attenuation.
              Furthermore, it explains the force-matching illusion and reproduces
              empirical results almost exactly. Finally, if attenuation is
              removed, the force-matching illusion disappears and false
              (delusional) inferences about agency emerge. This is important,
              given the negative correlation between sensory attenuation and
              delusional beliefs in normal subjects—and the reduction in the
              magnitude of the illusion in schizophrenia. Active inference
              therefore links the neuromodulatory optimisation of precision to
              sensory attenuation and illusory phenomena during the attribution
              of agency in normal subjects. It also provides a functional account
              of deficits in syndromes characterised by false inference and
              impaired movement—like schizophrenia and Parkinsonism—syndromes
              that implicate abnormal modulatory neurotransmission.},
  number   = {4},
  urldate  = {2017-10-24},
  journal  = {Cognitive Processing},
  author   = {Brown, Harriet and Adams, Rick A. and Parees, Isabel and Edwards,
              Mark and Friston, Karl},
  year     = {2013},
  pmid     = {23744445},
  pmcid    = {PMC3824582},
  pages    = {411--427},
  file     = {Brown e.a. - 2013 - Active inference, sensory attenuation and
              illusion.pdf:/Users/apodusenko/Zotero/storage/8IYVNZ34/Brown e.a. -
              2013 - Active inference, sensory attenuation and
              illusion.pdf:application/pdf}
}

@inproceedings{han_convolutional_2016,
  title     = {Convolutional neural network with multiple-width frequency-delta data
               augmentation for acoustic scene classification},
  booktitle = {{IEEE} {AASP} {Challenge} on {Detection} and {Classification} of
               {Acoustic} {Scenes} and {Events} ({DCASE})},
  author    = {Han, Yoonchang and Lee, Kyogu},
  year      = {2016},
  file      = {
               DCASE2016_Lee_1034.pdf:/Users/apodusenko/Zotero/storage/HW7G8S37/DCASE2016_Lee_1034.pdf:application/pdf
               }
}

@inproceedings{eghbal-zadeh_cp-jku_2016,
  title      = {{CP}-{JKU} submissions for {DCASE}-2016: {A} hybrid approach using
                binaural i-vectors and deep convolutional neural networks},
  shorttitle = {{CP}-{JKU} submissions for {DCASE}-2016},
  url        = {
                https://www.researchgate.net/profile/Bernhard_Lehner/publication/306011374_CP-JKU_Submissions_for_DCASE-2016_a_Hybrid_Approach_Using_Binaural_I-Vectors_and_Deep_Convolutional_Neural_Networks/links/57bd4f0808ae6c703bc5eaa7.pdf
                },
  urldate    = {2017-09-27},
  booktitle  = {{IEEE} {AASP} {Challenge} on {Detection} and {Classification} of
                {Acoustic} {Scenes} and {Events} ({DCASE})},
  author     = {Eghbal-Zadeh, Hamid and Lehner, Bernhard and Dorfer, Matthias and
                Widmer, Gerhard},
  year       = {2016},
  file       = {Eghbal-Zadeh et al. - 2016 - CP-JKU submissions for DCASE-2016 A
                hybrid
                approa.pdf:/Users/apodusenko/Zotero/storage/HGVTXZX2/Eghbal-Zadeh et
                al. - 2016 - CP-JKU submissions for DCASE-2016 A hybrid
                approa.pdf:application/pdf}
}

@inproceedings{van_diepen_-situ_2017,
  address   = {Technische Universiteit Eindhoven},
  title     = {An {In}-situ {Trainable} {Gesture} {Classifier}},
  booktitle = {Proceedings of the {Twenty}-{Sixth} {Benelux} {Conference} on {
               Machine} {Learning}},
  author    = {van Diepen, Anouk and Cox, Marco and de Vries, Bert},
  month     = jun,
  year      = {2017},
  pages     = {66--69},
  file      = {van Diepen et al. - 2017 - An In-situ Trainable Gesture
               Classifier.pdf:/Users/apodusenko/Zotero/storage/T37U5RM9/van Diepen et
               al. - 2017 - An In-situ Trainable Gesture
               Classifier.pdf:application/pdf}
}

@article{yu_hidden_2010,
  title    = {Hidden semi-{Markov} models},
  volume   = {174},
  issn     = {00043702},
  url      = {http://linkinghub.elsevier.com/retrieve/pii/S0004370209001416},
  doi      = {10.1016/j.artint.2009.11.011},
  language = {en},
  number   = {2},
  urldate  = {2017-10-16},
  journal  = {Artificial Intelligence},
  author   = {Yu, Shun-Zheng},
  month    = feb,
  year     = {2010},
  pages    = {215--243},
  file     = {
              1-s2.0-S0004370209001416-main.pdf:/Users/apodusenko/Zotero/storage/KY4M8E5C/1-s2.0-S0004370209001416-main.pdf:application/pdf
              }
}

@article{brown_complete_1981,
  title    = {A {Complete} {Class} {Theorem} for {Statistical} {Problems} with {
              Finite} {Sample} {Spaces}},
  volume   = {9},
  issn     = {0090-5364},
  url      = {http://www.jstor.org/stable/2240418},
  abstract = {This paper contains a complete class theorem (Theorem 3.2) which
              applies to most statistical estimation problems having a finite
              sample space. This theorem also applies to many other statistical
              problems with finite sample spaces. The description of this
              complete class involves a stepwise algorithm. At each step of the
              process it is necessary to construct the Bayes procedures in a
              suitably modified version of the original problem. The complete
              class is a minimal complete class if the loss function is strictly
              convex. Some examples are given to illustrate the application of
              this complete class theorem. Among these is a new result concerning
              the estimation of the parameters of a multinomial distribution
              under a normalized quadratic loss function. (See Example 4.5).},
  number   = {6},
  urldate  = {2017-10-13},
  journal  = {The Annals of Statistics},
  author   = {Brown, Lawrence D.},
  year     = {1981},
  pages    = {1289--1300},
  file     = {Brown - 1981 - A Complete Class Theorem for Statistical Problems
              .pdf:/Users/apodusenko/Zotero/storage/ZV8KHXFV/Brown - 1981 - A
              Complete Class Theorem for Statistical Problems .pdf:application/pdf}
}

@article{levine_reinforcement_2018,
  title      = {Reinforcement {Learning} and {Control} as {Probabilistic} {Inference}
                : {Tutorial} and {Review}},
  shorttitle = {Reinforcement {Learning} and {Control} as {Probabilistic} {
                Inference}},
  url        = {http://arxiv.org/abs/1805.00909},
  abstract   = {The framework of reinforcement learning or optimal control
                provides a mathematical formalization of intelligent decision
                making that is powerful and broadly applicable. While the general
                form of the reinforcement learning problem enables effective
                reasoning about uncertainty, the connection between reinforcement
                learning and inference in probabilistic models is not immediately
                obvious. However, such a connection has considerable value when it
                comes to algorithm design: formalizing a problem as probabilistic
                inference in principle allows us to bring to bear a wide array of
                approximate inference tools, extend the model in flexible and
                powerful ways, and reason about compositionality and partial
                observability. In this article, we will discuss how a
                generalization of the reinforcement learning or optimal control
                problem, which is sometimes termed maximum entropy reinforcement
                learning, is equivalent to exact probabilistic inference in the
                case of deterministic dynamics, and variational inference in the
                case of stochastic dynamics. We will present a detailed derivation
                of this framework, overview prior work that has drawn on this and
                related ideas to propose new reinforcement learning and control
                algorithms, and describe perspectives on future research.},
  urldate    = {2019-03-15},
  journal    = {arXiv:1805.00909 [cs, stat]},
  author     = {Levine, Sergey},
  month      = may,
  year       = {2018},
  note       = {arXiv: 1805.00909},
  keywords   = {Statistics - Machine Learning, Computer Science - Artificial
                Intelligence, Computer Science - Robotics, Computer Science -
                Machine Learning},
  file       = {arXiv.org
                Snapshot:/Users/apodusenko/Zotero/storage/LFAAINZ3/1805.html:text/html;Levine
                - 2018 - Reinforcement Learning and Control as
                Probabilisti.pdf:/Users/apodusenko/Zotero/storage/894BFFMF/Levine -
                2018 - Reinforcement Learning and Control as
                Probabilisti.pdf:application/pdf}
}

@inproceedings{rawlik_stochastic_2012,
  title     = {On {Stochastic} {Optimal} {Control} and {Reinforcement} {Learning} by
               {Approximate} {Inference}},
  isbn      = {978-0-262-51968-7},
  url       = {http://www.roboticsproceedings.org/rss08/p45.pdf},
  doi       = {10.15607/RSS.2012.VIII.045},
  abstract  = {We present a reformulation of the stochastic optimal control
               problem in terms of KL divergence minimisation, not only providing
               a unifying perspective of previous approaches in this area, but
               also demonstrating that the formalism leads to novel practical
               approaches to the control problem. Speciﬁcally, a natural
               relaxation of the dual formulation gives rise to exact iterative
               solutions to the ﬁnite and inﬁnite horizon stochastic optimal
               control problem, while direct application of Bayesian inference
               methods yields instances of risk sensitive control. We furthermore
               study corresponding formulations in the reinforcement learning
               setting and present model free algorithms for problems with both
               discrete and continuous state and action spaces. Evaluation of the
               proposed methods on the standard Gridworld and Cart-Pole benchmarks
               veriﬁes the theoretical insights and shows that the proposed
               methods improve upon current approaches.},
  language  = {en},
  urldate   = {2019-03-15},
  booktitle = {Robotics: {Science} and {Systems} {VIII}},
  publisher = {Robotics: Science and Systems Foundation},
  author    = {Rawlik, Konrad and Toussaint, Marc and Vijayakumar, Sethu},
  month     = jul,
  year      = {2012},
  file      = {Rawlik et al. - 2012 - On Stochastic Optimal Control and Reinforcement
               Le.pdf:/Users/apodusenko/Zotero/storage/7SMC7DS6/Rawlik et al. - 2012 -
               On Stochastic Optimal Control and Reinforcement Le.pdf:application/pdf}
}

@article{doucet_rao-blackwellised_2013,
  title    = {Rao-{Blackwellised} {Particle} {Filtering} for {Dynamic} {Bayesian} {
              Networks}},
  url      = {http://arxiv.org/abs/1301.3853},
  abstract = {Particle filters (PFs) are powerful sampling-based
              inference/learning algorithms for dynamic Bayesian networks (DBNs).
              They allow us to treat, in a principled way, any type of
              probability distribution, nonlinearity and non-stationarity. They
              have appeared in several fields under such names as "condensation",
              "sequential Monte Carlo" and "survival of the fittest". In this
              paper, we show how we can exploit the structure of the DBN to
              increase the efficiency of particle filtering, using a technique
              known as Rao-Blackwellisation. Essentially, this samples some of
              the variables, and marginalizes out the rest exactly, using the
              Kalman filter, HMM filter, junction tree algorithm, or any other
              finite dimensional optimal filter. We show that Rao-Blackwellised
              particle filters (RBPFs) lead to more accurate estimates than
              standard PFs. We demonstrate RBPFs on two problems, namely
              non-stationary online regression with radial basis function
              networks and robot localization and map building. We also discuss
              other potential application areas and provide references to some
              finite dimensional optimal filters.},
  urldate  = {2019-03-15},
  journal  = {arXiv:1301.3853 [cs, stat]},
  author   = {Doucet, Arnaud and de Freitas, Nando and Murphy, Kevin and Russell,
              Stuart},
  month    = jan,
  year     = {2013},
  note     = {arXiv: 1301.3853},
  keywords = {Computer Science - Artificial Intelligence, Statistics -
              Computation, Computer Science - Machine Learning},
  file     = {arXiv\:1301.3853 PDF:/Users/apodusenko/Zotero/storage/MEQLNSDQ/Doucet
              et al. - 2013 - Rao-Blackwellised Particle Filtering for Dynamic
              B.pdf:application/pdf;arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/SU77QY92/1301.html:text/html}
}

@article{dedecius_autoregressive_2012,
  title    = {Autoregressive {Model} with {Partial} {Forgetting} within {Rao}-{
              Blackwellized} {Particle} {Filter}},
  volume   = {41},
  issn     = {0361-0918, 1532-4141},
  url      = {http://www.tandfonline.com/doi/abs/10.1080/03610918.2011.598992},
  doi      = {10.1080/03610918.2011.598992},
  language = {en},
  number   = {5},
  urldate  = {2019-03-15},
  journal  = {Communications in Statistics - Simulation and Computation},
  author   = {Dedecius, Kamil and Hofman, Radek},
  month    = may,
  year     = {2012},
  pages    = {582--589},
  file     = {Dedecius and Hofman - 2012 - Autoregressive Model with Partial
              Forgetting withi.pdf:/Users/apodusenko/Zotero/storage/8VG99BZG/Dedecius
              and Hofman - 2012 - Autoregressive Model with Partial Forgetting
              withi.pdf:application/pdf}
}

@article{kappen_path_2005,
  title   = {Path integrals and symmetry breaking for optimal control theory},
  volume  = {2005},
  issn    = {1742-5468},
  url     = {
             http://stacks.iop.org/1742-5468/2005/i=11/a=P11011?key=crossref.1f78a17a4703600ed252cfaeca103970
             },
  doi     = {10.1088/1742-5468/2005/11/P11011},
  number  = {11},
  urldate = {2019-03-08},
  journal = {Journal of Statistical Mechanics: Theory and Experiment},
  author  = {Kappen, H J},
  month   = nov,
  year    = {2005},
  pages   = {P11011--P11011},
  file    = {Submitted Version:/Users/apodusenko/Zotero/storage/RL5FRW7B/Kappen -
             2005 - Path integrals and symmetry breaking for optimal
             c.pdf:application/pdf}
}

@article{kaplan_planning_2018,
  title    = {Planning and navigation as active inference},
  volume   = {112},
  issn     = {0340-1200, 1432-0770},
  url      = {http://link.springer.com/10.1007/s00422-018-0753-2},
  doi      = {10.1007/s00422-018-0753-2},
  abstract = {This paper introduces an active inference formulation of planning
              and navigation. It illustrates how the exploitation–exploration
              dilemma is dissolved by acting to minimise uncertainty (i.e.
              expected surprise or free energy). We use simulations of a maze
              problem to illustrate how agents can solve quite complicated
              problems using context sensitive prior preferences to form
              subgoals. Our focus is on how epistemic behaviour—driven by novelty
              and the imperative to reduce uncertainty about the
              world—contextualises pragmatic or goal-directed behaviour. Using
              simulations, we illustrate the underlying process theory with
              synthetic behavioural and electrophysiological responses during
              exploration of a maze and subsequent navigation to a target
              location. An interesting phenomenon that emerged from the
              simulations was a putative distinction between ‘place cells’—that
              ﬁre when a subgoal is reached—and ‘path cells’—that ﬁre until a
              subgoal is reached.},
  language = {en},
  number   = {4},
  urldate  = {2019-03-04},
  journal  = {Biological Cybernetics},
  author   = {Kaplan, Raphael and Friston, Karl J.},
  month    = aug,
  year     = {2018},
  pages    = {323--343},
  file     = {Kaplan and Friston - 2018 - Planning and navigation as active
              inference.pdf:/Users/apodusenko/Zotero/storage/HKTZTZLU/Kaplan and
              Friston - 2018 - Planning and navigation as active
              inference.pdf:application/pdf}
}

@incollection{frigola_variational_2014,
  title     = {Variational {Gaussian} {Process} {State}-{Space} {Models}},
  url       = {
               http://papers.nips.cc/paper/5375-variational-gaussian-process-state-space-models.pdf
               },
  urldate   = {2019-03-04},
  booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 27},
  publisher = {Curran Associates, Inc.},
  author    = {Frigola, Roger and Chen, Yutian and Rasmussen, Carl Edward},
  editor    = {Ghahramani, Z. and Welling, M. and Cortes, C. and Lawrence, N. D.
               and Weinberger, K. Q.},
  year      = {2014},
  pages     = {3680--3688},
  file      = {NIPS Full Text PDF:/Users/apodusenko/Zotero/storage/878F9TD9/Frigola
               et al. - 2014 - Variational Gaussian Process State-Space
               Models.pdf:application/pdf;NIPS
               Snapshot:/Users/apodusenko/Zotero/storage/4R9CKK75/5375-variational-gaussian-process-state-space-models.html:text/html
               }
}

@inproceedings{song_hilbert_2009,
  address   = {Montreal, Quebec, Canada},
  title     = {Hilbert space embeddings of conditional distributions with
               applications to dynamical systems},
  isbn      = {978-1-60558-516-1},
  url       = {http://portal.acm.org/citation.cfm?doid=1553374.1553497},
  doi       = {10.1145/1553374.1553497},
  abstract  = {In this paper, we extend the Hilbert space embedding approach to
               handle conditional distributions. We derive a kernel estimate for
               the conditional embedding, and show its connection to ordinary
               embeddings. Conditional embeddings largely extend our ability to
               manipulate distributions in Hibert spaces, and as an example, we
               derive a nonparametric method for modeling dynamical systems where
               the belief state of the system is maintained as a conditional
               embedding. Our method is very general in terms of both the domains
               and the types of distributions that it can handle, and we
               demonstrate the effectiveness of our method in various dynamical
               systems. We expect that conditional embeddings will have wider
               applications beyond modeling dynamical systems.},
  language  = {en},
  urldate   = {2019-03-04},
  booktitle = {Proceedings of the 26th {Annual} {International} {Conference} on
               {Machine} {Learning} - {ICML} '09},
  publisher = {ACM Press},
  author    = {Song, Le and Huang, Jonathan and Smola, Alex and Fukumizu, Kenji},
  year      = {2009},
  pages     = {1--8},
  file      = {Song et al. - 2009 - Hilbert space embeddings of conditional
               distributi.pdf:/Users/apodusenko/Zotero/storage/PW8L62FV/Song et al. -
               2009 - Hilbert space embeddings of conditional
               distributi.pdf:application/pdf}
}

@inproceedings{ghosh_assumed_2016,
  title     = {Assumed density filtering methods for learning bayesian neural
               networks},
  booktitle = {Thirtieth {AAAI} {Conference} on {Artificial} {Intelligence}},
  author    = {Ghosh, Soumya and Delle Fave, Francesco Maria and Yedidia, Jonathan},
  year      = {2016},
  file      = {Ghosh e.a. - 2016 - Assumed density filtering methods for learning
               bay.pdf:/Users/apodusenko/Zotero/storage/38TRBR2I/Ghosh e.a. - 2016 -
               Assumed density filtering methods for learning
               bay.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/PDVA2GU4/12391.html:text/html
               }
}

@article{barto_recent_2003,
  title   = {Recent advances in hierarchical reinforcement learning},
  volume  = {13},
  number  = {1-2},
  journal = {Discrete event dynamic systems},
  author  = {Barto, Andrew G. and Mahadevan, Sridhar},
  year    = {2003},
  pages   = {41--77},
  file    = {Barto en Mahadevan - 2003 - Recent advances in hierarchical
             reinforcement lear.pdf:/Users/apodusenko/Zotero/storage/GMTPXFL2/Barto
             en Mahadevan - 2003 - Recent advances in hierarchical reinforcement
             lear.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/VGKRWUVY/A1022140919877.html:text/html
             }
}

@book{hammack_book_2018,
  title    = {Book of proof},
  isbn     = {978-0-9894721-2-8},
  abstract = {La 4e de couverture indique : "This book is an introduction to the
              language and standard proof methods of mathematics. It is a bridge
              from the computational courses (such as calculus or differential
              equations) that students typically encounter in their first year of
              college to a more abstract outlook. It lays a foundation for more
              theoretical courses such as topology, analysis and abstract
              algebra. Although it may be more meaningful to the student who has
              had some calculus, there is really no prerequisite other than a
              measure of mathematical maturity.Topics include sets, logic,
              counting, methods of conditional and non-conditional proof,
              disproof, induction, relations, functions, calculus proofs and
              infinite cardinality."},
  language = {English},
  author   = {Hammack, Richard H},
  year     = {2018},
  note     = {OCLC: 1078896723}
}

@misc{noauthor_chebyshev_2019,
  title     = {Chebyshev polynomials},
  copyright = {Creative Commons Attribution-ShareAlike License},
  url       = {
               https://en.wikipedia.org/w/index.php?title=Chebyshev_polynomials&oldid=879027427
               },
  abstract  = {In mathematics the Chebyshev polynomials, named after Pafnuty
               Chebyshev, are a sequence of orthogonal polynomials which are
               related to de Moivre's formula and which can be defined
               recursively. One usually distinguishes between Chebyshev
               polynomials of the first kind which are denoted Tn and Chebyshev
               polynomials of the second kind which are denoted Un. The letter T
               is used because of the alternative transliterations of the name
               Chebyshev as Tchebycheff, Tchebyshev (French) or Tschebyschow
               (German). The Chebyshev polynomials Tn or Un are polynomials of
               degree n and the sequence of Chebyshev polynomials of either kind
               composes a polynomial sequence. The Chebyshev polynomials Tn are
               polynomials with the largest possible leading coefficient, but
               subject to the condition that their absolute value on the interval
               [−1,1] is bounded by 1. They are also the extremal polynomials for
               many other properties.Chebyshev polynomials are important in
               approximation theory because the roots of the Chebyshev polynomials
               of the first kind, which are also called Chebyshev nodes, are used
               as nodes in polynomial interpolation. The resulting interpolation
               polynomial minimizes the problem of Runge's phenomenon and provides
               an approximation that is close to the polynomial of best
               approximation to a continuous function under the maximum norm. This
               approximation leads directly to the method of Clenshaw–Curtis
               quadrature. In the study of differential equations they arise as
               the solution to the Chebyshev differential equations ( 1 − x 2 ) y
               ″ − x y ′ + n 2 y = 0 \{{\textbackslash}displaystyle {
               \textbackslash}left(1-x{\textasciicircum}\{2\}{\textbackslash}
               right)y''-xy'+n{\textasciicircum}\{2\}y=0\} and ( 1 − x 2 ) y ″ − 3
               x y ′ + n ( n + 2 ) y = 0 \{{\textbackslash}displaystyle {
               \textbackslash}left(1-x{\textasciicircum}\{2\}{\textbackslash}
               right)y''-3xy'+n(n+2)y=0\} for the polynomials of the first and
               second kind, respectively. These equations are special cases of the
               Sturm–Liouville differential equation.},
  language  = {en},
  urldate   = {2019-02-18},
  journal   = {Wikipedia},
  month     = jan,
  year      = {2019},
  note      = {Page Version ID: 879027427},
  file      = {
               Snapshot:/Users/apodusenko/Zotero/storage/AGLRLJAJ/index.html:text/html
               }
}

@article{obermeyer_tensor_2019,
  title    = {Tensor {Variable} {Elimination} for {Plated} {Factor} {Graphs}},
  url      = {http://arxiv.org/abs/1902.03210},
  abstract = {A wide class of machine learning algorithms can be reduced to
              variable elimination on factor graphs. While factor graphs provide
              a unifying notation for these algorithms, they do not provide a
              compact way to express repeated structure when compared to plate
              diagrams for directed graphical models. To exploit efficient tensor
              algebra in graphs with plates of variables, we generalize
              undirected factor graphs to plated factor graphs and variable
              elimination to a tensor variable elimination algorithm that
              operates directly on plated factor graphs. Moreover, we generalize
              complexity bounds based on treewidth and characterize the class of
              plated factor graphs for which inference is tractable. As an
              application, we integrate tensor variable elimination into the Pyro
              probabilistic programming language to enable exact inference in
              discrete latent variable models with repeated structure. We
              validate our methods with experiments on both directed and
              undirected graphical models, including applications to polyphonic
              music modeling, animal movement modeling, and latent sentiment
              analysis.},
  urldate  = {2019-02-14},
  journal  = {arXiv:1902.03210 [cs, stat]},
  author   = {Obermeyer, Fritz and Bingham, Eli and Jankowiak, Martin and Chiu,
              Justin and Pradhan, Neeraj and Rush, Alexander and Goodman, Noah},
  month    = feb,
  year     = {2019},
  note     = {arXiv: 1902.03210},
  keywords = {Statistics - Machine Learning, Computer Science - Machine Learning
              },
  file     = {arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/46JKAVDU/1902.html:text/html;Obermeyer
              et al. - 2019 - Tensor Variable Elimination for Plated Factor
              Grap.pdf:/Users/apodusenko/Zotero/storage/CA55Q6AH/Obermeyer et al. -
              2019 - Tensor Variable Elimination for Plated Factor
              Grap.pdf:application/pdf}
}

@article{williams_performance_nodate,
  title    = {Performance {Guarantees} for {Information} {Theoretic} {Active} {
              Inference}},
  abstract = {In many estimation problems, the measurement process can be
              actively controlled to alter the information received. The control
              choices made in turn determine the performance that is possible in
              the underlying inference task. In this paper, we discuss
              performance guarantees for heuristic algorithms for adaptive
              measurement selection in sequential estimation problems, where the
              inference criterion is mutual information. We also demonstrate the
              performance of our tighter online computable performance guarantees
              through computational simulations.},
  language = {en},
  author   = {Williams, Jason L and Iii, John W Fisher and Willsky, Alan S},
  pages    = {8},
  file     = {Williams et al. - Performance Guarantees for Information Theoretic
              A.pdf:/Users/apodusenko/Zotero/storage/4WGCBBJI/Williams et al. -
              Performance Guarantees for Information Theoretic A.pdf:application/pdf}
}

@article{caticha_relative_2004,
  title    = {Relative {Entropy} and {Inductive} {Inference}},
  volume   = {707},
  issn     = {0094243X},
  url      = {http://arxiv.org/abs/physics/0311093},
  doi      = {10.1063/1.1751358},
  abstract = {We discuss how the method of maximum entropy, MaxEnt, can be
              extended beyond its original scope, as a rule to assign a
              probability distribution, to a full-fledged method for inductive
              inference. The main concept is the (relative) entropy S[p{\textbar}
              q] which is designed as a tool to update from a prior probability
              distribution q to a posterior probability distribution p when new
              information in the form of a constraint becomes available. The
              extended method goes beyond the mere selection of a single
              posterior p, but also addresses the question of how much less
              probable other distributions might be. Our approach clarifies how
              the entropy S[p{\textbar}q] is used while avoiding the question of
              its meaning. Ultimately, entropy is a tool for induction which
              needs no interpretation. Finally, being a tool for generalization
              from special examples, we ask whether the functional form of the
              entropy depends on the choice of the examples and we find that it
              does. The conclusion is that there is no single general theory of
              inductive inference and that alternative expressions for the
              entropy are possible.},
  urldate  = {2019-02-05},
  journal  = {AIP Conference Proceedings},
  author   = {Caticha, Ariel},
  year     = {2004},
  note     = {arXiv: physics/0311093},
  keywords = {Physics - General Physics, Physics - Data Analysis, Statistics and
              Probability},
  pages    = {75--96},
  file     = {arXiv\:physics/0311093
              PDF:/Users/apodusenko/Zotero/storage/3DIJMIXK/Caticha - 2004 - Relative
              Entropy and Inductive Inference.pdf:application/pdf;arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/8BLXMC3Q/0311093.html:text/html
              }
}

@article{blaiotta_variational_2016,
  title    = {Variational inference for medical image segmentation},
  volume   = {151},
  issn     = {10773142},
  url      = {https://linkinghub.elsevier.com/retrieve/pii/S1077314216300285},
  doi      = {10.1016/j.cviu.2016.04.004},
  language = {en},
  urldate  = {2019-02-03},
  journal  = {Computer Vision and Image Understanding},
  author   = {Blaiotta, Claudia and Jorge Cardoso, M. and Ashburner, John},
  month    = oct,
  year     = {2016},
  pages    = {14--28},
  file     = {Blaiotta et al. - 2016 - Variational inference for medical image
              segmentati.pdf:/Users/apodusenko/Zotero/storage/B4KQ6GKD/Blaiotta et
              al. - 2016 - Variational inference for medical image
              segmentati.pdf:application/pdf}
}

@article{wayne_unsupervised_2018,
  title    = {Unsupervised {Predictive} {Memory} in a {Goal}-{Directed} {Agent}},
  url      = {http://arxiv.org/abs/1803.10760},
  abstract = {Animals execute goal-directed behaviours despite the limited range
              and scope of their sensors. To cope, they explore environments and
              store memories maintaining estimates of important information that
              is not presently available. Recently, progress has been made with
              artificial intelligence (AI) agents that learn to perform tasks
              from sensory input, even at a human level, by merging reinforcement
              learning (RL) algorithms with deep neural networks, and the
              excitement surrounding these results has led to the pursuit of
              related ideas as explanations of non-human animal learning. However
              , we demonstrate that contemporary RL algorithms struggle to solve
              simple tasks when enough information is concealed from the sensors
              of the agent, a property called "partial observability". An obvious
              requirement for handling partially observed tasks is access to
              extensive memory, but we show memory is not enough; it is critical
              that the right information be stored in the right format. We
              develop a model, the Memory, RL, and Inference Network (MERLIN), in
              which memory formation is guided by a process of predictive
              modeling. MERLIN facilitates the solution of tasks in 3D virtual
              reality environments for which partial observability is severe and
              memories must be maintained over long durations. Our model
              demonstrates a single learning agent architecture that can solve
              canonical behavioural tasks in psychology and neurobiology without
              strong simplifying assumptions about the dimensionality of sensory
              input or the duration of experiences.},
  urldate  = {2019-02-01},
  journal  = {arXiv:1803.10760 [cs, stat]},
  author   = {Wayne, Greg and Hung, Chia-Chun and Amos, David and Mirza, Mehdi and
              Ahuja, Arun and Grabska-Barwinska, Agnieszka and Rae, Jack and
              Mirowski, Piotr and Leibo, Joel Z. and Santoro, Adam and Gemici,
              Mevlana and Reynolds, Malcolm and Harley, Tim and Abramson, Josh and
              Mohamed, Shakir and Rezende, Danilo and Saxton, David and Cain, Adam
              and Hillier, Chloe and Silver, David and Kavukcuoglu, Koray and
              Botvinick, Matt and Hassabis, Demis and Lillicrap, Timothy},
  month    = mar,
  year     = {2018},
  note     = {arXiv: 1803.10760},
  keywords = {Statistics - Machine Learning, Computer Science - Machine Learning
              },
  file     = {arXiv\:1803.10760 PDF:/Users/apodusenko/Zotero/storage/GSELMKAI/Wayne
              et al. - 2018 - Unsupervised Predictive Memory in a Goal-Directed
              .pdf:application/pdf;arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/XAPMXHGH/1803.html:text/html}
}

@article{fellows_virel:_2018,
  title      = {{VIREL}: {A} {Variational} {Inference} {Framework} for {Reinforcement
                } {Learning}},
  shorttitle = {{VIREL}},
  url        = {http://arxiv.org/abs/1811.01132},
  abstract   = {Applying probabilistic models to reinforcement learning (RL)
                enables the application of powerful optimisation tools such as
                variational inference to RL. However, existing inference frameworks
                and their algorithms pose significant challenges for learning
                optimal policies, e.g., the absence of mode capturing behaviour in
                pseudo-likelihood methods and difficulties learning deterministic
                policies in maximum entropy RL based approaches. We propose VIREL,
                a novel, theoretically grounded probabilistic inference framework
                for RL that utilises a parametrised action-value function to
                summarise future dynamics of the underlying MDP. This gives VIREL a
                mode-seeking form of KL divergence, the ability to learn
                deterministic optimal polices naturally from inference and the
                ability to optimise value functions and policies in separate,
                iterative steps. In applying variational expectation-maximisation
                to VIREL we thus show that the actor-critic algorithm can be
                reduced to expectation-maximisation, with policy improvement
                equivalent to an E-step and policy evaluation to an M-step. We then
                derive a family of actor-critic methods from VIREL, including a
                scheme for adaptive exploration. Finally, we demonstrate that
                actor-critic algorithms from this family outperform
                state-of-the-art methods based on soft value functions in several
                domains.},
  urldate    = {2019-02-01},
  journal    = {arXiv:1811.01132 [cs, stat]},
  author     = {Fellows, Matthew and Mahajan, Anuj and Rudner, Tim G. J. and
                Whiteson, Shimon},
  month      = nov,
  year       = {2018},
  note       = {arXiv: 1811.01132},
  keywords   = {Statistics - Machine Learning, Computer Science - Machine Learning
                },
  file       = {arXiv\:1811.01132
                PDF:/Users/apodusenko/Zotero/storage/UHIUF5PW/Fellows et al. - 2018 -
                VIREL A Variational Inference Framework for
                Reinf.pdf:application/pdf;arXiv.org
                Snapshot:/Users/apodusenko/Zotero/storage/AG2IDTZY/1811.html:text/html}
}

@inproceedings{ghavamzadeh_bayesian_2007,
  address   = {Corvalis, Oregon},
  title     = {Bayesian actor-critic algorithms},
  isbn      = {978-1-59593-793-3},
  url       = {http://portal.acm.org/citation.cfm?doid=1273496.1273534},
  doi       = {10.1145/1273496.1273534},
  abstract  = {We1 present a new actor-critic learning model in which a Bayesian
               class of non-parametric critics, using Gaussian process temporal
               difference learning is used. Such critics model the state-action
               value function as a Gaussian process, allowing Bayes’ rule to be
               used in computing the posterior distribution over state-action
               value functions, conditioned on the observed data. Appropriate
               choices of the prior covariance (kernel) between stateaction values
               and of the parametrization of the policy allow us to obtain
               closed-form expressions for the posterior distribution of the
               gradient of the average discounted return with respect to the
               policy parameters. The posterior mean, which serves as our estimate
               of the policy gradient, is used to update the policy, while the
               posterior covariance allows us to gauge the reliability of the
               update.},
  language  = {en},
  urldate   = {2019-01-30},
  booktitle = {Proceedings of the 24th international conference on {Machine}
               learning - {ICML} '07},
  publisher = {ACM Press},
  author    = {Ghavamzadeh, Mohammad and Engel, Yaakov},
  year      = {2007},
  pages     = {297--304},
  file      = {Ghavamzadeh and Engel - 2007 - Bayesian actor-critic
               algorithms.pdf:/Users/apodusenko/Zotero/storage/6CNSR4U8/Ghavamzadeh
               and Engel - 2007 - Bayesian actor-critic algorithms.pdf:application/pdf
               }
}

@inproceedings{vontobel_factor-graph_2011,
  title     = {A factor-graph approach to {Lagrangian} and {Hamiltonian} dynamics},
  doi       = {10.1109/ISIT.2011.6033945},
  abstract  = {Factor graphs are graphical models with origins in coding theory.
               The sum-product, the max-product, and the min-sum algorithms, which
               operate by message passing on a factor graph, subsume a great
               variety of algorithms in coding, signal processing, and artificial
               intelligence. This paper aims at extending the field of possible
               applications of factor graphs to Lagrangian and Hamiltonian
               dynamics. The starting point is the principle of least action (more
               precisely, the principle of stationary action). The resulting
               factor graphs require a new message-passing algorithm that we call
               the stationary-sum algorithm. As it turns out, some of the
               properties of this algorithm are equivalent to Liouville's theorem.
               Moreover, duality results for factor graphs allow to easily derive
               Noether's theorem. We also discuss connections and differences to
               Kalman filtering.},
  booktitle = {2011 {IEEE} {International} {Symposium} on {Information} {Theory}
               {Proceedings}},
  author    = {Vontobel, P. O.},
  month     = jul,
  year      = {2011},
  keywords  = {Kalman filters, graph theory, message passing, message passing
               algorithm, Equations, Mathematical model, Kalman filtering, signal
               processing, coding theory, factor graph approach, Hamiltonian
               dynamics, Heuristic algorithms, Lagrangian dynamics, Lagrangian
               functions, least action principle, Liouville theorem, stationary
               action principle, stationary sum algorithm, Trajectory, Transforms},
  pages     = {2183--2187},
  file      = {IEEE Xplore Abstract
               Record:/Users/apodusenko/Zotero/storage/98Z6QZTR/6033945.html:text/html;IEEE
               Xplore Full Text PDF:/Users/apodusenko/Zotero/storage/QM9A4A48/Vontobel
               - 2011 - A factor-graph approach to Lagrangian and
               Hamilton.pdf:application/pdf}
}

@incollection{ghavamzadeh_bayesian_2007-1,
  title     = {Bayesian {Policy} {Gradient} {Algorithms}},
  url       = {
               http://papers.nips.cc/paper/2993-bayesian-policy-gradient-algorithms.pdf
               },
  urldate   = {2019-01-29},
  booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 19},
  publisher = {MIT Press},
  author    = {Ghavamzadeh, Mohammad and Engel, Yaakov},
  editor    = {Schölkopf, B. and Platt, J. C. and Hoffman, T.},
  year      = {2007},
  pages     = {457--464},
  file      = {NIPS Full Text
               PDF:/Users/apodusenko/Zotero/storage/DKCC4KPY/Ghavamzadeh and Engel -
               2007 - Bayesian Policy Gradient Algorithms.pdf:application/pdf;NIPS
               Snapshot:/Users/apodusenko/Zotero/storage/7LH4SIEV/2993-bayesian-policy-gradient-algorithms.html:text/html
               }
}

@article{ghavamzadeh_bayesian_nodate,
  title    = {Bayesian {Policy} {Gradient} and {Actor}-{Critic} {Algorithms}},
  abstract = {Policy gradient methods are reinforcement learning algorithms that
              adapt a parameterized policy by following a performance gradient
              estimate. Many conventional policy gradient methods use Monte-Carlo
              techniques to estimate this gradient. The policy is improved by
              adjusting the parameters in the direction of the gradient estimate.
              Since Monte-Carlo methods tend to have high variance, a large
              number of samples is required to attain accurate estimates,
              resulting in slow convergence. In this paper, we ﬁrst propose a
              Bayesian framework for policy gradient, based on modeling the
              policy gradient as a Gaussian process. This reduces the number of
              samples needed to obtain accurate gradient estimates. Moreover,
              estimates of the natural gradient as well as a measure of the
              uncertainty in the gradient estimates, namely, the gradient
              covariance, are provided at little extra cost. Since the proposed
              Bayesian framework considers system trajectories as its basic
              observable unit, it does not require the dynamics within
              trajectories to be of any particular form, and thus, can be easily
              extended to partially observable problems. On the downside, it
              cannot take advantage of the Markov property when the system is
              Markovian.},
  language = {en},
  author   = {Ghavamzadeh, Mohammad and Engel, Yaakov and Valko, Michal},
  pages    = {53},
  file     = {Ghavamzadeh et al. - Bayesian Policy Gradient and Actor-Critic
              Algorith.pdf:/Users/apodusenko/Zotero/storage/AXZUWKE6/Ghavamzadeh et
              al. - Bayesian Policy Gradient and Actor-Critic
              Algorith.pdf:application/pdf}
}

@inproceedings{van_den_broek_risk_2010,
  address   = {Arlington, Virginia, United States},
  series    = {{UAI}'10},
  title     = {Risk {Sensitive} {Path} {Integral} {Control}},
  isbn      = {978-0-9749039-6-5},
  url       = {http://dl.acm.org/citation.cfm?id=3023549.3023622},
  abstract  = {Recently path integral methods have been developed for stochastic
               optimal control for a wide class of models with non-linear dynamics
               in continuous space-time. Path integral methods find the control
               that minimizes the expected cost-to-go. In this paper we show that
               under the same assumptions, path integral methods generalize
               directly to risk sensitive stochastic optimal control. Here the
               method minimizes in expectation an exponentially weighted
               cost-to-go. Depending on the exponential weight, risk seeking or
               risk averse behaviour is obtained. We demonstrate the approach on
               risk sensitive stochastic optimal control problems beyond the
               linear-quadratic case, showing the intricate interaction of
               multi-modal control with risk sensitivity.},
  urldate   = {2019-01-25},
  booktitle = {Proceedings of the {Twenty}-{Sixth} {Conference} on {Uncertainty}
               in {Artificial} {Intelligence}},
  publisher = {AUAI Press},
  author    = {van den Broek, Bart and Wiegerinck, Wim and Kappen, Bert},
  year      = {2010},
  pages     = {615--622}
}

@inproceedings{wadehn_new_2016,
  address   = {Monticello, IL, USA},
  title     = {New square-root and diagonalized {Kalman} smoothers},
  isbn      = {978-1-5090-4550-1},
  url       = {http://ieeexplore.ieee.org/document/7852382/},
  doi       = {10.1109/ALLERTON.2016.7852382},
  abstract  = {Standard implementations of Kalman ﬁlters and smoothers often
               suffer from numerical instability issues, due to round-off errors,
               even for moderate-sized state space models. Recently, two
               inversion-free and computationally efﬁcient Kalman smoothers, an
               adapted version of the Modiﬁed-Bryson Frasier smoother (MBF),
               mainly tailored to input estimation and the Backward Information
               Filter Forward Marginal (BIFM) smoother for state estimation and
               output interpolation were presented. In this paper, we will ﬁrst
               suggest improvements to both the MBF and BIFM smoother
               implementations aimed at improving computational efﬁciency and,
               using this improved version of the BIFM smoother, we will elaborate
               on its usage in sensor networks with spatially correlated noise.
               The main novelty in this paper is the square-root version of the
               BIFM smoother, which can be used in numerically critical smoothing
               problems, as exempliﬁed in a force estimation problem using a
               multi-mass resonator model of an industrial milling machine.},
  language  = {en},
  urldate   = {2019-01-23},
  booktitle = {2016 54th {Annual} {Allerton} {Conference} on {Communication}, {
               Control}, and {Computing} ({Allerton})},
  publisher = {IEEE},
  author    = {Wadehn, Federico and Bruderer, Lukas and Sahdeva, Vijay and Loeliger
               , Hans-Andrea},
  month     = sep,
  year      = {2016},
  pages     = {1282--1290},
  file      = {Wadehn e.a. - 2016 - New square-root and diagonalized Kalman
               smoothers.pdf:/Users/apodusenko/Zotero/storage/XXQ3RV52/Wadehn e.a. -
               2016 - New square-root and diagonalized Kalman
               smoothers.pdf:application/pdf}
}

@incollection{blahut_least_2002,
  address   = {Boston, MA},
  title     = {Least {Squares} and {Kalman} {Filtering} on {Forney} {Graphs}},
  volume    = {670},
  isbn      = {978-1-4613-5292-1 978-1-4615-0895-3},
  url       = {http://link.springer.com/10.1007/978-1-4615-0895-3_7},
  abstract  = {General versions of Kalman ﬁltering and recursive least-squares
               algorithms are derived as instances of the sum(mary)-product
               algorithm on Forney-style factor graphs.},
  language  = {en},
  urldate   = {2019-01-14},
  booktitle = {Codes, {Graphs}, and {Systems}},
  publisher = {Springer US},
  author    = {Loeliger, H.-A.},
  editor    = {Blahut, Richard E. and Koetter, Ralf},
  year      = {2002},
  doi       = {10.1007/978-1-4615-0895-3_7},
  pages     = {113--135}
}

@article{marsh_introduction_nodate,
  title    = {Introduction to {Continuous} {Entropy}},
  abstract = {Classically, Shannon entropy was formalized over discrete
              probability distributions. However, the concept of entropy can be
              extended to continuous distributions through a quantity known as
              continuous (or differential ) entropy. The most common deﬁnition
              for continuous entropy is seemingly straightforward; however,
              further analysis reveals a number of shortcomings that render it
              far less useful than it appears. Instead, relative entropy (or KL
              divergence) proves to be the key to information theory in the
              continuous case, as the notion of comparing entropy across
              probability distributions retains value. Expanding oﬀ this notion,
              we present several results in the ﬁeld of maximum entropy and, in
              particular, conclude with an information-theoretic proof of the
              Central Limit Theorem using continuous relative entropy.},
  language = {en},
  author   = {Marsh, Charles},
  pages    = {17},
  file     = {Marsh - Introduction to Continuous
              Entropy.pdf:/Users/apodusenko/Zotero/storage/6X9HWYWU/Marsh -
              Introduction to Continuous Entropy.pdf:application/pdf}
}

@article{koelsch_predictive_2019,
  title    = {Predictive {Processes} and the {Peculiar} {Case} of {Music}},
  volume   = {23},
  issn     = {1364-6613, 1879-307X},
  url      = {
              https://www.cell.com/trends/cognitive-sciences/abstract/S1364-6613(18)30254-7
              },
  doi      = {10.1016/j.tics.2018.10.006},
  language = {English},
  number   = {1},
  urldate  = {2019-01-10},
  journal  = {Trends in Cognitive Sciences},
  author   = {Koelsch, Stefan and Vuust, Peter and Friston, Karl},
  month    = jan,
  year     = {2019},
  pmid     = {30471869},
  keywords = {predictive coding, active inference, auditory processing,
              embodiment, ERAN, MMN, music perception},
  pages    = {63--77},
  file     = {Koelsch et al. - 2019 - Predictive Processes and the Peculiar Case of
              Musi.pdf:/Users/apodusenko/Zotero/storage/WKIZ8NSR/Koelsch et al. -
              2019 - Predictive Processes and the Peculiar Case of
              Musi.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/YN5XXL4E/S1364-6613(18)30254-7.html:text/html
              }
}

@article{schulman_gradient_nodate,
  title    = {Gradient {Estimation} {Using} {Stochastic} {Computation} {Graphs}},
  abstract = {In a variety of problems originating in supervised, unsupervised,
              and reinforcement learning, the loss function is deﬁned by an
              expectation over a collection of random variables, which might be
              part of a probabilistic model or the external world. Estimating the
              gradient of this loss function, using samples, lies at the core of
              gradient-based learning algorithms for these problems. We introduce
              the formalism of stochastic computation graphs—directed acyclic
              graphs that include both deterministic functions and conditional
              probability distributions—and describe how to easily and
              automatically derive an unbiased estimator of the loss function’s
              gradient. The resulting algorithm for computing the gradient
              estimator is a simple modiﬁcation of the standard backpropagation
              algorithm. The generic scheme we propose uniﬁes estimators derived
              in variety of prior work, along with variance-reduction techniques
              therein. It could assist researchers in developing intricate models
              involving a combination of stochastic and deterministic operations,
              enabling, for example, attention, memory, and control actions.},
  language = {en},
  author   = {Schulman, John and Heess, Nicolas and Weber, Theophane and Abbeel,
              Pieter},
  pages    = {9},
  file     = {Schulman et al. - Gradient Estimation Using Stochastic Computation
              G.pdf:/Users/apodusenko/Zotero/storage/KFXCRBKQ/Schulman et al. -
              Gradient Estimation Using Stochastic Computation G.pdf:application/pdf}
}

@inproceedings{li_variance_2018,
  address   = {Stockholm, Sweden},
  title     = {Variance {Reduction} in {Black}-box {Variational} {Inference} by {
               Adaptive} {Importance} {Sampling}},
  isbn      = {978-0-9992411-2-7},
  url       = {https://www.ijcai.org/proceedings/2018/333},
  doi       = {10.24963/ijcai.2018/333},
  abstract  = {Overdispersed black-box variational inference employs importance
               sampling to reduce the variance of the Monte Carlo gradient in
               black-box variational inference. A simple overdispersed proposal
               distribution is used. This paper aims to investigate how to
               adaptively obtain better proposal distribution for lower variance.
               To this end, we directly approximate the optimal proposal in theory
               using a Monte Carlo moment matching step at each variational
               iteration. We call this adaptive proposal moment matching proposal
               (MMP). Experimental results on two Bayesian models show that the
               MMP can effectively reduce variance in black-box learning, and
               perform better than baseline inference algorithms.},
  language  = {en},
  urldate   = {2019-01-08},
  booktitle = {Proceedings of the {Twenty}-{Seventh} {International} {Joint} {
               Conference} on {Artificial} {Intelligence}},
  publisher = {International Joint Conferences on Artificial Intelligence
               Organization},
  author    = {Li, Ximing and Li, Changchun and Chi, Jinjin and Ouyang, Jihong},
  month     = jul,
  year      = {2018},
  pages     = {2404--2410},
  file      = {Li et al. - 2018 - Variance Reduction in Black-box Variational
               Infere.pdf:/Users/apodusenko/Zotero/storage/F48GB29W/Li et al. - 2018 -
               Variance Reduction in Black-box Variational Infere.pdf:application/pdf}
}

@article{ahn_bayesian_2012,
  title    = {Bayesian {Posterior} {Sampling} via {Stochastic} {Gradient} {Fisher}
              {Scoring}},
  url      = {http://arxiv.org/abs/1206.6380},
  abstract = {In this paper we address the following question: Can we
              approximately sample from a Bayesian posterior distribution if we
              are only allowed to touch a small mini-batch of data-items for
              every sample we generate?. An algorithm based on the Langevin
              equation with stochastic gradients (SGLD) was previously proposed
              to solve this, but its mixing rate was slow. By leveraging the
              Bayesian Central Limit Theorem, we extend the SGLD algorithm so
              that at high mixing rates it will sample from a normal
              approximation of the posterior, while for slow mixing rates it will
              mimic the behavior of SGLD with a pre-conditioner matrix. As a
              bonus, the proposed algorithm is reminiscent of Fisher scoring
              (with stochastic gradients) and as such an efficient optimizer
              during burn-in.},
  urldate  = {2019-01-08},
  journal  = {arXiv:1206.6380 [cs, stat]},
  author   = {Ahn, Sungjin and Korattikara, Anoop and Welling, Max},
  month    = jun,
  year     = {2012},
  note     = {arXiv: 1206.6380},
  keywords = {Statistics - Machine Learning, Statistics - Computation, Computer
              Science - Machine Learning},
  file     = {arXiv\:1206.6380 PDF:/Users/apodusenko/Zotero/storage/D5JBWDZJ/Ahn et
              al. - 2012 - Bayesian Posterior Sampling via Stochastic
              Gradien.pdf:application/pdf;arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/K2NXD3DL/1206.html:text/html}
}

@article{korattikara_bayesian_2015,
  title    = {Bayesian {Dark} {Knowledge}},
  url      = {http://arxiv.org/abs/1506.04416},
  abstract = {We consider the problem of Bayesian parameter estimation for deep
              neural networks, which is important in problem settings where we
              may have little data, and/ or where we need accurate posterior
              predictive densities, e.g., for applications involving bandits or
              active learning. One simple approach to this is to use online Monte
              Carlo methods, such as SGLD (stochastic gradient Langevin
              dynamics). Unfortunately, such a method needs to store many copies
              of the parameters (which wastes memory), and needs to make
              predictions using many versions of the model (which wastes time).
              We describe a method for "distilling" a Monte Carlo approximation
              to the posterior predictive density into a more compact form,
              namely a single deep neural network. We compare to two very recent
              approaches to Bayesian neural networks, namely an approach based on
              expectation propagation [Hernandez-Lobato and Adams, 2015] and an
              approach based on variational Bayes [Blundell et al., 2015]. Our
              method performs better than both of these, is much simpler to
              implement, and uses less computation at test time.},
  urldate  = {2019-01-08},
  journal  = {arXiv:1506.04416 [cs, stat]},
  author   = {Korattikara, Anoop and Rathod, Vivek and Murphy, Kevin and Welling,
              Max},
  month    = jun,
  year     = {2015},
  note     = {arXiv: 1506.04416},
  keywords = {Statistics - Machine Learning, Computer Science - Machine Learning
              },
  file     = {arXiv\:1506.04416
              PDF:/Users/apodusenko/Zotero/storage/9DABEU9N/Korattikara et al. - 2015
              - Bayesian Dark Knowledge.pdf:application/pdf;arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/DXN9MNSB/1506.html:text/html}
}

@article{salimans_markov_nodate,
  title    = {Markov {Chain} {Monte} {Carlo} and {Variational} {Inference}:{
              Bridging} the {Gap}},
  abstract = {Recent advances in stochastic gradient variational inference have
              made it possible to perform variational Bayesian inference with
              posterior approximations containing auxiliary random variables.
              This enables us to explore a new synthesis of variational inference
              and Monte Carlo methods where we incorporate one or more steps of
              MCMC into our variational approximation. By doing so we obtain a
              rich class of inference algorithms bridging the gap between
              variational methods and MCMC, and offering the best of both worlds:
              fast posterior approximation through the maximization of an
              explicit objective, with the option of trading off additional
              computation for additional accuracy. We describe the theoretical
              foundations that make this possible and show some promising ﬁrst
              results.},
  language = {en},
  journal  = {Bridging the Gap},
  author   = {Salimans, Tim and Kingma, Diederik P and Welling, Max},
  pages    = {9},
  file     = {Salimans et al. - Markov Chain Monte Carlo and Variational
              Inference.pdf:/Users/apodusenko/Zotero/storage/5L2I7QSF/Salimans et al.
              - Markov Chain Monte Carlo and Variational
              Inference.pdf:application/pdf}
}

@article{angelino_patterns_2016,
  title    = {Patterns of {Scalable} {Bayesian} {Inference}},
  url      = {http://arxiv.org/abs/1602.05221},
  abstract = {Datasets are growing not just in size but in complexity, creating
              a demand for rich models and quantification of uncertainty.
              Bayesian methods are an excellent fit for this demand, but scaling
              Bayesian inference is a challenge. In response to this challenge,
              there has been considerable recent work based on varying
              assumptions about model structure, underlying computational
              resources, and the importance of asymptotic correctness. As a
              result, there is a zoo of ideas with few clear overarching
              principles. In this paper, we seek to identify unifying principles,
              patterns, and intuitions for scaling Bayesian inference. We review
              existing work on utilizing modern computing resources with both
              MCMC and variational approximation techniques. From this taxonomy
              of ideas, we characterize the general principles that have proven
              successful for designing scalable inference procedures and comment
              on the path forward.},
  urldate  = {2019-01-08},
  journal  = {arXiv:1602.05221 [stat]},
  author   = {Angelino, Elaine and Johnson, Matthew James and Adams, Ryan P.},
  month    = feb,
  year     = {2016},
  note     = {arXiv: 1602.05221},
  keywords = {Statistics - Machine Learning},
  file     = {arXiv\:1602.05221
              PDF:/Users/apodusenko/Zotero/storage/BQ9KWSF9/Angelino et al. - 2016 -
              Patterns of Scalable Bayesian Inference.pdf:application/pdf;arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/24PCJ3AU/1602.html:text/html}
}

@article{grover_variational_2014,
  title    = {Variational {Rejection} {Sampling}},
  abstract = {Learning latent variable models with stochastic variational
              inference is challenging when the approximate posterior is far from
              the true posterior, due to high variance in the gradient estimates.
              We propose a novel rejection sampling step that discards samples
              from the variational posterior which are assigned low likelihoods
              by the model. Our approach provides an arbitrarily accurate
              approximation of the true posterior at the expense of extra
              computation. Using a new gradient estimator for the resulting
              unnormalized proposal distribution, we achieve average improvements
              of 3.71 nats and 0.21 nats over state-of-theart single-sample and
              multi-sample alternatives respectively for estimating marginal
              loglikelihoods using sigmoid belief networks on the MNIST dataset.},
  language = {en},
  author   = {Grover, Aditya and Gummadi, Ramki and Lazaro-Gredilla, Miguel and
              Schuurmans, Dale and Ermon, Stefano},
  year     = {2014},
  pages    = {10},
  file     = {Grover et al. - 2014 - Variational Rejection
              Sampling.pdf:/Users/apodusenko/Zotero/storage/PIP9VT53/Grover et al. -
              2014 - Variational Rejection Sampling.pdf:application/pdf}
}

@article{welling_bayesian_nodate,
  title    = {Bayesian {Learning} via {Stochastic} {Gradient} {Langevin} {Dynamics}
              },
  abstract = {In this paper we propose a new framework for learning from large
              scale datasets based on iterative learning from small mini-batches.
              By adding the right amount of noise to a standard stochastic
              gradient optimization algorithm we show that the iterates will
              converge to samples from the true posterior distribution as we
              anneal the stepsize. This seamless transition between optimization
              and Bayesian posterior sampling provides an inbuilt protection
              against overﬁtting. We also propose a practical method for Monte
              Carlo estimates of posterior statistics which monitors a “sampling
              threshold” and collects samples after it has been surpassed. We
              apply the method to three models: a mixture of Gaussians, logistic
              regression and ICA with natural gradients.},
  language = {en},
  author   = {Welling, Max and Teh, Yee Whye},
  pages    = {8},
  file     = {Welling and Teh - Bayesian Learning via Stochastic Gradient
              Langevin.pdf:/Users/apodusenko/Zotero/storage/P39VKENY/Welling and Teh
              - Bayesian Learning via Stochastic Gradient
              Langevin.pdf:application/pdf}
}

@inproceedings{pathak_curiosity-driven_2017,
  address   = {Honolulu, HI, USA},
  title     = {Curiosity-{Driven} {Exploration} by {Self}-{Supervised} {Prediction}},
  isbn      = {978-1-5386-0733-6},
  url       = {http://ieeexplore.ieee.org/document/8014804/},
  doi       = {10.1109/CVPRW.2017.70},
  abstract  = {In many real-world scenarios, rewards extrinsic to the agent are
               extremely sparse, or absent altogether. In such cases, curiosity
               can serve as an intrinsic reward signal to enable the agent to
               explore its environment and learn skills that might be useful later
               in its life. We formulate curiosity as the error in an agent’s
               ability to predict the consequence of its own actions in a visual
               feature space learned by a self-supervised inverse dynamics model.
               Our formulation scales to high-dimensional continuous state spaces
               like images, bypasses the difﬁculties of directly predicting pixels
               , and, critically, ignores the aspects of the environment that
               cannot affect the agent. The proposed approach is evaluated in two
               environments: VizDoom and Super Mario Bros. Three broad settings
               are investigated: 1) sparse extrinsic reward, where curiosity
               allows for far fewer interactions with the environment to reach the
               goal; 2) exploration with no extrinsic reward, where curiosity
               pushes the agent to explore more efﬁciently; and 3) generalization
               to unseen scenarios (e.g. new levels of the same game) where the
               knowledge gained from earlier experience helps the agent explore
               new places much faster than starting from scratch.},
  language  = {en},
  urldate   = {2019-01-03},
  booktitle = {2017 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {
               Recognition} {Workshops} ({CVPRW})},
  publisher = {IEEE},
  author    = {Pathak, Deepak and Agrawal, Pulkit and Efros, Alexei A. and Darrell,
               Trevor},
  month     = jul,
  year      = {2017},
  pages     = {488--489},
  file      = {Pathak et al. - 2017 - Curiosity-Driven Exploration by Self-Supervised
               Pr.pdf:/Users/apodusenko/Zotero/storage/QZKVZ92J/Pathak et al. - 2017 -
               Curiosity-Driven Exploration by Self-Supervised Pr.pdf:application/pdf}
}

@article{van_der_heijden_learning_2014,
  title    = {Learning {Bayesian} networks for clinical time series analysis},
  volume   = {48},
  issn     = {15320464},
  url      = {https://linkinghub.elsevier.com/retrieve/pii/S1532046413001986},
  doi      = {10.1016/j.jbi.2013.12.007},
  abstract = {Introduction: Autonomous chronic disease management requires
              models that are able to interpret time series data from patients.
              However, construction of such models by means of machine learning
              requires the availability of costly health-care data, often
              resulting in small samples. We analysed data from chronic
              obstructive pulmonary disease (COPD) patients with the goal of
              constructing a model to predict the occurrence of exacerbation
              events, i.e., episodes of decreased pulmonary health status.},
  language = {en},
  urldate  = {2019-01-03},
  journal  = {Journal of Biomedical Informatics},
  author   = {van der Heijden, Maarten and Velikova, Marina and Lucas, Peter J.F.},
  month    = apr,
  year     = {2014},
  pages    = {94--105},
  file     = {van der Heijden et al. - 2014 - Learning Bayesian networks for
              clinical time serie.pdf:/Users/apodusenko/Zotero/storage/ZCTNWLFU/van
              der Heijden et al. - 2014 - Learning Bayesian networks for clinical
              time serie.pdf:application/pdf}
}

@article{kuremoto_predicting_nodate,
  title    = {Predicting {Chaotic} {Time} {Series} by {Reinforcement} {Learning}},
  abstract = {Although a large number of researches have been carried out into
              the analysis of nonlinear phenomena, little is reported about using
              reinforcement learning, which is widely used in artiﬁcial
              intelligent, intelligent control, and other ﬁelds. Here, we
              consider the problem of chaotic time series using a self-organized
              fuzzy neural network and reinforcement learning, in particular, a
              learning algorithm called Stochastic Gradient Ascent(SGA). The
              proposed fuzzy neural network is similar to a radial basis function
              network(RBFN), but has self-organization ability dealing with its
              dynamical inputs,and provides stochastic outputs. The outputs are
              values of predicted time series, which called actions in
              reinforcement learning. After feeding some training data of chaotic
              time series to the initial frame of system, the structure and
              synaptic weights will be organized, and the predictor begins to
              provide correct dynamics of time series. Applying our proposed
              method to the Lorenz system, we obtained a high accuracy estimation
              of short-term prediction, and a reasonable result of long-term
              prediction.},
  language = {en},
  author   = {Kuremoto, T and Obayashi, M and Yamamoto, A and Kobayashi, K},
  pages    = {7},
  file     = {Kuremoto et al. - Predicting Chaotic Time Series by Reinforcement
              Le.pdf:/Users/apodusenko/Zotero/storage/LNM9TL49/Kuremoto et al. -
              Predicting Chaotic Time Series by Reinforcement Le.pdf:application/pdf}
}

@article{pedersen_drift_2017,
  title    = {The drift diffusion model as the choice rule in reinforcement
              learning},
  volume   = {24},
  issn     = {1531-5320},
  url      = {https://doi.org/10.3758/s13423-016-1199-y},
  doi      = {10.3758/s13423-016-1199-y},
  abstract = {Current reinforcement-learning models often assume simplified
              decision processes that do not fully reflect the dynamic
              complexities of choice processes. Conversely, sequential-sampling
              models of decision making account for both choice accuracy and
              response time, but assume that decisions are based on static
              decision values. To combine these two computational models of
              decision making and learning, we implemented reinforcement-learning
              models in which the drift diffusion model describes the choice
              process, thereby capturing both within- and across-trial dynamics.
              To exemplify the utility of this approach, we quantitatively fit
              data from a common reinforcement-learning paradigm using
              hierarchical Bayesian parameter estimation, and compared model
              variants to determine whether they could capture the effects of
              stimulant medication in adult patients with attention-deficit
              hyperactivity disorder (ADHD). The model with the best relative fit
              provided a good description of the learning process, choices, and
              response times. A parameter recovery experiment showed that the
              hierarchical Bayesian modeling approach enabled accurate estimation
              of the model parameters. The model approach described here, using
              simultaneous estimation of reinforcement-learning and drift
              diffusion model parameters, shows promise for revealing new
              insights into the cognitive and neural mechanisms of learning and
              decision making, as well as the alteration of such processes in
              clinical groups.},
  language = {en},
  number   = {4},
  urldate  = {2019-01-02},
  journal  = {Psychonomic Bulletin \& Review},
  author   = {Pedersen, Mads Lund and Frank, Michael J. and Biele, Guido},
  month    = aug,
  year     = {2017},
  keywords = {Bayesian modeling, Reinforcement learning, Decision making,
              Mathematical models},
  pages    = {1234--1251},
  file     = {Springer Full Text
              PDF:/Users/apodusenko/Zotero/storage/I3HRDLHT/Pedersen et al. - 2017 -
              The drift diffusion model as the choice rule in re.pdf:application/pdf}
}

@article{ishiguro_averaged_nodate,
  title    = {Averaged {Collapsed} {Variational} {Bayes} {Inference}},
  abstract = {This paper presents the Averaged CVB (ACVB) inference and oﬀers
              convergence-guaranteed and practically useful fast Collapsed
              Variational Bayes (CVB) inferences. CVB inferences yield more
              precise inferences of Bayesian probabilistic models than
              Variational Bayes (VB) inferences. However, their convergence
              aspect is fairly unknown and has not been scrutinized. To make CVB
              more useful, we study their convergence behaviors in a empirical
              and practical approach. We develop a convergence-guaranteed
              algorithm for any CVB-based inference called ACVB, which enables
              automatic convergence detection and frees non-expert practitioners
              from the diﬃcult and costly manual monitoring of inference
              processes. In experiments, ACVB inferences are comparable to or
              better than those of existing inference methods and deterministic,
              fast, and provide easier convergence detection. These features are
              especially convenient for practitioners who want precise Bayesian
              inference with assured convergence.},
  language = {en},
  author   = {Ishiguro, Katsuhiko and Sato, Issei and Ueda, Naonori},
  pages    = {29},
  file     = {Ishiguro et al. - Averaged Collapsed Variational Bayes
              Inference.pdf:/Users/apodusenko/Zotero/storage/5DJY88GE/Ishiguro et al.
              - Averaged Collapsed Variational Bayes Inference.pdf:application/pdf}
}

@article{vaswani_attention_2017,
  title    = {Attention {Is} {All} {You} {Need}},
  url      = {http://arxiv.org/abs/1706.03762},
  abstract = {The dominant sequence transduction models are based on complex
              recurrent or convolutional neural networks in an encoder-decoder
              configuration. The best performing models also connect the encoder
              and decoder through an attention mechanism. We propose a new simple
              network architecture, the Transformer, based solely on attention
              mechanisms, dispensing with recurrence and convolutions entirely.
              Experiments on two machine translation tasks show these models to
              be superior in quality while being more parallelizable and
              requiring significantly less time to train. Our model achieves 28.4
              BLEU on the WMT 2014 English-to-German translation task, improving
              over the existing best results, including ensembles by over 2 BLEU.
              On the WMT 2014 English-to-French translation task, our model
              establishes a new single-model state-of-the-art BLEU score of 41.8
              after training for 3.5 days on eight GPUs, a small fraction of the
              training costs of the best models from the literature. We show that
              the Transformer generalizes well to other tasks by applying it
              successfully to English constituency parsing both with large and
              limited training data.},
  urldate  = {2018-12-21},
  journal  = {arXiv:1706.03762 [cs]},
  author   = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit,
              Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and
              Polosukhin, Illia},
  month    = jun,
  year     = {2017},
  note     = {arXiv: 1706.03762},
  keywords = {Computer Science - Computation and Language, Computer Science -
              Machine Learning},
  file     = {arXiv\:1706.03762
              PDF:/Users/apodusenko/Zotero/storage/2CGCY6N7/Vaswani et al. - 2017 -
              Attention Is All You Need.pdf:application/pdf;arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/V8A6A8HM/1706.html:text/html}
}

@article{hudson_compositional_2018,
  title    = {Compositional {Attention} {Networks} for {Machine} {Reasoning}},
  url      = {http://arxiv.org/abs/1803.03067},
  abstract = {We present the MAC network, a novel fully differentiable neural
              network architecture, designed to facilitate explicit and
              expressive reasoning. MAC moves away from monolithic black-box
              neural architectures towards a design that encourages both
              transparency and versatility. The model approaches problems by
              decomposing them into a series of attention-based reasoning steps,
              each performed by a novel recurrent Memory, Attention, and
              Composition (MAC) cell that maintains a separation between control
              and memory. By stringing the cells together and imposing structural
              constraints that regulate their interaction, MAC effectively learns
              to perform iterative reasoning processes that are directly inferred
              from the data in an end-to-end approach. We demonstrate the model's
              strength, robustness and interpretability on the challenging CLEVR
              dataset for visual reasoning, achieving a new state-of-the-art 98.9
              \% accuracy, halving the error rate of the previous best model.
              More importantly, we show that the model is
              computationally-efficient and data-efficient, in particular
              requiring 5x less data than existing models to achieve strong
              results.},
  urldate  = {2018-12-21},
  journal  = {arXiv:1803.03067 [cs]},
  author   = {Hudson, Drew A. and Manning, Christopher D.},
  month    = mar,
  year     = {2018},
  note     = {arXiv: 1803.03067},
  keywords = {Computer Science - Artificial Intelligence},
  file     = {arXiv\:1803.03067 PDF:/Users/apodusenko/Zotero/storage/K97UNEC7/Hudson
              and Manning - 2018 - Compositional Attention Networks for Machine
              Reaso.pdf:application/pdf;arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/FUFJW4GR/1803.html:text/html}
}

@article{burda_large-scale_nodate,
  title    = {Large-{Scale} {Study} of {Curiosity}-{Driven} {Learning}},
  abstract = {Reinforcement learning algorithms rely on carefully engineering
              environment rewards that are extrinsic to the agent. However,
              annotating each environment with hand-designed, dense rewards is
              not scalable, motivating the need for developing reward functions
              that are intrinsic to the agent. Curiosity is a type of intrinsic
              reward function which uses prediction error as reward signal. In
              this paper: (a) We perform the ﬁrst large-scale study of purely
              curiosity-driven learning, i.e. without any extrinsic rewards,
              across 54 standard benchmark environments, including the Atari game
              suite. Our results show surprisingly good performance, and a high
              degree of alignment between the intrinsic curiosity objective and
              the handdesigned extrinsic rewards of many game environments. (b)
              We investigate the effect of using different feature spaces for
              computing prediction error and show that random features are
              sufﬁcient for many popular RL game benchmarks, but learned features
              appear to generalize better (e.g. to novel game levels in Super
              Mario Bros.). (c) We demonstrate limitations of the
              prediction-based rewards in stochastic setups. Game-play videos and
              code are at https://pathak22.github. io/large-scale-curiosity/.},
  language = {en},
  author   = {Burda, Yuri and Edwards, Harri and Pathak, Deepak and Storkey, Amos
              and Darrell, Trevor and Efros, Alexei A},
  pages    = {15},
  file     = {Burda et al. - Large-Scale Study of Curiosity-Driven
              Learning.pdf:/Users/apodusenko/Zotero/storage/NWX4GT28/Burda et al. -
              Large-Scale Study of Curiosity-Driven Learning.pdf:application/pdf}
}

@inproceedings{wan_unscented_2000,
  address   = {Lake Louise, Alta., Canada},
  title     = {The unscented {Kalman} filter for nonlinear estimation},
  isbn      = {978-0-7803-5800-3},
  url       = {http://ieeexplore.ieee.org/document/882463/},
  doi       = {10.1109/ASSPCC.2000.882463},
  abstract  = {The Extended Kalman Filter (EKF) has become a standard technique
               used in a number of nonlinear estimation and machine learning
               applications. These include estimating the state of a nonlinear
               dynamic system, estimating parameters for nonlinear system
               identiﬁcation (e.g., learning the weights of a neural network), and
               dual estimation (e.g., the Expectation Maximization (EM) algorithm)
               where both states and parameters are estimated simultaneously. This
               paper points out the ﬂaws in using the EKF, and introduces an
               improvement, the Unscented Kalman Filter (UKF), proposed by Julier
               and Uhlman [5]. A central and vital operation performed in the
               Kalman Filter is the propagation of a Gaussian random variable
               (GRV) through the system dynamics. In the EKF, the state
               distribution is approximated by a GRV, which is then propagated
               analytically through the ﬁrst-order linearization of the nonlinear
               system. This can introduce large errors in the true posterior mean
               and covariance of the transformed GRV, which may lead to
               sub-optimal performance and sometimes divergence of the ﬁlter. The
               UKF addresses this problem by using a deterministic sampling
               approach. The state distribution is again approximated by a GRV,
               but is now represented using a minimal set of carefully chosen
               sample points. These sample points completely capture the true mean
               and covariance of the GRV, and when propagated through the true
               nonlinear system, captures the posterior mean and covariance
               accurately to the 3rd order (Taylor series expansion) for any
               nonlinearity. The EKF, in contrast, only achieves ﬁrst-order
               accuracy. Remarkably, the computational complexity of the UKF is
               the same order as that of the EKF.},
  language  = {en},
  urldate   = {2018-12-17},
  booktitle = {Proceedings of the {IEEE} 2000 {Adaptive} {Systems} for {Signal}
               {Processing}, {Communications}, and {Control} {Symposium} ({Cat}.
               {No}.{00EX373})},
  publisher = {IEEE},
  author    = {Wan, E.A. and Van Der Merwe, R.},
  year      = {2000},
  pages     = {153--158},
  file      = {Wan and Van Der Merwe - 2000 - The unscented Kalman filter for
               nonlinear estimati.pdf:/Users/apodusenko/Zotero/storage/62HHHSEY/Wan
               and Van Der Merwe - 2000 - The unscented Kalman filter for nonlinear
               estimati.pdf:application/pdf}
}

@article{friston_dynamic_2001,
  title    = {Dynamic representations and generative models of brain function},
  volume   = {54},
  issn     = {0361-9230},
  url      = {http://www.sciencedirect.com/science/article/pii/S0361923000004366},
  doi      = {10.1016/S0361-9230(00)00436-6},
  abstract = {The main point made in this article is that the representational
              capacity and inherent function of any neuron, neuronal population
              or cortical area is dynamic and context-sensitive. This adaptive
              and contextual specialisation is mediated by functional integration
              or interactions among brain systems with a special emphasis on
              backwards or top-down connections. The critical notion is that
              neuronal responses, in any given cortical area, can represent
              different things at different times. Our argument is developed
              under the perspective of generative models of functional brain
              architectures, where higher-level systems provide a prediction of
              the inputs to lower-level regions. Conflict between the two is
              resolved by changes in the higher-level representations, driven by
              the resulting error in lower regions, until the mismatch is
              ‘cancelled’. In this model the specialisation of any region is
              determined both by bottom-up driving inputs and by top-down
              predictions. Specialisation is therefore not an intrinsic property
              of any region but depends on both forward and backward connections
              with other areas. Because these other areas have access to the
              context in which the inputs are generated they are in a position to
              modulate the selectivity or specialisation of lower areas. The
              implications for ‘classical’ models (e.g., classical receptive
              fields in electrophysiology, classical specialisation in
              neuroimaging and connectionism in cognitive models) are severe and
              suggest these models provide incomplete accounts of real brain
              architectures. Generative models represent a far more plausible
              framework for understanding selective neurophysiological responses
              and how representations are constructed in the brain.},
  number   = {3},
  urldate  = {2018-12-17},
  journal  = {Brain Research Bulletin},
  author   = {Friston, Karl J and Price, Cathy J},
  month    = feb,
  year     = {2001},
  keywords = {Effective connectivity, Predictive coding, Representations},
  pages    = {275--285},
  file     = {Friston en Price - 2001 - Dynamic representations and generative
              models of b.pdf:/Users/apodusenko/Zotero/storage/I4XM3PW2/Friston en
              Price - 2001 - Dynamic representations and generative models of
              b.pdf:application/pdf;ScienceDirect
              Snapshot:/Users/apodusenko/Zotero/storage/A4Y68EEQ/S0361923000004366.html:text/html
              }
}

@article{ramstead_answering_2018,
  title      = {Answering {Schrödinger}'s question: {A} free-energy formulation},
  issn       = {1571-0645},
  shorttitle = {Answering {Schrödinger}'s question},
  url        = {http://www.sciencedirect.com/science/article/pii/S1571064517301409},
  doi        = {10.1016/j.plrev.2017.09.001},
  abstract   = {The free-energy principle (FEP) is a formal model of neuronal
                processes that is widely recognised in neuroscience as a unifying
                theory of the brain and biobehaviour. More recently, however, it
                has been extended beyond the brain to explain the dynamics of
                living systems, and their unique capacity to avoid decay. The aim
                of this review is to synthesise these advances with a
                meta-theoretical ontology of biological systems called variational
                neuroethology, which integrates the FEP with Tinbergen's four
                research questions to explain biological systems across spatial and
                temporal scales. We exemplify this framework by applying it to Homo
                sapiens, before translating variational neuroethology into a
                systematic research heuristic that supplies the biological,
                cognitive, and social sciences with a computationally tractable
                guide to discovery.},
  urldate    = {2017-09-29},
  journal    = {Physics of Life Reviews},
  author     = {Ramstead and Badcock and Friston},
  year       = {2018},
  keywords   = {Free energy principle, Complex adaptive systems, Evolutionary
                systems theory, Hierarchically mechanistic mind, Physics of the
                mind, Variational neuroethology},
  file       = {Désormeau Ramstead et al. - Answering Schrödinger's question A
                free-energy fo.pdf:/Users/apodusenko/Zotero/storage/BFWJ8I6D/Désormeau
                Ramstead et al. - Answering Schrödinger's question A free-energy
                fo.pdf:application/pdf;Full
                Text:/Users/apodusenko/Zotero/storage/Z6NM9RGM/S1571064517301409.html:text/html;ScienceDirect
                Snapshot:/Users/apodusenko/Zotero/storage/D7SJNMWL/S1571064517301409.html:text/html
                }
}

@article{friston_active_2017-1,
  title   = {Active inference, curiosity and insight},
  volume  = {29},
  number  = {10},
  journal = {Neural computation},
  author  = {Friston, Karl J. and Lin, Marco and Frith, Christopher D. and
             Pezzulo, Giovanni and Hobson, J. Allan and Ondobaka, Sasha},
  year    = {2017},
  pages   = {2633--2683},
  file    = {Friston e.a. - 2017 - Active inference, curiosity and
             insight.html:/Users/apodusenko/Zotero/storage/KFX2D95F/Friston e.a. -
             2017 - Active inference, curiosity and
             insight.html:text/html;Snapshot:/Users/apodusenko/Zotero/storage/FTNKWHQU/neco_a_00999.html:text/html
             }
}

@misc{akbayrak_reparameterization_nodate,
  title    = {Reparameterization {Gradient} {Message} {Passing} - {Automatic} {
              Variational} {Inference} for {Factor} {Graphs}},
  language = {en},
  author   = {Akbayrak, Semih},
  file     = {Akbayrak - Reparameterization Gradient Message Passing -
              Auto.pdf:/Users/apodusenko/Zotero/storage/C86T9Q72/Akbayrak -
              Reparameterization Gradient Message Passing - Auto.pdf:application/pdf}
}

@article{cullen_active_2018,
  title      = {Active {Inference} in {OpenAI} {Gym}: {A} {Paradigm} for {
                Computational} {Investigations} {Into} {Psychiatric} {Illness}},
  volume     = {3},
  issn       = {24519022},
  shorttitle = {Active {Inference} in {OpenAI} {Gym}},
  url        = {https://linkinghub.elsevier.com/retrieve/pii/S2451902218301617},
  doi        = {10.1016/j.bpsc.2018.06.010},
  abstract   = {BACKGROUND: Artiﬁcial intelligence has recently attained humanlike
                performance in a number of gamelike domains. These advances have
                been spurred by brain-inspired architectures and algorithms such as
                hierarchical ﬁltering and reinforcement learning. OpenAI Gym is an
                open-source platform in which to train, test, and benchmark
                algorithms—it provides a range of tasks, including those of classic
                arcade games such as Doom. Here we describe how the platform might
                be used as a simulation, test, and diagnostic paradigm for
                psychiatric conditions. METHODS: To illustrate how active inference
                models of game play could be used to test mechanistic and
                algorithmic properties of psychiatric disorders, we provide two
                exemplar analyses. The ﬁrst speaks to the impact of aging on
                cognition, examining game-play behaviors in a model of aging in
                which we compared age-dependent changes of younger (n = 9, 22 6 1
                years of age) and older (n = 7, 56 6 5 years of age) adult players.
                The second is an illustration of a putative feature of anhedonia in
                which we simulated diminished sensitivity to reward. RESULTS: These
                simulations demonstrate how active inference can be used to test
                predicted changes in both neurobiology and beliefs in psychiatric
                cohorts. We show that, as well as behavioral measures, putative
                neural correlates of active inference can be simulated, and
                hypothesized (model-based) differences in local ﬁeld potentials and
                blood oxygen level–dependent responses can be produced.
                CONCLUSIONS: We show that active inference, through epistemic and
                value-based goals, enables simulated subjects to actively develop
                detailed representations of gaming environments, and we demonstrate
                the use of a principled algorithmic and neurobiological framework
                for testing hypotheses in psychiatric illness.},
  language   = {en},
  number     = {9},
  urldate    = {2018-12-12},
  journal    = {Biological Psychiatry: Cognitive Neuroscience and Neuroimaging},
  author     = {Cullen, Maell and Davey, Ben and Friston, Karl J. and Moran, Rosalyn
                J.},
  month      = sep,
  year       = {2018},
  pages      = {809--818},
  file       = {Cullen et al. - 2018 - Active Inference in OpenAI Gym A Paradigm for
                Com.pdf:/Users/apodusenko/Zotero/storage/5JBD8254/Cullen et al. - 2018
                - Active Inference in OpenAI Gym A Paradigm for
                Com.pdf:application/pdf;mmc1.pdf:/Users/apodusenko/Zotero/storage/KDEQP8TS/mmc1.pdf:application/pdf
                }
}

@article{heinonen_learning_2018,
  title    = {Learning unknown {ODE} models with {Gaussian} processes},
  url      = {http://arxiv.org/abs/1803.04303},
  abstract = {In conventional ODE modelling coefficients of an equation driving
              the system state forward in time are estimated. However, for many
              complex systems it is practically impossible to determine the
              equations or interactions governing the underlying dynamics. In
              these settings, parametric ODE model cannot be formulated. Here, we
              overcome this issue by introducing a novel paradigm of
              nonparametric ODE modelling that can learn the underlying dynamics
              of arbitrary continuous-time systems without prior knowledge. We
              propose to learn non-linear, unknown differential functions from
              state observations using Gaussian process vector fields within the
              exact ODE formalism. We demonstrate the model's capabilities to
              infer dynamics from sparse data and to simulate the system forward
              into future.},
  urldate  = {2018-12-11},
  journal  = {arXiv:1803.04303 [stat]},
  author   = {Heinonen, Markus and Yildiz, Cagatay and Mannerström, Henrik and
              Intosalmi, Jukka and Lähdesmäki, Harri},
  month    = mar,
  year     = {2018},
  note     = {arXiv: 1803.04303},
  keywords = {Statistics - Machine Learning},
  file     = {arXiv\:1803.04303
              PDF:/Users/apodusenko/Zotero/storage/TZUKPHW7/Heinonen et al. - 2018 -
              Learning unknown ODE models with Gaussian
              processe.pdf:application/pdf;arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/4W9QSN76/1803.html:text/html}
}

@article{baydin_automatic_2015,
  title      = {Automatic differentiation in machine learning: a survey},
  shorttitle = {Automatic differentiation in machine learning},
  url        = {http://arxiv.org/abs/1502.05767},
  abstract   = {Derivatives, mostly in the form of gradients and Hessians, are
                ubiquitous in machine learning. Automatic differentiation (AD),
                also called algorithmic differentiation or simply "autodiff", is a
                family of techniques similar to but more general than
                backpropagation for efficiently and accurately evaluating
                derivatives of numeric functions expressed as computer programs. AD
                is a small but established field with applications in areas
                including computational fluid dynamics, atmospheric sciences, and
                engineering design optimization. Until very recently, the fields of
                machine learning and AD have largely been unaware of each other and
                , in some cases, have independently discovered each other's
                results. Despite its relevance, general-purpose AD has been missing
                from the machine learning toolbox, a situation slowly changing with
                its ongoing adoption under the names "dynamic computational graphs"
                and "differentiable programming". We survey the intersection of AD
                and machine learning, cover applications where AD has direct
                relevance, and address the main implementation techniques. By
                precisely defining the main differentiation techniques and their
                interrelationships, we aim to bring clarity to the usage of the
                terms "autodiff", "automatic differentiation", and "symbolic
                differentiation" as these are encountered more and more in machine
                learning settings.},
  urldate    = {2018-12-11},
  journal    = {arXiv:1502.05767 [cs, stat]},
  author     = {Baydin, Atilim Gunes and Pearlmutter, Barak A. and Radul, Alexey
                Andreyevich and Siskind, Jeffrey Mark},
  month      = feb,
  year       = {2015},
  note       = {arXiv: 1502.05767},
  keywords   = {Statistics - Machine Learning, I.2.6, Computer Science - Machine
                Learning, 68W30, 65D25, 68T05, Computer Science - Symbolic
                Computation, G.1.4},
  file       = {arXiv.org
                Snapshot:/Users/apodusenko/Zotero/storage/36ZCM83X/1502.html:text/html;Baydin
                et al. - 2015 - Automatic differentiation in machine learning a
                s.pdf:/Users/apodusenko/Zotero/storage/MJP9M5ME/Baydin et al. - 2015 -
                Automatic differentiation in machine learning a s.pdf:application/pdf}
}

@misc{de_vries_biaslab_2018,
  title  = {{BIASlab} reseach theme},
  author = {De Vries, Bert},
  month  = sep,
  year   = {2018},
  file   = {DeVries - sep2018- BIASlab research
            theme.pptx:/Users/apodusenko/Zotero/storage/EASS4J7F/DeVries - sep2018-
            BIASlab research
            theme.pptx:application/vnd.openxmlformats-officedocument.presentationml.presentation
            }
}

@article{van_diepen_probabilistic_2018,
  title    = {A {Probabilistic} {Modelling} {Approach} to {One}-{Shot} {Gesture} {
              Recogntion}},
  url      = {http://arxiv.org/abs/1806.11408},
  abstract = {Gesture recognition enables a natural extension of the way we
              currently interact with devices. Commercially available gesture
              recognition systems are usually pre-trained and offer no option for
              customization by the user. In order to improve the user experience,
              it is desirable to allow end users to define their own gestures.
              This scenario requires learning from just a few training examples
              if we want to impose only a light training load on the user. To
              this end, we propose a gesture classifier based on a hierarchical
              probabilistic modeling approach. In this framework, high-level
              features that are shared among different gestures can be extracted
              from a large labeled data set, yielding a prior distribution for
              gestures. When learning new types of gestures, the learned shared
              prior reduces the number of required training examples for
              individual gestures. We implemented the proposed gesture classifier
              for a Myo sensor bracelet and show favorable results for the tested
              system on a database of 17 different gesture types. Furthermore, we
              propose and implement two methods to incorporate the gesture
              classifier in a real-time gesture recognition system.},
  urldate  = {2018-12-07},
  journal  = {arXiv:1806.11408 [eess]},
  author   = {van Diepen, Anouk and Cox, Marco and de Vries, Bert},
  month    = jun,
  year     = {2018},
  note     = {arXiv: 1806.11408},
  keywords = {Electrical Engineering and Systems Science - Signal Processing},
  file     = {arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/D2NH2833/1806.html:text/html;van
              Diepen et al. - 2018 - A Probabilistic Modelling Approach to One-Shot
              Ges.pdf:/Users/apodusenko/Zotero/storage/G7RPI3BV/van Diepen et al. -
              2018 - A Probabilistic Modelling Approach to One-Shot
              Ges.pdf:application/pdf}
}

@article{chen_neural_2018,
  title    = {Neural {Ordinary} {Differential} {Equations}},
  url      = {http://arxiv.org/abs/1806.07366},
  abstract = {We introduce a new family of deep neural network models. Instead
              of specifying a discrete sequence of hidden layers, we parameterize
              the derivative of the hidden state using a neural network. The
              output of the network is computed using a blackbox differential
              equation solver. These continuous-depth models have constant memory
              cost, adapt their evaluation strategy to each input, and can
              explicitly trade numerical precision for speed. We demonstrate
              these properties in continuous-depth residual networks and
              continuous-time latent variable models. We also construct
              continuous normalizing ﬂows, a generative model that can train by
              maximum likelihood, without partitioning or ordering the data
              dimensions. For training, we show how to scalably backpropagate
              through any ODE solver, without access to its internal operations.
              This allows end-to-end training of ODEs within larger models.},
  language = {en},
  urldate  = {2018-12-07},
  journal  = {arXiv:1806.07366 [cs, stat]},
  author   = {Chen, Ricky T. Q. and Rubanova, Yulia and Bettencourt, Jesse and
              Duvenaud, David},
  month    = jun,
  year     = {2018},
  note     = {arXiv: 1806.07366},
  keywords = {Statistics - Machine Learning, Computer Science - Artificial
              Intelligence, Computer Science - Machine Learning},
  file     = {Chen et al. - 2018 - Neural Ordinary Differential
              Equations.pdf:/Users/apodusenko/Zotero/storage/ET5QD4UN/Chen et al. -
              2018 - Neural Ordinary Differential Equations.pdf:application/pdf}
}

@phdthesis{fraccaro_deep_nodate,
  title    = {Deep {Latent} {Variable} {Models} for {Sequential} {Data}},
  language = {en},
  author   = {Fraccaro, Marco},
  file     = {Fraccaro - Deep Latent Variable Models for Sequential
              Data.pdf:/Users/apodusenko/Zotero/storage/JAUXPLI3/Fraccaro - Deep
              Latent Variable Models for Sequential Data.pdf:application/pdf}
}

@inproceedings{bocharov_acoustic_2018,
  address   = {Rome, Italy},
  title     = {Acoustic {Scene} {Classification} from {Few} {Examples}},
  abstract  = {In order to personalize the behavior of hearing aid devices in
               different acoustic environments, we need to develop personalized
               acoustic scene classiﬁers. Since we cannot afford to burden an
               individual hearing aid user with the task to collect a large
               acoustic database, we aim instead to train a scene classiﬁer on
               just one (or maximally a few) in-situ recorded acoustic waveform of
               a few seconds duration per scene. In this paper we develop such a
               ”one-shot” personalized scene classiﬁer, based on a Hidden
               Semi-Markov model. The presented classiﬁer consistently outperforms
               a more classical Dynamic-Time-Warping-NearestNeighbor classiﬁer,
               and correctly classiﬁes acoustic scenes about twice as well as a
               (random) chance classiﬁer after training on just one recording of
               10 seconds duration per scene.},
  language  = {en},
  booktitle = {26th {European} {Signal} {Processing} {Conference} ({EUSIPCO})},
  author    = {Bocharov, Ivan and Tjalkens, Tjalling and De Vries, Bert},
  year      = {2018},
  pages     = {5},
  file      = {Bocharov et al. - 2018 - Acoustic Scene Classification from Few
               Examples.pdf:/Users/apodusenko/Zotero/storage/DHFQFIYH/Bocharov et al.
               - 2018 - Acoustic Scene Classification from Few
               Examples.pdf:application/pdf}
}

@article{friston_what_2011,
  title    = {What {Is} {Optimal} about {Motor} {Control}?},
  volume   = {72},
  issn     = {0896-6273},
  url      = {http://www.sciencedirect.com/science/article/pii/S0896627311009305},
  doi      = {10.1016/j.neuron.2011.10.018},
  abstract = {This article poses a controversial question: is optimal control
              theory useful for understanding motor behavior or is it a
              misdirection? This question is becoming acute as people start to
              conflate internal models in motor control and perception (Poeppel
              et al., 2008; Hickok et al., 2011). However, the forward models in
              motor control are not the generative models used in perceptual
              inference. This Perspective tries to highlight the differences
              between internal models in motor control and perception and asks
              whether optimal control is the right way to think about things. The
              issues considered here may have broader implications for optimal
              decision theory and Bayesian approaches to learning and behavior in
              general.},
  number   = {3},
  urldate  = {2015-02-02},
  journal  = {Neuron},
  author   = {Friston, Karl},
  month    = nov,
  year     = {2011},
  pages    = {488--498},
  file     = {Friston - 2011 - What Is Optimal about Motor
              Control.pdf:/Users/apodusenko/Zotero/storage/GBU8X3G8/Friston - 2011 -
              What Is Optimal about Motor Control.pdf:application/pdf;ScienceDirect
              Snapshot:/Users/apodusenko/Zotero/storage/UZXQFIDE/S0896627311009305.html:text/html;ScienceDirect
              Snapshot:/Users/apodusenko/Zotero/storage/PI8CMJPL/S0896627311009305.html:text/html
              }
}

@article{mandt_variational_2014,
  title    = {Variational {Tempering}},
  url      = {http://arxiv.org/abs/1411.1810},
  abstract = {Variational inference (VI) combined with data subsampling enables
              approximate posterior inference over large data sets, but suffers
              from poor local optima. We first formulate a deterministic
              annealing approach for the generic class of conditionally conjugate
              exponential family models. This approach uses a decreasing
              temperature parameter which deterministically deforms the objective
              during the course of the optimization. A well-known drawback to
              this annealing approach is the choice of the cooling schedule. We
              therefore introduce variational tempering, a variational algorithm
              that introduces a temperature latent variable to the model. In
              contrast to related work in the Markov chain Monte Carlo literature
              , this algorithm results in adaptive annealing schedules. Lastly,
              we develop local variational tempering, which assigns a latent
              temperature to each data point; this allows for dynamic annealing
              that varies across data. Compared to the traditional VI, all
              proposed approaches find improved predictive likelihoods on
              held-out data.},
  urldate  = {2018-05-28},
  journal  = {arXiv:1411.1810 [cs, stat]},
  author   = {Mandt, Stephan and McInerney, James and Abrol, Farhan and Ranganath,
              Rajesh and Blei, David},
  month    = nov,
  year     = {2014},
  note     = {arXiv: 1411.1810},
  keywords = {Computer Science - Learning, Statistics - Machine Learning},
  file     = {arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/7QNPMEKF/1411.html:text/html;arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/HTUWUDFL/1411.html:text/html;Mandt
              et al. - 2014 - Variational
              Tempering.pdf:/Users/apodusenko/Zotero/storage/WDW2JXF6/Mandt et al. -
              2014 - Variational Tempering.pdf:application/pdf;Mandt et al. - 2014 -
              Variational
              Tempering.pdf:/Users/apodusenko/Zotero/storage/NLWXKFIU/Mandt et al. -
              2014 - Variational Tempering.pdf:application/pdf}
}

@article{mathys_uncertainty_2014,
  title    = {Uncertainty in perception and the {Hierarchical} {Gaussian} {Filter}},
  volume   = {8},
  issn     = {1662-5161},
  url      = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4237059/},
  doi      = {10.3389/fnhum.2014.00825},
  abstract = {In its full sense, perception rests on an agent's model of how its
              sensory input comes about and the inferences it draws based on this
              model. These inferences are necessarily uncertain. Here, we
              illustrate how the Hierarchical Gaussian Filter (HGF) offers a
              principled and generic way to deal with the several forms that
              uncertainty in perception takes. The HGF is a recent derivation of
              one-step update equations from Bayesian principles that rests on a
              hierarchical generative model of the environment and its
              (in)stability. It is computationally highly efficient, allows for
              online estimates of hidden states, and has found numerous
              applications to experimental data from human subjects. In this
              paper, we generalize previous descriptions of the HGF and its
              account of perceptual uncertainty. First, we explicitly formulate
              the extension of the HGF's hierarchy to any number of levels;
              second, we discuss how various forms of uncertainty are
              accommodated by the minimization of variational free energy as
              encoded in the update equations; third, we combine the HGF with
              decision models and demonstrate the inversion of this combination;
              finally, we report a simulation study that compared four
              optimization methods for inverting the HGF/decision model
              combination at different noise levels. These four methods
              (Nelder–Mead simplex algorithm, Gaussian process-based global
              optimization, variational Bayes and Markov chain Monte Carlo
              sampling) all performed well even under considerable noise, with
              variational Bayes offering the best combination of efficiency and
              informativeness of inference. Our results demonstrate that the HGF
              provides a principled, flexible, and efficient—but at the same time
              intuitive—framework for the resolution of perceptual uncertainty in
              behaving agents.},
  urldate  = {2018-11-16},
  journal  = {Frontiers in Human Neuroscience},
  author   = {Mathys, Christoph D. and Lomakina, Ekaterina I. and Daunizeau, Jean
              and Iglesias, Sandra and Brodersen, Kay H. and Friston, Karl J. and
              Stephan, Klaas E.},
  month    = nov,
  year     = {2014},
  pmid     = {25477800},
  pmcid    = {PMC4237059},
  keywords = {Bayesian inference, Learning, Free energy, decision-making,
              hierarchical modeling, Uncertainty, filtering, volatility},
  file     = {Mathys et al. - 2014 - Uncertainty in perception and the Hierarchical
              Gau.pdf:/Users/apodusenko/Zotero/storage/E8HNWM8Y/Mathys et al. - 2014
              - Uncertainty in perception and the Hierarchical
              Gau.pdf:application/pdf}
}

@article{friston_graphical_2017,
  title      = {The graphical brain: belief propagation and active inference},
  shorttitle = {The graphical brain},
  url        = {http://dx.doi.org/10.1162/NETN_a_00018},
  doi        = {10.1162/NETN_a_00018},
  urldate    = {2017-07-28},
  journal    = {Network Neuroscience},
  author     = {Friston, Karl J and Parr, Thomas and de Vries, Bert},
  month      = jun,
  year       = {2017},
  pages      = {1--78},
  file       = {Friston et al. - 2017 - The graphical brain Belief propagation and
                active.pdf:/Users/apodusenko/Zotero/storage/QNSDEINJ/Friston et al. -
                2017 - The graphical brain Belief propagation and
                active.pdf:application/pdf;Network Neuroscience
                Snapshot:/Users/apodusenko/Zotero/storage/N9A8ZMFM/NETN_a_00018.html:text/html;Snapshot:/Users/apodusenko/Zotero/storage/9HA3BX98/NETN_a_00018.html:text/html
                }
}

@article{friston_anatomy_2014,
  title      = {The anatomy of choice: dopamine and decision-making},
  volume     = {369},
  issn       = {0962-8436, 1471-2970},
  shorttitle = {The anatomy of choice},
  url        = {http://rstb.royalsocietypublishing.org/content/369/1655/20130481},
  doi        = {10.1098/rstb.2013.0481},
  abstract   = {This paper considers goal-directed decision-making in terms of
                embodied or active inference. We associate bounded rationality with
                approximate Bayesian inference that optimizes a free energy bound
                on model evidence. Several constructs such as expected utility,
                exploration or novelty bonuses, softmax choice rules and optimism
                bias emerge as natural consequences of free energy minimization.
                Previous accounts of active inference have focused on predictive
                coding. In this paper, we consider variational Bayes as a scheme
                that the brain might use for approximate Bayesian inference. This
                scheme provides formal constraints on the computational anatomy of
                inference and action, which appear to be remarkably consistent with
                neuroanatomy. Active inference contextualizes optimal decision
                theory within embodied inference, where goals become prior beliefs.
                For example, expected utility theory emerges as a special case of
                free energy minimization, where the sensitivity or inverse
                temperature (associated with softmax functions and quantal response
                equilibria) has a unique and Bayes-optimal solution. Crucially,
                this sensitivity corresponds to the precision of beliefs about
                behaviour. The changes in precision during variational updates are
                remarkably reminiscent of empirical dopaminergic responses—and they
                may provide a new perspective on the role of dopamine in
                assimilating reward prediction errors to optimize decision-making.},
  language   = {en},
  number     = {1655},
  urldate    = {2014-10-14},
  journal    = {Philosophical Transactions of the Royal Society B: Biological
                Sciences},
  author     = {Friston, Karl and Schwartenbeck, Philipp and FitzGerald, Thomas and
                Moutoussis, Michael and Behrens, Timothy and Dolan, Raymond J.},
  month      = may,
  year       = {2014},
  pmid       = {25267823},
  keywords   = {Bayesian inference, agency, bounded rationality, Active Inference,
                free energy, utility theory, active inference},
  pages      = {20130481},
  file       = {Friston et al. - 2014 - The anatomy of choice dopamine and
                decision-makin.pdf:/Users/apodusenko/Zotero/storage/YSSBMMKS/Friston et
                al. - 2014 - The anatomy of choice dopamine and
                decision-makin.pdf:application/pdf;Friston et al. - 2014 - The anatomy
                of choice dopamine and
                decision-makin.pdf:/Users/apodusenko/Zotero/storage/D6BP9DYU/Friston et
                al. - 2014 - The anatomy of choice dopamine and
                decision-makin.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/XHEWQNLU/20130481.html:text/html;Snapshot:/Users/apodusenko/Zotero/storage/HWUVIUPI/20130481.html:text/html
                }
}

@article{bamler_structured_2017,
  title    = {Structured {Black} {Box} {Variational} {Inference} for {Latent} {Time
              } {Series} {Models}},
  url      = {http://arxiv.org/abs/1707.01069},
  abstract = {Continuous latent time series models are prevalent in Bayesian
              modeling; examples include the Kalman filter, dynamic collaborative
              filtering, or dynamic topic models. These models often benefit from
              structured, non mean field variational approximations that capture
              correlations between time steps. Black box variational inference
              with reparameterization gradients (BBVI) allows us to explore a
              rich new class of Bayesian non-conjugate latent time series models;
              however, a naive application of BBVI to such structured variational
              models would scale quadratically in the number of time steps. We
              describe a BBVI algorithm analogous to the forward-backward
              algorithm which instead scales linearly in time. It allows us to
              efficiently sample from the variational distribution and estimate
              the gradients of the ELBO. Finally, we show results on the recently
              proposed dynamic word embedding model, which was trained using our
              method.},
  urldate  = {2018-11-29},
  journal  = {arXiv:1707.01069 [cs, stat]},
  author   = {Bamler, Robert and Mandt, Stephan},
  month    = jul,
  year     = {2017},
  note     = {arXiv: 1707.01069},
  keywords = {Computer Science - Learning, Statistics - Machine Learning,
              Computer Science - Machine Learning},
  file     = {arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/QKRLUYQ7/1707.html:text/html;arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/6GVGQXI5/1707.html:text/html;Bamler
              and Mandt - 2017 - Structured Black Box Variational Inference for
              Lat.pdf:/Users/apodusenko/Zotero/storage/JZQ9Z3UM/Bamler and Mandt -
              2017 - Structured Black Box Variational Inference for
              Lat.pdf:application/pdf;Bamler and Mandt - 2017 - Structured Black Box
              Variational Inference for
              Lat.pdf:/Users/apodusenko/Zotero/storage/YUYTYFJK/Bamler and Mandt -
              2017 - Structured Black Box Variational Inference for
              Lat.pdf:application/pdf}
}

@inproceedings{rao_speech_2016,
  title     = {Speech enhancement using combination of digital audio effects with {
               Kalman} filter},
  doi       = {10.1109/SCOPES.2016.7955633},
  abstract  = {The term “Quality of Speech” in Speech Enhancement techniques is
               associated with Clarity and Intelligibility. Till now due to the
               variable nature and characteristics of noise with time and process
               to process, Speech Enhancement is a difficult problem in Noisy
               environment. In this paper, we proposed a method to improve the
               quality of speech based on combination of Digital Audio Effects
               with Improved Adaptive Kalman Filter when only corrupted speech is
               available. In this approach to enhance the Speech content in the
               Noisy speech signal, Digital audio effects are used. A Digital
               Expander generates an audio effect which operates on a low signal
               level and create more likely sound characteristics. And further,
               noise is removed by Auto Regressive modeled improved adaptive
               Kalman filter. The performance of the proposed method with additive
               color noise is found to be better compared to other spectral
               subtraction, wiener and Kalman filter methods in terms of
               Signal-to-Noise ratio and intelligibility.},
  booktitle = {2016 {International} {Conference} on {Signal} {Processing}, {
               Communication}, {Power} and {Embedded} {System} ({SCOPES})},
  author    = {Rao, G. M. and Kumar, U. S.},
  month     = oct,
  year      = {2016},
  keywords  = {Estimation, Kalman filters, Kalman filter, autoregressive
               processes, Speech enhancement, intelligibility, Noise measurement,
               Signal to noise ratio, Speech, Spectral Subtraction, speech
               enhancement, adaptive Kalman filters, additive color noise,
               autoregressive modeled, Digital audio effect, digital audio effects
               , digital expander, improved adaptive Kalman filter, noisy speech
               signal, Quality of Speech, signal denoising, signal-to-noise ratio,
               spectral subtraction, speech content, speech enhancement techniques
               , Wiener filter, Wiener filter methods, Wiener filters},
  pages     = {1208--1211},
  file      = {Rao and Kumar - 2016 - Speech enhancement using combination of digital
               au.html:/Users/apodusenko/Zotero/storage/2AM6YVGY/Rao and Kumar - 2016
               - Speech enhancement using combination of digital au.html:text/html;Rao
               and Kumar - 2016 - Speech enhancement using combination of digital
               au.pdf:/Users/apodusenko/Zotero/storage/N78V7SKD/Rao and Kumar - 2016 -
               Speech enhancement using combination of digital au.pdf:application/pdf}
}

@article{kim_robust_2008,
  title    = {Robust {Beamforming} via {Worst}-{Case} {SINR} {Maximization}},
  volume   = {56},
  issn     = {1053-587X},
  doi      = {10.1109/TSP.2007.911498},
  abstract = {Minimum variance beamforming, which uses a weight vector that
              maximizes the signal-to-interference-plus-noise ratio (SINR), is
              often sensitive to estimation error and uncertainty in the
              parameters, steering vector and covariance matrix. Robust
              beamforming attempts to systematically alleviate this sensitivity
              by explicitly incorporating a data uncertainty model in the
              optimization problem. In this paper, we consider robust beamforming
              via worst-case SINR maximization, that is, the problem of finding a
              weight vector that maximizes the worst-case SINR over the
              uncertainty model. We show that with a general convex uncertainty
              model, the worst-case SINR maximization problem can be solved by
              using convex optimization. In particular, when the uncertainty
              model can be represented by linear matrix inequalities, the
              worst-case SINR maximization problem can be solved via semidefinite
              programming. The convex formulation result allows us to handle more
              general uncertainty models than prior work using a special form of
              uncertainty model. We illustrate the method with a numerical
              example.},
  number   = {4},
  journal  = {IEEE Transactions on Signal Processing},
  author   = {Kim, S. J. and Magnani, A. and Mutapcic, A. and Boyd, S. P. and Luo,
              Z. Q.},
  month    = apr,
  year     = {2008},
  keywords = {optimization problem, Uncertainty, Signal to noise ratio,
              Covariance matrix, Additive noise, Array signal processing, sensor
              arrays, Beamforming, Covariance matrices, Robustness, array signal
              processing, Interference, Sensor arrays, Cities and towns,
              Contracts, convex optimization, convex programming, covariance
              matrices, covariance matrix, data uncertainty model, linear matrix
              inequalities, minimum variance beamforming, robust beamforming,
              semidefinite programming, signal-to-interference-plus-noise ratio,
              signal-to-interference-plus-noise ratio (SINR), steering vector,
              worst-case SINR maximization},
  pages    = {1539--1547},
  file     = {Kim et al. - 2008 - Robust Beamforming via Worst-Case SINR
              Maximizatio.html:/Users/apodusenko/Zotero/storage/XT66FUII/Kim et al. -
              2008 - Robust Beamforming via Worst-Case SINR
              Maximizatio.html:text/html;Kim et al. - 2008 - Robust Beamforming via
              Worst-Case SINR
              Maximizatio.pdf:/Users/apodusenko/Zotero/storage/383XBPIE/Kim et al. -
              2008 - Robust Beamforming via Worst-Case SINR
              Maximizatio.pdf:application/pdf}
}

@article{friston_reinforcement_2009,
  title    = {Reinforcement {Learning} or {Active} {Inference}?},
  volume   = {4},
  issn     = {1932-6203},
  url      = {https://dx.plos.org/10.1371/journal.pone.0006421},
  doi      = {10.1371/journal.pone.0006421},
  abstract = {This paper questions the need for reinforcement learning or
              control theory when optimising behaviour. We show that it is fairly
              simple to teach an agent complicated and adaptive behaviours using
              a free-energy formulation of perception. In this formulation,
              agents adjust their internal states and sampling of the environment
              to minimize their free-energy. Such agents learn causal structure
              in the environment and sample it in an adaptive and self-supervised
              fashion. This results in behavioural policies that reproduce those
              optimised by reinforcement learning and dynamic programming.
              Critically, we do not need to invoke the notion of reward, value or
              utility. We illustrate these points by solving a benchmark problem
              in dynamic programming; namely the mountain-car problem, using
              active perception or inference under the free-energy principle. The
              ensuing proof-of-concept may be important because the free-energy
              formulation furnishes a unified account of both action and
              perception and may speak to a reappraisal of the role of dopamine
              in the brain.},
  language = {en},
  number   = {7},
  urldate  = {2018-11-02},
  journal  = {PLoS ONE},
  author   = {Friston, Karl J. and Daunizeau, Jean and Kiebel, Stefan J.},
  editor   = {Sporns, Olaf},
  month    = jul,
  year     = {2009},
  pages    = {e6421},
  file     = {Friston et al. - 2009 - Reinforcement Learning or Active
              Inference.pdf:/Users/apodusenko/Zotero/storage/3WUF93HP/Friston et al.
              - 2009 - Reinforcement Learning or Active Inference.pdf:application/pdf
              }
}

@article{bitzer_perceptual_2014,
  title      = {Perceptual decision making: drift-diffusion model is equivalent to a
                {Bayesian} model},
  volume     = {8},
  shorttitle = {Perceptual decision making},
  url        = {
                http://journal.frontiersin.org/Journal/10.3389/fnhum.2014.00102/abstract
                },
  doi        = {10.3389/fnhum.2014.00102},
  abstract   = {Behavioral data obtained with perceptual decision making
                experiments are typically analyzed with the drift-diffusion model.
                This parsimonious model accumulates noisy pieces of evidence toward
                a decision bound to explain the accuracy and reaction times of
                subjects. Recently, Bayesian models have been proposed to explain
                how the brain extracts information from noisy input as typically
                presented in perceptual decision making tasks. It has long been
                known that the drift-diffusion model is tightly linked with such
                functional Bayesian models but the precise relationship of the two
                mechanisms was never made explicit. Using a Bayesian model, we
                derived the equations which relate parameter values between these
                models. In practice we show that this equivalence is useful when
                fitting multi-subject data. We further show that the Bayesian model
                suggests different decision variables which all predict equal
                responses and discuss how these may be discriminated based on
                neural correlates of accumulated evidence. In addition, we discuss
                extensions to the Bayesian model which would be difficult to derive
                for the drift-diffusion model. We suggest that these and other
                extensions may be highly useful for deriving new experiments which
                test novel hypotheses.},
  urldate    = {2014-10-05},
  journal    = {Frontiers in Human Neuroscience},
  author     = {Bitzer, Sebastian and Park, Hame and Blankenburg, Felix and Kiebel,
                Stefan J.},
  year       = {2014},
  keywords   = {drift diffusion model, Bayesian models, uncertainty, Uncertainty,
                parameter fitting, perceptual decision making, decision variable,
                reaction time},
  pages      = {102},
  file       = {Bitzer et al. - 2014 - Perceptual decision making drift-diffusion
                model .pdf:/Users/apodusenko/Zotero/storage/NCU4GBSQ/Bitzer et al. -
                2014 - Perceptual decision making drift-diffusion model
                .pdf:application/pdf;Bitzer et al. - 2014 - Perceptual decision making
                drift-diffusion model
                .pdf:/Users/apodusenko/Zotero/storage/DXJA453P/Bitzer et al. - 2014 -
                Perceptual decision making drift-diffusion model
                .pdf:application/pdf;Bitzer et al. - 2014 - Perceptual decision making
                drift-diffusion model
                .pdf:/Users/apodusenko/Zotero/storage/JA4SNGM7/Bitzer et al. - 2014 -
                Perceptual decision making drift-diffusion model .pdf:application/pdf}
}

@inproceedings{hoffmann_linear_2017,
  address = {Toulouse, France},
  title   = {Linear {Optimal} {Control} on {Factor} {Graphs} - a {Message} {
             Passing} {Perspective}},
  author  = {Hoffmann, Christian and Rostalski, Philipp},
  month   = jul,
  year    = {2017},
  file    = {Hoffmann and Rostalski - 2017 - Linear Optimal Control on Factor
             Graphs - a
             Messag.pdf:/Users/apodusenko/Zotero/storage/9RLXYQTU/Hoffmann and
             Rostalski - 2017 - Linear Optimal Control on Factor Graphs - a
             Messag.pdf:application/pdf}
}

@article{moens_learning_2018,
  title     = {Learning and {Forgetting} {Using} {Reinforced} {Bayesian} {Change} {
               Detection}},
  copyright = {© 2018, Posted by Cold Spring Harbor Laboratory. This pre-print
               is available under a Creative Commons License
               (Attribution-NonCommercial-NoDerivs 4.0 International), CC
               BY-NC-ND 4.0, as described at
               http://creativecommons.org/licenses/by-nc-nd/4.0/},
  url       = {https://www.biorxiv.org/content/early/2018/04/06/294959},
  doi       = {10.1101/294959},
  abstract  = {Agents living in volatile environments must be able to detect
               changes in contingencies while refraining to adapt to unexpected
               events that are caused by noise. In Reinforcement Learning (RL)
               frameworks, this requires learning rates that adapt to past
               reliability of the model. The observation that behavioural
               flexibility in animals tends to decrease following prolonged
               training in stable environment provides experimental evidence for
               such adaptive learning rates. However, in classical RL models,
               learning rate is either fixed or scheduled and can thus not adapt
               dynamically to environmental changes. Here, we propose a new
               Bayesian learning model, using variational inference, that achieves
               adaptive change detection by the use of Stabilized Forgetting,
               updating its current belief based on a mixture of fixed, initial
               priors and previous posterior beliefs. The weight given to these
               two sources is optimized alongside the other parameters, allowing
               the model to adapt dynamically to changes in environmental
               volatility and to unexpected observations. This approach is used to
               implement the "critic" of an actor-critic RL model, while the actor
               samples the resulting value distributions to choose which action to
               undertake. We show that our model can emulate different adaptation
               strategies to contingency changes, depending on its prior
               assumptions of environmental stability, and that model parameters
               can be fit to real data with high accuracy. The model also exhibits
               trade-offs between flexibility and computational costs that mirror
               those observed in real data. Overall, the proposed method provides
               a general framework to study learning flexibility and decision
               making in RL contexts.},
  language  = {en},
  urldate   = {2018-04-16},
  journal   = {bioRxiv},
  author    = {Moens, Vincent and Zenon, Alexandre},
  month     = apr,
  year      = {2018},
  pages     = {294959},
  file      = {Full Text PDF:/Users/apodusenko/Zotero/storage/NK5PEXJQ/Moens and
               Zenon - 2018 - Learning and Forgetting Using Reinforced Bayesian
               .pdf:application/pdf;Moens en Zenon - 2018 - Learning and Forgetting
               Using Reinforced Bayesian
               .pdf:/Users/apodusenko/Zotero/storage/G5LX5XWB/Moens en Zenon - 2018 -
               Learning and Forgetting Using Reinforced Bayesian
               .pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/85M4M6E2/294959.html:text/html;Snapshot:/Users/apodusenko/Zotero/storage/4LKD2MHA/294959.html:text/html
               }
}

@article{yildiz_birdsong_2013,
  title      = {From {Birdsong} to {Human} {Speech} {Recognition}: {Bayesian} {
                Inference} on a {Hierarchy} of {Nonlinear} {Dynamical} {Systems}},
  volume     = {9},
  issn       = {1553-7358},
  shorttitle = {From {Birdsong} to {Human} {Speech} {Recognition}},
  url        = {http://dx.plos.org/10.1371/journal.pcbi.1003219},
  doi        = {10.1371/journal.pcbi.1003219},
  language   = {en},
  number     = {9},
  urldate    = {2014-04-10},
  journal    = {PLoS Computational Biology},
  author     = {Yildiz, Izzet B. and von Kriegstein, Katharina and Kiebel, Stefan J.
                },
  editor     = {Jirsa, Viktor K.},
  month      = sep,
  year       = {2013},
  keywords   = {Neurons, Dynamical systems, Learning, Speech, Birds, Language,
                Language acquisition, Word recognition},
  pages      = {e1003219},
  file       = {
                Snapshot:/Users/apodusenko/Zotero/storage/IJIDD7KI/article.html:text/html;Yildiz
                et al. - 2013 - From Birdsong to Human Speech Recognition
                Bayesia.pdf:/Users/apodusenko/Zotero/storage/PQZ84ZDG/Yildiz et al. -
                2013 - From Birdsong to Human Speech Recognition
                Bayesia.pdf:application/pdf;Yildiz et al. - 2013 - From Birdsong to
                Human Speech Recognition
                Bayesia.pdf:/Users/apodusenko/Zotero/storage/BZQVQ4U6/Yildiz et al. -
                2013 - From Birdsong to Human Speech Recognition
                Bayesia.pdf:application/pdf}
}

@article{dauwels_expectation_2009,
  title      = {Expectation maximization as message passing-part {I}: {Principles}
                and gaussian messages},
  shorttitle = {Expectation maximization as message passing-part {I}},
  url        = {http://arxiv.org/abs/0910.2832},
  urldate    = {2014-04-10},
  journal    = {arXiv preprint arXiv:0910.2832},
  author     = {Dauwels, Justin and Eckford, Andrew and Korl, Sascha and Loeliger,
                Hans-Andrea},
  year       = {2009},
  keywords   = {EM Algorithm},
  file       = {Dauwels et al. - 2009 - Expectation maximization as message
                passing-part I.html:/Users/apodusenko/Zotero/storage/E5QWE34P/Dauwels
                et al. - 2009 - Expectation maximization as message passing-part
                I.html:text/html;Dauwels et al. - 2009 - Expectation maximization as
                message passing-part
                I.pdf:/Users/apodusenko/Zotero/storage/P5TGMP8H/Dauwels et al. - 2009 -
                Expectation maximization as message passing-part I.pdf:application/pdf}
}

@incollection{doclo_acoustic_2010,
  title     = {Acoustic {Beamforming} for {Hearing} {Aid} {Applications}},
  copyright = {Copyright © 2009 John Wiley \& Sons, Inc.},
  isbn      = {978-0-470-48706-8},
  url       = {https://onlinelibrary.wiley.com/doi/abs/10.1002/9780470487068.ch9},
  abstract  = {This chapter contains sections titled: Introduction Overview of
               noise reduction techniques Monaural beamforming Binaural
               beamforming Conclusion References},
  language  = {en},
  urldate   = {2018-10-29},
  booktitle = {Handbook on {Array} {Processing} and {Sensor} {Networks}},
  publisher = {Wiley-Blackwell},
  author    = {Doclo, Simon and Gannot, Sharon and Moonen, Marc and Spriet, Ann},
  year      = {2010},
  doi       = {10.1002/9780470487068.ch9},
  keywords  = {acoustic beamforming for hearing aid applications, differences
               between monaural and binaural beamforming - two sets of microphones
               , multimicrophone beamforming techniques - noise reduction in
               monaural and binaural hearing aids},
  pages     = {269--302},
  file      = {Doclo et al. - 2010 - Acoustic Beamforming for Hearing Aid
               Applications.html:/Users/apodusenko/Zotero/storage/SP2TMDZF/Doclo et
               al. - 2010 - Acoustic Beamforming for Hearing Aid
               Applications.html:text/html}
}

@misc{van_de_laar_simulating_2018,
  title    = {Simulating {Active} {Inference} {Processes} {With} {Message} {Passing
              }},
  language = {en},
  author   = {van de Laar, Thijs},
  month    = nov,
  year     = {2018},
  file     = {van de Laar - 2018 - Simulating Active Inference Processes With
              Message.pdf:/Users/apodusenko/Zotero/storage/3UPBXEU5/van de Laar -
              2018 - Simulating Active Inference Processes With
              Message.pdf:application/pdf}
}

@misc{van_de_laar_you_2018,
  title  = {Do {You} {Wanna} {Build} a {Node}, {Man}?},
  author = {van de Laar, Thijs},
  month  = dec,
  year   = {2018},
  file   = {van de Laar - 2018 - Do You Wanna Build a Node,
            Man.pdf:/Users/apodusenko/Zotero/storage/5J58JGS7/van de Laar - 2018 -
            Do You Wanna Build a Node, Man.pdf:application/pdf}
}

@article{mandt_variational_2016,
  title    = {A {Variational} {Analysis} of {Stochastic} {Gradient} {Algorithms}},
  url      = {http://arxiv.org/abs/1602.02666},
  abstract = {Stochastic Gradient Descent (SGD) is an important algorithm in
              machine learning. With constant learning rates, it is a stochastic
              process that, after an initial phase of convergence, generates
              samples from a stationary distribution. We show that SGD with
              constant rates can be eﬀectively used as an approximate posterior
              inference algorithm for probabilistic modeling. Speciﬁcally, we
              show how to adjust the tuning parameters of SGD such as to match
              the resulting stationary distribution to the posterior. This
              analysis rests on interpreting SGD as a continuoustime stochastic
              process and then minimizing the Kullback-Leibler divergence between
              its stationary distribution and the target posterior. (This is in
              the spirit of variational inference.) In more detail, we model SGD
              as a multivariate Ornstein-Uhlenbeck process and then use
              properties of this process to derive the optimal parameters. This
              theoretical framework also connects SGD to modern scalable
              inference algorithms; we analyze the recently proposed stochastic
              gradient Fisher scoring under this perspective. We demonstrate that
              SGD with properly chosen constant rates gives a new way to optimize
              hyperparameters in probabilistic models.},
  language = {en},
  urldate  = {2018-12-03},
  journal  = {arXiv:1602.02666 [cs, stat]},
  author   = {Mandt, Stephan and Hoffman, Matthew D. and Blei, David M.},
  month    = feb,
  year     = {2016},
  note     = {arXiv: 1602.02666},
  keywords = {Statistics - Machine Learning, Computer Science - Machine Learning
              },
  file     = {Mandt et al. - 2016 - A Variational Analysis of Stochastic Gradient
              Algo.pdf:/Users/apodusenko/Zotero/storage/F4V9UUQ4/Mandt et al. - 2016
              - A Variational Analysis of Stochastic Gradient
              Algo.pdf:application/pdf}
}

@article{bamler_dynamic_2017,
  title    = {Dynamic {Word} {Embeddings}},
  url      = {http://arxiv.org/abs/1702.08359},
  abstract = {We present a probabilistic language model for time-stamped text
              data which tracks the semantic evolution of individual words over
              time. The model represents words and contexts by latent
              trajectories in an embedding space. At each moment in time, the
              embedding vectors are inferred from a probabilistic version of
              word2vec [Mikolov et al., 2013]. These embedding vectors are
              connected in time through a latent diffusion process. We describe
              two scalable variational inference algorithms--skip-gram smoothing
              and skip-gram filtering--that allow us to train the model jointly
              over all times; thus learning on all data while simultaneously
              allowing word and context vectors to drift. Experimental results on
              three different corpora demonstrate that our dynamic model infers
              word embedding trajectories that are more interpretable and lead to
              higher predictive likelihoods than competing methods that are based
              on static models trained separately on time slices.},
  urldate  = {2018-12-03},
  journal  = {arXiv:1702.08359 [cs, stat]},
  author   = {Bamler, Robert and Mandt, Stephan},
  month    = feb,
  year     = {2017},
  note     = {arXiv: 1702.08359},
  keywords = {Statistics - Machine Learning, Computer Science - Machine Learning
              },
  file     = {arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/APPA2HFH/1702.html:text/html;Bamler
              and Mandt - 2017 - Dynamic Word
              Embeddings.pdf:/Users/apodusenko/Zotero/storage/CZC8GE8K/Bamler and
              Mandt - 2017 - Dynamic Word Embeddings.pdf:application/pdf}
}

@article{jang_categorical_2016,
  title    = {Categorical {Reparameterization} with {Gumbel}-{Softmax}},
  url      = {http://arxiv.org/abs/1611.01144},
  abstract = {Categorical variables are a natural choice for representing
              discrete structure in the world. However, stochastic neural
              networks rarely use categorical latent variables due to the
              inability to backpropagate through samples. In this work, we
              present an efficient gradient estimator that replaces the
              non-differentiable sample from a categorical distribution with a
              differentiable sample from a novel Gumbel-Softmax distribution.
              This distribution has the essential property that it can be
              smoothly annealed into a categorical distribution. We show that our
              Gumbel-Softmax estimator outperforms state-of-the-art gradient
              estimators on structured output prediction and unsupervised
              generative modeling tasks with categorical latent variables, and
              enables large speedups on semi-supervised classification.},
  urldate  = {2018-12-03},
  journal  = {arXiv:1611.01144 [cs, stat]},
  author   = {Jang, Eric and Gu, Shixiang and Poole, Ben},
  month    = nov,
  year     = {2016},
  note     = {arXiv: 1611.01144},
  keywords = {Statistics - Machine Learning, Computer Science - Machine Learning
              },
  file     = {arXiv\:1611.01144 PDF:/Users/apodusenko/Zotero/storage/4I45K86J/Jang
              et al. - 2016 - Categorical Reparameterization with
              Gumbel-Softmax.pdf:application/pdf;arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/AL4PWURT/1611.html:text/html}
}

@article{roberts_variational_2002,
  title    = {Variational {Bayes} for generalized autoregressive models},
  volume   = {50},
  issn     = {1053-587X},
  doi      = {10.1109/TSP.2002.801921},
  abstract = {We describe a variational Bayes (VB) learning algorithm for
              generalized autoregressive (GAR) models. The noise is modeled as a
              mixture of Gaussians rather than the usual single Gaussian. This
              allows different data points to be associated with different noise
              levels and effectively provides robust estimation of AR
              coefficients. The VB framework is used to prevent overfitting and
              provides model-order selection criteria both for AR order and noise
              model order. We show that for the special case of Gaussian noise
              and uninformative priors on the noise and weight precisions, the VB
              framework reduces to the Bayesian evidence framework. The algorithm
              is applied to synthetic and real data with encouraging results.},
  number   = {9},
  journal  = {IEEE Transactions on Signal Processing},
  author   = {Roberts, S. J. and Penny, W. D.},
  month    = sep,
  year     = {2002},
  keywords = {Inference algorithms, Bayes methods, medical signal processing,
              Bayesian methods, Bayesian evidence, autoregressive processes,
              History, variational techniques, learning (artificial intelligence)
              , electroencephalography, Noise robustness, Least squares methods,
              Gaussian noise, AR coefficients, AR order, Cost function, EEG data,
              generalized autoregressive models, mixture of Gaussians,
              model-order selection criteria, Noise level, noise model order,
              noise precision, Noise reduction, real data, robust estimation,
              sampling, signal sampling, synthetic data, uninformative priors,
              variational Bayes learning algorithm, weight precision},
  pages    = {2245--2257},
  file     = {Roberts and Penny - 2002 - Variational Bayes for generalized
              autoregressive m.html:/Users/apodusenko/Zotero/storage/67V2VYFC/Roberts
              and Penny - 2002 - Variational Bayes for generalized autoregressive
              m.html:text/html;Roberts and Penny - 2002 - Variational Bayes for
              generalized autoregressive
              m.pdf:/Users/apodusenko/Zotero/storage/AHVB7ECJ/Roberts and Penny -
              2002 - Variational Bayes for generalized autoregressive
              m.pdf:application/pdf}
}

@article{krishnan_deep_2015,
  title    = {Deep {Kalman} {Filters}},
  url      = {http://arxiv.org/abs/1511.05121},
  abstract = {Kalman Filters are one of the most influential models of
              time-varying phenomena. They admit an intuitive probabilistic
              interpretation, have a simple functional form, and enjoy widespread
              adoption in a variety of disciplines. Motivated by recent
              variational methods for learning deep generative models, we
              introduce a unified algorithm to efficiently learn a broad spectrum
              of Kalman filters. Of particular interest is the use of temporal
              generative models for counterfactual inference. We investigate the
              efficacy of such models for counterfactual inference, and to that
              end we introduce the "Healing MNIST" dataset where long-term
              structure, noise and actions are applied to sequences of digits. We
              show the efficacy of our method for modeling this dataset. We
              further show how our model can be used for counterfactual inference
              for patients, based on electronic health record data of 8,000
              patients over 4.5 years.},
  urldate  = {2018-12-03},
  journal  = {arXiv:1511.05121 [cs, stat]},
  author   = {Krishnan, Rahul G. and Shalit, Uri and Sontag, David},
  month    = nov,
  year     = {2015},
  note     = {arXiv: 1511.05121},
  keywords = {Statistics - Machine Learning, Computer Science - Machine Learning
              },
  file     = {arXiv\:1511.05121
              PDF:/Users/apodusenko/Zotero/storage/8396GWVF/Krishnan et al. - 2015 -
              Deep Kalman Filters.pdf:application/pdf;arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/2YHPPIAT/1511.html:text/html}
}

@misc{mathys_tapas_2018,
  title      = {{TAPAS} - {Translational} {Algorithms} for {Psychiatry}-{Advancing} {
                Science}},
  copyright  = {GPL-3.0},
  shorttitle = {tapas},
  url        = {https://github.com/translationalneuromodeling/tapas},
  urldate    = {2018-04-24},
  publisher  = {Translational Neuromodeling Unit (TNU)},
  author     = {Mathys, Christoph D.},
  month      = apr,
  year       = {2018},
  note       = {original-date: 2017-01-14T13:14:23Z}
}

@article{wang_variational_2012,
  title    = {Variational {Inference} in {Nonconjugate} {Models}},
  url      = {http://arxiv.org/abs/1209.4360},
  journal  = {Journal of Machine Learning Research},
  author   = {Wang, Chong and Blei, David M.},
  year     = {2012},
  keywords = {Statistics - Machine Learning, variational Bayes},
  file     = {Wang and Blei - Variational Inference in Nonconjugate
              Models.pdf:/Users/apodusenko/Zotero/storage/E49CUAP3/Wang and Blei -
              Variational Inference in Nonconjugate Models.pdf:application/pdf}
}

@inproceedings{dauwels_particle_2006,
  title     = {Particle {Methods} as {Message} {Passing}},
  isbn      = {978-1-4244-0505-3 978-1-4244-0504-6},
  url       = {http://ieeexplore.ieee.org/document/4036329},
  doi       = {10.1109/ISIT.2006.261910},
  abstract  = {It is shown how particle methods can be viewed as message passing
               on factor graphs. In this setting, particle methods can readily be
               combined with other message-passing techniques such as the
               sum-product and max-product algorithm, expectation maximization,
               iterative conditional modes, steepest descent, Kalman ﬁlters, etc.
               Generic message computation rules for particle-based
               representations of sum-product messages are formulated. Various
               existing particle methods are described as instances of those
               generic rules, i.e., Gibbs sampling, importance sampling,
               Markov-chain Monte Carlo methods (MCMC), particle ﬁltering, and
               simulated annealing.},
  language  = {en},
  urldate   = {2018-04-19},
  booktitle = {{IEEE} {International} {Symposium} on {Information} {Theory}},
  publisher = {IEEE},
  author    = {Dauwels, Justin and Korl, Sascha and Loeliger, Hans-andrea},
  month     = jul,
  year      = {2006},
  pages     = {2052--2056},
  file      = {Dauwels et al. - 2006 - Particle Methods as Message
               Passing.pdf:/Users/apodusenko/Zotero/storage/V8PW3EQ6/Dauwels et al. -
               2006 - Particle Methods as Message Passing.pdf:application/pdf}
}

@article{ranganath_hierarchical_2016,
  title    = {Hierarchical {Variational} {Models}},
  volume   = {48},
  abstract = {Black box variational inference allows researchers to easily
              prototype and evaluate an array of models. Recent advances allow
              such algorithms to scale to high dimensions. However, a central
              question remains: How to specify an expressive variational
              distribution that maintains efﬁcient computation? To address this,
              we develop hierarchical variational models (HVMs). HVMs augment a
              variational approximation with a prior on its parameters, which
              allows it to capture complex structure for both discrete and
              continuous latent variables. The algorithm we develop is black box,
              can be used for any HVM, and has the same computational efﬁciency
              as the original approximation. We study HVMs on a variety of deep
              discrete latent variable models. HVMs generalize other expressive
              variational distributions and maintains higher ﬁdelity to the
              posterior.},
  language = {en},
  journal  = {Journal of Machine Learning Research},
  author   = {Ranganath, Rajesh and Tran, Dustin and Blei, David M},
  year     = {2016},
  pages    = {10},
  file     = {Ranganath et al. - Hierarchical Variational
              Models.pdf:/Users/apodusenko/Zotero/storage/VME6D8JN/Ranganath et al. -
              Hierarchical Variational Models.pdf:application/pdf}
}

@inproceedings{cox_robust_2018,
  address   = {Rome, Italy},
  title     = {Robust {Expectation} {Propagation} in {Factor} {Graphs} {Involving} {
               Both} {Continuous} and {Binary} {Variables}},
  abstract  = {Factor graphs provide a convenient framework for automatically
               generating (approximate) Bayesian inference algorithms based on
               message passing. Examples include the sumproduct algorithm (belief
               propagation), expectation maximization (EM), expectation
               propagation (EP) and variational message passing (VMP). While these
               message passing algorithms can be generated automatically, they
               depend on a library of precomputed message update rules. As a
               result, the applicability of the factor graph approach depends on
               the availability of such rules for all involved nodes. This paper
               describes the probit factor node for linking continuous and binary
               random variables in a factor graph. We derive (approximate)
               sum-product message update rules for this node through constrained
               moment matching, which leads to a robust version of the EP
               algorithm in which all messages are guaranteed to be proper. This
               enables automatic Bayesian inference in probabilistic models that
               involve both continuous and discrete latent variables, without the
               need for model-speciﬁc derivations. The usefulness of the node as a
               factor graph building block is demonstrated by applying it to
               perform Bayesian inference in a linear classiﬁcation model with
               corrupted class labels.},
  language  = {en},
  booktitle = {26th {European} {Signal} {Processing} {Conference} ({EUSIPCO})},
  author    = {Cox, Marco},
  year      = {2018},
  pages     = {5},
  file      = {Cox - 2018 - Robust Expectation Propagation in Factor Graphs
               In.pdf:/Users/apodusenko/Zotero/storage/FPMBIW25/Cox - 2018 - Robust
               Expectation Propagation in Factor Graphs In.pdf:application/pdf}
}

@book{suchman_human-machine_2007,
  title      = {Human-{Machine} {Reconfigurations}: {Plans} and {Situated} {Actions}},
  isbn       = {978-0-521-67588-8},
  shorttitle = {Human-{Machine} {Reconfigurations}},
  abstract   = {This 2007 book considers how agencies are currently figured at the
                human-machine interface, and how they might be imaginatively and
                materially reconfigured. Contrary to the apparent enlivening of
                objects promised by the sciences of the artificial, the author
                proposes that the rhetorics and practices of those sciences work to
                obscure the performative nature of both persons and things. The
                question then shifts from debates over the status of human-like
                machines, to that of how humans and machines are enacted as similar
                or different in practice, and with what theoretical, practical and
                political consequences. Drawing on scholarship across the social
                sciences, humanities and computing, the author argues for research
                aimed at tracing the differences within specific sociomaterial
                arrangements without resorting to essentialist divides. This
                requires expanding our unit of analysis, while recognizing the
                inevitable cuts or boundaries through which technological systems
                are constituted.},
  language   = {en},
  publisher  = {Cambridge University Press},
  author     = {Suchman, Lucy},
  year       = {2007},
  keywords   = {Computers / Human-Computer Interaction (HCI), Computers /
                Programming / General, Psychology / Developmental / General, Social
                Science / Sociology / General, Technology \& Engineering /
                Industrial Health \& Safety}
}

@book{suchman_plans_1987,
  title      = {Plans and situated actions: the problem of human machine interaction},
  shorttitle = {Plans and situated actions},
  publisher  = {Cambridge: Cambridge University Press},
  author     = {Suchman, Lucy},
  year       = {1987}
}

@article{beck_manifesto_2001,
  title  = {Manifesto for agile software development},
  author = {Beck, Kent and Beedle, Mike and Van Bennekum, Arie and Cockburn,
            Alistair and Cunningham, Ward and Fowler, Martin and Grenning, James
            and Highsmith, Jim and Hunt, Andrew and Jeffries, Ron},
  year   = {2001},
  file   = {Beck e.a. - 2001 - Manifesto for agile software
            development.pdf:/Users/apodusenko/Zotero/storage/FHGXB9GW/Beck e.a. -
            2001 - Manifesto for agile software development.pdf:application/pdf}
}

@article{lee_mobile_2006,
  title   = {A mobile pet wearable computer and mixed reality system for
             human–poultry interaction through the internet},
  volume  = {10},
  number  = {5},
  journal = {Personal and Ubiquitous Computing},
  author  = {Lee, Ping and Cheok, David and James, Soon and Debra, Lyn and Jie,
             Wen and Chuang, Wang and Farbiz, Farzam},
  year    = {2006},
  pages   = {301--317},
  file    = {Lee e.a. - 2006 - A mobile pet wearable computer and mixed reality
             s.pdf:/Users/apodusenko/Zotero/storage/I2B4YDT4/Lee e.a. - 2006 - A
             mobile pet wearable computer and mixed reality
             s.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/LRJN9JME/citation.html:text/html
             }
}

@article{huisman_academic_2002,
  title      = {Academic careers from a {European} perspective: {The} declining
                desirability of the faculty position},
  volume     = {73},
  shorttitle = {Academic careers from a {European} perspective},
  number     = {1},
  journal    = {The journal of higher education},
  author     = {Huisman, Jeroen and de Weert, Egbert and Bartelse, Jeroen},
  year       = {2002},
  pages      = {141--160},
  file       = {Huisman e.a. - 2002 - Academic careers from a European perspective The
                .pdf:/Users/apodusenko/Zotero/storage/UPKX8T4K/Huisman e.a. - 2002 -
                Academic careers from a European perspective The .pdf:application/pdf}
}

@inproceedings{ranganath_deep_2015,
  title     = {Deep exponential families},
  booktitle = {Artificial {Intelligence} and {Statistics}},
  author    = {Ranganath, Rajesh and Tang, Linpeng and Charlin, Laurent and Blei,
               David},
  year      = {2015},
  pages     = {762--771},
  file      = {Ranganath e.a. - 2015 - Deep exponential
               families.pdf:/Users/apodusenko/Zotero/storage/S8STC8QF/Ranganath e.a. -
               2015 - Deep exponential families.pdf:application/pdf}
}

@inproceedings{fei-fei_bayesian_2005,
  title     = {A bayesian hierarchical model for learning natural scene categories},
  volume    = {2},
  booktitle = {Computer {Vision} and {Pattern} {Recognition}, 2005. {CVPR} 2005.
               {IEEE} {Computer} {Society} {Conference} on},
  publisher = {IEEE},
  author    = {Fei-Fei, Li and Perona, Pietro},
  year      = {2005},
  pages     = {524--531},
  file      = {Fei-Fei en Perona - 2005 - A bayesian hierarchical model for learning
               natural.pdf:/Users/apodusenko/Zotero/storage/WEKKYV27/Fei-Fei en Perona
               - 2005 - A bayesian hierarchical model for learning
               natural.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/FUDXMX6F/1467486.html:text/html
               }
}

@inproceedings{dean_scalable_2006,
  title     = {Scalable {Inference} in {Hierarchical} {Generative} {Models}.},
  booktitle = {{ISAIM}},
  publisher = {Citeseer},
  author    = {Dean, Thomas},
  year      = {2006},
  file      = {Dean - 2006 - Scalable Inference in Hierarchical Generative
               Mode.pdf:/Users/apodusenko/Zotero/storage/VTUF4L6X/Dean - 2006 -
               Scalable Inference in Hierarchical Generative Mode.pdf:application/pdf}
}

@article{innes_flux:_2018,
  title      = {Flux: {Elegant} machine learning with {Julia}},
  volume     = {3},
  shorttitle = {Flux},
  number     = {25},
  journal    = {Journal of Open Source Software},
  author     = {Innes, Mike},
  year       = {2018},
  pages      = {602}
}

@article{seth_interoceptive_2013,
  title    = {Interoceptive inference, emotion, and the embodied self},
  volume   = {17},
  issn     = {1364-6613},
  url      = {http://www.sciencedirect.com/science/article/pii/S1364661313002118},
  doi      = {10.1016/j.tics.2013.09.007},
  abstract = {The concept of the brain as a prediction machine has enjoyed a
              resurgence in the context of the Bayesian brain and predictive
              coding approaches within cognitive science. To date, this
              perspective has been applied primarily to exteroceptive perception
              (e.g., vision, audition), and action. Here, I describe a predictive
              , inferential perspective on interoception: ‘interoceptive
              inference’ conceives of subjective feeling states (emotions) as
              arising from actively-inferred generative (predictive) models of
              the causes of interoceptive afferents. The model generalizes
              ‘appraisal’ theories that view emotions as emerging from cognitive
              evaluations of physiological changes, and it sheds new light on the
              neurocognitive mechanisms that underlie the experience of body
              ownership and conscious selfhood in health and in neuropsychiatric
              illness.},
  number   = {11},
  urldate  = {2018-11-29},
  journal  = {Trends in Cognitive Sciences},
  author   = {Seth, Anil K.},
  month    = nov,
  year     = {2013},
  keywords = {predictive coding, active inference, emotion, experience of body
              ownership, interoception, rubber hand illusion},
  pages    = {565--573},
  file     = {ScienceDirect
              Snapshot:/Users/apodusenko/Zotero/storage/2JELANN7/S1364661313002118.html:text/html;Seth
              - 2013 - Interoceptive inference, emotion, and the
              embodied.pdf:/Users/apodusenko/Zotero/storage/R5BJR2JK/Seth - 2013 -
              Interoceptive inference, emotion, and the embodied.pdf:application/pdf}
}

@article{singh_structured_nodate,
  title    = {Structured {Variational} {Autoencoders} for the {Beta}-{Bernoulli} {
              Process}},
  abstract = {Beta-Bernoulli processes, also known as Indian buffet processes,
              are nonparametric priors that allow generative models to
              automatically infer the number of features in datasets. However,
              inference for these models proves to be challenging, often relying
              on speciﬁc forms of the likelihood for computational tractability.},
  language = {en},
  author   = {Singh, Rachit and Ling, Jeffrey and Doshi-Velez, Finale},
  pages    = {9},
  journal  = {Neural Information Processing Systems (NIPS) Workshop on Advances in Approximate Bayesian Inference},
  year     = {2017},
  file     = {Singh et al. - Structured Variational Autoencoders for the
              Beta-B.pdf:/Users/apodusenko/Zotero/storage/MGE4PTD6/Singh et al. -
              Structured Variational Autoencoders for the Beta-B.pdf:application/pdf}
}

@article{sudderth_nonparametric_2010,
  title    = {Nonparametric belief propagation},
  volume   = {53},
  issn     = {00010782},
  url      = {http://portal.acm.org/citation.cfm?doid=1831407.1831431},
  doi      = {10.1145/1831407.1831431},
  abstract = {Continuous quantities are ubiquitous in models of realworld
              phenomena, but are surprisingly difficult to reason about
              automatically. Probabilistic graphical models such as Bayesian
              networks and Markov random fields, and algorithms for approximate
              inference such as belief propagation (BP), have proven to be
              powerful tools in a wide range of applications in statistics and
              artificial intelligence. However, applying these methods to models
              with continuous variables remains a challenging task. In this work
              we describe an extension of BP to continuous variable models,
              generalizing particle filtering, and Gaussian mixture filtering
              techniques for time series to more complex models. We illustrate
              the power of the resulting nonparametric BP algorithm via two
              applications: kinematic tracking of visual motion and distributed
              localization in sensor networks.},
  language = {en},
  number   = {10},
  urldate  = {2018-11-29},
  journal  = {Communications of the ACM},
  author   = {Sudderth, Erik B. and Ihler, Alexander T. and Isard, Michael and
              Freeman, William T. and Willsky, Alan S.},
  month    = oct,
  year     = {2010},
  pages    = {95},
  file     = {Sudderth et al. - 2010 - Nonparametric belief
              propagation.pdf:/Users/apodusenko/Zotero/storage/QTBDCSQQ/Sudderth et
              al. - 2010 - Nonparametric belief propagation.pdf:application/pdf}
}

@article{ryder_black-box_2018,
  title    = {Black-box {Variational} {Inference} for {Stochastic} {Differential} {
              Equations}},
  url      = {http://arxiv.org/abs/1802.03335},
  abstract = {Parameter inference for stochastic differential equations is
              challenging due to the presence of a latent diffusion process.
              Working with an Euler-Maruyama discretisation for the diffusion, we
              use variational inference to jointly learn the parameters and the
              diffusion paths. We use a standard mean-field variational
              approximation of the parameter posterior, and introduce a recurrent
              neural network to approximate the posterior for the diffusion paths
              conditional on the parameters. This neural network learns how to
              provide Gaussian state transitions which bridge between
              observations in a very similar way to the conditioned diffusion
              process. The resulting black-box inference method can be applied to
              any SDE system with light tuning requirements. We illustrate the
              method on a Lotka-Volterra system and an epidemic model, producing
              accurate parameter estimates in a few hours.},
  urldate  = {2018-11-29},
  journal  = {arXiv:1802.03335 [stat]},
  author   = {Ryder, Thomas and Golightly, Andrew and McGough, A. Stephen and
              Prangle, Dennis},
  month    = feb,
  year     = {2018},
  note     = {arXiv: 1802.03335},
  keywords = {Statistics - Machine Learning, Statistics - Computation},
  file     = {arXiv\:1802.03335 PDF:/Users/apodusenko/Zotero/storage/FCAXNJWG/Ryder
              et al. - 2018 - Black-box Variational Inference for Stochastic
              Dif.pdf:application/pdf;arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/STHFR8RJ/1802.html:text/html}
}

@book{puterman_markov_2014,
  title      = {Markov {Decision} {Processes}: {Discrete} {Stochastic} {Dynamic} {
                Programming}},
  shorttitle = {Markov {Decision} {Processes}.},
  publisher  = {John Wiley \& Sons},
  author     = {Puterman, Martin L.},
  year       = {2014},
  file       = {
                Snapshot:/Users/apodusenko/Zotero/storage/TVS4MSA2/books.html:text/html
                }
}

@article{lee_model_2011,
  title      = {Model predictive control: {Review} of the three decades of
                development},
  volume     = {9},
  shorttitle = {Model predictive control},
  number     = {3},
  journal    = {International Journal of Control, Automation and Systems},
  author     = {Lee, Jay H.},
  year       = {2011},
  pages      = {415},
  file       = {Lee - 2011 - Model predictive control Review of the three
                deca.pdf:/Users/apodusenko/Zotero/storage/RNYAGY5W/Lee - 2011 - Model
                predictive control Review of the three
                deca.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/ZUHQHXH9/s12555-011-0300-6.html:text/html
                }
}

@article{benrimoh_active_2018,
  title    = {Active {Inference} and {Auditory} {Hallucinations}},
  url      = {https://doi.org/10.1162/cpsy_a_00022},
  doi      = {10.1162/cpsy_a_00022},
  abstract = {Auditory verbal hallucinations (AVH) are often distressing
              symptoms of several neuropsychiatric conditions, including
              schizophrenia. Using a Markov decision process formulation of
              active inference, we develop a novel model of AVH as false
              (positive) inference. Active inference treats perception as a
              process of hypothesis testing, in which sensory data are used to
              disambiguate between alternative hypotheses about the world.
              Crucially, this depends upon a delicate balance between prior
              beliefs about unobserved (hidden) variables and the sensations they
              cause. A false inference that a voice is present, even in the
              absence of auditory sensations, suggests that prior beliefs
              dominate perceptual inference. Here we consider the computational
              mechanisms that could cause this imbalance in perception. Through
              simulation, we show that the content of (and confidence in) prior
              beliefs depends on beliefs about policies (here sequences of
              listening and talking) and on beliefs about the reliability of
              sensory data. We demonstrate several ways in which hallucinatory
              percepts could occur when an agent expects to hear a voice in the
              presence of imprecise sensory data. This model expresses, in formal
              terms, alternative computational mechanisms that underwrite AVH and
              , speculatively, can be mapped onto neurobiological changes
              associated with schizophrenia. The interaction of action and
              perception is important in modeling AVH, given that speech is a
              fundamentally enactive and interactive process—and that
              hallucinators often actively engage with their voices.},
  urldate  = {2018-11-24},
  journal  = {Computational Psychiatry},
  author   = {Benrimoh, David and Parr, Thomas and Vincent, Peter and Adams, Rick
              A. and Friston, Karl},
  month    = nov,
  year     = {2018},
  pages    = {1--22},
  file     = {Benrimoh et al. - 2018 - Active Inference and Auditory
              Hallucinations.pdf:/Users/apodusenko/Zotero/storage/DKA8VQZM/Benrimoh
              et al. - 2018 - Active Inference and Auditory
              Hallucinations.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/GZES9S88/cpsy_a_00022.html:text/html
              }
}

@inproceedings{sallans_using_2001,
  title     = {Using free energies to represent {Q}-values in a multiagent
               reinforcement learning task},
  booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
  author    = {Sallans, Brian and Hinton, Geoffrey E.},
  year      = {2001},
  pages     = {1075--1081},
  file      = {Sallans en Hinton - 2001 - Using free energies to represent Q-values
               in a mul.pdf:/Users/apodusenko/Zotero/storage/J2I6WHUQ/Sallans en
               Hinton - 2001 - Using free energies to represent Q-values in a
               mul.pdf:application/pdf}
}

@inproceedings{williams_information_2017,
  title     = {Information theoretic {MPC} for model-based reinforcement learning},
  doi       = {10.1109/ICRA.2017.7989202},
  abstract  = {We introduce an information theoretic model predictive control
               (MPC) algorithm capable of handling complex cost criteria and
               general nonlinear dynamics. The generality of the approach makes it
               possible to use multi-layer neural networks as dynamics models,
               which we incorporate into our MPC algorithm in order to solve
               model-based reinforcement learning tasks. We test the algorithm in
               simulation on a cart-pole swing up and quadrotor navigation task,
               as well as on actual hardware in an aggressive driving task.
               Empirical results demonstrate that the algorithm is capable of
               achieving a high level of performance and does so only utilizing
               data collected from the system.},
  booktitle = {2017 {IEEE} {International} {Conference} on {Robotics} and {
               Automation} ({ICRA})},
  author    = {Williams, Grady and Wagener, Nolan and Goldfain, Brian and Drews,
               Paul and Rehg, James M. and Boots, Byron and Theodorou, Evangelos A.},
  month     = may,
  year      = {2017},
  keywords  = {information theory, optimal control, learning (artificial
               intelligence), Heuristic algorithms, Trajectory, Cost function,
               cart-pole swing up, control engineering computing, dynamics models,
               information theoretic model predictive control, information
               theoretic MPC, Learning (artificial intelligence), mobile robots,
               model-based reinforcement learning, multilayer neural networks,
               multilayer perceptrons, nonlinear dynamical systems, nonlinear
               dynamics, Optimal control, predictive control, quadrotor navigation
               , Robots},
  pages     = {1714--1721},
  file      = {IEEE Xplore Abstract
               Record:/Users/apodusenko/Zotero/storage/ITZRZR8C/7989202.html:text/html
               }
}

@article{kamthe_data-efficient_2017,
  title    = {Data-{Efficient} {Reinforcement} {Learning} with {Probabilistic} {
              Model} {Predictive} {Control}},
  url      = {http://arxiv.org/abs/1706.06491},
  abstract = {Trial-and-error based reinforcement learning (RL) has seen rapid
              advancements in recent times, especially with the advent of deep
              neural networks. However, the majority of autonomous RL algorithms
              require a large number of interactions with the environment. A
              large number of interactions may be impractical in many real-world
              applications, such as robotics, and many practical systems have to
              obey limitations in the form of state space or control constraints.
              To reduce the number of system interactions while simultaneously
              handling constraints, we propose a model-based RL framework based
              on probabilistic Model Predictive Control (MPC). In particular, we
              propose to learn a probabilistic transition model using Gaussian
              Processes (GPs) to incorporate model uncertainty into long-term
              predictions, thereby, reducing the impact of model errors. We then
              use MPC to find a control sequence that minimises the expected
              long-term cost. We provide theoretical guarantees for first-order
              optimality in the GP-based transition models with deterministic
              approximate inference for long-term planning. We demonstrate that
              our approach does not only achieve state-of-the-art data efficiency
              , but also is a principled way for RL in constrained environments.},
  urldate  = {2019-10-15},
  journal  = {arXiv:1706.06491 [cs, stat]},
  author   = {Kamthe, Sanket and Deisenroth, Marc Peter},
  month    = jun,
  year     = {2017},
  note     = {arXiv: 1706.06491},
  keywords = {Statistics - Machine Learning, Electrical Engineering and Systems
              Science - Systems and Control},
  file     = {arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/ZIELHSSU/1706.html:text/html;Kamthe
              en Deisenroth - 2017 - Data-Efficient Reinforcement Learning with
              Probabi.pdf:/Users/apodusenko/Zotero/storage/QRHIDS2F/Kamthe en
              Deisenroth - 2017 - Data-Efficient Reinforcement Learning with
              Probabi.pdf:application/pdf}
}

@article{ziebart_maximum_nodate,
  title    = {Maximum {Entropy} {Inverse} {Reinforcement} {Learning}},
  abstract = {Recent research has shown the beneﬁt of framing problems of
              imitation learning as solutions to Markov Decision Problems. This
              approach reduces learning to the problem of recovering a utility
              function that makes the behavior induced by a near-optimal policy
              closely mimic demonstrated behavior. In this work, we develop a
              probabilistic approach based on the principle of maximum entropy.
              Our approach provides a well-deﬁned, globally normalized
              distribution over decision sequences, while providing the same
              performance guarantees as existing methods.},
  language = {en},
  author   = {Ziebart, Brian D and Maas, Andrew and Bagnell, J Andrew and Dey,
              Anind K},
  pages    = {6},
  file     = {Ziebart e.a. - Maximum Entropy Inverse Reinforcement
              Learning.pdf:/Users/apodusenko/Zotero/storage/K6Q4MQA7/Ziebart e.a. -
              Maximum Entropy Inverse Reinforcement Learning.pdf:application/pdf}
}

@article{tschantz_learning_2019,
  title     = {Learning action-oriented models through active inference},
  copyright = {© 2019, Posted by Cold Spring Harbor Laboratory. This pre-print
               is available under a Creative Commons License
               (Attribution-NonCommercial 4.0 International), CC BY-NC 4.0, as
               described at http://creativecommons.org/licenses/by-nc/4.0/},
  url       = {https://www.biorxiv.org/content/10.1101/764969v1},
  doi       = {10.1101/764969},
  abstract  = {{\textless}p{\textgreater}Converging theories suggest that
               organisms learn and exploit probabilistic models of their
               environment. However, it remains unclear how such models can be
               learned in practice. The open-ended complexity of natural
               environments means that it is generally infeasible for organisms to
               model their environment comprehensively. Alternatively,
               action-oriented models attempt to encode a parsimonious
               representation of adaptive agent-environment interactions. One
               approach to learning action-oriented models is to learn online in
               the presence of goal-directed behaviours. This constrains an agent
               to behaviourally relevant trajectories, reducing the diversity of
               the data a model need account for. Unfortunately, this approach can
               cause models to prematurely converge to sub-optimal solutions,
               through a process we refer to as a bad-bootstrap. Here, we exploit
               the normative framework of active inference to show that efficient
               action-oriented models can be learned by balancing goal-oriented
               and epistemic (information-seeking) behaviours in a principled
               manner. We illustrate our approach using a simple agent-based model
               of bacterial chemotaxis. We first demonstrate that learning via
               goal-directed behaviour indeed constrains models to behaviorally
               relevant aspects of the environment, but that this approach is
               prone to sub-optimal convergence. We then demonstrate that
               epistemic behaviours facilitate the construction of accurate and
               comprehensive models, but that these models are not tailored to any
               specific behavioural niche and are therefore less efficient in
               their use of data. Finally, we show that active inference agents
               learn models that are parsimonious, tailored to action, and which
               avoid bad bootstraps and sub-optimal convergence. Critically, our
               results indicate that models learned through active inference can
               support adaptive behaviour in spite of, and indeed because of,
               their departure from veridical representations of the environment.
               Our approach provides a principled method for learning adaptive
               models from limited interactions with an environment, highlighting
               a route to sample efficient learning algorithms.{\textless}/p{
               \textgreater}},
  language  = {en},
  urldate   = {2019-09-12},
  journal   = {bioRxiv},
  author    = {Tschantz, Alexander and Seth, Anil K. and Buckley, Christopher L.},
  month     = sep,
  year      = {2019},
  pages     = {764969},
  file      = {Tschantz et al. - 2019 - Learning action-oriented models through
               active inf.pdf:/Users/apodusenko/Zotero/storage/LIIF5K26/Tschantz et
               al. - 2019 - Learning action-oriented models through active
               inf.pdf:application/pdf}
}

@article{watson_stochastic_2019,
  title    = {Stochastic {Optimal} {Control} as {Approximate} {Input} {Inference}},
  url      = {http://arxiv.org/abs/1910.03003},
  abstract = {Optimal control of stochastic nonlinear dynamical systems is a
              major challenge in the domain of robot learning. Given the
              intractability of the global control problem, state-of-the-art
              algorithms focus on approximate sequential optimization techniques,
              that heavily rely on heuristics for regularization in order to
              achieve stable convergence. By building upon the duality between
              inference and control, we develop the view of Optimal Control as
              Input Estimation, devising a probabilistic stochastic optimal
              control formulation that iteratively infers the optimal input
              distributions by minimizing an upper bound of the control cost.
              Inference is performed through Expectation Maximization and message
              passing on a probabilistic graphical model of the dynamical system,
              and time-varying linear Gaussian feedback controllers are extracted
              from the joint state-action distribution. This perspective
              incorporates uncertainty quantification, effective initialization
              through priors, and the principled regularization inherent to the
              Bayesian treatment. Moreover, it can be shown that for
              deterministic linearized systems, our framework derives the maximum
              entropy linear quadratic optimal control law. We provide a complete
              and detailed derivation of our probabilistic approach and highlight
              its advantages in comparison to other deterministic and
              probabilistic solvers.},
  urldate  = {2019-10-10},
  journal  = {arXiv:1910.03003 [cs, eess, stat]},
  author   = {Watson, Joe and Abdulsamad, Hany and Peters, Jan},
  month    = oct,
  year     = {2019},
  note     = {arXiv: 1910.03003},
  keywords = {Statistics - Machine Learning, Computer Science - Robotics,
              Computer Science - Machine Learning, Electrical Engineering and
              Systems Science - Systems and Control},
  file     = {arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/TKS4VG2I/1910.html:text/html;Watson
              e.a. - 2019 - Stochastic Optimal Control as Approximate Input
              In.pdf:/Users/apodusenko/Zotero/storage/54T595SK/Watson e.a. - 2019 -
              Stochastic Optimal Control as Approximate Input In.pdf:application/pdf}
}

@article{spiegelhalter_sequential_1990,
  title     = {Sequential updating of conditional probabilities on directed
               graphical structures},
  volume    = {20},
  copyright = {Copyright © 1990 Wiley Periodicals, Inc., A Wiley Company},
  issn      = {1097-0037},
  url       = {https://onlinelibrary.wiley.com/doi/abs/10.1002/net.3230200507},
  doi       = {10.1002/net.3230200507},
  abstract  = {A directed acyclic graph or influence diagram is frequently used
               as a representation for qualitative knowledge in some domains in
               which expert system techniques have been applied, and conditional
               probability tables on appropriate sets of variables form the
               quantitative part of the accumulated experience. It is shown how
               one can introduce imprecision into such probabilities as a data
               base of cases accumulates. By exploiting the graphical structure,
               the updating can be performed locally, either approximately or
               exactly, and the setup makes it possible to take advantage of a
               range of well-established statistical techniques. As examples we
               discuss discrete models, models based on Dirichlet distributions
               and models of the logistic regression type.},
  language  = {en},
  number    = {5},
  urldate   = {2019-10-05},
  journal   = {Networks},
  author    = {Spiegelhalter, David J. and Lauritzen, Steffen L.},
  year      = {1990},
  pages     = {579--605},
  file      = {
               Snapshot:/Users/apodusenko/Zotero/storage/YESV3ZLL/net.html:text/html;Spiegelhalter
               and Lauritzen - 1990 - Sequential updating of conditional probabilities
               o.pdf:/Users/apodusenko/Zotero/storage/VSNE8JJ3/Spiegelhalter and
               Lauritzen - 1990 - Sequential updating of conditional probabilities
               o.pdf:application/pdf}
}

@inproceedings{niranjan_sequential_1999,
  title     = {Sequential {Bayesian} computation of logistic regression models},
  volume    = {2},
  doi       = {10.1109/ICASSP.1999.759927},
  abstract  = {The extended Kalman filter (EKF) algorithm for identification of a
               state space model is shown to be a sensible tool in estimating a
               logistic regression model sequentially. A Gaussian probability
               density over the parameters of the logistic model is propagated on
               a sample by sample basis. Two other approaches, the Laplace
               approximation and the variational approximation are compared with
               the state space formulation. Features of the latter approach, such
               as the possibility of inferring noise levels by maximising the "
               innovation probability" are indicated. Experimental illustrations
               of these ideas on a synthetic problem and two real world problems
               are discussed.},
  booktitle = {1999 {IEEE} {International} {Conference} on {Acoustics}, {Speech}
               , and {Signal} {Processing}. {Proceedings}. {ICASSP99} ({Cat}. {No
               }.{99CH36258})},
  author    = {Niranjan, M.},
  month     = mar,
  year      = {1999},
  keywords  = {Kalman filters, Approximation algorithms, Bayes methods, Bayesian
               methods, Equations, Gaussian distribution, filtering theory,
               approximation theory, State estimation, statistical analysis, Noise
               level, Control system analysis, experiment, extended Kalman filter
               algorithm, Gaussian probability density, innovation probability,
               Laplace approximation, logistic model parameters, logistic
               regression models, Logistics, noise levels, nonlinear filters,
               parameter estimation, Parametric statistics, Power system modeling,
               real world problems, sequential Bayesian computation, state space
               formulation, state space model identification, state-space methods,
               State-space methods, synthetic problem, variational approximation},
  pages     = {1065--1068 vol.2},
  file      = {IEEE Xplore Abstract
               Record:/Users/apodusenko/Zotero/storage/SNRUSR5A/759927.html:text/html;Niranjan
               - 1999 - Sequential Bayesian computation of logistic
               regres.pdf:/Users/apodusenko/Zotero/storage/C84Q2XXL/Niranjan - 1999 -
               Sequential Bayesian computation of logistic regres.pdf:application/pdf}
}

@article{ghavamzadeh_bayesian_2015,
  title      = {Bayesian {Reinforcement} {Learning}: {A} {Survey}},
  volume     = {8},
  issn       = {1935-8237, 1935-8245},
  shorttitle = {Bayesian {Reinforcement} {Learning}},
  url        = {http://arxiv.org/abs/1609.04436},
  doi        = {10.1561/2200000049},
  abstract   = {Bayesian methods for machine learning have been widely
                investigated, yielding principled methods for incorporating prior
                information into inference algorithms. In this survey, we provide
                an in-depth review of the role of Bayesian methods for the
                reinforcement learning (RL) paradigm. The major incentives for
                incorporating Bayesian reasoning in RL are: 1) it provides an
                elegant approach to action-selection (exploration/exploitation) as
                a function of the uncertainty in learning; and 2) it provides a
                machinery to incorporate prior knowledge into the algorithms. We
                first discuss models and methods for Bayesian inference in the
                simple single-step Bandit model. We then review the extensive
                recent literature on Bayesian methods for model-based RL, where
                prior information can be expressed on the parameters of the Markov
                model. We also present Bayesian methods for model-free RL, where
                priors are expressed over the value function or policy class. The
                objective of the paper is to provide a comprehensive survey on
                Bayesian RL algorithms and their theoretical and empirical
                properties.},
  number     = {5-6},
  urldate    = {2019-10-03},
  journal    = {Foundations and Trends® in Machine Learning},
  author     = {Ghavamzadeh, Mohammad and Mannor, Shie and Pineau, Joelle and Tamar,
                Aviv},
  year       = {2015},
  note       = {arXiv: 1609.04436},
  keywords   = {Statistics - Machine Learning, Computer Science - Artificial
                Intelligence, Computer Science - Machine Learning},
  pages      = {359--483},
  file       = {arXiv.org
                Snapshot:/Users/apodusenko/Zotero/storage/BF8LU7QV/1609.html:text/html;Ghavamzadeh
                et al. - 2015 - Bayesian Reinforcement Learning A
                Survey.pdf:/Users/apodusenko/Zotero/storage/NAGHNR28/Ghavamzadeh et al.
                - 2015 - Bayesian Reinforcement Learning A Survey.pdf:application/pdf}
}

@misc{bocharov_autodiff_2019,
  address = {Eindhoven University of Technology},
  title   = {{AutoDiff} (in {Julia})},
  author  = {Bocharov, Ivan},
  month   = sep,
  year    = {2019},
  file    = {Bocharov - 2019 - AutoDiff (in
             Julia).pdf:/Users/apodusenko/Zotero/storage/SQNM2CKV/Bocharov - 2019 -
             AutoDiff (in Julia).pdf:application/pdf}
}

@incollection{hutchison_unifying_2006,
  address   = {Berlin, Heidelberg},
  title     = {Unifying {Divergence} {Minimization} and {Statistical} {Inference} {
               Via} {Convex} {Duality}},
  volume    = {4005},
  isbn      = {978-3-540-35294-5 978-3-540-35296-9},
  url       = {http://link.springer.com/10.1007/11776420_13},
  abstract  = {In this paper we unify divergence minimization and statistical
               inference by means of convex duality. In the process of doing so,
               we prove that the dual of approximate maximum entropy estimation is
               maximum a posteriori estimation. Moreover, our treatment leads to
               stability and convergence bounds for many statistical learning
               problems. Finally, we show how an algorithm by Zhang can be used to
               solve this class of optimization problems eﬃciently.},
  language  = {en},
  urldate   = {2019-09-05},
  booktitle = {Learning {Theory}},
  publisher = {Springer Berlin Heidelberg},
  author    = {Altun, Yasemin and Smola, Alex},
  editor    = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg,
               Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni
               and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and
               Sudan, Madhu and Terzopoulos, Demetri and Tygar, Dough and Vardi,
               Moshe Y. and Weikum, Gerhard and Lugosi, Gábor and Simon, Hans Ulrich
               },
  year      = {2006},
  doi       = {10.1007/11776420_13},
  pages     = {139--153},
  file      = {Altun and Smola - 2006 - Unifying Divergence Minimization and
               Statistical I.pdf:/Users/apodusenko/Zotero/storage/472ZX9J6/Altun and
               Smola - 2006 - Unifying Divergence Minimization and Statistical
               I.pdf:application/pdf}
}

@book{colombo_andy_2019,
  address   = {New York, NY},
  title     = {Andy {Clark} and {His} {Critics}},
  isbn      = {978-0-19-066281-3},
  abstract  = {Andy Clark is a leading philosopher of cognitive science, whose
               work has had an extraordinary impact throughout philosophy,
               psychology, neuroscience, and robotics. His monographs have led the
               way for new research programs in the philosophy of mind and
               cognition: Microcognition (1989) and Associative Engines (1993)
               introduced the philosophical community to connectionist research
               and the novel issues it raised; Being There (1997) showed the
               relevance of embodiment, dynamical systems theory, and minimal
               computation frameworks for the study of the mind; Natural Born
               Cyborgs (OUP 2003) presented an accessible development of embodied
               and embedded approaches to understanding human nature and
               cognition; Supersizing the Mind (OUP 2008) developed this yet
               further along with the famous "Extended Mind" hypothesis; and
               Surfing Uncertainty (OUP 2017) presents a framework for uniting
               perception, action, and the embodied mind.In Andy Clark and His
               Critics, a range of high-profile researchers in philosophy of mind,
               philosophy of cognitive science, and empirical cognitive science,
               critically engage with Clark's work across the themes of: Extended,
               Embodied, Embedded, Enactive, and Affective Minds; Natural Born
               Cyborgs; and Perception, Action, and Prediction. Daniel Dennett
               provides a foreword on the significance of Clark's work, and Clark
               replies to each section of the book, thus advancing current
               literature with original contributions that will form the basis for
               new discussions, debates and directions in the discipline.},
  language  = {English},
  publisher = {Oxford University Press},
  editor    = {Colombo, Matteo and Irvine, Elizabeth and Stapleton, Mog},
  month     = may,
  year      = {2019},
  file      = {Colombo et al. - 2019 - Andy Clark and His
               Critics.pdf:/Users/apodusenko/Zotero/storage/4BD5QE8Z/Colombo et al. -
               2019 - Andy Clark and His Critics.pdf:application/pdf}
}

@article{spieksma_introduction_nodate,
  title    = {An {Introduction} to {Stochastic} {Processes} in {Continuous} {Time}:
              },
  language = {en},
  author   = {Spieksma, Flora},
  pages    = {145},
  file     = {Spieksma - An Introduction to Stochastic Processes in
              Continu.pdf:/Users/apodusenko/Zotero/storage/WDLVFDBY/Spieksma - An
              Introduction to Stochastic Processes in Continu.pdf:application/pdf}
}

@article{spreij_measure_nodate,
  title    = {Measure {Theoretic} {Probability}},
  language = {en},
  author   = {Spreij, P J C},
  pages    = {169},
  file     = {Spreij - Measure Theoretic
              Probability.pdf:/Users/apodusenko/Zotero/storage/K9UA46ZA/Spreij -
              Measure Theoretic Probability.pdf:application/pdf}
}

@article{karny_towards_1996,
  title   = {Towards fully probabilistic control design},
  volume  = {32},
  number  = {12},
  journal = {Automatica},
  author  = {Kárnỳ, Miroslav},
  year    = {1996},
  pages   = {1719--1722},
  file    = {
             Snapshot:/Users/apodusenko/Zotero/storage/KRT26V3Y/S0005109896800094.html:text/html
             }
}

@article{parr_anatomy_2018,
  title      = {The {Anatomy} of {Inference}: {Generative} {Models} and {Brain} {
                Structure}},
  volume     = {12},
  issn       = {1662-5188},
  shorttitle = {The {Anatomy} of {Inference}},
  url        = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6243103/},
  doi        = {10.3389/fncom.2018.00090},
  abstract   = {To infer the causes of its sensations, the brain must call on a
                generative (predictive) model. This necessitates passing local
                messages between populations of neurons to update beliefs about
                hidden variables in the world beyond its sensory samples. It also
                entails inferences about how we will act. Active inference is a
                principled framework that frames perception and action as
                approximate Bayesian inference. This has been successful in
                accounting for a wide range of physiological and behavioral
                phenomena. Recently, a process theory has emerged that attempts to
                relate inferences to their neurobiological substrates. In this
                paper, we review and develop the anatomical aspects of this process
                theory. We argue that the form of the generative models required
                for inference constrains the way in which brain regions connect to
                one another. Specifically, neuronal populations representing
                beliefs about a variable must receive input from populations
                representing the Markov blanket of that variable. We illustrate
                this idea in four different domains: perception, planning,
                attention, and movement. In doing so, we attempt to show how
                appealing to generative models enables us to account for anatomical
                brain architectures. Ultimately, committing to an anatomical theory
                of inference ensures we can form empirical hypotheses that can be
                tested using neuroimaging, neuropsychological, and
                electrophysiological experiments.},
  urldate    = {2018-12-01},
  journal    = {Frontiers in Computational Neuroscience},
  author     = {Parr, Thomas and Friston, Karl J.},
  month      = nov,
  year       = {2018},
  pmid       = {null},
  pmcid      = {PMC6243103},
  keywords   = {message passing, Bayesian, Active inference., Generative Model,
                Neuroanatomy, predictive processing},
  file       = {Parr and Friston - 2018 - The Anatomy of Inference Generative Models
                and Br.pdf:/Users/apodusenko/Zotero/storage/7I46YRX7/Parr and Friston -
                2018 - The Anatomy of Inference Generative Models and
                Br.pdf:application/pdf}
}

@article{hoffmann_iterative_2019,
  title    = {Iterative {Approximate} {Nonlinear} {Inference} via {Gaussian} {
              Message} {Passing} on {Factor} {Graphs}},
  volume   = {3},
  issn     = {2475-1456},
  doi      = {10.1109/LCSYS.2019.2919260},
  abstract = {Factor graphs are graphical models able to represent the
              factorization of probability density functions. By visualizing
              conditional independence statements, they provide an intuitive and
              versatile interface to sparsity exploiting message passing
              algorithms as a unified framework for constructing algorithms in
              signal processing, estimation, and control in a mix-and-match
              style. Especially, when assuming Gaussian distributed variables,
              tabulated message passing rules allow for easy automated
              derivations of algorithms. This letter's contribution consists in
              the combination of statistical or Jacobian-based linearization
              approaches to handling nonlinear factors with efficient message
              parametrizations in a Gaussian message passing setting. Tabulated
              message passing rules for a multivariate nonlinear factor node are
              presented that implement a re-linearization about the most current
              belief (marginal) of each adjacent variable. When utilized in a
              nonlinear Kalman smoothing setting, the iterated nonlinear modified
              Bryson-Frazier smoother is recovered, while retaining the
              flexibility of the factor graph framework. This application is
              illustrated by deriving an input estimation algorithm for a
              nonlinear system.},
  number   = {4},
  journal  = {IEEE Control Systems Letters},
  author   = {Hoffmann, C. Herzog né and Petersen, E. and Rostalski, P.},
  month    = oct,
  year     = {2019},
  keywords = {Estimation, Kalman filters, Message passing, smoothing methods,
              machine learning, graph theory, Inference algorithms, inference
              mechanisms, graphical models, message passing, Signal processing
              algorithms, iterative methods, Gaussian distribution, Random
              variables, Gaussian processes, approximation theory, Kalman
              filtering, Smoothing methods, signal processing, nonlinear filters,
              conditional independence statements, efficient message
              parametrizations, estimation, factor graph framework, factorization
              , Gaussian distributed variables, Gaussian message passing setting,
              input estimation algorithm, iterated nonlinear modified
              Bryson-Frazier smoother, iterative approximate nonlinear inference,
              Jacobian-based linearization, mix-and-match style, multivariate
              nonlinear factor node, nonlinear factors, nonlinear Kalman
              smoothing setting, nonlinear system, probability density functions,
              sparsity exploiting message passing algorithms, statistical
              learning, statistical linearization, stochastic systems, tabulated
              message passing rules},
  pages    = {978--983},
  file     = {Hoffmann et al. - 2019 - Iterative Approximate Nonlinear Inference via
              Gaus.pdf:/Users/apodusenko/Zotero/storage/7ZW27VW4/Hoffmann et al. -
              2019 - Iterative Approximate Nonlinear Inference via
              Gaus.pdf:application/pdf;IEEE Xplore Abstract
              Record:/Users/apodusenko/Zotero/storage/YSWZSGQW/8723648.html:text/html
              }
}

@article{friston_life_2013,
  title     = {Life as we know it},
  volume    = {10},
  copyright = {. © 2013 The Authors. Published by the Royal Society under the
               terms of the Creative Commons Attribution License
               http://creativecommons.org/licenses/by/3.0/, which permits
               unrestricted use, provided the original author and source are
               credited.},
  issn      = {1742-5689, 1742-5662},
  url       = {http://rsif.royalsocietypublishing.org/content/10/86/20130475},
  doi       = {10.1098/rsif.2013.0475},
  abstract  = {This paper presents a heuristic proof (and simulations of a
               primordial soup) suggesting that life—or biological
               self-organization—is an inevitable and emergent property of any
               (ergodic) random dynamical system that possesses a Markov blanket.
               This conclusion is based on the following arguments: if the
               coupling among an ensemble of dynamical systems is mediated by
               short-range forces, then the states of remote systems must be
               conditionally independent. These independencies induce a Markov
               blanket that separates internal and external states in a
               statistical sense. The existence of a Markov blanket means that
               internal states will appear to minimize a free energy functional of
               the states of their Markov blanket. Crucially, this is the same
               quantity that is optimized in Bayesian inference. Therefore, the
               internal states (and their blanket) will appear to engage in active
               Bayesian inference. In other words, they will appear to model—and
               act on—their world to preserve their functional and structural
               integrity, leading to homoeostasis and a simple form of
               autopoiesis.},
  number    = {86},
  journal   = {Journal of The Royal Society Interface},
  author    = {Friston, Karl},
  month     = sep,
  year      = {2013},
  pmid      = {23825119},
  pages     = {20130475},
  file      = {Friston - 2013 - Life as we know
               it.pdf:/Users/apodusenko/Zotero/storage/QHYUUF8Q/Friston - 2013 - Life
               as we know it.pdf:application/pdf}
}

@article{mcgregor_minimal_2015,
  title    = {A {Minimal} {Active} {Inference} {Agent}},
  url      = {http://arxiv.org/abs/1503.04187},
  abstract = {Research on the so-called "free-energy principle'' (FEP) in
              cognitive neuroscience is becoming increasingly high-profile. To
              date, introductions to this theory have proved difficult for many
              readers to follow, but it depends mainly upon two relatively simple
              ideas: firstly that normative or teleological values can be
              expressed as probability distributions (active inference), and
              secondly that approximate Bayesian reasoning can be effectively
              performed by gradient descent on model parameters (the free-energy
              principle). The notion of active inference is of great interest for
              a number of disciplines including cognitive science and artificial
              intelligence, as well as cognitive neuroscience, and deserves to be
              more widely known. This paper attempts to provide an accessible
              introduction to active inference and informational free-energy, for
              readers from a range of scientific backgrounds. In this work
              introduce an agent-based model with an agent trying to make
              predictions about its position in a one-dimensional discretized
              world using methods from the FEP.},
  urldate  = {2015-08-30},
  journal  = {arXiv:1503.04187 [cs]},
  author   = {McGregor, Simon and Baltieri, Manuel and Buckley, Christopher L.},
  month    = mar,
  year     = {2015},
  note     = {arXiv: 1503.04187},
  keywords = {Computer Science - Artificial Intelligence},
  file     = {arXiv\:1503.04187
              PDF:/Users/apodusenko/Zotero/storage/FCICLSCE/McGregor et al. - 2015 -
              A Minimal Active Inference Agent.pdf:application/pdf;arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/YPSLFYHK/1503.html:text/html;arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/PSEE9NPH/1503.html:text/html;McGregor
              et al. - 2015 - A Minimal Active Inference
              Agent.pdf:/Users/apodusenko/Zotero/storage/5JQTSFKB/McGregor et al. -
              2015 - A Minimal Active Inference Agent.pdf:application/pdf}
}

@article{schwobel_active_2018,
  title    = {Active {Inference}, {Belief} {Propagation}, and the {Bethe} {
              Approximation}},
  volume   = {30},
  issn     = {1530-888X},
  doi      = {10.1162/neco_a_01108},
  abstract = {When modeling goal-directed behavior in the presence of various
              sources of uncertainty, planning can be described as an inference
              process. A solution to the problem of planning as inference was
              previously proposed in the active inference framework in the form
              of an approximate inference scheme based on variational free
              energy. However, this approximate scheme was based on the
              mean-field approximation, which assumes statistical independence of
              hidden variables and is known to show overconfidence and may
              converge to local minima of the free energy. To better capture the
              spatiotemporal properties of an environment, we reformulated the
              approximate inference process using the so-called Bethe
              approximation. Importantly, the Bethe approximation allows for
              representation of pairwise statistical dependencies. Under these
              assumptions, the minimizer of the variational free energy
              corresponds to the belief propagation algorithm, commonly used in
              machine learning. To illustrate the differences between the
              mean-field approximation and the Bethe approximation, we have
              simulated agent behavior in a simple goal-reaching task with
              different types of uncertainties. Overall, the Bethe agent achieves
              higher success rates in reaching goal states. We relate the better
              performance of the Bethe agent to more accurate predictions about
              the consequences of its own actions. Consequently, active inference
              based on the Bethe approximation extends the application range of
              active inference to more complex behavioral tasks.},
  language = {eng},
  number   = {9},
  journal  = {Neural Computation},
  author   = {Schwöbel, Sarah and Kiebel, Stefan and Marković, Dimitrije},
  month    = sep,
  year     = {2018},
  pmid     = {29949461},
  pages    = {2530--2567},
  file     = {Schwöbel et al. - 2018 - Active Inference, Belief Propagation, and the
              Beth.pdf:/Users/apodusenko/Zotero/storage/WV3WA2M3/Schwöbel et al. -
              2018 - Active Inference, Belief Propagation, and the
              Beth.pdf:application/pdf}
}

@book{barber_bayesian_2012,
  title     = {Bayesian {Reasoning} and {Machine} {Learning}},
  url       = {http://www.cs.ucl.ac.uk/staff/d.barber/brml/},
  publisher = {Cambridge University Press},
  author    = {Barber, David},
  year      = {2012},
  file      = {Barber - 2011 - Bayesian Reasoning and Machine
               Learning.pdf:/Users/apodusenko/Zotero/storage/QZ4TQBPQ/Barber - 2011 -
               Bayesian Reasoning and Machine Learning.pdf:application/pdf;Book 25 feb
               2014:/Users/apodusenko/Zotero/storage/EVM4665P/250214.pdf:application/pdf
               }
}

@article{yedidia_generalized_nodate,
  title    = {Generalized {Belief} {Propagation}},
  abstract = {Belief propagation (BP) was only supposed to work for tree-like
              networks but works surprisingly well in many applications involving
              networks with loops, including turbo codes. However, there has been
              little understanding of the algorithm or the nature of the
              solutions it finds for general graphs.},
  language = {en},
  author   = {Yedidia, Jonathan S and Freeman, William T and Weiss, Yair},
  pages    = {7},
  file     = {Yedidia et al. - Generalized Belief
              Propagation.pdf:/Users/apodusenko/Zotero/storage/C42PRPMI/Yedidia et
              al. - Generalized Belief Propagation.pdf:application/pdf}
}

@book{kurt_bayesian_2019,
  title   = {Bayesian {Statistics} the {Fun} {Way}: {Understanding} {Statistics}
             and {Probability} with {Star} {Wars}, {LEGO}, and {Rubber} {Ducks}},
  url     = {
             https://www.amazon.com/Bayesian-Statistics-Fun-Will-Kurt-ebook/dp/B07J461Q2K/ref=sr_1_4?keywords=Doing+Bayesian+Data+Analysis%3A+A+Tutorial+with+R%2C+JAGS%2C+and+Stan&qid=1566042596&s=books&sr=1-4
             },
  urldate = {2019-08-17},
  author  = {Kurt, Will},
  year    = {2019},
  file    = {Kurt - 2019 - Bayesian Statistics the Fun Way Understanding
             Sta.pdf:/Users/apodusenko/Zotero/storage/AHFFMGC2/Kurt - 2019 -
             Bayesian Statistics the Fun Way Understanding Sta.pdf:application/pdf}
}

@article{bishop_variational_1999,
  title    = {Variational {Principal} {Components}},
  volume   = {1},
  url      = {
              https://www.microsoft.com/en-us/research/publication/variational-principal-components/
              },
  abstract = {One of the central issues in the use of principal component
              analysis (PCA) for data modelling is that of choosing the
              appropriate number of retained components. This problem was
              recently addressed through the formulation of a Bayesian treatment
              of PCA (Bishop, 1998) in terms of a probabilistic latent variable
              model. A central feature of this …},
  language = {en-US},
  urldate  = {2019-08-09},
  author   = {Bishop, Christopher},
  month    = jan,
  year     = {1999},
  file     = {Bishop - 1999 - Variational Principal
              Components.pdf:/Users/apodusenko/Zotero/storage/M6PZ5LEN/Bishop - 1999
              - Variational Principal
              Components.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/8YWZG2LP/variational-principal-components.html:text/html
              }
}

@article{kirchhoff_markov_2018,
  title      = {The {Markov} blankets of life: autonomy, active inference and the
                free energy principle},
  volume     = {15},
  issn       = {1742-5689, 1742-5662},
  shorttitle = {The {Markov} blankets of life},
  url        = {https://royalsocietypublishing.org/doi/10.1098/rsif.2017.0792},
  doi        = {10.1098/rsif.2017.0792},
  language   = {en},
  number     = {138},
  urldate    = {2019-08-07},
  journal    = {Journal of The Royal Society Interface},
  author     = {Kirchhoff, Michael and Parr, Thomas and Palacios, Ensor and Friston,
                Karl and Kiverstein, Julian},
  month      = jan,
  year       = {2018},
  pages      = {20170792},
  file       = {Full Text:/Users/apodusenko/Zotero/storage/IDIMZGTD/Kirchhoff et al. -
                2018 - The Markov blankets of life autonomy, active
                infe.pdf:application/pdf}
}

@book{mohri_foundations_2012,
  address   = {Cambridge, MA},
  series    = {Adaptive computation and machine learning series},
  title     = {Foundations of machine learning},
  isbn      = {978-0-262-01825-8},
  publisher = {MIT Press},
  author    = {Mohri, Mehryar and Rostamizadeh, Afshin and Talwalkar, Ameet},
  year      = {2012},
  keywords  = {Machine learning, Computer algorithms}
}

@incollection{mickens_unreasonable_1990,
  title        = {The {Unreasonable} {Effectiveness} of {Mathematics} in the {Natural}
                  {Sciences}},
  isbn         = {978-981-02-0233-0 978-981-4503-48-8},
  url          = {http://www.worldscientific.com/doi/abs/10.1142/9789814503488_0018},
  language     = {en},
  urldate      = {2019-07-25},
  booktitle    = {Mathematics and {Science}},
  publisher    = {WORLD SCIENTIFIC},
  author       = {Wigner, Eugene P.},
  collaborator = {Mickens, Ronald E},
  month        = aug,
  year         = {1990},
  doi          = {10.1142/9789814503488_0018},
  pages        = {291--306}
}

@article{halevy_unreasonable_2009,
  title   = {The {Unreasonable} {Effectiveness} of {Data}},
  volume  = {24},
  issn    = {1541-1672},
  url     = {http://ieeexplore.ieee.org/document/4804817/},
  doi     = {10.1109/MIS.2009.36},
  number  = {2},
  urldate = {2019-07-25},
  journal = {IEEE Intelligent Systems},
  author  = {Halevy, Alon and Norvig, Peter and Pereira, Fernando},
  month   = mar,
  year    = {2009},
  pages   = {8--12}
}

@article{hamming_unreasonable_1980,
  title    = {The {Unreasonable} {Effectiveness} of {Mathematics}},
  volume   = {87},
  issn     = {0002-9890, 1930-0972},
  url      = {https://www.tandfonline.com/doi/full/10.1080/00029890.1980.11994966},
  doi      = {10.1080/00029890.1980.11994966},
  language = {en},
  number   = {2},
  urldate  = {2019-07-25},
  journal  = {The American Mathematical Monthly},
  author   = {Hamming, R. W.},
  month    = feb,
  year     = {1980},
  pages    = {81--90},
  file     = {Submitted Version:/Users/apodusenko/Zotero/storage/KU92283Y/Hamming -
              1980 - The Unreasonable Effectiveness of
              Mathematics.pdf:application/pdf}
}

@article{sarkka_unscented_2008,
  title   = {Unscented {Rauch}–{Tung}–{Striebel} {Smoother}},
  volume  = {53},
  number  = {3},
  journal = {IEEE Transactions on Automatic Control},
  author  = {Särkkä, Simo},
  year    = {2008},
  pages   = {845--849},
  file    = {
             Snapshot:/Users/apodusenko/Zotero/storage/GYKBYDYQ/4484208.html:text/html
             }
}

@article{menegaz_systematization_2015,
  title   = {A systematization of the unscented {Kalman} filter theory},
  volume  = {60},
  number  = {10},
  journal = {IEEE Transactions on automatic control},
  author  = {Menegaz, Henrique MT and Ishihara, João Y. and Borges, Geovany A.
             and Vargas, Alessandro N.},
  year    = {2015},
  pages   = {2583--2598},
  file    = {
             Snapshot:/Users/apodusenko/Zotero/storage/RBNP4ABP/7042740.html:text/html
             }
}

@phdthesis{uhlmann_dynamic_1995,
  type       = {{PhD} {Thesis}},
  title      = {Dynamic map building and localization: {New} theoretical foundations},
  shorttitle = {Dynamic map building and localization},
  school     = {University of Oxford Oxford},
  author     = {Uhlmann, Jeffrey K.},
  year       = {1995},
  file       = {Uhlmann - 1995 - Dynamic map building and localization New
                theoret.pdf:/Users/apodusenko/Zotero/storage/C77YUBUS/Uhlmann - 1995 -
                Dynamic map building and localization New theoret.pdf:application/pdf}
}

@article{ahmed_bayesian_2012,
  title   = {Bayesian multicategorical soft data fusion for human-robot
             collaboration},
  volume  = {29},
  number  = {1},
  journal = {IEEE Transactions on Robotics},
  author  = {Ahmed, Nisar R. and Sample, Eric M. and Campbell, Mark},
  year    = {2012},
  pages   = {189--206},
  file    = {Ahmed e.a. - 2012 - Bayesian multicategorical soft data fusion for
             human-robot
             collaboration.pdf:/Users/apodusenko/Zotero/storage/YTHBN9WP/Ahmed e.a.
             - 2012 - Bayesian multicategorical soft data fusion for human-robot
             collaboration.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/CX7MKSEX/6301744.html:text/html
             }
}

@inproceedings{bouchard_efficient_2007,
  title     = {Efficient bounds for the softmax function and applications to
               approximate inference in hybrid models},
  booktitle = {{NIPS} 2007 workshop for approximate {Bayesian} inference in
               continuous/hybrid systems},
  author    = {Bouchard, Guillaume},
  year      = {2007},
  file      = {Bouchard - 2007 - Efficient bounds for the softmax function and
               appl.pdf:/Users/apodusenko/Zotero/storage/F8537QBP/Bouchard - 2007 -
               Efficient bounds for the softmax function and appl.pdf:application/pdf}
}

@article{wagenmakers_accumulative_2006,
  series   = {Special {Issue} on {Model} {Selection}: {Theoretical} {Developments}
              and {Applications}},
  title    = {Accumulative prediction error and the selection of time series models
              },
  volume   = {50},
  issn     = {0022-2496},
  url      = {http://www.sciencedirect.com/science/article/pii/S0022249606000058},
  doi      = {10.1016/j.jmp.2006.01.004},
  abstract = {This article reviews the rationale for using accumulative
              one-step-ahead prediction error (APE) as a data-driven method for
              model selection. Theoretically, APE is closely related to Bayesian
              model selection and the method of minimum description length (MDL).
              The sole requirement for using APE is that the models under
              consideration are capable of generating a prediction for the next,
              unseen data point. This means that APE may be readily applied to
              selection problems involving very complex models. APE automatically
              takes the functional form of parameters into account, and the
              ‘plug-in’ version of APE does not require the specification of
              priors. APE is particularly easy to compute for data that have a
              natural ordering, such as time series. Here, we explore the
              possibility of using APE to discriminate the short-range ARMA(1,1)
              model from the long-range ARFIMA(0,d,0) model. We also illustrate
              how APE may be used for model meta-selection, allowing one to
              choose between different model selection methods.},
  number   = {2},
  urldate  = {2019-07-08},
  journal  = {Journal of Mathematical Psychology},
  author   = {Wagenmakers, Eric-Jan and Grünwald, Peter and Steyvers, Mark},
  month    = apr,
  year     = {2006},
  keywords = {Model selection, Prediction error, Time series analysis,
              Long-range dependence, Predictive MDL, Prequential methods},
  pages    = {149--166},
  file     = {ScienceDirect
              Snapshot:/Users/apodusenko/Zotero/storage/B6ZQG4X3/S0022249606000058.html:text/html;Wagenmakers
              et al. - 2006 - Accumulative prediction error and the selection
              of.pdf:/Users/apodusenko/Zotero/storage/ZPDN79X3/Wagenmakers et al. -
              2006 - Accumulative prediction error and the selection
              of.pdf:application/pdf}
}

@article{catal_bayesian_2019,
  title   = {Bayesian policy selection using active inference},
  journal = {arXiv preprint arXiv:1904.08149},
  author  = {Catal, Ozan and Nauta, Johannes and Verbelen, Tim and Simoens,
             Pieter and Dhoedt, Bart},
  year    = {2019},
  file    = {Catal e.a. - 2019 - Bayesian policy selection using active
             inference.html:/Users/apodusenko/Zotero/storage/SHGMLSXY/Catal e.a. -
             2019 - Bayesian policy selection using active
             inference.html:text/html;Catal e.a. - 2019 - Bayesian policy selection
             using active
             inference.pdf:/Users/apodusenko/Zotero/storage/QGS4YBK3/Catal e.a. -
             2019 - Bayesian policy selection using active
             inference.pdf:application/pdf}
}

@book{izikevic_dynamical_2010,
  address    = {Cambridge, Mass.},
  edition    = {1. MIT Press paperback ed},
  series     = {Computational neuroscience},
  title      = {Dynamical systems in neuroscience: the geometry of excitability and
                bursting},
  isbn       = {978-0-262-51420-0 978-0-262-09043-8},
  shorttitle = {Dynamical systems in neuroscience},
  language   = {eng},
  publisher  = {MIT Press},
  author     = {Ižikevič, Eugene M.},
  year       = {2010},
  note       = {OCLC: 699607639},
  file       = {Table of Contents
                PDF:/Users/apodusenko/Zotero/storage/EGWDUINM/Ižikevič - 2010 -
                Dynamical systems in neuroscience the geometry of.pdf:application/pdf}
}

@book{williams_probability_2018,
  address   = {Cambridge},
  edition   = {22nd printing},
  series    = {Cambridge mathematical textbooks},
  title     = {Probability with martingales},
  isbn      = {978-0-521-40605-5},
  language  = {eng},
  publisher = {Cambridge Univ. Pr},
  author    = {Williams, David},
  year      = {2018},
  note      = {OCLC: 1080326332}
}

@book{billingsley_probability_2012,
  address   = {Hoboken, N.J},
  edition   = {Anniversary ed},
  series    = {Wiley series in probability and statistics},
  title     = {Probability and measure},
  isbn      = {978-1-118-12237-2},
  publisher = {Wiley},
  author    = {Billingsley, Patrick},
  year      = {2012},
  keywords  = {Measure theory, Probabilities}
}

@book{rudin_real_2013,
  address   = {New York, NY},
  edition   = {3. ed., internat. ed., [Nachdr.]},
  series    = {{McGraw}-{Hill} international editions {Mathematics} series},
  title     = {Real and complex analysis},
  isbn      = {978-0-07-100276-9 978-0-07-054234-1},
  language  = {eng},
  publisher = {McGraw-Hill},
  author    = {Rudin, Walter},
  year      = {2013},
  note      = {OCLC: 957461070},
  file      = {Table of Contents PDF:/Users/apodusenko/Zotero/storage/W5JXD4ZK/Rudin
               - 2013 - Real and complex analysis.pdf:application/pdf}
}

@book{ripley_statistical_1999,
  address    = {Cambridge},
  edition    = {Transferred to digital printing},
  title      = {Statistical inference for spatial processes: an essay awarded the {
                Adams} {Prize} of the {University} of {Cambridge}},
  isbn       = {978-0-521-42420-2 978-0-521-35234-5},
  shorttitle = {Statistical inference for spatial processes},
  language   = {eng},
  publisher  = {Cambridge Univ. Press},
  author     = {Ripley, Brian D.},
  year       = {1999},
  note       = {OCLC: 255331455}
}

@book{young_essentials_2005,
  address    = {Cambridge, UK ; New York},
  series     = {Cambridge series in statistical and probabilistic mathematics},
  title      = {Essentials of statistical inference: {G}.{A}. {Young}, {R}.{L}. {
                Smith}},
  isbn       = {978-0-521-83971-6},
  shorttitle = {Essentials of statistical inference},
  publisher  = {Cambridge University Press},
  author     = {Young, G. A. and Smith, Richard L.},
  year       = {2005},
  note       = {OCLC: ocm58999135},
  keywords   = {Probabilities, Mathematical statistics}
}

@book{hastie_elements_2009,
  address    = {New York, NY},
  edition    = {2nd ed},
  series     = {Springer series in statistics},
  title      = {The elements of statistical learning: data mining, inference, and
                prediction},
  isbn       = {978-0-387-84857-0 978-0-387-84858-7},
  shorttitle = {The elements of statistical learning},
  publisher  = {Springer},
  author     = {Hastie, Trevor and Tibshirani, Robert and Friedman, J. H.},
  year       = {2009},
  keywords   = {Machine learning, Bioinformatics, Inference, Statistics,
                Computational intelligence, Data mining, Forecasting, Methodology}
}

@book{rudin_principles_1976,
  address   = {New York},
  edition   = {3d ed},
  series    = {International series in pure and applied mathematics},
  title     = {Principles of mathematical analysis},
  isbn      = {978-0-07-054235-8},
  publisher = {McGraw-Hill},
  author    = {Rudin, Walter},
  year      = {1976},
  keywords  = {Mathematical analysis}
}

@book{boyd_convex_2004,
  address   = {Cambridge, UK ; New York},
  title     = {Convex optimization},
  isbn      = {978-0-521-83378-3},
  publisher = {Cambridge University Press},
  author    = {Boyd, Stephen P. and Vandenberghe, Lieven},
  year      = {2004},
  keywords  = {Convex functions, Mathematical optimization}
}

@book{jordan_graphical_2001,
  address    = {Cambridge, Mass},
  series     = {Computational neuroscience},
  title      = {Graphical models: foundations of neural computation},
  isbn       = {978-0-262-60042-2},
  shorttitle = {Graphical models},
  publisher  = {MIT Press},
  editor     = {Jordan, Michael Irwin and Sejnowski, Terrence J.},
  year       = {2001},
  keywords   = {Computer graphics, Neural networks (Computer science)}
}

@book{quinonero-candela_dataset_2009,
  address   = {Cambridge, Mass},
  series    = {Neural information processing series},
  title     = {Dataset shift in machine learning},
  isbn      = {978-0-262-17005-5},
  publisher = {MIT Press},
  editor    = {Quiñonero-Candela, Joaquin},
  year      = {2009},
  note      = {OCLC: ocn227205909},
  keywords  = {Machine learning}
}

@book{rieke_spikes:_1999,
  address    = {Cambridge, Mass.},
  edition    = {1. paperback ed},
  series     = {Computational neuroscience series},
  title      = {Spikes: exploring the neural code},
  isbn       = {978-0-262-68108-7 978-0-262-18174-7},
  shorttitle = {Spikes},
  language   = {eng},
  publisher  = {MIT Press},
  editor     = {Rieke, Fred},
  year       = {1999},
  note       = {OCLC: 633854747}
}

@book{vaart_asymptotic_2007,
  address   = {Cambridge},
  edition   = {1. paperback ed., 8. printing},
  series    = {Cambridge series in statistical and probabilistic mathematics},
  title     = {Asymptotic statistics},
  isbn      = {978-0-521-78450-4 978-0-521-49603-2},
  language  = {eng},
  publisher = {Cambridge Univ. Press},
  author    = {Vaart, Aad W. van der},
  year      = {2007},
  note      = {OCLC: 838749444}
}

@book{gerstner_spiking_2002,
  address    = {Cambridge, U.K. ; New York},
  title      = {Spiking neuron models: single neurons, populations, plasticity},
  isbn       = {978-0-521-81384-6 978-0-521-89079-3},
  shorttitle = {Spiking neuron models},
  publisher  = {Cambridge University Press},
  author     = {Gerstner, Wulfram and Kistler, Werner M.},
  year       = {2002},
  keywords   = {Neurons, Computational neuroscience, Neural networks
                (Neurobiology), Neuroplasticity}
}

@book{pollard_convergence_2012,
  address   = {Place of publication not identified},
  title     = {Convergence of stochastic processes.},
  isbn      = {978-1-4612-5254-2},
  language  = {English},
  publisher = {Springer},
  author    = {Pollard, D},
  year      = {2012},
  note      = {OCLC: 968649830}
}

@book{boucheron_concentration_2016,
  address    = {Oxford New York, NY},
  edition    = {Paperback},
  title      = {Concentration inequalities: a nonasymptotic theory of independence},
  isbn       = {978-0-19-876765-7},
  shorttitle = {Concentration inequalities},
  language   = {eng},
  publisher  = {Oxford University Press},
  author     = {Boucheron, Stéphane and Lugosi, Gábor and Massart, Pascal},
  year       = {2016},
  note       = {OCLC: 947132616},
  file       = {Table of Contents
                PDF:/Users/apodusenko/Zotero/storage/VWHBLMMT/Boucheron et al. - 2016 -
                Concentration inequalities a nonasymptotic theory.pdf:application/pdf}
}

@misc{noauthor_neural_nodate,
  title   = {Neural {Dynamics} as {Sampling}: {A} {Model} for {Stochastic} {
             Computation} in {Recurrent} {Networks} of {Spiking} {Neurons}},
  url     = {
             https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1002211
             },
  urldate = {2019-06-25}
}

@book{doya_bayesian_2006,
  title      = {Bayesian {Brain}: {Probabilistic} {Approaches} to {Neural} {Coding}},
  isbn       = {978-0-262-29418-8},
  shorttitle = {Bayesian {Brain}},
  url        = {
                https://direct.mit.edu/books/book/2884/bayesian-brainprobabilistic-approaches-to-neural
                },
  language   = {en},
  urldate    = {2019-06-24},
  publisher  = {The MIT Press},
  editor     = {Doya, Kenji and Ishii, Shin and Pouget, Alexandre and Rao, Rajesh
                P.N.},
  year       = {2006},
  doi        = {10.7551/mitpress/9780262042383.001.0001}
}

@techreport{george_d_belief_2006,
  title       = {Belief propagation and wiring length optimization as organizing
                 principles for cortical microcircuits},
  url         = {http://nemoai.narod.ru/Library/CorticalCircuits.pdf},
  abstract    = {n this paper we explore how functional and anatomical constraints
                 andresource optimization could be combined to obtain a canonical
                 corticalmicro-circuit and an explanation for its laminar
                 organization. We startwith the assumption that cortical regions are
                 involved in Bayesian BeliefPropagation. This imposes a set of
                 constraints on the type of neurons andthe connection patterns
                 between neurons in that region. In addition thereare anatomical
                 constraints that a region has to adhere to. There are sev-eral
                 different configurations of neurons consistent with both these
                 con-straints. Among all such configurations, it is reasonable to
                 expect thatNature has chosen the configuration with the minimum
                 wiring length.We cast the problem of finding the optimum
                 configuration as a combi-natorial optimization problem. A
                 near-optimal solution to this problemmatched anatomical and
                 physiological data. As the result of this inves-tigation, we
                 propose a canonical cortical micro-circuit that will
                 supportBayesian Belief Propagation computation and whose laminar
                 organiza-tion is near optimal in its wiring length. We describe how
                 the details ofthis circuit match many of the anatomical and
                 physiological findings anddiscuss the implications of these results
                 to experimenters and theorists.},
  institution = {Numenta},
  author      = {George, D and Hawkins, J},
  year        = {2006},
  pages       = {1--8},
  file        = {George, D and Hawkins, J - 2006 - Belief propagation and wiring length
                 optimization .pdf:/Users/apodusenko/Zotero/storage/IVGJ68D2/George, D
                 and Hawkins, J - 2006 - Belief propagation and wiring length
                 optimization .pdf:application/pdf}
}

@article{steimer_belief_2009,
  title    = {Belief {Propagation} in {Networks} of {Spiking} {Neurons}},
  volume   = {21},
  issn     = {0899-7667, 1530-888X},
  url      = {http://www.mitpressjournals.org/doi/10.1162/neco.2009.08-08-837},
  doi      = {10.1162/neco.2009.08-08-837},
  language = {en},
  number   = {9},
  urldate  = {2019-06-24},
  journal  = {Neural Computation},
  author   = {Steimer, Andreas and Maass, Wolfgang and Douglas, Rodney},
  month    = sep,
  year     = {2009},
  pages    = {2502--2523},
  file     = {Submitted Version:/Users/apodusenko/Zotero/storage/JTVCGG89/Steimer et
              al. - 2009 - Belief Propagation in Networks of Spiking
              Neurons.pdf:application/pdf}
}

@book{dayan_theoretical_2005,
  address    = {Cambridge, Mass.},
  edition    = {First paperback ed},
  series     = {Computational neuroscience},
  title      = {Theoretical neuroscience: computational and mathematical modeling of
                neural systems},
  isbn       = {978-0-262-54185-5},
  shorttitle = {Theoretical neuroscience},
  language   = {eng},
  publisher  = {MIT Press},
  author     = {Dayan, Peter and Abbott, L. F.},
  year       = {2005},
  note       = {OCLC: 255027508}
}

@article{parr_neuronal_2019,
  title    = {Neuronal message passing using {Mean}-field, {Bethe}, and {Marginal}
              approximations},
  volume   = {9},
  issn     = {2045-2322},
  url      = {http://www.nature.com/articles/s41598-018-38246-3},
  doi      = {10.1038/s41598-018-38246-3},
  language = {en},
  number   = {1},
  urldate  = {2019-06-22},
  journal  = {Scientific Reports},
  author   = {Parr, Thomas and Markovic, Dimitrije and Kiebel, Stefan J. and
              Friston, Karl J.},
  month    = dec,
  year     = {2019},
  pages    = {1889},
  file     = {Parr et al. - 2019 - Neuronal message passing using Mean-field, Bethe,
              .pdf:/Users/apodusenko/Zotero/storage/H6LUIBJP/Parr et al. - 2019 -
              Neuronal message passing using Mean-field, Bethe, .pdf:application/pdf}
}

@article{kullback_information_1951,
  title   = {On {Information} and {Sufficiency}},
  volume  = {22},
  issn    = {0003-4851},
  url     = {https://www.jstor.org/stable/2236703},
  number  = {1},
  urldate = {2019-06-17},
  journal = {The Annals of Mathematical Statistics},
  author  = {Kullback, S. and Leibler, R. A.},
  year    = {1951},
  pages   = {79--86}
}

@book{dauwels_1_2009,
  title      = {1 {Expectation} {Maximization} as {Message} {Passing}—{Part} {I}: {
                Principles} and {Gaussian} {Messages}},
  shorttitle = {1 {Expectation} {Maximization} as {Message} {Passing}—{Part} {I}
                },
  abstract   = {Abstract—It is shown how expectation maximization (EM) may be
                viewed as a message passing algorithm in factor graphs. In
                particular, a new general EM message computation rule is
                identified. As a factor graph tool, EM may be used to break cycles
                in a factor graph, and “nice ” messages may in some cases be
                obtained where the standard sum-product messages are unwieldy. As
                an exemplary application, the paper considers linear Gaussian state
                space models with multipliers. Such multipliers arise naturally
                from unknown model coefficients. A main attraction of EM in such
                cases is that it results in purely Gaussian message passing
                algorithms. These Gaussian EM messages are tabulated for several
                (scalar, vector, matrix) multipliers that frequently appear in
                applications. I.},
  author     = {Dauwels, Justin and Eckford, Andrew and Korl, Sascha and Loeliger,
                Hans-andrea},
  year       = {2009},
  file       = {Citeseer - Full Text
                PDF:/Users/apodusenko/Zotero/storage/WW3RQ322/Dauwels et al. - 2009 - 1
                Expectation Maximization as Message
                Passing—Part.pdf:application/pdf;Citeseer -
                Snapshot:/Users/apodusenko/Zotero/storage/DDFAJ499/summary.html:text/html
                }
}

@article{friston_free-energy_2012,
  title    = {Free-{Energy} {Minimization} and the {Dark}-{Room} {Problem}},
  volume   = {3},
  issn     = {1664-1078},
  url      = {https://www.frontiersin.org/articles/10.3389/fpsyg.2012.00130/full},
  doi      = {10.3389/fpsyg.2012.00130},
  abstract = {Recent years have seen the emergence of an important new
              fundamental theory of brain function. This theory brings
              information-theoretic, Bayesian, neuroscientific, and machine
              learning approaches into a single framework whose overarching
              principle is the minimization of surprise (or, equivalently, the
              maximization of expectation). The most comprehensive such treatment
              is the ‘free energy minimization’ formulation due to Karl Friston
              (see e.g. Friston and Stephan (2007), Friston (2010) – see also
              Thornton (2010), Fiorillo (2010) A recurrent puzzle raised by
              critics of these models is that biological systems do not seem to
              avoid surprises. We do not simply seek a dark, unchanging chamber
              and stay there. This is the ‘Dark Room Problem’. Here, we describe
              the problem and further unpack the issues to which it speaks. Using
              the same format as the prologue of Eddington’s Space, Time and
              Gravitation (Eddington 1920) we present our discussion as a
              conversation between: An Information THEORIST (Thornton) A
              PHYSICIST (Friston) A PHILOSOPHER (Clark)},
  language = {English},
  urldate  = {2019-06-13},
  journal  = {Frontiers in Psychology},
  author   = {Friston, Karl and Thornton, Christopher and Clark, Andy},
  year     = {2012},
  keywords = {surprise, Bayesian Brain, Free Energy Principle, optimality},
  file     = {Full Text PDF:/Users/apodusenko/Zotero/storage/NHUFH6EP/Friston et al.
              - 2012 - Free-Energy Minimization and the Dark-Room
              Problem.pdf:application/pdf}
}

@article{batselier_constructive_2014,
  title    = {A {Constructive} {Algorithm} for {Decomposing} a {Tensor} into a {
              Finite} {Sum} of {Orthonormal} {Rank}-1 {Terms}},
  url      = {http://arxiv.org/abs/1407.1593},
  abstract = {We propose a constructive algorithm that decomposes an arbitrary
              real tensor into a finite sum of orthonormal rank-1 outer products.
              The algorithm, named TTr1SVD, works by converting the tensor into a
              tensor-train rank-1 (TTr1) series via the singular value
              decomposition (SVD). TTr1SVD naturally generalizes the SVD to the
              tensor regime with properties such as uniqueness for a fixed order
              of indices, orthogonal rank-1 outer product terms, and easy
              truncation error quantification. Using an outer product column
              table it also allows, for the first time, a complete
              characterization of all tensors orthogonal with the original
              tensor. Incidentally, this leads to a strikingly simple
              constructive proof showing that the maximum rank of a real \$2 {
              \textbackslash}times 2 {\textbackslash}times 2\$ tensor over the
              real field is 3. We also derive a conversion of the TTr1
              decomposition into a Tucker decomposition with a sparse core
              tensor. Numerical examples illustrate each of the favorable
              properties of the TTr1 decomposition.},
  urldate  = {2019-06-06},
  journal  = {arXiv:1407.1593 [cs, math]},
  author   = {Batselier, Kim and Liu, Haotian and Wong, Ngai},
  month    = jul,
  year     = {2014},
  note     = {arXiv: 1407.1593},
  keywords = {Mathematics - Numerical Analysis, Computer Science - Numerical
              Analysis},
  file     = {arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/QB882RF8/1407.html:text/html;Batselier
              et al. - 2014 - A Constructive Algorithm for Decomposing a Tensor
              .pdf:/Users/apodusenko/Zotero/storage/V3HPNG93/Batselier et al. - 2014
              - A Constructive Algorithm for Decomposing a Tensor
              .pdf:application/pdf}
}

@misc{noauthor_autoregressive_nodate,
  title    = {Autoregressive {Hidden} {Markov} {Model} and the {Speech} {Signal} {
              \textbar} {Elsevier} {Enhanced} {Reader}},
  url      = {
              https://reader.elsevier.com/reader/sd/pii/S1877050915029816?token=F896AFB53AAA03AC877594FD96C2107848BADBDEF643C51EC68DFC13A2905478276ECA43788D7B2939416919E49F54C5
              },
  language = {en},
  urldate  = {2019-06-05},
  doi      = {10.1016/j.procs.2015.09.151},
  file     = {
              Snapshot:/Users/apodusenko/Zotero/storage/98KKYZDA/S1877050915029816.html:text/html
              }
}

@inproceedings{togami_noise_2013,
  title     = {Noise robust speech dereverberation with {Kalman} smoother},
  doi       = {10.1109/ICASSP.2013.6639110},
  abstract  = {A speech dereverberation method is proposed that is robust against
               background noise. In contrast to conventional methods based on the
               linear prediction of the given microphone input signal, in which
               the linear prediction coefficients are not fully optimized when
               there is background noise, the proposed method optimizes the
               coefficients by linear prediction of the noiseless reverberant
               speech signal even when there is background noise. The noiseless
               reverberant speech signal and the parameters are iteratively
               updated on the basis of the expectation maximization algorithm. In
               the expectation step, sufficient statistics of latent variables
               which include noiseless reverberant speech signal are estimated
               using the Kalman smoother. Unlike the standard Kalman smoother,
               which uses a time-invariant covariance matrix as a state-transition
               covariance matrix, the proposed method utilizes a time-varying
               covariance matrix, enabling it to meet the time-varying speech
               characteristics. The parameters are updated so that the Q function
               is increased in the maximization step. Experimental results show
               that the proposed method is superior to conventional methods under
               noisy conditions.},
  booktitle = {2013 {IEEE} {International} {Conference} on {Acoustics}, {Speech}
               and {Signal} {Processing}},
  author    = {Togami, M. and Kawaguchi, Y.},
  month     = may,
  year      = {2013},
  keywords  = {Kalman filters, expectation-maximisation algorithm, Noise
               measurement, Speech, microphones, Kalman smoother, Optimization,
               Covariance matrices, Microphones, speech processing, background
               noise, signal denoising, covariance matrices, Noise reduction,
               dereverberation, expectation maximization algorithm, kalman
               smoother, linear prediction coefficients, linear prediction-based
               methods, microphone input signal, noise robust speech
               dereverberation, noiseless reverberant speech signal, noisy
               conditions, Q function, Reverberation, state-transition covariance
               matrix, time-invariant covariance matrix, time-varying covariance
               matrix, time-varying speech characteristics},
  pages     = {7447--7451},
  file      = {IEEE Xplore Abstract
               Record:/Users/apodusenko/Zotero/storage/UEXJWTQT/6639110.html:text/html;IEEE
               Xplore Full Text PDF:/Users/apodusenko/Zotero/storage/JFJWTGAT/Togami
               and Kawaguchi - 2013 - Noise robust speech dereverberation with Kalman
               sm.pdf:application/pdf}
}

@article{kahneman_prospect_1979,
  title      = {Prospect {Theory}: {An} {Analysis} of {Decision} under {Risk}},
  volume     = {47},
  issn       = {00129682},
  shorttitle = {Prospect {Theory}},
  url        = {https://www.jstor.org/stable/1914185?origin=crossref},
  doi        = {10.2307/1914185},
  language   = {en},
  number     = {2},
  urldate    = {2019-06-03},
  journal    = {Econometrica},
  author     = {Kahneman, Daniel and Tversky, Amos},
  month      = mar,
  year       = {1979},
  pages      = {263},
  file       = {Kahneman and Tversky - 1979 - Prospect Theory An Analysis of Decision
                under Ris.pdf:/Users/apodusenko/Zotero/storage/WXQ6IGDY/Kahneman and
                Tversky - 1979 - Prospect Theory An Analysis of Decision under
                Ris.pdf:application/pdf}
}

@article{tagliasacchi_self-supervised_2019,
  title    = {Self-supervised audio representation learning for mobile devices},
  url      = {http://arxiv.org/abs/1905.11796},
  abstract = {We explore self-supervised models that can be potentially deployed
              on mobile devices to learn general purpose audio representations.
              Specifically, we propose methods that exploit the temporal context
              in the spectrogram domain. One method estimates the temporal gap
              between two short audio segments extracted at random from the same
              audio clip. The other methods are inspired by Word2Vec, a popular
              technique used to learn word embeddings, and aim at reconstructing
              a temporal spectrogram slice from past and future slices or,
              alternatively, at reconstructing the context of surrounding slices
              from the current slice. We focus our evaluation on small encoder
              architectures, which can be potentially run on mobile devices
              during both inference (re-using a common learned representation
              across multiple downstream tasks) and training (capturing the true
              data distribution without compromising users' privacy when combined
              with federated learning). We evaluate the quality of the embeddings
              produced by the self-supervised learning models, and show that they
              can be re-used for a variety of downstream tasks, and for some
              tasks even approach the performance of fully supervised models of
              similar size.},
  urldate  = {2019-05-29},
  journal  = {arXiv:1905.11796 [cs, eess, stat]},
  author   = {Tagliasacchi, Marco and Gfeller, Beat and Quitry, Félix de Chaumont
              and Roblek, Dominik},
  month    = may,
  year     = {2019},
  note     = {arXiv: 1905.11796},
  keywords = {Computer Science - Sound, Statistics - Machine Learning,
              Electrical Engineering and Systems Science - Audio and Speech
              Processing, Computer Science - Machine Learning},
  file     = {arXiv\:1905.11796
              PDF:/Users/apodusenko/Zotero/storage/X39FPWJS/Tagliasacchi et al. -
              2019 - Self-supervised audio representation learning for
              .pdf:application/pdf;arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/35HZN2WZ/1905.html:text/html}
}

@incollection{indelman_towards_2016,
  title     = {Towards planning in generalized belief space},
  booktitle = {Robotics {Research}},
  publisher = {Springer},
  author    = {Indelman, Vadim and Carlone, Luca and Dellaert, Frank},
  year      = {2016},
  pages     = {593--609},
  file      = {Indelman e.a. - 2016 - Towards planning in generalized belief
               space.pdf:/Users/apodusenko/Zotero/storage/DL8EAQTT/Indelman e.a. -
               2016 - Towards planning in generalized belief
               space.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/2VQZYMB8/978-3-319-28872-7_34.html:text/html
               }
}

@article{van_den_berg_motion_2012,
  title   = {Motion planning under uncertainty using iterative local optimization
             in belief space},
  volume  = {31},
  number  = {11},
  journal = {The International Journal of Robotics Research},
  author  = {Van Den Berg, Jur and Patil, Sachin and Alterovitz, Ron},
  year    = {2012},
  pages   = {1263--1278},
  file    = {
             Snapshot:/Users/apodusenko/Zotero/storage/7G8ZBFG2/0278364912456319.html:text/html;Van
             Den Berg e.a. - 2012 - Motion planning under uncertainty using
             iterative .pdf:/Users/apodusenko/Zotero/storage/FYBHCFPC/Van Den Berg
             e.a. - 2012 - Motion planning under uncertainty using iterative
             .pdf:application/pdf}
}

@article{carvalho_simulation-based_2007,
  title    = {Simulation-based sequential analysis of {Markov} switching stochastic
              volatility models},
  volume   = {51},
  issn     = {01679473},
  url      = {https://linkinghub.elsevier.com/retrieve/pii/S0167947306002349},
  doi      = {10.1016/j.csda.2006.07.019},
  abstract = {We propose a simulation-based algorithm for inference in
              stochastic volatility models with possible regime switching in
              which the regime state is governed by a ﬁrst-order Markov process.
              Using auxiliary particle ﬁlters we developed a strategy to
              sequentially learn about states and parameters of the model. The
              methodology is tested against a synthetic time series and validated
              with a real ﬁnancial time series: the IBOVESPA stock index (São
              Paulo Stock Exchange).},
  language = {en},
  number   = {9},
  urldate  = {2019-05-10},
  journal  = {Computational Statistics \& Data Analysis},
  author   = {Carvalho, Carlos M. and Lopes, Hedibert F.},
  month    = may,
  year     = {2007},
  pages    = {4526--4542},
  file     = {Carvalho and Lopes - 2007 - Simulation-based sequential analysis of
              Markov swi.pdf:/Users/apodusenko/Zotero/storage/6GWB2QSB/Carvalho and
              Lopes - 2007 - Simulation-based sequential analysis of Markov
              swi.pdf:application/pdf}
}

@inproceedings{minka_gates_2009,
  title     = {Gates},
  url       = {http://papers.nips.cc/paper/3379-gates.pdf},
  urldate   = {2019-05-09},
  booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 21},
  publisher = {Curran Associates, Inc.},
  author    = {Minka, Tom and Winn, John},
  editor    = {Koller, D. and Schuurmans, D. and Bengio, Y. and Bottou, L.},
  year      = {2009},
  pages     = {1073--1080},
  file      = {Minka and Winn - 2009 -
               Gates.pdf:/Users/apodusenko/Zotero/storage/XWSWJTNG/Minka and Winn -
               2009 - Gates.pdf:application/pdf;NIPS
               Snapshot:/Users/apodusenko/Zotero/storage/2IAU7P69/3379-gates.html:text/html
               }
}

@article{friston_perception_2012,
  title   = {Perception and self-organized instability},
  volume  = {6},
  journal = {Frontiers in computational neuroscience},
  author  = {Friston, Karl and Breakspear, Michael and Deco, Gustavo},
  year    = {2012},
  pages   = {44},
  file    = {Full
             Text:/Users/apodusenko/Zotero/storage/XQ6H9HT5/full.html:text/html}
}

@article{lu_bayesian_2018,
  title    = {Bayesian hierarchical vector autoregressive models for patient-level
              predictive modeling},
  volume   = {13},
  issn     = {1932-6203},
  url      = {
              https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0208082
              },
  doi      = {10.1371/journal.pone.0208082},
  abstract = {Predicting health outcomes from longitudinal health histories is
              of central importance to healthcare. Observational healthcare
              databases such as patient diary databases provide a rich resource
              for patient-level predictive modeling. In this paper, we propose a
              Bayesian hierarchical vector autoregressive (VAR) model to predict
              medical and psychological conditions using multivariate time series
              data. Compared to the existing patient-specific predictive VAR
              models, our model demonstrated higher accuracy in predicting future
              observations in terms of both point and interval estimates due to
              the pooling effect of the hierarchical model specification. In
              addition, by adopting an elastic-net prior, our model offers
              greater interpretability about the associations between variables
              of interest on both the population level and the patient level, as
              well as between-patient heterogeneity. We apply the model to two
              examples: 1) predicting substance use craving, negative affect and
              tobacco use among college students, and 2) predicting functional
              somatic symptoms and psychological discomforts.},
  language = {en},
  number   = {12},
  urldate  = {2019-05-07},
  journal  = {PLOS ONE},
  author   = {Lu, Feihan and Zheng, Yao and Cleveland, Harrington and Burton,
              Chris and Madigan, David},
  month    = dec,
  year     = {2018},
  keywords = {Depression, Forecasting, Fatigue, Headaches, Linear regression
              analysis, Myalgia, Psychological stress, Skeletal joints},
  pages    = {e0208082},
  file     = {Full Text PDF:/Users/apodusenko/Zotero/storage/9N67NEQD/Lu et al. -
              2018 - Bayesian hierarchical vector autoregressive
              models.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/E2BX99VI/article.html:text/html
              }
}

@misc{friston_2016_nodate,
  title   = {2016 {CCN} {Workshop}: {Predictive} {Coding}},
  url     = {https://www.youtube.com/watch?v=b1hEc6vay_k&t=4019s},
  urldate = {2019-05-01},
  author  = {Friston, Karl},
  file    = {2016 CCN Workshop\: Predictive
             Coding:/Users/apodusenko/Zotero/storage/8RFF4V5B/2016 CCN Workshop
             Predictive Coding.html:text/html}
}

@book{chen_stochastic_nodate,
  title    = {Stochastic {Filtering} {Theory}},
  abstract = {Abstract — In this self-contained survey/review paper, we
              systematically investigate the roots of Bayesian filtering as well
              as its rich leaves in the literature. Stochastic filtering theory
              is briefly reviewed with emphasis on nonlinear and non-Gaussian
              filtering. Following the Bayesian statistics, different Bayesian
              filtering techniques are developed given different scenarios. Under
              linear quadratic Gaussian circumstance, the celebrated Kalman
              filter can be derived within the Bayesian framework.
              Optimal/suboptimal nonlinear filtering techniques are extensively
              investigated. In particular, we focus our attention on the Bayesian
              filtering approach based on sequential Monte Carlo sampling, the
              so-called particle filters. Many variants of the particle filter as
              well as their features (strengths and weaknesses) are discussed.
              Related theoretical and practical issues are addressed in detail.
              In addition, some other (new) directions on Bayesian filtering are
              also explored.},
  author   = {Chen, Zhe},
  file     = {Chen - I-A Stochastic Filtering Theory...............
              2.pdf:/Users/apodusenko/Zotero/storage/JK6ASIVJ/Chen - I-A Stochastic
              Filtering Theory............... 2.pdf:application/pdf;Citeseer -
              Snapshot:/Users/apodusenko/Zotero/storage/JVJQGJCN/summary.html:text/html
              }
}

@misc{senoz_constrained_nodate,
  title    = {Constrained {Free} {Energy} {Minimization}},
  language = {en},
  author   = {Senoz, Ismail},
  file     = {Senoz - Constrained Free Energy
              Minimization.pdf:/Users/apodusenko/Zotero/storage/5TUFIFFN/Senoz -
              Constrained Free Energy Minimization.pdf:application/pdf}
}

@article{ueltzhoffer_deep_2018,
  title    = {Deep {Active} {Inference}},
  issn     = {0340-1200, 1432-0770},
  url      = {http://arxiv.org/abs/1709.02341},
  doi      = {10.1007/s00422-018-0785-7},
  abstract = {This work combines the free energy principle from cognitive
              neuroscience and the ensuing active inference dynamics with recent
              advances in variational inference on deep generative models and
              evolution strategies as eﬃcient large-scale black-box optimisation
              technique, to introduce the ”deep active inference” agent. This
              agent tries to minimize a variational free energy bound on the
              average surprise of its sensations, which is motivated by a
              homeostatic argument. It does so by changing the parameters of its
              generative model, together with a variational density approximating
              the posterior distribution over latent variables, given its
              observations, and by acting on its environment to actively sample
              input that is likely under its generative model. The internal
              dynamics of the agent are implemented using deep neural networks,
              as used in machine learning, and recurrent dynamics, making the
              deep active inference agent a scalable and very ﬂexible class of
              active inference agents. Using the mountaincar problem, we show how
              goal-directed behaviour can be implemented by deﬁning sensible
              prior expectations on the latent states in the agent’s model, that
              it will try to fulﬁl. Furthermore, we show that the deep active
              inference agent can learn a generative model of the environment,
              which can be sampled from to understand the agent’s beliefs about
              the environment and its interaction with it.},
  language = {en},
  urldate  = {2018-10-29},
  journal  = {Biological Cybernetics},
  author   = {Ueltzhöffer, Kai},
  month    = oct,
  year     = {2018},
  keywords = {Quantitative Biology - Neurons and Cognition},
  file     = {Ueltzhöffer - 2018 - Deep Active
              Inference.pdf:/Users/apodusenko/Zotero/storage/HAQLC3I6/Ueltzhöffer -
              2018 - Deep Active Inference.pdf:application/pdf}
}

@article{jitkrittum_kernel-based_2015,
  title   = {Kernel-based just-in-time learning for passing expectation
             propagation messages},
  journal = {arXiv preprint arXiv:1503.02551},
  author  = {Jitkrittum, Wittawat and Gretton, Arthur and Heess, Nicolas and
             Eslami, S. M. and Lakshminarayanan, Balaji and Sejdinovic, Dino and
             Szabó, Zoltán},
  year    = {2015},
  file    = {Jitkrittum e.a. - 2015 - Kernel-based just-in-time learning for
             passing exp.pdf:/Users/apodusenko/Zotero/storage/9E6ZPE7C/Jitkrittum
             e.a. - 2015 - Kernel-based just-in-time learning for passing
             exp.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/F53TT95E/1503.html:text/html
             }
}

@book{boltzmann_vorlesungen_1898,
  title      = {Vorlesungen über {Gastheorie}: {Th}. {Theorie} van der {Waals}'; {
                Gase} mit zusammengesetzten {Molekülen}; {Gasdissociation}; {
                Schlussbemerkungen}},
  shorttitle = {Vorlesungen über {Gastheorie}},
  language   = {de},
  publisher  = {J. A. Barth},
  author     = {Boltzmann, Ludwig},
  year       = {1898}
}

@inproceedings{mathys_uncertainty_2014-1,
  title     = {Uncertainty, precision, and prediction errors},
  booktitle = {{UCL} {Computational} {Psychiatry} {Course}},
  author    = {Mathys, Christoph D.},
  year      = {2014},
  file      = {Mathys - 2014 - Uncertainty, precision, and prediction
               errors.pdf:/Users/apodusenko/Zotero/storage/G2IQPXV5/Mathys - 2014 -
               Uncertainty, precision, and prediction errors.pdf:application/pdf}
}

@book{schott_matrix_2016,
  title     = {Matrix analysis for statistics},
  publisher = {John Wiley \& Sons},
  author    = {Schott, James R.},
  year      = {2016},
  file      = {
               Snapshot:/Users/apodusenko/Zotero/storage/RWXUA762/books.html:text/html
               }
}

@inproceedings{heskes_incremental_2005,
  title     = {Incremental utility elicitation for adaptive personalization},
  booktitle = {{BNIAC}},
  publisher = {Contactforum KVAB},
  author    = {Heskes, T. and de Vries, B.},
  year      = {2005},
  pages     = {127--134},
  file      = {
               Snapshot:/Users/apodusenko/Zotero/storage/NYNAHTAE/incremental-utility-elicitation-for-adaptive-personalization.html:text/html
               }
}

@inproceedings{azizi_data-driven_2015,
  title     = {A data-driven forgetting factor for stabilized forgetting in
               approximate {Bayesian} filtering},
  doi       = {10.1109/ISSC.2015.7163747},
  abstract  = {The main focus of this paper is to extend Bayesian filtering to
               allow for time-variant parameters in the transition kernels. Since
               a finite-dimensional exact solution is not available, we adopt
               stabilized forgetting in order to restore a recursive signal
               processing algorithm in this case, involving the processing of
               fixed, finite-dimensional statistics. This approximate solution is
               amenable to online sequential estimation, and is derived for a rich
               class of observation models. The data-driven forgetting factor is
               optimized sequentially using an iterative variational Bayes
               approach. A number of Bayesian filtering problems involving
               parameter-variant Gaussian processes is addressed in this way. In
               simulations, we emphasize the performance enhancements achieved
               using the data-driven sequential assignment of the forgetting
               factor, when compared to the conventional approach, which adopts a
               fixed value.},
  booktitle = {Signals and {Systems} {Conference} ({ISSC}), 2015 26th {Irish}},
  author    = {Azizi, S. and Quinn, A.},
  month     = jun,
  year      = {2015},
  keywords  = {Noise, Bayesian filtering, Bayes methods, iterative methods,
               Gaussian processes, computational modeling, Signal processing,
               Approximate Bayesian filtering, Approximation methods, Context,
               data-driven forgetting factor, finite-dimensional exact solution,
               finite-dimensional statistics, iterative variational Bayes, online
               sequential estimation, parameter-variant Gaussian processes,
               recursive signal processing algorithm, stabilized forgetting,
               time-variant parameter},
  pages     = {1--6},
  file      = {Azizi and Quinn - 2015 - A data-driven forgetting factor for
               stabilized for.pdf:/Users/apodusenko/Zotero/storage/49F4DIQN/Azizi and
               Quinn - 2015 - A data-driven forgetting factor for stabilized
               for.pdf:application/pdf;IEEE Xplore Abstract
               Record:/Users/apodusenko/Zotero/storage/H2JIRTDC/login.html:text/html}
}

@inproceedings{azizi_approximate_2015,
  title     = {Approximate {Bayesian} {Filtering} using {Stabilized} {Forgetting}},
  booktitle = {23rd {European} {Signal} {Processing} {Conference} ({EUSIPCO})},
  author    = {Azizi, S. and Quinn, A.},
  year      = {2015},
  file      = {Azizi, S. and Quinn, A. - 2015 - Approximate Bayesian Filtering using
               Stabilized Fo.pdf:/Users/apodusenko/Zotero/storage/R9E4LXCZ/Azizi, S.
               and Quinn, A. - 2015 - Approximate Bayesian Filtering using Stabilized
               Fo.pdf:application/pdf}
}

@book{dillon_hearing_2012,
  address   = {Sydney},
  edition   = {2},
  title     = {Hearing {Aids}},
  isbn      = {978-1-60406-810-8},
  abstract  = {Praise for this book:This book succeeds in its aim of providing a
               practically useful and theoretically sound comprehensive book on
               hearing aids ... It makes the most complicated concepts easy to
               understand, with excellent cross referencing ... I certainly
               wouldn't want to be without this book and I highly recommend it to
               everyone with an interest in hearing aids. -- Pauline Smith, ENT \&
               Audiology News Key Features: Completely revised to reflect the
               research and technological advances of the last decadeNew chapters
               on directional microphones and the latest digital signal processing
               strategies Extensive coverage of all aspects of open-canal,
               thin-tube hearing aids Practical tips, tables, and procedures
               designed to be pinned on the walls of clinicsEach cross-referenced
               chapter builds on the previous chaptersHearing Aids, Second Edition
               , is a book within a book: Each chapter has a one-page synopsis
               that captures the key concepts of each topicThe material that
               students most need is contained in marked paragraphs that flow
               after each other to form a coherent thin book inside the larger
               book Intervening additional paragraphs add satisfying depthWritten,
               comprehensively referenced, and extensively reviewed by leaders in
               the field, this book is ideal as a core graduate text as well as a
               standard reference for clinicians.},
  language  = {English},
  publisher = {Thieme},
  author    = {Dillon, Harvey},
  month     = jun,
  year      = {2012}
}

@book{kates_digital_2008,
  address   = {San Diego},
  edition   = {1},
  title     = {Digital {Hearing} {Aids}},
  isbn      = {978-1-59756-317-8},
  language  = {English},
  publisher = {Plural Publishing},
  author    = {Kates, James},
  month     = mar,
  year      = {2008}
}

@patent{kudo_pedometer_2007,
  title        = {Pedometer for pets},
  url          = {http://www.freepatentsonline.com/7246033.html},
  abstract     = {An exercise monitoring pedometer for pets is directed toward
                  measuring a pet's exercise over some, period of time, such as a day
                  or week or month. The pet pedometer includes a solid state
                  three-axis accelerometer, a signal processing unit, a CPU, a memory
                  chip and a display with settable controls, and may include a voice
                  recorder/player; or these functions may reside mainly on an
                  application specific integrated circuit. The settable controls are
                  directed toward providing a setting for pet stride size for
                  conversion for walking and running, and for manual resetting. The
                  pet pedometer auto-selects automatically for a pet's stride both a
                  walking stride and a running stride. The present invention may also
                  contain a recorder, typically a solid state recorder, which provide
                  for a recording of the pet “owner's” voice, or selected music, so
                  that the owner may record encouragement, etc., to his/her pet.},
  nationality  = {United States},
  number       = {7246033},
  urldate      = {2019-04-03},
  author       = {Kudo, Susan Leeds},
  collaborator = {Bui, Bryan and Seidman, Abraham N.},
  month        = jul,
  year         = {2007},
  note         = {U.S. Patent 7246033},
  file         = {
                  Snaptshot:/Users/apodusenko/Zotero/storage/XWP5PYNN/7246033.html:text/html
                  }
}

@misc{lamkin_wearable_nodate,
  title    = {Wearable {Tech} {Market} {To} {Double} {By} 2021},
  url      = {
              https://www.forbes.com/sites/paullamkin/2017/06/22/wearable-tech-market-to-double-by-2021/
              },
  abstract = {Smart clothing and connected watches will be the main players in
              an ever expanding industry.},
  language = {en},
  urldate  = {2019-04-03},
  journal  = {Forbes},
  author   = {Lamkin, Paul},
  note     = {Forbes,
              https://www.forbes.com/sites/paullamkin/2017/06/22/wearable-tech-market-to-double-by-2021/
              , last accessed on 3-4-2019},
  file     = {
              Snapshot:/Users/apodusenko/Zotero/storage/4Y5FEWR6/wearable-tech-market-to-double-by-2021.html:text/html
              }
}

@article{holmes_marss:_2012,
  title      = {{MARSS}: {Multivariate} {Autoregressive} {State}-space {Models} for {
                Analyzing} {Time}-series {Data}},
  volume     = {4},
  issn       = {2073-4859},
  shorttitle = {{MARSS}},
  url        = {https://journal.r-project.org/archive/2012/RJ-2012-002/index.html},
  doi        = {10.32614/RJ-2012-002},
  abstract   = {MARSS is a package for ﬁtting multivariate autoregressive
                state-space models to time-series data. The MARSS package
                implements state-space models in a maximum likelihood framework.
                The core functionality of MARSS is based on likelihood maximization
                using the Kalman ﬁlter/smoother, combined with an EM algorithm. To
                make comparisons with other packages available, parameter
                estimation is also permitted via direct search routines available
                in ’optim’. The MARSS package allows data to contain missing values
                and allows a wide variety of model structures and constraints to be
                speciﬁed (such as ﬁxed or shared parameters). In addition to
                model-ﬁtting, the package provides bootstrap routines for
                simulating data and generating conﬁdence intervals, and multiple
                options for calculating model selection criteria (such as AIC).},
  language   = {en},
  number     = {1},
  urldate    = {2019-03-27},
  journal    = {The R Journal},
  author     = {Holmes, E., Elizabeth and Ward, J., Eric and Wills, Kellie},
  year       = {2012},
  pages      = {11},
  file       = {Holmes et al. - 2012 - MARSS Multivariate Autoregressive State-space
                Mod.pdf:/Users/apodusenko/Zotero/storage/IZHN2MEG/Holmes et al. - 2012
                - MARSS Multivariate Autoregressive State-space Mod.pdf:application/pdf
                }
}

@inproceedings{petersen_approximate_2018,
  title     = {On {Approximate} {Nonlinear} {Gaussian} {Message} {Passing} on {
               Factor} {Graphs}},
  doi       = {10.1109/SSP.2018.8450699},
  abstract  = {Factor graphs have recently gained increasing attention as a
               unified framework for representing and constructing algorithms for
               signal processing, estimation, and control. One capability that
               does not seem to be well explored within the factor graph tool kit
               is the ability to handle deterministic nonlinear transformations,
               such as those occuring in nonlinear filtering and smoothing
               problems, using tabulated message passing rules. In this
               contribution, we provide general forward (filtering) and backward
               (smoothing) approximate Gaussian message passing rules for
               deterministic nonlinear transformation nodes in arbitrary factor
               graphs fulfilling a Markov property, based on numerical quadrature
               procedures for the forward pass and a Rauch-Tung-Striebel-type
               approximation of the backward pass. These message passing rules can
               be employed for deriving many algorithms for solving nonlinear
               problems using factor graphs, as is illustrated by the proposition
               of a nonlinear modified Bryson-Frazier (MBF) smoother based on the
               presented message passing rules.},
  booktitle = {2018 {IEEE} {Statistical} {Signal} {Processing} {Workshop} ({SSP}
               )},
  author    = {Petersen, E. and Hoffmann, C. and Rostalski, P.},
  month     = jun,
  year      = {2018},
  keywords  = {Message passing, Approximation algorithms, Signal processing
               algorithms, Markov processes, Conferences, Smoothing methods,
               Signal processing, Factor Graphs, message Passing, nonlinear
               Filtering, nonlinear Smoothing, sigma Point Filtering},
  pages     = {513--517},
  file      = {IEEE Xplore Abstract
               Record:/Users/apodusenko/Zotero/storage/JXLWGCSC/8450699.html:text/html;Petersen
               et al. - 2018 - On Approximate Nonlinear Gaussian Message Passing
               .pdf:/Users/apodusenko/Zotero/storage/8BQRIZJC/Petersen et al. - 2018 -
               On Approximate Nonlinear Gaussian Message Passing .pdf:application/pdf}
}

@article{sahu_hierarchical_2012,
  title    = {Hierarchical {Bayesian} autoregressive models for large space-time
              data with applications to ozone concentration modelling},
  volume   = {28},
  issn     = {15241904},
  url      = {http://doi.wiley.com/10.1002/asmb.1951},
  doi      = {10.1002/asmb.1951},
  abstract = {Increasingly large volumes of space-time data are collected
              everywhere by mobile computing applications, and in many of these
              cases temporal data are obtained by registering events, for example
              telecommunication or web traﬃc data. Having both the spatial and
              temporal dimensions adds substantial complexity to data analysis
              and inference tasks. The computational complexity increases rapidly
              for ﬁtting Bayesian hierarchical models, as such a task involves
              repeated inversion of large matrices. The primary focus of this
              paper is on developing space-time auto-regressive models under the
              hierarchical Bayesian setup. To handle large data sets, a recently
              developed Gaussian predictive process approximation method
              (Banerjee et al. [1]) is extended to include auto-regressive terms
              of latent space-time processes. Speciﬁcally, a spacetime
              auto-regressive process, supported on a set of a smaller number of
              knot locations, is spatially interpolated to approximate the
              original space-time process. The resulting model is speciﬁed within
              a hierarchical Bayesian framework and Markov chain Monte Carlo
              techniques are used to make inference. The proposed model is
              applied for analysing the daily maximum 8-hour average ground level
              ozone concentration data from 1997 to 2006 from a large study
              region in the eastern United States. The developed methods allow
              accurate spatial prediction of a temporally aggregated ozone
              summary, known as the primary ozone standard, along with its
              uncertainty, at any unmonitored location during the study period.
              Trends in spatial patterns of many features of the posterior
              predictive distribution of the primary standard, such as the
              probability of non-compliance with respect to the standard, are
              obtained and illustrated.},
  language = {en},
  number   = {5},
  urldate  = {2019-03-21},
  journal  = {Applied Stochastic Models in Business and Industry},
  author   = {Sahu, Sujit Kumar and Bakar, Khandoker Shuvo},
  month    = sep,
  year     = {2012},
  pages    = {395--415},
  file     = {Sahu and Bakar - 2012 - Hierarchical Bayesian autoregressive models
              for la.pdf:/Users/apodusenko/Zotero/storage/8QF88H3C/Sahu and Bakar -
              2012 - Hierarchical Bayesian autoregressive models for
              la.pdf:application/pdf}
}

@inproceedings{loeliger_signal_2013,
  title      = {Signal processing with factor graphs: {Beamforming} and {Hilbert}
                transform},
  shorttitle = {Signal processing with factor graphs},
  doi        = {10.1109/ITA.2013.6502952},
  abstract   = {Continuous-time linear state space models with discrete-time
                observations enable digital estimation of continuous-time signals
                with arbitrary temporal resolution by means of Kalman
                filtering/smoothing or Gaussian message passing in the
                corresponding factor graph. In this paper, we demonstrate the
                application of this approach to time-domain sensor array processing
                and to an emulation of the Hilbert transform.},
  booktitle  = {2013 {Information} {Theory} and {Applications} {Workshop} ({ITA})
                },
  author     = {Loeliger, H. and Reller, C.},
  month      = feb,
  year       = {2013},
  keywords   = {Estimation, Kalman filters, smoothing methods, factor graphs,
                graph theory, message passing, Vectors, time-domain analysis,
                Kalman filtering, Array signal processing, array signal processing,
                beamforming, signal processing, Kalman smoothing, Transforms,
                arbitrary temporal resolution, Computational modeling,
                continuous-time linear state space models, digital continuous-time
                signal estimation, discrete-time observations, Gaussian message
                passing, Hilbert transform, Hilbert transforms, time-domain sensor
                array processing},
  pages      = {1--4},
  file       = {Loeliger and Reller - 2013 - Signal processing with factor graphs
                Beamforming .html:/Users/apodusenko/Zotero/storage/6VYBY346/Loeliger
                and Reller - 2013 - Signal processing with factor graphs Beamforming
                .html:text/html;Loeliger and Reller - 2013 - Signal processing with
                factor graphs Beamforming
                .pdf:/Users/apodusenko/Zotero/storage/GMNZNV2L/Loeliger and Reller -
                2013 - Signal processing with factor graphs Beamforming
                .pdf:application/pdf}
}

@article{millidge_combining_nodate,
  title      = {Combining {Active} {Inference} and {Hierarchical} {Predictive} {
                Coding}: {A} {Tutorial} {Introduction} and {Case} {Study}},
  shorttitle = {Combining {Active} {Inference} and {Hierarchical} {Predictive} {
                Coding}},
  url        = {https://osf.io/kf6wc},
  doi        = {10.31234/osf.io/kf6wc},
  abstract   = {This paper combines the active inference formulation of action
                (Friston, 2009) with hierarchical predictive coding models (Friston
                , 2003) to provide a proof-of-concept implementation of an active
                inference agent able to solve a common reinforcement learning
                baseline -- the cart-pole environment in OpenAI gym. It
                demonstrates empirically that predictive coding and active
                inference approaches can be successfully scaled up to tasks more
                challenging than the mountain car (Friston 2009, 2012). We show
                that hierarchical predictive coding models can be learned from
                scratch during the task, and can successfully drive action
                selection via active inference. To our knowledge, it is the first
                implemented active inference agent to combine active inference with
                a hierarchical predictive coding perceptual model. We also provide
                a tutorial walk-through of the free-energy principle, hierarchical
                predictive coding, and active inference, including an in-depth
                derivation of our agent.},
  urldate    = {2019-03-19},
  author     = {Millidge, Beren}
}

@article{bishop_variational_2013,
  title    = {Variational {Relevance} {Vector} {Machines}},
  url      = {http://arxiv.org/abs/1301.3838},
  abstract = {The Support Vector Machine (SVM) of Vapnik (1998) has become
              widely established as one of the leading approaches to pattern
              recognition and machine learning. It expresses predictions in terms
              of a linear combination of kernel functions centred on a subset of
              the training data, known as support vectors. Despite its widespread
              success, the SVM suffers from some important limitations, one of
              the most significant being that it makes point predictions rather
              than generating predictive distributions. Recently Tipping (1999)
              has formulated the Relevance Vector Machine (RVM), a probabilistic
              model whose functional form is equivalent to the SVM. It achieves
              comparable recognition accuracy to the SVM, yet provides a full
              predictive distribution, and also requires substantially fewer
              kernel functions. The original treatment of the RVM relied on the
              use of type II maximum likelihood (the `evidence framework') to
              provide point estimates of the hyperparameters which govern model
              sparsity. In this paper we show how the RVM can be formulated and
              solved within a completely Bayesian paradigm through the use of
              variational inference, thereby giving a posterior distribution over
              both parameters and hyperparameters. We demonstrate the
              practicality and performance of the variational RVM using both
              synthetic and real world examples.},
  urldate  = {2019-03-18},
  journal  = {arXiv:1301.3838 [cs, stat]},
  author   = {Bishop, Christopher M. and Tipping, Michael},
  month    = jan,
  year     = {2013},
  note     = {arXiv: 1301.3838},
  keywords = {Statistics - Machine Learning, Computer Science - Machine Learning
              },
  file     = {Bishop and Tipping - 2013 - Variational Relevance Vector
              Machines.html:/Users/apodusenko/Zotero/storage/979C7Q4N/Bishop and
              Tipping - 2013 - Variational Relevance Vector
              Machines.html:text/html;Bishop and Tipping - 2013 - Variational
              Relevance Vector
              Machines.pdf:/Users/apodusenko/Zotero/storage/DX294L4Y/Bishop and
              Tipping - 2013 - Variational Relevance Vector
              Machines.pdf:application/pdf}
}

@article{godsill_bayesian_1997,
  title    = {Bayesian {Enhancement} of {Speech} and {Audio} {Signals} which can be
              {Modelled} as {ARMA} {Processes}},
  volume   = {65},
  issn     = {1751-5823},
  url      = {
              https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1751-5823.1997.tb00365.x
              },
  doi      = {10.1111/j.1751-5823.1997.tb00365.x},
  abstract = {In application areas which involve digitised speech and audio
              signals, such as coding, digital remastering of old recordings and
              recognition of speech, it is often desirable to reduce the effects
              of noise with the aim of enhancing intelligibility and perceived
              sound quality. We consider the case where noise sources contain
              non-Gaussian, impulsive elements superimposed upon a continuous
              Gaussian background. Such a situation arises in areas such as
              communications channels, telephony and gramophone recordings where
              impulsive effects might be caused by electromagnetic interference
              (lightning strikes), electrical switching noise or defects in
              recording media, while electrical circuit noise or the combined
              effect of many distant atmospheric events lead to a continuous
              Gaussian component. In this paper we discuss the background to this
              type of noise degradation and describe briefly some existing
              statistical techniques for noise reduction. We propose new methods
              for enhancement based upon Markov chain Monte Carlo (MCMC)
              simulation. Signals are modelled as autoregressive moving-average
              (ARMA); while noise sources are treated as discrete and continuous
              mixtures of Gaussian distributions. Results are presented for both
              real and artificially corrupted data sequences, illustrating the
              potential of the new methods.},
  language = {en},
  number   = {1},
  urldate  = {2020-06-17},
  journal  = {International Statistical Review},
  author   = {Godsill, Simon J.},
  year     = {1997},
  keywords = {Markov chain Monte Carlo, noise reduction, ARMA model, impulsive
              noise, outliers, robust},
  pages    = {1--21},
  file     = {Full Text PDF:/Users/apodusenko/Zotero/storage/95VJA2LJ/Godsill - 1997
              - Bayesian Enhancement of Speech and Audio Signals
              w.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/CPLK43NH/j.1751-5823.1997.tb00365.html:text/html
              }
}

@article{van_de_laar_application_2019,
  title   = {Application of the {Free} {Energy} {Principle} to {Estimation} and {
             Control}},
  journal = {arXiv preprint arXiv:1910.09823},
  author  = {van de Laar, Thijs and Özçelikkale, Ayça and Wymeersch, Henk},
  year    = {2019},
  file    = {
             Snapshot:/Users/apodusenko/Zotero/storage/AB9IJY98/1910.html:text/html;van
             de Laar et al. - 2019 - Application of the Free Energy Principle to
             Estima.pdf:/Users/apodusenko/Zotero/storage/MS96PCE8/van de Laar et al.
             - 2019 - Application of the Free Energy Principle to
             Estima.pdf:application/pdf}
}

@article{alameda-pineda_variational_2020,
  title    = {Variational {Inference} and {Learning} of {Piecewise}-linear {
              Dynamical} {Systems}},
  url      = {http://arxiv.org/abs/2006.01668},
  abstract = {Modeling the temporal behavior of data is of primordial importance
              in many scientific and engineering fields. The baseline method
              assumes that both the dynamic and observation models follow
              linear-Gaussian models. Non-linear extensions lead to intractable
              solvers. It is also possible to consider several linear models, or
              a piecewise linear model, and to combine them with a switching
              mechanism, which is also intractable because of the exponential
              explosion of the number of Gaussian components. In this paper, we
              propose a variational approximation of piecewise linear dynamic
              systems. We provide full details of the derivation of a variational
              expectation-maximization algorithm that can be used either as a
              filter or as a smoother. We show that the model parameters can be
              split into two sets, a set of static (or observation parameters)
              and a set of dynamic parameters. The immediate consequences are
              that the former set can be estimated off-line and that the number
              of linear models (or the number of states of the switching
              variable) can be learned based on model selection. We apply the
              proposed method to the problem of visual tracking and we thoroughly
              compare our algorithm with several visual trackers applied to the
              problem of head-pose estimation.},
  urldate  = {2020-06-03},
  journal  = {arXiv:2006.01668 [cs, stat]},
  author   = {Alameda-Pineda, Xavier and Drouard, Vincent and Horaud, Radu},
  month    = jun,
  year     = {2020},
  note     = {arXiv: 2006.01668},
  keywords = {Statistics - Machine Learning, Computer Science - Computer Vision
              and Pattern Recognition, Computer Science - Machine Learning},
  file     = {arXiv Fulltext
              PDF:/Users/apodusenko/Zotero/storage/MRSBLWLQ/Alameda-Pineda et al. -
              2020 - Variational Inference and Learning of
              Piecewise-li.pdf:application/pdf;arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/EMAJAQQZ/2006.html:text/html}
}

@article{hughes_why_1989,
  title    = {Why {Functional} {Programming} {Matters}},
  volume   = {32},
  issn     = {0010-4620, 1460-2067},
  url      = {
              https://academic.oup.com/comjnl/article-lookup/doi/10.1093/comjnl/32.2.98
              },
  doi      = {10.1093/comjnl/32.2.98},
  abstract = {As software becomes more and more complex, it is more and more
              important to structure it well. Well-structured software is easy to
              write and to debug, and provides a collection of modules that can
              be reused to reduce future programming costs. In this paper we show
              that two features of functional languages in particular,
              higher-order functions and lazy evaluation, can contribute
              signiﬁcantly to modularity. As examples, we manipulate lists and
              trees, program several numerical algorithms, and implement the
              alpha-beta heuristic (an algorithm from Artiﬁcial Intelligence used
              in game-playing programs). We conclude that since modularity is the
              key to successful programming, functional programming oﬀers
              important advantages for software development.},
  language = {en},
  number   = {2},
  urldate  = {2020-06-01},
  journal  = {The Computer Journal},
  author   = {Hughes, J.},
  month    = feb,
  year     = {1989},
  pages    = {98--107},
  file     = {Hughes - 1989 - Why Functional Programming
              Matters.pdf:/Users/apodusenko/Zotero/storage/QZBT98MT/Hughes - 1989 -
              Why Functional Programming Matters.pdf:application/pdf}
}

@article{ma_complete_2015,
  title    = {A {Complete} {Recipe} for {Stochastic} {Gradient} {MCMC}},
  url      = {http://arxiv.org/abs/1506.04696},
  abstract = {Many recent Markov chain Monte Carlo (MCMC) samplers leverage
              continuous dynamics to deﬁne a transition kernel that efﬁciently
              explores a target distribution. In tandem, a focus has been on
              devising scalable variants that subsample the data and use
              stochastic gradients in place of full-data gradients in the dynamic
              simulations. However, such stochastic gradient MCMC samplers have
              lagged behind their full-data counterparts in terms of the
              complexity of dynamics considered since proving convergence in the
              presence of the stochastic gradient noise is nontrivial. Even with
              simple dynamics, signiﬁcant physical intuition is often required to
              modify the dynamical system to account for the stochastic gradient
              noise. In this paper, we provide a general recipe for constructing
              MCMC samplers—including stochastic gradient versions—based on
              continuous Markov processes speciﬁed via two matrices. We
              constructively prove that the framework is complete. That is, any
              continuous Markov process that provides samples from the target
              distribution can be written in our framework. We show how previous
              continuous-dynamic samplers can be trivially “reinvented” in our
              framework, avoiding the complicated sampler-speciﬁc proofs. We
              likewise use our recipe to straightforwardly propose a new
              state-adaptive sampler: stochastic gradient Riemann Hamiltonian
              Monte Carlo (SGRHMC). Our experiments on simulated data and a
              streaming Wikipedia analysis demonstrate that the proposed SGRHMC
              sampler inherits the beneﬁts of Riemann HMC, with the scalability
              of stochastic gradient methods.},
  language = {en},
  urldate  = {2020-02-13},
  journal  = {arXiv:1506.04696 [math, stat]},
  author   = {Ma, Yi-An and Chen, Tianqi and Fox, Emily B.},
  month    = oct,
  year     = {2015},
  note     = {arXiv: 1506.04696},
  keywords = {Statistics - Machine Learning, Mathematics - Statistics Theory,
              Statistics - Methodology},
  file     = {Ma et al. - 2015 - A Complete Recipe for Stochastic Gradient
              MCMC.pdf:/Users/apodusenko/Zotero/storage/JU27PMWT/Ma et al. - 2015 - A
              Complete Recipe for Stochastic Gradient MCMC.pdf:application/pdf}
}

@article{gannot_iterative_1998,
  title    = {Iterative and sequential {Kalman} filter-based speech enhancement
              algorithms},
  volume   = {6},
  issn     = {1558-2353},
  doi      = {10.1109/89.701367},
  abstract = {Speech quality and intelligibility might significantly deteriorate
              in the presence of background noise, especially when the speech
              signal is subject to subsequent processing. In particular, speech
              coders and automatic speech recognition (ASR) systems that were
              designed or trained to act on clean speech signals might be
              rendered useless in the presence of background noise. Speech
              enhancement algorithms have therefore attracted a great deal of
              interest. In this paper, we present a class of Kalman filter-based
              algorithms with some extensions, modifications, and improvements of
              previous work. The first algorithm employs the estimate-maximize
              (EM) method to iteratively estimate the spectral parameters of the
              speech and noise parameters. The enhanced speech signal is obtained
              as a byproduct of the parameter estimation algorithm. The second
              algorithm is a sequential, computationally efficient, gradient
              descent algorithm. We discuss various topics concerning the
              practical implementation of these algorithms. Extensive
              experimental study using real speech and noise signals is provided
              to compare these algorithms with alternative speech enhancement
              algorithms, and to compare the performance of the iterative and
              sequential algorithms.},
  number   = {4},
  journal  = {IEEE Transactions on Speech and Audio Processing},
  author   = {Gannot, S. and Burshtein, D. and Weinstein, E.},
  month    = jul,
  year     = {1998},
  keywords = {Kalman filters, Parameter estimation, Iterative algorithms,
              iterative methods, Equations, Background noise, Speech enhancement,
              intelligibility, speech quality, Speech processing, spectral
              analysis, Maximum likelihood estimation, Automatic speech
              recognition, speech enhancement, background noise, Wiener filter,
              parameter estimation, acoustic noise, automatic speech recognition,
              estimate-maximize method, gradient descent algorithm, iterative
              Kalman filter-based speech enhancement algorithm, noise parameters,
              sequential Kalman filter-based speech enhancement algorithm,
              spectral parameters, speech coders, speech signal},
  pages    = {373--385},
  file     = {IEEE Xplore Abstract
              Record:/Users/apodusenko/Zotero/storage/I96X5ZCI/701367.html:text/html;IEEE
              Xplore Full Text PDF:/Users/apodusenko/Zotero/storage/AGAJL5MN/Gannot
              et al. - 1998 - Iterative and sequential Kalman filter-based
              speec.pdf:application/pdf}
}

@inproceedings{mathe_speech_2012,
  title     = {Speech enhancement using {Kalman} {Filter} for white, random and
               color noise},
  doi       = {10.1109/ICDCSyst.2012.6188703},
  abstract  = {In this paper, we investigated the enhancement of speech by
               applying kalman filter. Noise removal is very important in many
               applications like telephone conversation, speech recognition, etc.
               The corruption of speech due to presence of additive background
               noise causes severe difficulties in various communication
               environments. If the background noise is evolving more slowly than
               the speech, i.e., if the noise is more stationary than the speech,
               it is easy to estimate the noise during the pauses in speech. If
               the Noise is varying rapidly then estimation is more difficult. In
               this work we used the Kalman filter which is an efficient recursive
               filter that estimates the internal state of a linear dynamic system
               from a series of noisy measurements. Compared the results of kalman
               filter with spectral substraction, weiner filter and found kalman
               filter has shown good improvement in SNR values.},
  booktitle = {2012 {International} {Conference} on {Devices}, {Circuits} and {
               Systems} ({ICDCS})},
  author    = {Mathe, Mariyadasu and Nandyala, Siva Prasad and Kishore Kumar, T.},
  month     = mar,
  year      = {2012},
  keywords  = {Kalman filters, Kalman filter, speech recognition, Signal to noise
               ratio, spectral analysis, random noise, speech enhancement, signal
               denoising, additive background noise, color noise, communication
               environment, linear dynamic system, noise removal, noisy
               measurement, recursive filter, recursive filters, SNR value,
               spectral substraction, speech corruption, telephone conversation,
               Weiner filter, white noise},
  pages     = {195--198},
  file      = {IEEE Xplore Abstract
               Record:/Users/apodusenko/Zotero/storage/SBF2BTS8/6188703.html:text/html;IEEE
               Xplore Full Text PDF:/Users/apodusenko/Zotero/storage/DVBPYIBG/Mathe et
               al. - 2012 - Speech enhancement using Kalman Filter for white,
               .pdf:application/pdf}
}

@article{tokdar_importance_2010,
  title      = {Importance sampling: a review},
  volume     = {2},
  copyright  = {Copyright © 2009 John Wiley \& Sons, Inc.},
  issn       = {1939-0068},
  shorttitle = {Importance sampling},
  url        = {https://onlinelibrary.wiley.com/doi/abs/10.1002/wics.56},
  doi        = {10.1002/wics.56},
  abstract   = {We provide a short overview of importance sampling—a popular
                sampling tool used for Monte Carlo computing. We discuss its
                mathematical foundation and properties that determine its accuracy
                in Monte Carlo approximations. We review the fundamental
                developments in designing efficient importance sampling (IS) for
                practical use. This includes parametric approximation with
                optimization-based adaptation, sequential sampling with dynamic
                adaptation through resampling and population-based approaches that
                make use of Markov chain sampling. Copyright © 2009 John Wiley \&
                Sons, Inc. This article is categorized under: Statistical and
                Graphical Methods of Data Analysis {\textgreater} Sampling},
  language   = {en},
  number     = {1},
  urldate    = {2020-05-07},
  journal    = {WIREs Computational Statistics},
  author     = {Tokdar, Surya T. and Kass, Robert E.},
  year       = {2010},
  note       = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/wics.56},
  keywords   = {importance sampling, Markov chain sampling, Monte Carlo
                approximation, resampling, sequential sampling},
  pages      = {54--60},
  file       = {Snapshot:/Users/apodusenko/Zotero/storage/H67YT5TA/wics.html:text/html
                }
}

@article{kucukelbir_automatic_2017,
  title   = {Automatic {Differentiation} {Variational} {Inference}},
  volume  = {18},
  url     = {http://www.jmlr.org/papers/volume18/16-107/16-107.pdf},
  number  = {1},
  journal = {Journal of Machine Learning Research},
  author  = {Kucukelbir, Alp and Tran, Dustin and Ranganath, Rajesh and Gelman,
             Andrew and Blei, David M.},
  year    = {2017},
  pages   = {430--474},
  file    = {Kucukelbir et al. - 2017 - Automatic differentiation variational
             inference.pdf:/Users/apodusenko/Zotero/storage/UIHNPM6P/Kucukelbir et
             al. - 2017 - Automatic differentiation variational
             inference.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/DJMG2NFZ/Kucukelbir
             e.a. - 2017 - Automatic differentiation variational
             inference.pdf:application/pdf}
}

@inproceedings{ge_turing_2018,
  title      = {Turing: {A} {Language} for {Flexible} {Probabilistic} {Inference}},
  shorttitle = {Turing},
  url        = {http://proceedings.mlr.press/v84/ge18b.html},
  abstract   = {Probabilistic programming promises to simplify and democratize
                probabilistic machine learning, but successful probabilistic
                programming systems require flexible, generic and efficient
                inference eng...},
  language   = {en},
  urldate    = {2020-05-06},
  booktitle  = {International {Conference} on {Artificial} {Intelligence} and {
                Statistics}},
  author     = {Ge, Hong and Xu, Kai and Ghahramani, Zoubin},
  month      = mar,
  year       = {2018},
  note       = {ISSN: 1938-7228 Section: Machine Learning},
  pages      = {1682--1690},
  file       = {Ge et al. - 2018 - Turing A Language for Flexible Probabilistic
                Infe.pdf:/Users/apodusenko/Zotero/storage/5NVKI2KG/Ge et al. - 2018 -
                Turing A Language for Flexible Probabilistic
                Infe.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/CHHP4WDM/ge18b.html:text/html
                }
}

@article{bingham_pyro_2019,
  title      = {Pyro: {Deep} {Universal} {Probabilistic} {Programming}},
  volume     = {20},
  issn       = {1533-7928},
  shorttitle = {Pyro},
  url        = {http://jmlr.org/papers/v20/18-403.html},
  number     = {28},
  urldate    = {2020-05-06},
  journal    = {Journal of Machine Learning Research},
  author     = {Bingham, Eli and Chen, Jonathan P. and Jankowiak, Martin and
                Obermeyer, Fritz and Pradhan, Neeraj and Karaletsos, Theofanis and
                Singh, Rohit and Szerlip, Paul and Horsfall, Paul and Goodman, Noah
                D.},
  year       = {2019},
  pages      = {1--6},
  file       = {Bingham et al. - 2019 - Pyro Deep Universal Probabilistic
                Programming.pdf:/Users/apodusenko/Zotero/storage/YD8CLQVE/Bingham et
                al. - 2019 - Pyro Deep Universal Probabilistic
                Programming.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/FFAB82Z3/18-403.html:text/html
                }
}

@article{dillon_tensorflow_2017,
  title    = {{TensorFlow} {Distributions}},
  url      = {http://arxiv.org/abs/1711.10604},
  abstract = {The TensorFlow Distributions library implements a vision of
              probability theory adapted to the modern deep-learning paradigm of
              end-to-end differentiable computation. Building on two basic
              abstractions, it offers flexible building blocks for probabilistic
              computation. Distributions provide fast, numerically stable methods
              for generating samples and computing statistics, e.g., log density.
              Bijectors provide composable volume-tracking transformations with
              automatic caching. Together these enable modular construction of
              high dimensional distributions and transformations not possible
              with previous libraries (e.g., pixelCNNs, autoregressive flows, and
              reversible residual networks). They are the workhorse behind deep
              probabilistic programming systems like Edward and empower fast
              black-box inference in probabilistic models built on deep-network
              components. TensorFlow Distributions has proven an important part
              of the TensorFlow toolkit within Google and in the broader deep
              learning community.},
  urldate  = {2020-05-06},
  journal  = {arXiv:1711.10604 [cs, stat]},
  author   = {Dillon, Joshua V. and Langmore, Ian and Tran, Dustin and Brevdo,
              Eugene and Vasudevan, Srinivas and Moore, Dave and Patton, Brian and
              Alemi, Alex and Hoffman, Matt and Saurous, Rif A.},
  month    = nov,
  year     = {2017},
  note     = {arXiv: 1711.10604},
  keywords = {Statistics - Machine Learning, Computer Science - Artificial
              Intelligence, Computer Science - Programming Languages, Computer
              Science - Machine Learning},
  file     = {arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/ZSSQJDFE/1711.html:text/html;Dillon
              et al. - 2017 - TensorFlow
              Distributions.pdf:/Users/apodusenko/Zotero/storage/7KF2MFTG/Dillon et
              al. - 2017 - TensorFlow Distributions.pdf:application/pdf}
}

@book{pearl_probabilistic_1988,
  address    = {San Francisco, CA, USA},
  title      = {Probabilistic reasoning in intelligent systems: networks of plausible
                inference},
  isbn       = {978-0-934613-73-6},
  shorttitle = {Probabilistic reasoning in intelligent systems},
  publisher  = {Morgan Kaufmann Publishers Inc.},
  author     = {Pearl, Judea},
  year       = {1988}
}

@article{chow_approximating_1968,
  title    = {Approximating discrete probability distributions with dependence
              trees},
  volume   = {14},
  issn     = {1557-9654},
  doi      = {10.1109/TIT.1968.1054142},
  abstract = {A method is presented to approximate optimally ann-dimensional
              discrete probability distribution by a product of second-order
              distributions, or the distribution of the first-order tree
              dependence. The problem is to find an optimum set ofn - 1first
              order dependence relationship among thenvariables. It is shown that
              the procedure derived in this paper yields an approximation of a
              minimum difference in information. It is further shown that when
              this procedure is applied to empirical observations from an unknown
              distribution of tree dependence, the procedure is the
              maximum-likelihood estimate of the distribution.},
  number   = {3},
  journal  = {IEEE Transactions on Information Theory},
  author   = {Chow, C. and Liu, C.},
  month    = may,
  year     = {1968},
  note     = {Conference Name: IEEE Transactions on Information Theory},
  keywords = {Approximation methods, Probability functions, Trees},
  pages    = {462--467},
  file     = {IEEE Xplore Abstract
              Record:/Users/apodusenko/Zotero/storage/QRK8X2NG/1054142.html:text/html;Submitted
              Version:/Users/apodusenko/Zotero/storage/WKRAZA9X/Chow and Liu - 1968 -
              Approximating discrete probability distributions w.pdf:application/pdf}
}

@misc{bocharov_discovering_nodate,
  title  = {Discovering {Hearthstone} archetypes using probabilistic modeling},
  author = {Bocharov, Ivan},
  file   = {Bocharov - Discovering Hearthstone archetypes using
            probabili.pdf:/Users/apodusenko/Zotero/storage/6VKMAQMG/Bocharov -
            Discovering Hearthstone archetypes using probabili.pdf:application/pdf}
}

@article{narayanan_symbolic_nodate,
  title    = {Symbolic disintegration with a variety of base measures},
  volume   = {1},
  language = {en},
  number   = {1},
  author   = {Narayanan, Praveen and Shan, Chung-chieh},
  pages    = {52},
  file     = {Narayanan and Shan - Symbolic disintegration with a variety of base
              mea.pdf:/Users/apodusenko/Zotero/storage/UUVG26JP/Narayanan and Shan -
              Symbolic disintegration with a variety of base mea.pdf:application/pdf}
}

@article{silver_few-shot_nodate,
  title    = {Few-{Shot} {Bayesian} {Imitation} {Learning} with {Logical} {Program}
              {Policies}},
  abstract = {Humans can learn many novel tasks from a very small number (1–5)
              of demonstrations, in stark contrast to the data requirements of
              nearly tabula rasa deep learning methods. We propose an expressive
              class of policies, a strong but general prior, and a learning
              algorithm that, together, can learn interesting policies from very
              few examples. We represent policies as logical combinations of
              programs drawn from a domainspeciﬁc language (DSL), deﬁne a prior
              over policies with a probabilistic grammar, and derive an
              approximate Bayesian inference algorithm to learn policies from
              demonstrations. In experiments, we study six strategy games played
              on a 2D grid with one shared DSL. After a few demonstrations of
              each game, the inferred policies generalize to new game instances
              that differ substantially from the demonstrations. Our policy
              learning is 20–1,000x more data efﬁcient than convolutional and
              fully convolutional policy learning and many orders of magnitude
              more computationally efﬁcient than vanilla program induction. We
              argue that the proposed method is an apt choice for tasks that have
              scarce training data and feature signiﬁcant, structured variation
              between task instances.},
  language = {en},
  author   = {Silver, Tom and Allen, Kelsey R and Lew, Alex K and Kaelbling,
              Leslie and Tenenbaum, Josh},
  pages    = {8},
  file     = {Silver et al. - Few-Shot Bayesian Imitation Learning with Logical
              .pdf:/Users/apodusenko/Zotero/storage/GZUALT7R/Silver et al. - Few-Shot
              Bayesian Imitation Learning with Logical .pdf:application/pdf}
}

@article{hill_application_2012,
  title    = {Application of {Auto}-{Regressive} {Models} to {U}.{K}. {Wind} {Speed
              } {Data} for {Power} {System} {Impact} {Studies}},
  volume   = {3},
  issn     = {1949-3037},
  doi      = {10.1109/TSTE.2011.2163324},
  abstract = {Scientific research to characterize the long-term wind energy
              resource is plentiful. However, if the impact of wind power on the
              electric power system is the goal of modeling, consideration must
              be given to diurnal and seasonal effects, as well as the
              correlation of wind speed between geographical areas. This paper
              provides such detail by modeling these effects explicitly, enabling
              accurate evaluations of wind power impact on future power systems
              to be carried out. This is increasingly important in the context of
              ambitious wind energy targets driven in the U.K., for example, by
              the requirement for 20\% of Europe's energy to be met from
              renewable energy sources by 2020. Both univariate and multivariate
              auto-regressive models are presented here and it is shown how they
              can be applied to geographically dispersed wind speed data. These
              models are applied to suitably de-trended data. The accuracy of the
              models is assessed both by inspection of the residuals and by
              assessment of the forecasting accuracy of the models. Finally, it
              is shown how the models can be used to synthesize wind speed and
              thus wind power time series with the correct seasonal, diurnal, and
              spatial diversity characteristics.},
  number   = {1},
  journal  = {IEEE Transactions on Sustainable Energy},
  author   = {Hill, David C. and McMillan, David and Bell, Keith R. W. and Infield
              , David},
  month    = jan,
  year     = {2012},
  keywords = {autoregressive processes, Mathematical model, time series, Data
              models, Correlation, Autoregression moving average (ARMA) models,
              electric power system, future power systems, geographically
              dispersed wind speed data, long term wind energy resource,
              multivariate autoregressive models, power system impact, power
              systems, Reactive power, regression, spatial diversity
              characteristics, wind, wind energy, wind forecasting, Wind power
              generation, wind power impact, wind power plants, wind power time
              series, Wind speed},
  pages    = {134--141},
  file     = {IEEE Xplore Abstract
              Record:/Users/apodusenko/Zotero/storage/9DQJJZS6/5966365.html:text/html;IEEE
              Xplore Full Text PDF:/Users/apodusenko/Zotero/storage/UPC95CHS/Hill et
              al. - 2012 - Application of Auto-Regressive Models to U.K.
              Wind.pdf:application/pdf}
}

@article{baddour_autoregressive_2005,
  title    = {Autoregressive modeling for fading channel simulation},
  volume   = {4},
  issn     = {1558-2248},
  doi      = {10.1109/TWC.2005.850327},
  abstract = {Autoregressive stochastic models for the computer simulation of
              correlated Rayleigh fading processes are investigated. The
              unavoidable numerical difficulties inherent in this method are
              elucidated and a simple heuristic approach is adopted to enable the
              synthesis of accurately correlated, bandlimited Rayleigh variates.
              Startup procedures are presented, which allow autoregressive
              simulators to produce stationary channel gain samples from the
              first output sample. Performance comparisons are then made with
              popular fading generation techniques to demonstrate the merits of
              the approach. The general applicability of the method is
              demonstrated by examples involving the accurate synthesis of
              nonisotropic fading channel models.},
  number   = {4},
  journal  = {IEEE Transactions on Wireless Communications},
  author   = {Baddour, K.E. and Beaulieu, N.C.},
  month    = jul,
  year     = {2005},
  keywords = {Stochastic processes, autoregressive processes, Statistics,
              Filtering, Computational modeling, Autoregressive processes,
              autoregressive stochastic model, bandlimited communication,
              bandlimited stochastic process, bandlimited stochastic processes,
              computer simulation, Computer simulation, correlated Rayleigh
              fading process, Fading, fading channel simulation, fading
              generation technique, heuristic approach, multipath channel,
              multipath channels, nonisotropic fading channel model, nonisotropic
              scattering, Predictive models, Rayleigh channels, Rayleigh
              scattering, scattering, Scholarships, simulation},
  pages    = {1650--1662},
  file     = {IEEE Xplore Abstract
              Record:/Users/apodusenko/Zotero/storage/9X2SVEXQ/1512123.html:text/html;IEEE
              Xplore Full Text PDF:/Users/apodusenko/Zotero/storage/J8XM8MDD/Baddour
              and Beaulieu - 2005 - Autoregressive modeling for fading channel
              simulat.pdf:application/pdf}
}

@article{krithikaivasan_arch-based_2007,
  title    = {{ARCH}-{Based} {Traffic} {Forecasting} and {Dynamic} {Bandwidth} {
              Provisioning} for {Periodically} {Measured} {Nonstationary} {Traffic}},
  volume   = {15},
  issn     = {1558-2566},
  doi      = {10.1109/TNET.2007.893217},
  abstract = {Network providers are often interested in providing dynamically
              provisioned bandwidth to customers based on periodically measured
              nonstationary traffic while meeting service level agreements
              (SLAs). In this paper, we propose a dynamic bandwidth provisioning
              framework for such a situation. In order to have a good sense of
              nonstationary periodically measured traffic data, measurements were
              first collected over a period of three weeks excluding the weekends
              in three different months from an Internet access link. To
              characterize the traffic data rate dynamics of these data sets, we
              develop a seasonal autoregressive conditional heteroskedasticity
              (ARCH) based model with the innovation process (disturbances)
              generalized to the class of heavy-tailed distributions. We observed
              a strong empirical evidence for the proposed model. Based on the
              ARCH-model, we present a probability-hop forecasting algorithm, an
              augmented forecast mechanism using the confidence-bounds of the
              mean forecast value from the conditional forecast distribution. For
              bandwidth estimation, we present different bandwidth provisioning
              schemes that allocate or deallocate the bandwidth based on the
              traffic forecast generated by our forecasting algorithm. These
              provisioning schemes are developed to allow trade off between the
              underprovisioning and the utilization, while addressing the
              overhead cost of updating bandwidth. Based on extensive studies
              with three different data sets, we have found that our approach
              provides a robust dynamic bandwidth provisioning framework for
              real-world periodically measured nonstationary traffic.},
  number   = {3},
  journal  = {IEEE/ACM Transactions on Networking},
  author   = {Krithikaivasan, Balaji and Zeng, Yong and Deka, Kaushik and Medhi,
              Deep},
  month    = jun,
  year     = {2007},
  keywords = {Resource management, autoregressive processes, Internet,
              Robustness, Cities and towns, ARCH-based traffic forecasting,
              augmented forecast mechanism, autoregressive conditional
              heteroskedasticity, Autoregressive conditional heteroskedasticity,
              Bandwidth, bandwidth allocation, bandwidth estimation, bandwidth
              provisioning, Costs, dynamic bandwidth provisioning, forecasting
              theory, heavy-tailedness, innovation process, Internet access link,
              Meeting services, network providers, nonstationary traffic,
              periodically measured nonstationary traffic, probability-hop
              forecasting, service level agreements, Technological innovation,
              telecommunication networks, telecommunication traffic,
              Telecommunication traffic, Traffic control, traffic data rate
              dynamics},
  pages    = {683--696},
  file     = {IEEE Xplore Abstract
              Record:/Users/apodusenko/Zotero/storage/PZ5LFHUT/4237145.html:text/html;IEEE
              Xplore Full Text
              PDF:/Users/apodusenko/Zotero/storage/MSHRDZ6D/Krithikaivasan et al. -
              2007 - ARCH-Based Traffic Forecasting and Dynamic
              Bandwid.pdf:application/pdf}
}

@article{magnant_computing_2015,
  title    = {On {Computing} {Jeffrey}’s {Divergence} {Between} {Time}-{Varying} {
              Autoregressive} {Models}},
  volume   = {22},
  issn     = {1558-2361},
  doi      = {10.1109/LSP.2014.2377473},
  abstract = {Autoregressive (AR) and time-varying AR (TVAR) models are widely
              used in various applications, from speech processing to biomedical
              signal analysis. Various dissimilarity measures such as the Itakura
              divergence have been proposed to compare two AR models. However,
              they do not take into account the variances of the driving
              processes and only apply to stationary processes. More generally,
              the comparison between Gaussian processes is based on the
              Kullback-Leibler (KL) divergence but only asymptotic expressions
              are classically used. In this letter, we suggest analyzing the
              similarities of two TVAR models, sample after sample, by
              recursively computing the Jeffrey's divergence between the joint
              distributions of the successive values of each TVAR model. Then, we
              show that, under some assumptions, this divergence tends to the
              Itakura divergence in the stationary case.},
  number   = {7},
  journal  = {IEEE Signal Processing Letters},
  author   = {Magnant, Clement and Giremus, Audrey and Grivel, Eric},
  month    = jul,
  year     = {2015},
  keywords = {medical signal processing, autoregressive processes, Mathematical
              model, Vectors, Gaussian process, Gaussian processes, Joints,
              biomedical signal analysis, Kullback-Leibler divergence, Biological
              system modeling, speech processing, signal sampling, Computational
              modeling, Analytical models, asymptotic expression, Autoregressive
              process, Biomedical measurement, driving process variance, Itakura
              divergence, Jeffrey divergence computing, Jeffrey’s divergence,
              joint distribution, KL divergence, time-varying autoregressive
              model, time-varying autoregressive process, time-varying systems,
              TVAR model},
  pages    = {915--919},
  file     = {IEEE Xplore Abstract
              Record:/Users/apodusenko/Zotero/storage/LH7M6DYV/6975076.html:text/html;IEEE
              Xplore Full Text PDF:/Users/apodusenko/Zotero/storage/ZTAZGCGH/Magnant
              et al. - 2015 - On Computing Jeffrey’s Divergence Between
              Time-Var.pdf:application/pdf}
}

@misc{kouw_schedule-free_2020,
  type     = {Short talk},
  title    = {Schedule-free variational message passing for {Bayesian} filtering},
  url      = {https://neuromatch.io},
  abstract = {In Bayesian filtering, states and parameters of probabilistic
              state-space models are inferred in an online manner. Using the Free
              Energy Principle, the state-space model is cast to a generative
              model p and the posterior distributions of interest are
              approximated using recognition distributions or beliefs q. The
              factorisation of state-space models into state transitions and
              observation likelihoods over time supports forming a factor graph
              and performing inference via message passing. Tools for message
              passing on factor graphs typically employ a scheduling procedure,
              in which a separate algorithm or compiler takes the model
              description and returns which nodes should pass messages where at
              what time. This can be sufficiently expensive to form a bottleneck.
              Moreover, it's not a biologically plausible mechanism for governing
              message passing. I explore the possibility of passing messages
              without a scheduler. A designated terminal node should pass an
              initial message, which will arrive at an initial variable. The
              corresponding belief is updated, a local Free Energy is computed
              and the belief is emitted to neighbouring factor nodes. From there
              on out, whenever an updated belief arrives at a factor node, the
              node fires messages to all other variables if the local Free Energy
              surpasses a threshold. If not, the node becomes silent.},
  author   = {Kouw, Wouter M},
  month    = mar,
  year     = {2020},
  file     = {Kouw - 2020 - Schedule-free variational message passing for
              Baye.pdf:/Users/apodusenko/Zotero/storage/96P7BI6B/Kouw - 2020 -
              Schedule-free variational message passing for Baye.pdf:application/pdf}
}

@article{weiss_correctness_2001,
  title    = {Correctness of {Belief} {Propagation} in {Gaussian} {Graphical} {
              Models} of {Arbitrary} {Topology}},
  volume   = {13},
  issn     = {0899-7667, 1530-888X},
  url      = {http://www.mitpressjournals.org/doi/10.1162/089976601750541769},
  doi      = {10.1162/089976601750541769},
  abstract = {Local ”belief propagation” rules of the sort proposed by Pearl
              (1988) are guaranteed to converge to the correct posterior
              probabilities in singly connected graphical models. Recently, a
              number of researchers have empirically demonstrated good
              performance of ”loopy belief propagation” – using these same rules
              on graphs with loops. Perhaps the most dramatic instance is the
              near Shannon-limit performance of ”Turbo codes”, whose decoding
              algorithm is equivalent to loopy belief propagation. These results
              motivate using the powerful belief propagation algorithm in a
              broader class of networks, and help clarify the empirical
              performance results.},
  language = {en},
  number   = {10},
  urldate  = {2020-03-26},
  journal  = {Neural Computation},
  author   = {Weiss, Yair and Freeman, William T.},
  month    = oct,
  year     = {2001},
  pages    = {2173--2200},
  file     = {Weiss and Freeman - 2001 - Correctness of Belief Propagation in
              Gaussian Grap.pdf:/Users/apodusenko/Zotero/storage/7S6XA2WF/Weiss and
              Freeman - 2001 - Correctness of Belief Propagation in Gaussian
              Grap.pdf:application/pdf}
}

@inproceedings{saleh_speech_1998,
  title     = {Speech enhancement in a {Bayesian} framework},
  volume    = {1},
  doi       = {10.1109/ICASSP.1998.674449},
  abstract  = {We present an approach for the enhancement of speech signals
               corrupted by additive white noise of Gaussian statistics. The
               speech enhancement problem is treated as a signal estimation
               problem within a Bayesian framework. The conventional all-pole
               speech production model is assumed to govern the behaviour of the
               clean speech signal. The additive noise level and all-pole model
               gain are automatically inferred during the speech enhancement
               process. The strength of the Bayesian approach developed in this
               paper lies in its ability to perform speech enhancement without the
               usual requirement of estimating the level of the corrupting noise
               from "silence" segments of the corrupted signal. The performance of
               the Bayesian approach is compared to that of the Lim \& Oppenheim
               (1978) framework, to which it follows a similar iterative nature. A
               significant quality improvement is obtained over the Lim \&
               Oppenheim framework.},
  booktitle = {Proceedings of the 1998 {IEEE} {International} {Conference} on {
               Acoustics}, {Speech} and {Signal} {Processing}, {ICASSP} '98 ({Cat
               }. {No}.{98CH36181})},
  author    = {Saleh, G.M.K. and Niranjan, M.},
  month     = may,
  year      = {1998},
  note      = {ISSN: 1520-6149},
  keywords  = {Estimation, Bayes methods, Bayesian methods, iterative methods,
               Statistics, Background noise, Hidden Markov models, Speech
               enhancement, speech intelligibility, Additive noise, Gaussian noise
               , speech enhancement, Noise level, white noise, additive noise
               level, additive white noise, Additive white noise, all-pole model
               gain, all-pole speech production model, AWGN, Bayesian framework,
               clean speech signal, Gaussian statistics, iterative method,
               Iterative methods, Lim \& Oppenheim framework, poles and zeros,
               signal estimation, speech quality improvement},
  pages     = {389--392 vol.1},
  file      = {IEEE Xplore Abstract
               Record:/Users/apodusenko/Zotero/storage/U3EEU4RS/674449.html:text/html;IEEE
               Xplore Full Text PDF:/Users/apodusenko/Zotero/storage/Q524V9ZP/Saleh
               and Niranjan - 1998 - Speech enhancement in a Bayesian
               framework.pdf:application/pdf}
}

@misc{noauthor_speech_nodate-2,
  title   = {Speech enhancement in a {Bayesian} framework - {IEEE} {Conference} {
             Publication}},
  url     = {https://ieeexplore.ieee.org/document/674449},
  urldate = {2020-03-24},
  file    = {Speech enhancement in a Bayesian framework - IEEE Conference
             Publication:/Users/apodusenko/Zotero/storage/ZT5LFAC5/674449.html:text/html
             }
}

@misc{kneale_caseyknealesmilesjl_2020,
  title     = {caseykneale/{SMILES}.jl},
  copyright = {MIT},
  url       = {https://github.com/caseykneale/SMILES.jl},
  abstract  = {SMILES parser in Julia. Contribute to caseykneale/SMILES.jl
               development by creating an account on GitHub.},
  urldate   = {2020-03-24},
  author    = {Kneale, Casey},
  month     = mar,
  year      = {2020},
  note      = {original-date: 2019-09-21T14:31:05Z}
}

@inproceedings{baudart_reactive_2019,
  author    = {Baudart, Guillaume and Mandel, Louis and Atkinson, Eric and Sherman, Benjamin and Pouzet, Marc and Carbin, Michael},
  title     = {Reactive Probabilistic Programming},
  year      = {2020},
  isbn      = {9781450376136},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3385412.3386009},
  doi       = {10.1145/3385412.3386009},
  abstract  = {Synchronous modeling is at the heart of programming languages like Lustre, Esterel, or Scade used routinely for implementing safety critical control software, e.g., fly-by-wire and engine control in planes. However, to date these languages have had limited modern support for modeling uncertainty --- probabilistic aspects of the software's environment or behavior --- even though modeling uncertainty is a primary activity when designing a control system. In this paper we present ProbZelus the first synchronous probabilistic programming language. ProbZelus conservatively provides the facilities of a synchronous language to write control software, with probabilistic constructs to model uncertainties and perform inference-in-the-loop. We present the design and implementation of the language. We propose a measure-theoretic semantics of probabilistic stream functions and a simple type discipline to separate deterministic and probabilistic expressions. We demonstrate a semantics-preserving compilation into a first-order functional language that lends itself to a simple presentation of inference algorithms for streaming models. We also redesign the delayed sampling inference algorithm to provide efficient streaming inference. Together with an evaluation on several reactive applications, our results demonstrate that ProbZelus enables the design of reactive probabilistic applications and efficient, bounded memory inference.},
  booktitle = {Proceedings of the 41st ACM SIGPLAN Conference on Programming Language Design and Implementation},
  pages     = {898–912},
  numpages  = {15},
  keywords  = {Reactive programming, Probabilistic programming, Streaming inference, Compilation, Semantics},
  location  = {London, UK},
  series    = {PLDI 2020}
}

@book{svensen_pattern_2008,
  title  = {Pattern {Recognition} and {Machine} {Learning} - {Solution} manual},
  author = {Svensen, Markus and Bishop, Christopher},
  year   = {2008},
  file   = {Svensen and Bishop - 2008 - Pattern Recognition and Machine Learning -
            Solutio.pdf:/Users/apodusenko/Zotero/storage/TI5BJNUL/Svensen and
            Bishop - 2008 - Pattern Recognition and Machine Learning -
            Solutio.pdf:application/pdf}
}

@book{bishop_pattern_2006,
  title     = {Pattern {Recognition} and {Machine} {Learning}},
  isbn      = {0-387-31073-8},
  url       = {
               http://www.springer.com/computer/image+processing/book/978-0-387-31073-2
               },
  abstract  = {The dramatic growth in practical applications for machine learning
               over the last ten years has been accompanied by many important
               developments in the underlying algorithms and techniques. For
               example, Bayesian methods have grown ...},
  urldate   = {2014-04-10},
  publisher = {Springer-Verlag New York, Inc.},
  author    = {Bishop, Christopher M.},
  year      = {2006},
  keywords  = {Artificial Intelligence (incl. Robotics), Pattern Recognition,
               Pattern Recognition and Machine Learning},
  file      = {Bishop - 2006 - Pattern Recognition and Machine
               Learning.pdf:/Users/apodusenko/Zotero/storage/YR9KNAM2/Bishop - 2006 -
               Pattern Recognition and Machine Learning.pdf:application/pdf}
}

@article{vermaak_particle_2002,
  title    = {Particle methods for {Bayesian} modeling and enhancement of speech
              signals},
  volume   = {10},
  issn     = {1558-2353},
  doi      = {10.1109/TSA.2002.1001982},
  abstract = {This paper applies time-varying autoregressive (TVAR) models with
              stochastically evolving parameters to the problem of speech
              modeling and enhancement. The stochastic evolution models for the
              TVAR parameters are Markovian diffusion processes. The main aim of
              the paper is to perform on-line estimation of the clean speech and
              model parameters and to determine the adequacy of the chosen
              statistical models. Efficient particle methods are developed to
              solve the optimal filtering and fixed-lag smoothing problems. The
              algorithms combine sequential importance sampling (SIS), a
              selection step and Markov chain Monte Carlo (MCMC) methods. They
              employ several variance reduction strategies to make the best use
              of the statistical structure of the model. It is also shown how
              model adequacy may be determined by combining the particle filter
              with frequentist methods. The modeling and enhancement performance
              of the models and estimation algorithms are evaluated in simulation
              studies on both synthetic and real speech data sets.},
  number   = {3},
  journal  = {IEEE Transactions on Speech and Audio Processing},
  author   = {Vermaak, J. and Andrieu, C. and Doucet, A. and Godsill, S.J.},
  month    = mar,
  year     = {2002},
  keywords = {smoothing methods, Stochastic processes, Bayesian modeling,
              Bayesian methods, autoregressive processes, Speech enhancement,
              Speech analysis, Markov processes, Speech processing, estimation
              algorithms, Context modeling, statistical models, speech
              enhancement, Signal processing, parameter estimation, importance
              sampling, simulation, Acoustical engineering, clean speech,
              Diffusion processes, enhancement performance, fixed-lag smoothing,
              frequentist methods, Markov chain Monte Carlo methods, Markovian
              diffusion processes, model parameters, modeling performance, Monte
              Carlo methods, on-line estimation, optimal filtering, particle
              filter, particle methods, real speech data set, selection step,
              sequential importance sampling, speech modeling, speech signals
              enhancement, statistical structure, stochastic evolution models,
              stochastically evolving parameters, synthetic speech data set,
              time-varying autoregressive models, TVAR models, TVAR parameters,
              variance reduction},
  pages    = {173--185},
  file     = {IEEE Xplore Abstract
              Record:/Users/apodusenko/Zotero/storage/6A7DTL5W/1001982.html:text/html;Submitted
              Version:/Users/apodusenko/Zotero/storage/25KN5LBR/Vermaak et al. - 2002
              - Particle methods for Bayesian modeling and
              enhance.pdf:application/pdf}
}

@inproceedings{huang_ar-based_2009,
  title     = {{AR}-{Based} {Bayesian} {Speech} {Enhancement} for {Nonstationary} {
               Environments}},
  volume    = {1},
  doi       = {10.1109/CSO.2009.171},
  abstract  = {A new technique for enhancing audio signal from a noisy
               nonstationary environment is presented in the paper. Autoregressive
               (AR) model is used to efficiently exploit the temporally correlated
               information of audio and noise signals during a short stationary
               frame. The temporal models of signals and noisy process are
               combined to construct a state space. The state space appropriately
               describes that the observed noisy signal is generated from two
               underlying sources which evolve with Markovian dynamics across
               successive step times. In the state space, the clean speech and the
               noise are two hidden source signals. The recovery of clean speech
               and the estimation of all the model parameters are carried out
               within the variational Bayesian framework. The original speech can
               be estimated as a state using a variational Kalman smoother. The
               experimental results show that our approach can obtain better
               performance in terms of signal-to-noise ratio (SNR) measure.},
  booktitle = {2009 {International} {Joint} {Conference} on {Computational} {
               Sciences} and {Optimization}},
  author    = {Huang, Qinghua and Liu, Kai},
  month     = apr,
  year      = {2009},
  note      = {ISSN: null},
  keywords  = {Kalman filters, Bayes methods, Bayesian methods, autoregressive
               processes, Speech enhancement, Working environment noise, Signal to
               noise ratio, Markov processes, State estimation, SNR, speech
               enhancement, Signal processing, signal-to-noise ratio, State-space
               methods, clean speech, AR model, audio signal, audio signal
               processing, autoregressive model, Bayesian speech enhancement,
               Markovian dynamics, Noise generators, noisy nonstationary
               environment, nonstationary environment, Signal generators, state
               space, temporally correlated information, variational Bayesian
               framework, variational Kalman smoother},
  pages     = {918--921},
  file      = {IEEE Xplore Abstract
               Record:/Users/apodusenko/Zotero/storage/4GXQH4DC/5193843.html:text/html;IEEE
               Xplore Full Text PDF:/Users/apodusenko/Zotero/storage/I34BLBPV/Huang
               and Liu - 2009 - AR-Based Bayesian Speech Enhancement for
               Nonstatio.pdf:application/pdf}
}

@article{yao_priori_2016,
  title    = {A priori {SNR} estimation and noise estimation for speech enhancement
              },
  volume   = {2016},
  issn     = {1687-6180},
  url      = {https://doi.org/10.1186/s13634-016-0398-z},
  doi      = {10.1186/s13634-016-0398-z},
  abstract = {A priori signal-to-noise ratio (SNR) estimation and noise
              estimation are important for speech enhancement. In this paper, a
              novel modified decision-directed (DD) a priori SNR estimation
              approach based on single-frequency entropy, named DDBSE, is
              proposed. DDBSE replaces the fixed weighting factor in the DD
              approach with an adaptive one calculated according to change of
              single-frequency entropy. Simultaneously, a new noise power
              estimation approach based on unbiased minimum mean square error
              (MMSE) and voice activity detection (VAD), named UMVAD, is
              proposed. UMVAD adopts different strategies to estimate noise in
              order to reduce over-estimation and under-estimation of noise.
              UMVAD improves the classical statistical model-based VAD by
              utilizing an adaptive threshold to replace the original fixed one
              and modifies the unbiased MMSE-based noise estimation approach
              using an adaptive a priori speech presence probability calculated
              by entropy instead of the original fixed one. Experimental results
              show that DDBSE can provide greater noise suppression than DD and
              UMVAD can improve the accuracy of noise estimation. Compared to
              existing approaches, speech enhancement based on UMVAD and DDBSE
              can obtain a better segment SNR score and composite measure covl
              score, especially in adverse environments such as non-stationary
              noise and low-SNR.},
  language = {en},
  number   = {1},
  urldate  = {2020-03-10},
  journal  = {EURASIP Journal on Advances in Signal Processing},
  author   = {Yao, Rui and Zeng, ZeQing and Zhu, Ping},
  month    = sep,
  year     = {2016},
  pages    = {101},
  file     = {Springer Full Text PDF:/Users/apodusenko/Zotero/storage/EYNM9DM5/Yao
              et al. - 2016 - A priori SNR estimation and noise estimation for
              s.pdf:application/pdf}
}

@article{paliwal_estimation_1988,
  title    = {Estimation of noise variance from the noisy {AR} signal and its
              application in speech enhancement},
  volume   = {36},
  issn     = {0096-3518},
  doi      = {10.1109/29.1523},
  abstract = {In a number of applications involving the processing of noisy
              signals, it is desirable to know a priori the noise variance. The
              author proposes a method of estimating the noise variance from the
              autoregressive (AR) signal corrupted by the additive white noise.
              This method first estimates the AR parameters from the high-order
              Yule-Walker equations, and then uses these AR parameters to
              estimate the noise variance from the low-order Yule-Walker
              equations. The method is used in a speech enhancement application
              where its performance is studied for stationary as well as
              nonstationary noise conditions. The results are found to be
              encouraging.{\textless}{\textgreater}},
  number   = {2},
  journal  = {IEEE Transactions on Acoustics, Speech, and Signal Processing},
  author   = {Paliwal, K.K.},
  month    = feb,
  year     = {1988},
  keywords = {Parameter estimation, Equations, White noise, Background noise,
              nonstationary noise, Speech enhancement, Signal to noise ratio,
              speech analysis and processing, Signal analysis, speech enhancement
              , Signal processing, signal processing, white noise, additive white
              noise, Additive white noise, AR parameters, Autocorrelation,
              autoregressive signal, high-order Yule-Walker equations, low-order
              Yule-Walker equations, noise variance estimation, noisy AR signal,
              noisy signals processing, stationary noise},
  pages    = {292--294},
  file     = {IEEE Xplore Abstract
              Record:/Users/apodusenko/Zotero/storage/QGAAHA9F/1523.html:text/html}
}

@inproceedings{berezina_autoregressive_2010,
  title     = {Autoregressive modeling of voiced speech},
  doi       = {10.1109/ICASSP.2010.5495058},
  abstract  = {It is well known that the classical linear predictive model for
               speech fails to take into account the quasi-periodic nature of the
               glottal flow typical of voiced speech. In this article we describe
               how to incorporate an estimate of the glottal flow directly into
               the traditional linear prediction framework, through the use of
               flexible basis function expansions that admit efficient estimation
               procedures. As we show, this not only allows for improved
               estimation of vocal tract transfer function parameters in a manner
               that is robust to pitch variation, but also precludes the need for
               nonlinear optimization procedures typically required in glottal
               waveform estimation. We illustrate our approach with experiments
               using synthesized and real speech waveforms, and show how it may be
               used to directly estimate the relative degree of voicing and
               aspiration present in a given utterance.},
  booktitle = {2010 {IEEE} {International} {Conference} on {Acoustics}, {Speech}
               and {Signal} {Processing}},
  author    = {Berezina, Maria A. and Rudoy, Daniel and Wolfe, Patrick J.},
  month     = mar,
  year      = {2010},
  note      = {ISSN: 1520-6149},
  keywords  = {Auditory system, Parameter estimation, autoregressive processes,
               Speech analysis, Filters, acoustic signal processing, Gaussian
               processes, speech, Additive noise, Robustness, speech processing,
               Predictive models, autoregressive modeling, classical linear
               predictive model, glottal flow, Glottal flow, linear prediction,
               nonlinear optimization, source harmonics-to-noise ratio, spectral
               estimation, Speech synthesis, speech waveforms, Transfer functions,
               vocal tract transfer function parameters, voiced speech, wavelet
               regression},
  pages     = {5042--5045},
  file      = {IEEE Xplore Abstract
               Record:/Users/apodusenko/Zotero/storage/JRDPWG5Q/5495058.html:text/html;IEEE
               Xplore Full Text PDF:/Users/apodusenko/Zotero/storage/7GLBALF8/Berezina
               et al. - 2010 - Autoregressive modeling of voiced
               speech.pdf:application/pdf}
}

@article{islam_speech_2018,
  title    = {Speech {Enhancement} {Based} on {Non}-stationary {Noise}-driven {
              Geometric} {Spectral} {Subtraction} and {Phase} {Spectrum} {
              Compensation}},
  abstract = {In this paper, a speech enhancement method based on noise
              compensation performed on short time magnitude as well phase
              spectra is presented. Unlike the conventional geometric approach
              (GA) to spectral subtraction (SS), here the noise estimate to be
              subtracted from the noisy speech spectrum is proposed to be
              determined by exploiting the low frequency regions of current frame
              of noisy speech rather than depending only on the initial silence
              frames. This approach gives the capability of tracking
              non-stationary noise thus resulting in a non-stationary
              noise-driven geometric approach of spectral subtraction for speech
              enhancement. The noise compensated magnitude spectrum from the GA
              step is then recombined with unchanged phase of noisy speech
              spectrum and used in phase compensation to obtain an enhanced
              complex spectrum, which is used to produce an enhanced speech
              frame. Extensive simulations are carried out using speech files
              available in the NOIZEUS database shows that the proposed method
              consistently outperforms some of the recent methods of speech
              enhancement when employed on the noisy speeches corrupted by street
              or babble noise at different levels of SNR in terms of objective
              measures, spectrogram analysis and formal subjective listening
              tests.},
  journal  = {ArXiv},
  author   = {Islam, Md Tauhidul and Saha, Udoy and Shahid, K. T. and Hussain,
              Ahmed Bin and Shahnaz, Celia},
  year     = {2018},
  file     = {Full Text PDF:/Users/apodusenko/Zotero/storage/7M8ZRCV5/Islam et al. -
              2018 - Speech Enhancement Based on Non-stationary
              Noise-d.pdf:application/pdf}
}

@article{paliwal_single-channel_2010,
  title    = {Single-channel speech enhancement using spectral subtraction in the
              short-time modulation domain},
  volume   = {52},
  issn     = {0167-6393},
  url      = {http://www.sciencedirect.com/science/article/pii/S0167639310000282},
  doi      = {10.1016/j.specom.2010.02.004},
  abstract = {In this paper we investigate the modulation domain as an
              alternative to the acoustic domain for speech enhancement. More
              specifically, we wish to determine how competitive the modulation
              domain is for spectral subtraction as compared to the acoustic
              domain. For this purpose, we extend the traditional
              analysis-modification-synthesis framework to include modulation
              domain processing. We then compensate the noisy modulation spectrum
              for additive noise distortion by applying the spectral subtraction
              algorithm in the modulation domain. Using an objective speech
              quality measure as well as formal subjective listening tests, we
              show that the proposed method results in improved speech quality.
              Furthermore, the proposed method achieves better noise suppression
              than the MMSE method. In this study, the effect of modulation frame
              duration on speech quality of the proposed enhancement method is
              also investigated. The results indicate that modulation frame
              durations of 180–280ms, provide a good compromise between different
              types of spectral distortions, namely musical noise and temporal
              slurring. Thus given a proper selection of modulation frame
              duration, the proposed modulation spectral subtraction does not
              suffer from musical noise artifacts typically associated with
              acoustic spectral subtraction. In order to achieve further
              improvements in speech quality, we also propose and investigate
              fusion of modulation spectral subtraction with the MMSE method. The
              fusion is performed in the short-time spectral domain by combining
              the magnitude spectra of the above speech enhancement algorithms.
              Subjective and objective evaluation of the speech enhancement
              fusion shows consistent speech quality improvements across input
              SNRs.},
  language = {en},
  number   = {5},
  urldate  = {2020-03-03},
  journal  = {Speech Communication},
  author   = {Paliwal, Kuldip and Wójcicki, Kamil and Schwerin, Belinda},
  month    = may,
  year     = {2010},
  keywords = {Speech enhancement, Analysis-modification-synthesis (AMS),
              Modulation spectral subtraction, Musical noise, Speech enhancement
              fusion},
  pages    = {450--475},
  file     = {Paliwal et al. - 2010 - Single-channel speech enhancement using
              spectral s.pdf:/Users/apodusenko/Zotero/storage/9H32BBTC/Paliwal et al.
              - 2010 - Single-channel speech enhancement using spectral
              s.pdf:application/pdf;ScienceDirect
              Snapshot:/Users/apodusenko/Zotero/storage/EGBC7TDX/S0167639310000282.html:text/html
              }
}

@misc{noauthor_role_nodate,
  title   = {Role of modulation magnitude and phase spectrum towards speech
             intelligibility {\textbar} {Speech} {Communication}},
  url     = {https://dl.acm.org/doi/10.1016/j.specom.2010.10.004},
  urldate = {2020-03-03},
  file    = {Role of modulation magnitude and phase spectrum towards speech
             intelligibility | Speech
             Communication:/Users/apodusenko/Zotero/storage/8DHM3XDB/j.specom.2010.10.html:text/html
             }
}

@article{singh_bayesian_2018,
  title    = {Bayesian noise estimation in the modulation domain},
  volume   = {96},
  issn     = {0167-6393},
  url      = {http://www.sciencedirect.com/science/article/pii/S0167639317301309},
  doi      = {10.1016/j.specom.2017.11.008},
  abstract = {Modulation domain has been reported to be a better alternative to
              time-frequency domain for speech enhancement, as speech
              intelligibility is closely linked with the modulation spectrum.
              Motivated by that, this paper investigates the use of modulation
              domain to model the noise density function. Results show that the
              modulation domain based Gamma density function better represents
              the noise density for all time-varying noise signals compared to
              the non-modulation domain. The modulation based Gamma density is
              then used to derive noise estimator via a Bayesian motivated MMSE
              approach. As the Gamma density closely matches the true noise
              spectrum in the modulation domain, the proposed noise estimator
              does not require bias compensation even for poor signal-to-noise
              ratio (SNR) conditions, i.e., ≤ 5 dB. The proposed method yields
              better noise suppression compared to the state of the art methods
              and provides higher improvements.},
  language = {en},
  urldate  = {2020-03-03},
  journal  = {Speech Communication},
  author   = {Singh, Maneesh K. and Low, Siow Yong and Nordholm, S. and Zang,
              Zhuquan},
  month    = feb,
  year     = {2018},
  keywords = {Analysis-modification-synthesis (AMS) framework, Gamma
              distribution, Minimum mean-square-error (MMSE), Modulation domain,
              Noise estimation, Single channel speech enhancement},
  pages    = {81--92},
  file     = {ScienceDirect Full Text
              PDF:/Users/apodusenko/Zotero/storage/R2MJ63SQ/Singh et al. - 2018 -
              Bayesian noise estimation in the modulation
              domain.pdf:application/pdf;ScienceDirect
              Snapshot:/Users/apodusenko/Zotero/storage/AAT2PW9B/S0167639317301309.html:text/html
              }
}

@article{deng_speech_2015,
  title    = {Speech enhancement based on {Bayesian} decision and spectral
              amplitude estimation},
  volume   = {2015},
  issn     = {1687-4722},
  url      = {https://doi.org/10.1186/s13636-015-0073-6},
  doi      = {10.1186/s13636-015-0073-6},
  abstract = {In this paper, a single-channel speech enhancement method based on
              Bayesian decision and spectral amplitude estimation is proposed, in
              which the speech detection module and spectral amplitude estimation
              module are included, and the two modules are strongly coupled.
              First, under the decisions of speech presence and speech absence,
              the optimal speech amplitude estimators are obtained by minimizing
              a combined Bayesian risk function, respectively. Second, using the
              obtained spectral amplitude estimators, the optimal speech detector
              is achieved by further minimizing the combined Bayesian risk
              function. Finally, according to the detection results of speech
              detector, the optimal decision rule is made and the optimal
              spectral amplitude estimator is chosen for enhancing noisy speech.
              Furthermore, by considering both detection and estimation errors,
              we propose a combined cost function which incorporates two general
              weighted distortion measures for the speech presence and speech
              absence of the spectral amplitudes, respectively. The cost
              parameters in the cost function are employed to balance the speech
              distortion and residual noise caused by missed detection and false
              alarm, respectively. In addition, we propose two adaptive
              calculation methods for the perceptual weighted order p and the
              spectral amplitude order β concerned in the proposed cost function,
              respectively. The objective and subjective test results indicate
              that the proposed method can achieve a more significant segmental
              signal-noise ratio (SNR) improvement, a lower log-spectral
              distortion, and a better speech quality than the reference methods.
              },
  language = {en},
  number   = {1},
  urldate  = {2020-03-03},
  journal  = {EURASIP Journal on Audio, Speech, and Music Processing},
  author   = {Deng, Feng and Bao, Chang-Chun},
  month    = oct,
  year     = {2015},
  pages    = {28},
  file     = {Springer Full Text PDF:/Users/apodusenko/Zotero/storage/DS5IV7LI/Deng
              and Bao - 2015 - Speech enhancement based on Bayesian decision and
              .pdf:application/pdf}
}

@article{so_suppressing_2011,
  title    = {Suppressing the influence of additive noise on the {Kalman} gain for
              low residual noise speech enhancement},
  volume   = {53},
  issn     = {0167-6393},
  url      = {http://www.sciencedirect.com/science/article/pii/S0167639310001652},
  doi      = {10.1016/j.specom.2010.10.006},
  abstract = {In this paper, we present a detailed analysis of the Kalman filter
              for the application of speech enhancement and identify its
              shortcomings when the linear predictor model parameters are
              estimated from speech that has been corrupted with additive noise.
              We show that when only noise-corrupted speech is available, the
              poor performance of the Kalman filter may be attributed to the
              presence of large values in the Kalman gain during low speech
              energy regions, which cause a large degree of residual noise to be
              present in the output. These large Kalman gain values result from
              poor estimates of the LPCs due to the presence of additive noise.
              This paper presents the analysis and application of the Kalman gain
              trajectory as a useful indicator of Kalman filter performance,
              which can be used to motivate further methods of improvement. As an
              example, we analyse the previously-reported application of long and
              overlapped tapered windows using Kalman gain trajectories to
              explain the reduction and smoothing of residual noise in the
              enhanced output. In addition, we investigate further extensions,
              such as Dolph–Chebychev windowing and iterative LPC estimation.
              This modified Kalman filter was found to have improved on the
              conventional and iterative versions of the Kalman filter in both
              objective and subjective testing.},
  language = {en},
  number   = {3},
  urldate  = {2020-03-03},
  journal  = {Speech Communication},
  author   = {So, Stephen and Paliwal, Kuldip K.},
  month    = mar,
  year     = {2011},
  keywords = {Speech enhancement, Kalman filtering, Dolph-Chebycher windows,
              Linear prediction},
  pages    = {355--378},
  file     = {ScienceDirect
              Snapshot:/Users/apodusenko/Zotero/storage/I8WCA86X/S0167639310001652.html:text/html
              }
}

@inproceedings{astudillo_integration_2010,
  title    = {Integration of short-time {Fourier} domain speech enhancement and
              observation uncertainty techniques for robust automatic speech
              recognition},
  doi      = {10.14279/depositonce-2483},
  abstract = {Semantic Scholar extracted view of "Integration of short-time
              Fourier domain speech enhancement and observation uncertainty
              techniques for robust automatic speech recognition" by Ramón
              Fernández Astudillo},
  author   = {Astudillo, Ramón Fernández},
  year     = {2010},
  file     = {Full Text PDF:/Users/apodusenko/Zotero/storage/MEZWBLKS/Astudillo -
              2010 - Integration of short-time Fourier domain speech
              en.pdf:application/pdf}
}

@inproceedings{senst_message_2012,
  title     = {A message passing approach to iterative {Bayesian} {SNR} estimation},
  doi       = {10.1109/ISSSE.2012.6374328},
  abstract  = {We study the design of iterative receiver structures for a simple
               communication system, operating over an AWGN channel with unknown
               gain and unknown noise power. Based on a generic message passing
               framework, which contains Belief Propagation (BP), Variational
               Message Passing (VMP), and Expectation Maximization (EM) as special
               cases, we first rederive a non-Bayesian EM-based SNR estimator
               which has been proposed before in the literature. We then switch to
               a Bayesian model and derive a refined algorithm, which uses VMP
               instead of EM for estimating the channel gain and noise precision.
               We demonstrate via simulations that the proposed VMP-based SNR
               estimator outperforms the EM-based estimator in terms of a lower
               frame error rate, at hardly any increase of computational
               complexity. While we focus on coherent SNR estimation in this work,
               we briefly discuss a possible extension to the non-coherent case.},
  booktitle = {2012 {International} {Symposium} on {Signals}, {Systems}, and {
               Electronics} ({ISSSE})},
  author    = {Senst, Martin and Ascheid, Gerd},
  month     = oct,
  year      = {2012},
  note      = {ISSN: 2161-0819},
  keywords  = {Estimation, Message passing, belief networks, expectation
               maximization, message passing, iterative methods, Signal to noise
               ratio, Mathematical model, belief propagation, AWGN channel, AWGN
               channels, coherent SNR estimation, communication system, Decoding,
               EM, iterative Bayesian SNR estimation, iterative receiver
               structures, message passing approach, nonBayesian EM-based SNR
               estimator, Receivers, variational message passing, VMP-based SNR
               estimator},
  pages     = {1--6},
  file      = {IEEE Xplore Abstract
               Record:/Users/apodusenko/Zotero/storage/VU3KM8NK/6374328.html:text/html;IEEE
               Xplore Full Text PDF:/Users/apodusenko/Zotero/storage/F5RV9W8P/Senst
               and Ascheid - 2012 - A message passing approach to iterative Bayesian
               S.pdf:application/pdf}
}

@misc{noauthor_message_nodate,
  title   = {A message passing approach to iterative {Bayesian} {SNR} estimation -
             {IEEE} {Conference} {Publication}},
  url     = {https://ieeexplore.ieee.org/document/6374328},
  urldate = {2020-02-26},
  file    = {A message passing approach to iterative Bayesian SNR estimation - IEEE
             Conference
             Publication:/Users/apodusenko/Zotero/storage/3UVJ8XQB/6374328.html:text/html
             }
}

@misc{noauthor_190308643_nodate,
  title   = {[1903.08643] {Online} {Gaussian} {Process} {State}-{Space} {Model}: {
             Learning} and {Planning} for {Partially} {Observable} {Dynamical} {
             Systems}},
  url     = {https://arxiv.org/abs/1903.08643},
  urldate = {2020-01-16},
  file    = {[1903.08643] Online Gaussian Process State-Space Model\: Learning and
             Planning for Partially Observable Dynamical
             Systems:/Users/apodusenko/Zotero/storage/9VNUX7XF/1903.html:text/html;[1903.08643]
             Online Gaussian Process State-Space Model\: Learning and Planning for
             Partially Observable Dynamical
             Systems:/Users/apodusenko/Zotero/storage/ITQEBCIA/1903.html:text/html}
}

@article{wilkinson_unifying_2019,
  title    = {Unifying {Probabilistic} {Models} for {Time}-{Frequency} {Analysis}},
  url      = {http://arxiv.org/abs/1811.02489},
  abstract = {In audio signal processing, probabilistic time-frequency models
              have many benefits over their non-probabilistic counterparts. They
              adapt to the incoming signal, quantify uncertainty, and measure
              correlation between the signal's amplitude and phase information,
              making time domain resynthesis straightforward. However, these
              models are still not widely used since they come at a high
              computational cost, and because they are formulated in such a way
              that it can be difficult to interpret all the modelling
              assumptions. By showing their equivalence to Spectral Mixture
              Gaussian processes, we illuminate the underlying model assumptions
              and provide a general framework for constructing more complex
              models that better approximate real-world signals. Our
              interpretation makes it intuitive to inspect, compare, and alter
              the models since all prior knowledge is encoded in the Gaussian
              process kernel functions. We utilise a state space representation
              to perform efficient inference via Kalman smoothing, and we
              demonstrate how our interpretation allows for efficient parameter
              learning in the frequency domain.},
  urldate  = {2020-02-07},
  journal  = {arXiv:1811.02489 [cs, eess, stat]},
  author   = {Wilkinson, William J. and Andersen, Michael Riis and Reiss, Joshua
              D. and Stowell, Dan and Solin, Arno},
  month    = feb,
  year     = {2019},
  note     = {arXiv: 1811.02489},
  keywords = {Computer Science - Sound, Statistics - Machine Learning,
              Electrical Engineering and Systems Science - Audio and Speech
              Processing, Computer Science - Machine Learning, Electrical
              Engineering and Systems Science - Signal Processing},
  file     = {arXiv Fulltext PDF:/Users/apodusenko/Zotero/storage/6868537W/Wilkinson
              et al. - 2019 - Unifying Probabilistic Models for Time-Frequency
              A.pdf:application/pdf;arXiv Fulltext
              PDF:/Users/apodusenko/Zotero/storage/TMTGA3EY/Wilkinson et al. - 2019 -
              Unifying Probabilistic Models for Time-Frequency
              A.pdf:application/pdf;arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/4J9UIDSK/1811.html:text/html;arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/A7BDK8XS/1811.html:text/html}
}

@article{van_de_laar_simulating_2019,
  title    = {Simulating {Active} {Inference} {Processes} by {Message} {Passing}},
  volume   = {6},
  doi      = {10.3389/frobt.2019.00020},
  journal  = {Frontiers in Robotics and AI},
  author   = {van de Laar, Thijs and de Vries, Bert},
  year     = {2019},
  keywords = {message passing, state-space models, active inference,
              Forney-style factor graphs, free-energy principle},
  pages    = {20},
  file     = {
              Snapshot:/Users/apodusenko/Zotero/storage/HPMKB26W/full.html:text/html;van
              de Laar and de Vries - 2019 - Simulating Active Inference Processes by
              Message P.pdf:/Users/apodusenko/Zotero/storage/VRYUIC69/van de Laar and
              de Vries - 2019 - Simulating Active Inference Processes by Message
              P.pdf:application/pdf;van de Laar en de Vries - 2019 - Simulating
              Active Inference Processes by Message
              P.pdf:/Users/apodusenko/Zotero/storage/KMFC2T35/van de Laar en de Vries
              - 2019 - Simulating Active Inference Processes by Message
              P.pdf:application/pdf}
}

@article{tipping_sparse_2001,
  title    = {Sparse {Bayesian} {Learning} and the {Relevance} {Vector} {Machine}},
  abstract = {This paper introduces a general Bayesian framework for obtaining
              sparse solutions to regression and classification tasks utilising
              models linear in the parameters. Although this framework is fully
              general, we illustrate our approach with a particular
              specialisation that we denote the 'relevance vector machine' (RVM),
              a model of identical functional form to the popular and
              state-of-the-art 'support vector machine' (SVM). We demonstrate
              that by exploiting a probabilistic Bayesian learning framework, we
              can derive accurate prediction models which typically utilise
              dramatically fewer basis functions than a comparable SVM while
              offering a number of additional advantages. These include the
              benefits of probabilistic predictions, automatic estimation of
              'nuisance' parameters, and the facility to utilise arbitrary basis
              functions (e.g. non-'Mercer' kernels). We detail the Bayesian
              framework and associated learning algorithm for the RVM, and give
              some illustrative examples of its application along with some
              comparative benchmarks. We offer some explanation for the
              exceptional degree of sparsity obtained, and discuss and
              demonstrate some of the advantageous features, and potential
              extensions, of Bayesian relevance learning.},
  number   = {1},
  journal  = {Journal of Machine Learning Research},
  author   = {Tipping, Michael},
  year     = {2001},
  pages    = {211--244},
  file     = {Tipping - 2001 - Sparse Bayesian Learning and the Relevance Vector
              .pdf:/Users/apodusenko/Zotero/storage/RTMJ53BT/Tipping - 2001 - Sparse
              Bayesian Learning and the Relevance Vector .pdf:application/pdf;Tipping
              - 2001 - Sparse Bayesian Learning and the Relevance Vector
              .pdf:/Users/apodusenko/Zotero/storage/6NNVEI6M/Tipping - 2001 - Sparse
              Bayesian Learning and the Relevance Vector .pdf:application/pdf}
}

@article{mootoovaloo_bayes_2016,
  title    = {Bayes {Factors} via {Savage}-{Dickey} {Supermodels}},
  url      = {http://arxiv.org/abs/1609.02186},
  abstract = {We outline a new method to compute the Bayes Factor for model
              selection which bypasses the Bayesian Evidence. Our method combines
              multiple models into a single, nested, Supermodel using one or more
              hyperparameters. Since the models are now nested the Bayes Factors
              between the models can be efficiently computed using the
              Savage-Dickey Density Ratio (SDDR). In this way model selection
              becomes a problem of parameter estimation. We consider two ways of
              constructing the supermodel in detail: one based on combined models
              , and a second based on combined likelihoods. We report on these
              two approaches for a Gaussian linear model for which the Bayesian
              evidence can be calculated analytically and a toy nonlinear
              problem. Unlike the combined model approach, where a standard Monte
              Carlo Markov Chain (MCMC) struggles, the combined-likelihood
              approach fares much better in providing a reliable estimate of the
              log-Bayes Factor. This scheme potentially opens the way to
              computationally efficient ways to compute Bayes Factors in high
              dimensions that exploit the good scaling properties of MCMC, as
              compared to methods such as nested sampling that fail for high
              dimensions.},
  urldate  = {2016-11-09},
  journal  = {arXiv:1609.02186 [astro-ph, stat]},
  author   = {Mootoovaloo, A. and Bassett, Bruce A. and Kunz, M.},
  month    = sep,
  year     = {2016},
  note     = {arXiv: 1609.02186},
  keywords = {Astrophysics - Instrumentation and Methods for Astrophysics,
              Statistics - Methodology},
  file     = {arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/Z5XUT8BE/1609.html:text/html;arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/TEBP4PBY/1609.html:text/html;Mootoovaloo
              et al. - 2016 - Bayes Factors via Savage-Dickey
              Supermodels.pdf:/Users/apodusenko/Zotero/storage/6CC6IXZZ/Mootoovaloo
              et al. - 2016 - Bayes Factors via Savage-Dickey
              Supermodels.pdf:application/pdf;Mootoovaloo et al. - 2016 - Bayes
              Factors via Savage-Dickey
              Supermodels.pdf:/Users/apodusenko/Zotero/storage/F8TPS4UK/Mootoovaloo
              et al. - 2016 - Bayes Factors via Savage-Dickey
              Supermodels.pdf:application/pdf}
}

@article{lin_variational_2018,
  title    = {{VARIATIONAL} {MESSAGE} {PASSING} {WITH} {STRUCTURED} {INFERENCE} {
              NETWORKS}},
  abstract = {Recent efforts on combining deep models with probabilistic
              graphical models are promising in providing ﬂexible models that are
              also easy to interpret. We propose a variational message-passing
              algorithm for variational inference in such models. We make three
              contributions. First, we propose structured inference networks that
              incorporate the structure of the graphical model in the inference
              network of variational auto-encoders (VAE). Second, we establish
              conditions under which such inference networks enable fast
              amortized inference similar to VAE. Finally, we derive a
              variational message passing algorithm to perform efﬁcient
              naturalgradient inference while retaining the efﬁciency of the
              amortized inference. By simultaneously enabling structured,
              amortized, and natural-gradient inference for deep structured
              models, our method simpliﬁes and generalizes existing methods.},
  language = {en},
  author   = {Lin, Wu and Hubacher, Nicolas and Khan, Mohammad Emtiyaz},
  year     = {2018},
  keywords = {Statistics - Machine Learning},
  pages    = {13},
  file     = {arXiv\:1803.05589 PDF:/Users/apodusenko/Zotero/storage/HCMFYCUC/Lin et
              al. - 2018 - Variational Message Passing with Structured
              Infere.pdf:application/pdf;arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/M9ADUK5B/1803.html:text/html;Lin
              et al. - 2018 - VARIATIONAL MESSAGE PASSING WITH STRUCTURED
              INFERE.pdf:/Users/apodusenko/Zotero/storage/TSVMZJG6/Lin et al. - 2018
              - VARIATIONAL MESSAGE PASSING WITH STRUCTURED
              INFERE.pdf:application/pdf}
}

@article{kulhavy_kullback-leibler_1996,
  title    = {A {Kullback}-{Leibler} distance approach to system identification},
  volume   = {20},
  issn     = {1367-5788},
  url      = {http://www.sciencedirect.com/science/article/pii/S1367578897000102},
  doi      = {10.1016/S1367-5788(97)00010-2},
  abstract = {The use of probability in system identification is shown to be
              equivalent to measuring Kullback-Leibler distance between the
              actual (empirical) and model distributions of data. When data are
              not known completely (being compressed, quantized, aggregated,
              missing etc.), the minimum distance approach can be seen as an
              asymptotic approximation of probabilistic inference. A class of
              problems is pointed out where inference via Kullback-Leibler
              distance brings an attractive, computationally less demanding
              alternative to maximum likelihood or Bayesian estimation.},
  urldate  = {2015-10-13},
  journal  = {Annual Reviews in Control},
  author   = {Kulhavý, Rudolf},
  year     = {1996},
  keywords = {Algorithms, system identification, adaptation, asymptotic
              approximation, large deviations, model approximation, statistical
              inference},
  pages    = {119--130},
  file     = {Kulhavý - 1996 - A Kullback-Leibler distance approach to system
              ide.pdf:/Users/apodusenko/Zotero/storage/SJDIDWEU/Kulhavý - 1996 - A
              Kullback-Leibler distance approach to system
              ide.pdf:application/pdf;Kulhavý - 1996 - A Kullback-Leibler distance
              approach to system
              ide.pdf:/Users/apodusenko/Zotero/storage/6T23HXYU/Kulhavý - 1996 - A
              Kullback-Leibler distance approach to system
              ide.pdf:application/pdf;ScienceDirect
              Snapshot:/Users/apodusenko/Zotero/storage/DKCG5KI4/S1367578897000102.html:text/html;ScienceDirect
              Snapshot:/Users/apodusenko/Zotero/storage/7GQP622H/S1367578897000102.html:text/html
              }
}

@article{deng_sparse_2015,
  title    = {Sparse {Hidden} {Markov} {Models} for {Speech} {Enhancement} in {Non}
              -{Stationary} {Noise} {Environments}},
  volume   = {23},
  issn     = {2329-9290},
  doi      = {10.1109/TASLP.2015.2458585},
  abstract = {We propose a sparse hidden Markov model (HMM)-based single-channel
              speech enhancement method that models the speech and noise gains
              accurately in non-stationary noise environments. Autoregressive
              models are employed to describe the speech and noise in a unified
              framework and the speech and noise gains are modeled as random
              processes with memory. The likelihood criterion for finding the
              model parameters is augmented with an regularization term resulting
              in a sparse autoregressive HMM (SARHMM) system that encourages
              sparsity in the speech- and noise- modeling. In the SARHMM only a
              small number of HMM states contribute significantly to the model of
              each particular observed speech segment. As it eliminates ambiguity
              between noise and speech spectra, the sparsity of speech and noise
              modeling helps to improve the tracking of the changes of both
              spectral shapes and power levels of non-stationary noise. Using the
              modeled speech and noise SARHMMs, we first construct a noise
              estimator to estimate the noise power spectrum. Then, a Bayesian
              speech estimator is derived to obtain the enhanced speech signal.
              The subjective and objective test results indicate that the
              proposed speech enhancement scheme can achieve a larger segmental
              SNR improvement, a lower log-spectral distortion and a better
              speech quality in stationary noise conditions than state-of-the-art
              reference methods. The advantage of the new method is largest for
              non-stationary noise conditions .},
  number   = {11},
  journal  = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  author   = {Deng, F. and Bao, C. and Kleijn, W.B.},
  month    = nov,
  year     = {2015},
  keywords = {Noise, belief networks, autoregressive processes, Bayesian speech
              estimator, Gain modeling, Hidden Markov models, Speech enhancement,
              Noise measurement, Speech, speech quality, Mathematical model, SE,
              hidden Markov models, speech enhancement, autoregressive models,
              HMM-based single-channel speech enhancement method, lower
              log-spectral distortion, noise estimator, non-stationary noise,
              nonstationary noise environments, random processes, SARHMM system,
              sparse autoregressive hidden Markov model (ARHMM), sparse hidden
              Markov models},
  pages    = {1973--1987},
  file     = {Deng et al. - 2015 - Sparse Hidden Markov Models for Speech
              Enhancement.pdf:/Users/apodusenko/Zotero/storage/QSRFGMQ3/Deng et al. -
              2015 - Sparse Hidden Markov Models for Speech
              Enhancement.pdf:application/pdf;IEEE Xplore Abstract
              Record:/Users/apodusenko/Zotero/storage/TMUU3TR4/abs_all.html:text/html;IEEE
              Xplore Abstract
              Record:/Users/apodusenko/Zotero/storage/3T3YCX9D/7163326.html:text/html;IEEE
              Xplore Full Text PDF:/Users/apodusenko/Zotero/storage/NIR4YTH5/Deng et
              al. - 2015 - Sparse Hidden Markov Models for Speech
              Enhancement.pdf:application/pdf}
}

@article{caticha_entropic_2011,
  title    = {Entropic {Inference}},
  url      = {http://arxiv.org/abs/1011.0723},
  doi      = {10.1063/1.3573619},
  abstract = {In this tutorial we review the essential arguments behing entropic
              inference. We focus on the epistemological notion of information
              and its relation to the Bayesian beliefs of rational agents. The
              problem of updating from a prior to a posterior probability
              distribution is tackled through an eliminative induction process
              that singles out the logarithmic relative entropy as the unique
              tool for inference. The resulting method of Maximum relative
              Entropy (ME), includes as special cases both MaxEnt and Bayes' rule
              , and therefore unifies the two themes of these workshops -- the
              Maximum Entropy and the Bayesian methods -- into a single general
              inference scheme.},
  urldate  = {2017-12-17},
  journal  = {arXiv:1011.0723 [cond-mat, physics:physics, stat]},
  author   = {Caticha, Ariel},
  year     = {2011},
  note     = {arXiv: 1011.0723},
  keywords = {Condensed Matter - Statistical Mechanics, Statistics - Methodology
              , Physics - Data Analysis, Statistics and Probability},
  pages    = {20--29},
  file     = {arXiv Fulltext PDF:/Users/apodusenko/Zotero/storage/S69VJSJS/Caticha -
              2011 - Entropic Inference.pdf:application/pdf;arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/2CUHKNGP/1011.html:text/html;arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/N9HWVPJE/1011.html:text/html;Caticha
              - 2011 - Entropic
              Inference.pdf:/Users/apodusenko/Zotero/storage/VAX4X77Q/Caticha - 2011
              - Entropic Inference.pdf:application/pdf}
}

@misc{noauthor_interspeech_nodate,
  title   = {{INTERSPEECH} 2010 {Abstract}: {Hershey} et al.},
  url     = {https://www.isca-speech.org/archive/interspeech_2010/i10_0334.html},
  urldate = {2020-02-21},
  file    = {INTERSPEECH 2010 Abstract\: Hershey et
             al.:/Users/apodusenko/Zotero/storage/95H2IIUT/i10_0334.html:text/html}
}

@article{kates_principles_2005,
  title    = {Principles of {Digital} {Dynamic}-{Range} {Compression}},
  volume   = {9},
  issn     = {1084-7138},
  url      = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4111488/},
  doi      = {10.1177/108471380500900202},
  abstract = {This article provides an overview of dynamic-range compression in
              digital hearing aids. Digital technology is becoming increasingly
              common in hearing aids, particularly because of the processing
              flexibility it offers and the opportunity to create more-effective
              devices. The focus of the paper is on the algorithms used to build
              digital compression systems. Of the various approaches that can be
              used to design a digital hearing aid, this paper considers
              broadband compression, multi-channel filter banks, a
              frequency-domain compressor using the FFT, the side-branch design
              that separates the filtering operation from the frequency analysis,
              and the frequency-warped version of the side-branch approach that
              modifies the analysis frequency spacing to more closely match
              auditory perception. Examples of the compressor frequency
              resolution, group delay, and compression behavior are provided for
              the different design approaches.},
  number   = {2},
  urldate  = {2020-02-20},
  journal  = {Trends in Amplification},
  author   = {Kates, James M.},
  year     = {2005},
  pmid     = {16012704},
  pmcid    = {PMC4111488},
  pages    = {45--76},
  file     = {PubMed Central Full Text
              PDF:/Users/apodusenko/Zotero/storage/9LYYL9GC/Kates - 2005 - Principles
              of Digital Dynamic-Range Compression.pdf:application/pdf}
}

@article{smith_active_2019,
  title     = {An active inference approach to modeling concept learning},
  copyright = {© 2019, Posted by Cold Spring Harbor Laboratory. This pre-print
               is available under a Creative Commons License
               (Attribution-NonCommercial-NoDerivs 4.0 International), CC
               BY-NC-ND 4.0, as described at
               http://creativecommons.org/licenses/by-nc-nd/4.0/},
  url       = {https://www.biorxiv.org/content/10.1101/633677v3},
  doi       = {10.1101/633677},
  abstract  = {{\textless}h3{\textgreater}Abstract{\textless}/h3{\textgreater} {
               \textless}p{\textgreater}Within computational neuroscience, the
               algorithmic and neural basis of concept learning remains poorly
               understood. Concept learning requires both a type of internal model
               expansion process (adding novel hidden states that explain new
               observations), and a model reduction process (merging different
               states into one underlying cause and thus reducing model complexity
               via meta-learning). Although various algorithmic models of concept
               learning have been proposed within machine learning and cognitive
               science, many are limited to various degrees by an inability to
               generalize, the need for very large amounts of training data,
               and/or insufficiently established biological plausibility. In this
               paper, we articulate a model of concept learning based on active
               inference and its accompanying neural process theory, with the idea
               that a generative model can be equipped with extra (hidden state or
               cause) ‘slots’ that can be engaged when an agent learns about novel
               concepts. This can be combined with a Bayesian model reduction
               process, in which any concept learning – associated with these
               slots – can be reset in favor of a simpler model with higher model
               evidence. We use simulations to illustrate this model’s ability to
               add new concepts to its state space (with relatively few
               observations) and increase the granularity of the concepts it
               currently possesses. We further show that it accomplishes a simple
               form of ‘one-shot’ generalization to new stimuli. Although
               deliberately simple, these results suggest that active inference
               may offer useful resources in developing neurocomputational models
               of concept learning.{\textless}/p{\textgreater}},
  language  = {en},
  urldate   = {2019-09-28},
  journal   = {bioRxiv},
  author    = {Smith, Ryan and Schwartenbeck, Philipp and Parr, Thomas and Friston,
               Karl J.},
  month     = may,
  year      = {2019},
  pages     = {633677},
  file      = {Smith et al. - 2019 - An active inference approach to modeling concept
               l.pdf:/Users/apodusenko/Zotero/storage/IHK2GNLH/Smith et al. - 2019 -
               An active inference approach to modeling concept
               l.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/ZAW6QZLC/633677v3.html:text/html
               }
}

@techreport{minka_gates:_2008,
  title    = {Gates: {A} {Graphical} {Notation} for {Mixture} {Models}},
  url      = {
              https://www.microsoft.com/en-us/research/publication/gates-a-graphical-notation-for-mixture-models/
              },
  abstract = {Gates are a new notation for representing mixture models and
              context-sensitive independence in factor graphs. Factor graphs
              provide a natural representation for message-passing algorithms for
              probabilistic inference, such as expectation propagation. However,
              message passing in mixture models is not well captured by factor
              graphs unless the entire mixture is represented by one factor,
              because the message equations have a containment structure. Gates
              capture this containment structure graphically, allowing both the
              independences and the message-passing equations for a model to be
              readily visualized. Different variational approximations for
              mixture models can be understood as different ways of drawing the
              gates in a model. We present general equations for expectation
              propagation and variational message passing in the presence of
              gates.},
  author   = {Minka, Tom and Winn, John},
  month    = dec,
  year     = {2008},
  file     = {Minka en Winn - 2008 - Gates A Graphical Notation for Mixture
              Models.pdf:/Users/apodusenko/Zotero/storage/MFAFEJBV/Minka en Winn -
              2008 - Gates A Graphical Notation for Mixture
              Models.pdf:application/pdf}
}

@article{meyer_sigma_2013,
  title    = {Sigma {Point} {Belief} {Propagation}},
  url      = {http://arxiv.org/abs/1309.0363},
  abstract = {The sigma point (SP) filter, also known as unscented Kalman filter
              , is an attractive alternative to the extended Kalman filter and
              the particle filter. Here, we extend the SP filter to nonsequential
              Bayesian inference corresponding to loopy factor graphs. We propose
              sigma point belief propagation (SPBP) as a low-complexity
              approximation of the belief propagation (BP) message passing
              scheme. SPBP achieves approximate marginalizations of posterior
              distributions corresponding to (generally) loopy factor graphs. It
              is well suited for decentralized inference because of its low
              communication requirements. For a decentralized, dynamic sensor
              localization problem, we demonstrate that SPBP can outperform
              nonparametric (particle-based) BP while requiring significantly
              less computations and communications.},
  urldate  = {2020-02-14},
  journal  = {arXiv:1309.0363 [cs]},
  author   = {Meyer, Florian and Hlinka, Ondrej and Hlawatsch, Franz},
  month    = nov,
  year     = {2013},
  note     = {arXiv: 1309.0363},
  keywords = {Computer Science - Artificial Intelligence, Computer Science -
              Distributed, Parallel, and Cluster Computing},
  file     = {arXiv Fulltext PDF:/Users/apodusenko/Zotero/storage/P5I3B9TB/Meyer et
              al. - 2013 - Sigma Point Belief
              Propagation.pdf:application/pdf;arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/FG78T2XR/1309.html:text/html}
}

@article{griffin_signal_1984,
  title    = {Signal estimation from modified short-time {Fourier} transform},
  volume   = {32},
  issn     = {0096-3518},
  doi      = {10.1109/TASSP.1984.1164317},
  abstract = {In this paper, we present an algorithm to estimate a signal from
              its modified short-time Fourier transform (STFT). This algorithm is
              computationally simple and is obtained by minimizing the mean
              squared error between the STFT of the estimated signal and the
              modified STFT. Using this algorithm, we also develop an iterative
              algorithm to estimate a signal from its modified STFT magnitude.
              The iterative algorithm is shown to decrease, in each iteration,
              the mean squared error between the STFT magnitude of the estimated
              signal and the modified STFT magnitude. The major computation
              involved in the iterative algorithm is the discrete Fourier
              transform (DFT) computation, and the algorithm appears to be
              real-time implementable with current hardware technology. The
              algorithm developed in this paper has been applied to the
              time-scale modification of speech. The resulting system generates
              very high-quality speech, and appears to be better in performance
              than any existing method.},
  number   = {2},
  journal  = {IEEE Transactions on Acoustics, Speech, and Signal Processing},
  author   = {Griffin, D. and Jae Lim},
  month    = apr,
  year     = {1984},
  keywords = {Hardware, Fourier transforms, Sampling methods, Iterative
              algorithms, Speech enhancement, Degradation, Monitoring, Signal
              processing, Discrete Fourier transforms, Estimation theory},
  pages    = {236--243},
  file     = {IEEE Xplore Abstract
              Record:/Users/apodusenko/Zotero/storage/9I3P49RC/1164317.html:text/html
              }
}

@article{achan_probabilistic_nodate,
  title    = {Probabilistic {Inference} of {Speech} {Signals} from {Phaseless} {
              Spectrograms}},
  abstract = {Many techniques for complex speech processing such as denoising
              and deconvolution, time/frequency warping, multiple speaker
              separation, and multiple microphone analysis operate on sequences
              of short-time power spectra (spectrograms), a representation which
              is often well-suited to these tasks. However, a signiﬁcant problem
              with algorithms that manipulate spectrograms is that the output
              spectrogram does not include a phase component, which is needed to
              create a time-domain signal that has good perceptual quality. Here
              we describe a generative model of time-domain speech signals and
              their spectrograms, and show how an efﬁcient optimizer can be used
              to ﬁnd the maximum a posteriori speech signal, given the
              spectrogram. In contrast to techniques that alternate between
              estimating the phase and a spectrally-consistent signal, our
              technique directly infers the speech signal, thus jointly
              optimizing the phase and a spectrally-consistent signal. We compare
              our technique with a standard method using signal-to-noise ratios,
              but we also provide audio ﬁles on the web for the purpose of
              demonstrating the improvement in perceptual quality that our
              technique offers.},
  language = {en},
  author   = {Achan, Kannan and Roweis, Sam T and Frey, Brendan J},
  pages    = {8},
  file     = {Achan et al. - Probabilistic Inference of Speech Signals from
              Pha.pdf:/Users/apodusenko/Zotero/storage/QH67QHRH/Achan et al. -
              Probabilistic Inference of Speech Signals from Pha.pdf:application/pdf}
}

@inproceedings{cemgil_probabilistic_2005,
  title     = {Probabilistic phase vocoder and its application to interpolation of
               missing values in audio signals},
  abstract  = {We formulate the phase vocoder - an audio synthesis method very
               closely related to inverse short time Fourier Transform synthesis -
               as a Gaussian state space model and demonstrate simulation results
               on interpolation of missing values. The audio signal is modelled as
               a superposition of quasi-sinusoidal signals generated by a linear
               dynamical system. The advantage of our “generative” perspective is
               that it allows a full Bayesian treatment of the problem; e.g. one
               can perform the analysis while arbitrary chunks of sample values
               are missing or model parameters are unknown. To perform audio
               restoration, we derive an expectation-maximisation (EM) algorithm
               that infers the expectations of missing samples and maximum
               a-posteriori model parameters. We demonstrate the validity of our
               approach on a set of challenging real audio examples and compare to
               existing methods.},
  booktitle = {2005 13th {European} {Signal} {Processing} {Conference}},
  author    = {Cemgil, Ali Taylan and Godsill, Simon J.},
  month     = sep,
  year      = {2005},
  note      = {ISSN: null},
  keywords  = {expectation-maximisation algorithm, Bayes methods, Signal to noise
               ratio, Vectors, linear dynamical system, audio coding, Discrete
               Fourier transforms, arbitrary chunks, audio restoration, audio
               signals, audio synthesis method, EM algorithm, full-Bayesian
               treatment, Gaussian state space model, interpolation, Interpolation
               , inverse short-time Fourier transform synthesis, maximum
               a-posteriori model parameters, missing sample expectation, missing
               value interpolation, probabilistic phase vocoder, quasisinusoidal
               signal superposition, vocoders, Vocoders},
  pages     = {1--4},
  file      = {IEEE Xplore Abstract
               Record:/Users/apodusenko/Zotero/storage/HFK7VY3S/7078066.html:text/html;IEEE
               Xplore Full Text PDF:/Users/apodusenko/Zotero/storage/FUNX4FQF/Cemgil
               and Godsill - 2005 - Probabilistic phase vocoder and its application
               to.pdf:application/pdf}
}

@article{wilson_gaussian_2013,
  title    = {Gaussian {Process} {Kernels} for {Pattern} {Discovery} and {
              Extrapolation}},
  url      = {http://arxiv.org/abs/1302.4245},
  abstract = {Gaussian processes are rich distributions over functions, which
              provide a Bayesian nonparametric approach to smoothing and
              interpolation. We introduce simple closed form kernels that can be
              used with Gaussian processes to discover patterns and enable
              extrapolation. These kernels are derived by modelling a spectral
              density -- the Fourier transform of a kernel -- with a Gaussian
              mixture. The proposed kernels support a broad class of stationary
              covariances, but Gaussian process inference remains simple and
              analytic. We demonstrate the proposed kernels by discovering
              patterns and performing long range extrapolation on synthetic
              examples, as well as atmospheric CO2 trends and airline passenger
              data. We also show that we can reconstruct standard covariances
              within our framework.},
  urldate  = {2020-02-11},
  journal  = {arXiv:1302.4245 [cs, stat]},
  author   = {Wilson, Andrew Gordon and Adams, Ryan Prescott},
  month    = dec,
  year     = {2013},
  note     = {arXiv: 1302.4245},
  keywords = {Statistics - Machine Learning, Computer Science - Artificial
              Intelligence, Statistics - Methodology},
  file     = {arXiv Fulltext PDF:/Users/apodusenko/Zotero/storage/9F58J53C/Wilson
              and Adams - 2013 - Gaussian Process Kernels for Pattern Discovery
              and.pdf:application/pdf;arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/WWZDNB9P/1302.html:text/html}
}

@inproceedings{alvarado_sparse_2019,
  title     = {Sparse {Gaussian} {Process} {Audio} {Source} {Separation} {Using} {
               Spectrum} {Priors} in the {Time}-domain},
  doi       = {10.1109/ICASSP.2019.8683287},
  abstract  = {Gaussian process (GP) audio source separation is a time- domain
               approach that circumvents the inherent phase approx- imation issue
               of spectrogram based methods. Furthermore, through its kernel, GPs
               elegantly incorporate prior knowl- edge about the sources into the
               separation model. Despite these compelling advantages, the
               computational complexity of GP inference scales cubically with the
               number of audio samples. As a result, source separation GP models
               have been restricted to the analysis of short audio frames. We
               intro- duce an efficient application of GPs to time-domain audio
               source separation, without compromising performance. For this
               purpose, we used GP regression, together with spectral mixture
               kernels, and variational sparse GPs. We compared our method with
               LD-PSDTF (positive semi-definite tensor factorization), KL-NMF
               (Kullback-Leibler non-negative ma- trix factorization), and IS-NMF
               (Itakura-Saito NMF). Results show that the proposed method
               outperforms these techniques.},
  booktitle = {{ICASSP} 2019 - 2019 {IEEE} {International} {Conference} on {
               Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
  author    = {Alvarado, Pablo A. and Alvarez, Mauricio A. and Stowell, Dan},
  month     = may,
  year      = {2019},
  note      = {ISSN: 1520-6149},
  keywords  = {computational complexity, Gaussian processes, source separation,
               variational inference, audio signal processing, audio samples,
               compressed sensing, GP regression, regression analysis, source
               separation GP models, sparse Gaussian process audio source
               separation, spectral mixture kernels, spectrogram based methods,
               spectrum priors, time-domain audio source separation, Time-domain
               source separation, variational sparse GPs},
  pages     = {995--999},
  file      = {IEEE Xplore Abstract
               Record:/Users/apodusenko/Zotero/storage/XKJ6WIEP/8683287.html:text/html;IEEE
               Xplore Full Text PDF:/Users/apodusenko/Zotero/storage/BDVF26DJ/Alvarado
               et al. - 2019 - Sparse Gaussian Process Audio Source Separation
               Us.pdf:application/pdf}
}

@book{makino_blind_2007,
  address   = {Dordrecht},
  series    = {Signals and communication technology},
  title     = {Blind speech separation},
  isbn      = {978-1-4020-6478-4 978-1-4020-6479-1},
  language  = {en},
  publisher = {Springer},
  editor    = {Makino, Shoji and Lee, Te-Won and Sawada, Hiroshi},
  year      = {2007},
  note      = {OCLC: 255606946},
  file      = {Makino et al. - 2007 - Blind speech
               separation.pdf:/Users/apodusenko/Zotero/storage/P4Q4YLU4/Makino et al.
               - 2007 - Blind speech separation.pdf:application/pdf}
}

@article{adiloglu_variational_2016,
  title    = {Variational {Bayesian} {Inference} for {Source} {Separation} and {
              Robust} {Feature} {Extraction}},
  volume   = {24},
  issn     = {2329-9290, 2329-9304},
  url      = {http://ieeexplore.ieee.org/document/7497597/},
  doi      = {10.1109/TASLP.2016.2583794},
  abstract = {We consider the task of separating and classifying individual
              sound sources mixed together. The main challenge is to achieve
              robust classiﬁcation despite residual distortion of the separated
              source signals. A promising paradigm is to estimate the uncertainty
              about the separated source signals and to propagate it through the
              subsequent feature extraction and classiﬁcation stages. We argue
              that variational Bayesian (VB) inference offers a mathematically
              rigorous way of deriving uncertainty estimators, which contrasts
              with state-of-theart estimators based on heuristics or on maximum
              likelihood (ML) estimation. We propose a general VB source
              separation algorithm, which makes it possible to jointly exploit
              spatial and spectral models of the sources. This algorithm achieves
              6\% and 5\% relative error reduction compared to ML uncertainty
              estimation on the CHiME noise-robust speaker identiﬁcation and
              speech recognition benchmarks, respectively, and it opens the way
              for more complex VB approximations of uncertainty.},
  language = {en},
  number   = {10},
  urldate  = {2020-02-10},
  journal  = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  author   = {Adiloglu, Kamil and Vincent, Emmanuel},
  month    = oct,
  year     = {2016},
  pages    = {1746--1758},
  file     = {Adiloglu and Vincent - 2016 - Variational Bayesian Inference for
              Source Separati.pdf:/Users/apodusenko/Zotero/storage/PU2W2K7K/Adiloglu
              and Vincent - 2016 - Variational Bayesian Inference for Source
              Separati.pdf:application/pdf}
}

@article{chien_bayesian_2016,
  title    = {Bayesian {Factorization} and {Learning} for {Monaural} {Source} {
              Separation}},
  volume   = {24},
  issn     = {2329-9304},
  doi      = {10.1109/TASLP.2015.2502141},
  abstract = {This paper presents a new Bayesian nonnegative matrix
              factorization (NMF) for monaural source separation. Using this
              approach, the reconstruction error based on NMF is represented by a
              Poisson distribution, and the NMF parameters, consisting of the
              basis and weight matrices, are characterized by the exponential
              priors. A variational Bayesian inference procedure is developed to
              learn variational parameters and model parameters. The randomness
              in separation process is faithfully represented so that the system
              robustness to model variations in heterogeneous environments could
              be achieved. Importantly, the exponential prior parameters are used
              to impose sparseness in basis representation. The variational lower
              bound of log marginal likelihood is adopted as the objective to
              control model complexity. The dependencies of variational objective
              on model parameters are fully characterized in the derived
              closed-form solution. A clustering algorithm is performed to find
              the groups of bases for unsupervised source separation. The
              experiments on speech/music separation and singing voice separation
              show that the proposed Bayesian NMF (BNMF) with adaptive basis
              representation outperforms the NMF with fixed number of bases and
              the other BNMFs in terms of signal-to-distortion ratio and the
              global normalized source to distortion ratio.},
  number   = {1},
  journal  = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  author   = {Chien, Jen-Tzung and Yang, Po-Kai},
  month    = jan,
  year     = {2016},
  keywords = {Standards, inference mechanisms, Bayes methods, variational
              techniques, Speech, learning (artificial intelligence), Maximum
              likelihood estimation, source separation, Robustness, Bayesian
              learning, Bayesian nonnegative matrix factorization, BNMF,
              closed-form solution, clustering algorithm, exponential prior
              parameters, global normalized source to distortion ratio,
              heterogeneous environments, log marginal likelihood, matrix
              decomposition, model complexity, monaural source separation, music
              separation, nonnegative matrix factorization, pattern clustering,
              Poisson distribution, reconstruction error, signal-to-distortion
              ratio, singing voice separation, Source separation, Spectrogram,
              speech separation, unsupervised source separation, variational
              Bayesian inference procedure, variational objective, weight
              matrices},
  pages    = {185--195},
  file     = {IEEE Xplore Abstract
              Record:/Users/apodusenko/Zotero/storage/MLMIFRVC/7331633.html:text/html;IEEE
              Xplore Full Text PDF:/Users/apodusenko/Zotero/storage/B9HE66JR/Chien
              and Yang - 2016 - Bayesian Factorization and Learning for Monaural
              S.pdf:application/pdf}
}

@article{kounades-bastian_variational_2016,
  title    = {A {Variational} {EM} {Algorithm} for the {Separation} of {Time}-{
              Varying} {Convolutive} {Audio} {Mixtures}},
  volume   = {24},
  issn     = {2329-9290, 2329-9304},
  url      = {http://arxiv.org/abs/1510.04595},
  doi      = {10.1109/TASLP.2016.2554286},
  abstract = {This paper addresses the problem of separating audio sources from
              time-varying convolutive mixtures. We propose a probabilistic
              framework based on the local complex-Gaussian model combined with
              non-negative matrix factorization. The time-varying mixing ﬁlters
              are modeled by a continuous temporal stochastic process. We present
              a variational expectationmaximization (VEM) algorithm that employs
              a Kalman smoother to estimate the time-varying mixing matrix, and
              that jointly estimate the source parameters. The sound sources are
              then separated by Wiener ﬁlters constructed with the estimators
              provided by the VEM algorithm. Extensive experiments on simulated
              data show that the proposed method outperforms a block-wise version
              of a state-of-the-art baseline method.},
  language = {en},
  number   = {8},
  urldate  = {2020-02-07},
  journal  = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  author   = {Kounades-Bastian, Dionyssos and Girin, Laurent and Alameda-Pineda,
              Xavier and Gannot, Sharon and Horaud, Radu},
  month    = aug,
  year     = {2016},
  note     = {arXiv: 1510.04595},
  keywords = {Computer Science - Sound},
  pages    = {1408--1423},
  file     = {Kounades-Bastian et al. - 2016 - A Variational EM Algorithm for the
              Separation of
              T.pdf:/Users/apodusenko/Zotero/storage/I24QI3QT/Kounades-Bastian et al.
              - 2016 - A Variational EM Algorithm for the Separation of
              T.pdf:application/pdf}
}

@article{turner_time-frequency_2014-1,
  title    = {Time-{Frequency} {Analysis} as {Probabilistic} {Inference}},
  volume   = {62},
  issn     = {1053-587X, 1941-0476},
  url      = {https://ieeexplore.ieee.org/document/6918491/},
  doi      = {10.1109/TSP.2014.2362100},
  abstract = {This paper proposes a new view of time-frequency analysis framed
              in terms of probabilistic inference. Natural signals are assumed to
              be formed by the superposition of distinct time-frequency
              components, with the analytic goal being to infer these components
              by application of Bayes’ rule. The framework serves to unify
              various existing models for natural time-series; it relates to both
              the Wiener and Kalman ﬁlters, and with suitable assumptions yields
              inferential interpretations of the short-time Fourier transform,
              spectrogram, ﬁlter bank, and wavelet representations. Value is
              gained by placing time-frequency analysis on the same probabilistic
              basis as is often employed in applications such as denoising,
              source separation, or recognition. Uncertainty in the
              time-frequency representation can be propagated correctly to
              application-speciﬁc stages, improving the handing of noise and
              missing data. Probabilistic learning allows modules to be
              co-adapted; thus, the time-frequency representation can be adapted
              to both the demands of the application and the time-varying
              statistics of the signal at hand. Similarly, the application module
              can be adapted to ﬁne properties of the signal propagated by the
              initial time-frequency processing. We demonstrate these beneﬁts by
              combining probabilistic time-frequency representations with
              non-negative matrix factorization, ﬁnding beneﬁts in audio
              denoising and inpainting tasks, albeit with higher computational
              cost than incurred by the standard approach.},
  language = {en},
  number   = {23},
  urldate  = {2020-02-07},
  journal  = {IEEE Transactions on Signal Processing},
  author   = {Turner, Richard E. and Sahani, Maneesh},
  month    = dec,
  year     = {2014},
  pages    = {6171--6183},
  file     = {Turner and Sahani - 2014 - Time-Frequency Analysis as Probabilistic
              Inference.pdf:/Users/apodusenko/Zotero/storage/4FZILJ23/Turner and
              Sahani - 2014 - Time-Frequency Analysis as Probabilistic
              Inference.pdf:application/pdf}
}

@article{itakura_bayesian_2018,
  title    = {Bayesian {Multichannel} {Audio} {Source} {Separation} {Based} on {
              Integrated} {Source} and {Spatial} {Models}},
  volume   = {26},
  issn     = {2329-9290, 2329-9304},
  url      = {http://ieeexplore.ieee.org/document/8276582/},
  doi      = {10.1109/TASLP.2017.2789320},
  abstract = {This paper presents new statistical methods of multichannel audio
              source separation based on uniﬁed source and spatial models that,
              respectively, represent the generative process of latent source
              spectrograms and that of observed mixture spectrograms. One
              possibility of the source model is a factor model based on
              nonnegative matrix factorization that represents each
              time-frequency (TF) bin as the weighted sum of basis spectra.
              Another possibility is a mixture model inspired by latent Dirichlet
              allocation that exclusively classiﬁes each TF bin into one of basis
              spectra. Similarly, the spatial model can either be a factor model
              that represents each TF bin as the weighted sum of source spectra
              or a mixture model that classiﬁes each bin into one of those
              spectra. To unify these models in a principled manner and
              incorporate prior knowledge of a microphone array, we propose
              hierarchical Bayesian models of all the source–spatial combinations
              (factor–factor, mixture–factor, factor–mixture, and mixture–mixture
              models) and derive efﬁcient Gibbs sampling algorithms for posterior
              inference. Experimental results showed that the proposed uniﬁed
              models outperformed the state-of-the-art method using only the
              spatial mixture model. Among the four uniﬁed models, the spatial
              factor model tended to work better than the spatial mixture model
              in exchange for larger computational cost, and the choice of source
              models had a little impact on the performance and computational
              cost.},
  language = {en},
  number   = {4},
  urldate  = {2020-02-07},
  journal  = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  author   = {Itakura, Kousuke and Bando, Yoshiaki and Nakamura, Eita and Itoyama,
              Katsutoshi and Yoshii, Kazuyoshi and Kawahara, Tatsuya},
  month    = apr,
  year     = {2018},
  pages    = {831--846},
  file     = {Itakura et al. - 2018 - Bayesian Multichannel Audio Source Separation
              Base.pdf:/Users/apodusenko/Zotero/storage/AMMICQM6/Itakura et al. -
              2018 - Bayesian Multichannel Audio Source Separation
              Base.pdf:application/pdf}
}

@article{li_deng_recursive_2003,
  title    = {Recursive estimation of nonstationary noise using iterative
              stochastic approximation for robust speech recognition},
  volume   = {11},
  issn     = {1558-2353},
  doi      = {10.1109/TSA.2003.818076},
  abstract = {We describe a novel algorithm for recursive estimation of
              nonstationary acoustic noise which corrupts clean speech, and a
              successful application of the algorithm in the speech feature
              enhancement framework of noise-normalized SPLICE for robust speech
              recognition. The noise estimation algorithm makes use of a
              nonlinear model of the acoustic environment in the cepstral domain.
              Central to the algorithm is the innovative iterative stochastic
              approximation technique that improves piecewise linear
              approximation to the nonlinearity involved and that subsequently
              increases the accuracy for noise estimation. We report
              comprehensive experiments on SPLICE-based, noise-robust speech
              recognition for the AURORA2 task using the results of iterative
              stochastic approximation. The effectiveness of the new technique is
              demonstrated in comparison with a more traditional, MMSE noise
              estimation algorithm under otherwise identical conditions. The word
              error rate reduction achieved by iterative stochastic approximation
              for recursive noise estimation in the framework of noise-normalized
              SPLICE is 27.9\% for the multicondition training mode, and 67.4\%
              for the clean-only training mode, respectively, compared with the
              results using the standard cepstra with no speech enhancement and
              using the baseline HMM supplied by AURORA2. These represent the
              best performance in the clean-training category of the
              September-2001 AURORA2 evaluation. The relative error rate
              reduction achieved by using the same noise estimate is increased to
              48.40\% and 76.86\%, respectively, for the two training modes after
              using a better designed HMM system. The experimental results
              demonstrated the crucial importance of using the newly introduced
              iterations in improving the earlier stochastic approximation
              technique, and showed sensitivity of the noise estimation
              algorithm's performance to the forgetting factor embedded in the
              algorithm.},
  number   = {6},
  journal  = {IEEE Transactions on Speech and Audio Processing},
  author   = {Li Deng and Droppo, J. and Acero, A.},
  month    = nov,
  year     = {2003},
  keywords = {Approximation algorithms, Iterative algorithms, iterative methods,
              Acoustic noise, hidden Markov model, Recursive estimation, Speech
              enhancement, speech recognition, Stochastic resonance, Working
              environment noise, least mean squares methods, cepstral analysis,
              Noise robustness, Piecewise linear approximation, hidden Markov
              models, speech enhancement, MMSE, acoustic noise, AURORA2 task,
              cepstral domain, clean-training category, HMM system, iterative
              stochastic approximation, minimum mean squared error,
              noise-normalized SPLICE, nonlinear model, nonstationary acoustic
              noise, piecewise linear approximation, piecewise linear techniques,
              recursive estimation, recursive noise estimation, Speech
              recognition, stereo-based piecewise linear compensation for
              environment, stochastic processes},
  pages    = {568--580},
  file     = {IEEE Xplore Abstract
              Record:/Users/apodusenko/Zotero/storage/3LEYNRE7/1255445.html:text/html;IEEE
              Xplore Full Text PDF:/Users/apodusenko/Zotero/storage/3AKGWKUL/Li Deng
              et al. - 2003 - Recursive estimation of nonstationary noise using
              .pdf:application/pdf}
}

@article{dionelis_phase-aware_2018,
  title    = {Phase-{Aware} {Single}-{Channel} {Speech} {Enhancement} {With} {
              Modulation}-{Domain} {Kalman} {Filtering}},
  volume   = {26},
  issn     = {2329-9304},
  doi      = {10.1109/TASLP.2018.2800525},
  abstract = {We present a speech enhancement algorithm that performs
              modulation-domain Kalman filtering to track the speech phase using
              circular statistics, along with the spectral log-amplitudes of
              speech and noise. In the proposed algorithm, the speech phase
              posterior is used to create an enhanced speech phase spectrum for
              the signal reconstruction of speech. The Kalman filter prediction
              step separately models the temporal inter-frame correlation of the
              speech and noise spectral log-amplitudes and of the speech phase,
              while the Kalman filter update step models their nonlinear
              relations under the assumption that speech and noise add in the
              complex short-time Fourier transform domain. The phase-sensitive
              enhancement algorithm is evaluated with speech quality and
              intelligibility metrics, using a variety of noise types over a
              range of SNRs. Instrumental measures predict that tracking the
              speech log-spectrum and phase with modulation-domain Kalman
              filtering leads to consistent improvements in speech quality, over
              both conventional enhancement algorithms and other algorithms that
              perform modulation-domain Kalman filtering.},
  number   = {5},
  journal  = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  author   = {Dionelis, Nikolaos and Brookes, Mike},
  month    = may,
  year     = {2018},
  keywords = {Kalman filters, Fourier transforms, Speech enhancement, Noise
              measurement, Speech, speech quality, filtering theory, Correlation,
              speech enhancement, enhanced speech phase spectrum, intelligibility
              metrics, Kalman filter prediction step, Kalman filter update step
              models, modulation-domain Kalman filtering, noise spectral
              log-amplitudes, nonlinear relations, phase-aware single-channel
              speech enhancement, phase-sensitive enhancement algorithm,
              Prediction algorithms, signal reconstruction, Spectral analysis,
              speech enhancement algorithm, speech log-spectrum, speech phase,
              speech phase posterior},
  pages    = {937--950},
  file     = {IEEE Xplore Abstract
              Record:/Users/apodusenko/Zotero/storage/34WS6X26/8276302.html:text/html;IEEE
              Xplore Full Text PDF:/Users/apodusenko/Zotero/storage/Y8MLK7XT/Dionelis
              and Brookes - 2018 - Phase-Aware Single-Channel Speech Enhancement
              With.pdf:application/pdf}
}

@article{da_costa_active_2020,
  title      = {Active inference on discrete state-spaces: a synthesis},
  shorttitle = {Active inference on discrete state-spaces},
  url        = {http://arxiv.org/abs/2001.07203},
  abstract   = {Active inference is a normative principle underwriting perception,
                action, planning, decision-making and learning in biological or
                artificial agents. From its inception, its associated process
                theory has grown to incorporate complex generative models, enabling
                simulation of a wide range of complex behaviours. Due to successive
                developments in active inference, it is often difficult to see how
                its underlying principle relates to process theories and practical
                implementation. In this paper, we try to bridge this gap by
                providing a complete mathematical synthesis of active inference on
                discrete state-space models. This technical summary provides an
                overview of the theory, derives neuronal dynamics from first
                principles and relates this dynamics to biological processes.
                Furthermore, this paper provides a fundamental building block
                needed to understand active inference for mixed generative models;
                allowing continuous sensations to inform discrete representations.
                This paper may be used as follows: to guide research towards
                outstanding challenges, a practical guide on how to implement
                active inference to simulate experimental behaviour, or a pointer
                towards various in-silico neurophysiological responses that may be
                used to make empirical predictions.},
  urldate    = {2020-01-27},
  journal    = {arXiv:2001.07203 [q-bio]},
  author     = {Da Costa, Lancelot and Parr, Thomas and Sajid, Noor and Veselic,
                Sebastijan and Neacsu, Victorita and Friston, Karl},
  month      = jan,
  year       = {2020},
  note       = {arXiv: 2001.07203},
  keywords   = {Quantitative Biology - Neurons and Cognition},
  file       = {arXiv.org
                Snapshot:/Users/apodusenko/Zotero/storage/4EPUH3D8/2001.html:text/html;Da
                Costa et al. - 2020 - Active inference on discrete state-spaces a
                synth.pdf:/Users/apodusenko/Zotero/storage/8UN7KDCC/Da Costa et al. -
                2020 - Active inference on discrete state-spaces a
                synth.pdf:application/pdf}
}

@misc{noauthor_modulation_nodate,
  title   = {Modulation frequency and efficient audio coding},
  url     = {
             https://www.spiedigitallibrary.org/conference-proceedings-of-spie/4474/0000/Modulation-frequency-and-efficient-audio-coding/10.1117/12.448636.full?SSO=1
             },
  urldate = {2020-01-24},
  file    = {Modulation frequency and efficient audio
             coding:/Users/apodusenko/Zotero/storage/6PBXXXPY/12.448636.html:text/html
             }
}

@article{low_compressive_2018,
  title    = {Compressive speech enhancement in the modulation domain},
  volume   = {102},
  issn     = {0167-6393},
  url      = {http://www.sciencedirect.com/science/article/pii/S0167639317303655},
  doi      = {10.1016/j.specom.2018.08.003},
  abstract = {Compressive speech enhancement (CSE) has gained popularity in
              recent years as it bypasses the need for noise estimation. Parallel
              to that, modulation domain has been widely studied in speech
              applications as it offers a more compact representation and is
              closely associated with speech intelligibility enhancement.
              Motivated by the development in modulation domain and CSE, this
              paper seeks to explore the suitability of modulation domain based
              sparse reconstruction for use in CSE. The main idea is to study if
              the increased sparsity in the modulation domain would benefit
              sparse reconstruction in CSE. The findings reveal that modulation
              transformation is sparser and offers a stronger restricted isometry
              property (RIP) compared to the frequency transformation, which is
              essential for sparse recovery with a high probability. The results
              are then extended to show that the sparse reconstruction error in
              the modulation domain is upper bounded by the frequency domain.
              Experimental results in a CSE setting concur with the theoretical
              derivations, with modulation domain CSE outperforming the frequency
              domain CSE through different speech quality measures.},
  language = {en},
  urldate  = {2020-01-24},
  journal  = {Speech Communication},
  author   = {Low, Siow Yong},
  month    = sep,
  year     = {2018},
  keywords = {Speech enhancement, Modulation domain, Compressed sensing,
              Compressibility, Modulation spectrum, Sparsity},
  pages    = {87--99},
  file     = {ScienceDirect Full Text
              PDF:/Users/apodusenko/Zotero/storage/QXCJZ7I3/Low - 2018 - Compressive
              speech enhancement in the modulation
              d.pdf:application/pdf;ScienceDirect
              Snapshot:/Users/apodusenko/Zotero/storage/HGJVEV5L/S0167639317303655.html:text/html
              }
}

@article{kuper_numerical_2019,
  title    = {Numerical {Gaussian} process {Kalman} filtering},
  url      = {http://arxiv.org/abs/1912.01234},
  abstract = {Numerical Gaussian processes have recently been developed to
              handle spatiotemporal models. The contribution of this paper is to
              embed numerical Gaussian processes into the well established
              recursive Kalman filter equations. This enables us to do Kalman
              filtering for infinite-dimensional systems with Gaussian processes.
              This is possible because i) we are obtaining a linear model from
              numerical Gaussian processes, and ii) the states of which are by
              definition Gaussian distributed random variables. Convenient
              properties of the numerical GPKF are that no spatial discretization
              is necessary, and setting up of the Kalman filter, namely the
              process and measurement noise levels, need not be fine-tuned by
              hand, as they are hyper-parameters of the Gaussian process and
              learned online on the data stream. We showcase the capability of
              the numerical GPKF in a simulation study of a heterogeneous cell
              population displaying cell-to-cell variability in cell size.},
  urldate  = {2020-01-16},
  journal  = {arXiv:1912.01234 [cs, eess, stat]},
  author   = {Küper, Armin and Waldherr, Steffen},
  month    = dec,
  year     = {2019},
  note     = {arXiv: 1912.01234},
  keywords = {Statistics - Machine Learning, Statistics - Computation, Computer
              Science - Machine Learning, Electrical Engineering and Systems
              Science - Signal Processing},
  file     = {arXiv Fulltext PDF:/Users/apodusenko/Zotero/storage/WRZDQ3G4/Küper and
              Waldherr - 2019 - Numerical Gaussian process Kalman
              filtering.pdf:application/pdf;arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/CYFNV9X3/1912.html:text/html}
}

@article{golub_calculation_1969,
  title    = {Calculation of {Gauss} {Quadrature} {Rules}},
  volume   = {23},
  url      = {https://www.jstor.org/stable/2004418},
  doi      = {10.2307/2004418},
  language = {en},
  journal  = {Mathematics of Computation},
  author   = {Golub, Gene H and Welsch, John H},
  month    = apr,
  year     = {1969},
  file     = {Golub and Welsch - Calculation of Gauss Quadrature
              Rules.pdf:/Users/apodusenko/Zotero/storage/IDAQEE9N/Golub and Welsch -
              Calculation of Gauss Quadrature Rules.pdf:application/pdf}
}

@article{shannon_autoregressive_2013,
  title    = {Autoregressive {Models} for {Statistical} {Parametric} {Speech} {
              Synthesis}},
  volume   = {21},
  issn     = {1558-7924},
  doi      = {10.1109/TASL.2012.2227740},
  abstract = {We propose using the autoregressive hidden Markov model (HMM) for
              speech synthesis. The autoregressive HMM uses the same model for
              parameter estimation and synthesis in a consistent way, in contrast
              to the standard approach to statistical parametric speech
              synthesis. It supports easy and efficient parameter estimation
              using expectation maximization, in contrast to the trajectory HMM.
              At the same time its similarities to the standard approach allow
              use of established high quality synthesis algorithms such as speech
              parameter generation considering global variance. The
              autoregressive HMM also supports a speech parameter generation
              algorithm not available for the standard approach or the trajectory
              HMM and which has particular advantages in the domain of real-time,
              low latency synthesis. We show how to do efficient parameter
              estimation and synthesis with the autoregressive HMM and look at
              some of the similarities and differences between the standard
              approach, the trajectory HMM and the autoregressive HMM. We compare
              the three approaches in subjective and objective evaluations. We
              also systematically investigate which choices of parameters such as
              autoregressive order and number of states are optimal for the
              autoregressive HMM.},
  number   = {3},
  journal  = {IEEE Transactions on Audio, Speech, and Language Processing},
  author   = {Shannon, Matt and Zen, Heiga and Byrne, William},
  month    = mar,
  year     = {2013},
  keywords = {Standards, expectation maximization, Parameter estimation,
              autoregressive processes, Hidden Markov models, Speech, Acoustics,
              Vectors, speech, hidden Markov models, speech synthesis,
              statistical analysis, parameter estimation, Speech synthesis,
              Acoustic modeling, autoregressive hidden Markov model,
              autoregressive HMM model, autoregressive order, global variance,
              hidden Markov models (HMMs), high quality synthesis algorithms,
              speech parameter generation algorithm, statistical parametric
              speech synthesis},
  pages    = {587--597},
  file     = {IEEE Xplore Abstract
              Record:/Users/apodusenko/Zotero/storage/HNVRJ5TI/6353548.html:text/html;IEEE
              Xplore Full Text PDF:/Users/apodusenko/Zotero/storage/FNK9SST6/Shannon
              et al. - 2013 - Autoregressive Models for Statistical Parametric
              S.pdf:application/pdf}
}

@book{liu_monte_2013,
  address   = {New York, NY},
  title     = {Monte {Carlo} {Strategies} in {Scientific} {Computing}.},
  isbn      = {978-0-387-76371-2},
  url       = {
               http://public.ebookcentral.proquest.com/choice/publicfullrecord.aspx?p=5575065
               },
  abstract  = {This book provides an up-to-date treatment of the Monte Carlo
               method and develops a common framework under which various Monte
               Carlo techniques can be ""standardized"" and compared. It can be
               used as a textbook for a graduate-level course on Monte Carlo
               methods.},
  language  = {English},
  urldate   = {2019-12-20},
  publisher = {Springer},
  author    = {Liu, Jun S},
  year      = {2013},
  note      = {OCLC: 1066196614}
}

@book{cappe_inference_2010,
  address   = {New York, NY},
  series    = {Springer series in statistics},
  title     = {Inference in hidden {Markov} models},
  isbn      = {978-0-387-28982-3 978-1-4419-2319-6},
  language  = {eng},
  publisher = {Springer},
  author    = {Cappé, Olivier and Moulines, Eric and Rydén, Tobias},
  year      = {2010},
  file      = {Table of Contents PDF:/Users/apodusenko/Zotero/storage/7BMKSHSS/Cappé
               et al. - 2010 - Inference in hidden Markov models.pdf:application/pdf}
}

@unpublished{bruyninckx_bayesian_2002,
  title  = {Bayesian {Probability}},
  author = {Bruyninckx, Herman},
  year   = {2002},
  file   = {Bruyninckx - 2002 - Bayesian
            Probability.pdf:/Users/apodusenko/Zotero/storage/34HN6TWF/Bruyninckx -
            2002 - Bayesian Probability.pdf:application/pdf}
}

@article{van_roosmalen_situ_2016,
  title    = {In {Situ} {Design} of {Noise} {Reduction} {Algorithms}},
  abstract = {In today’s hearing aids (HA), noise reduction algorithms are
              designed by engineers prior to the operational phase of the hearing
              aid. It is not possible for a HA user to personalize his HA to
              unforeseen disturbing noisy conditions. In this paper, we develop a
              method that allows a HA user to design a noise reduction algorithm
              under in situ conditions. The HA user will only need to indicate
              (by pressing a button) when observations of the clean noise or
              speech signals are available. We follow a full Bayesian approach,
              leading to inference problems that relate to signal processing,
              parameter estimation and model comparison tasks. Our method is
              efﬁciently implemented in (Forney-style) factor graphs that are
              capable to solve these inference tasks through automated message
              passing algorithms. The validity of the proposed method is
              supported by informal listening tests.},
  language = {en},
  author   = {van Roosmalen, Wouter},
  year     = {2016},
  pages    = {13},
  file     = {van Roosmalen - 2016 - In Situ Design of Noise Reduction
              Algorithms.pdf:/Users/apodusenko/Zotero/storage/YW4QJIPW/van Roosmalen
              - 2016 - In Situ Design of Noise Reduction
              Algorithms.pdf:application/pdf}
}

@book{noauthor_lebesgue_nodate,
  title = {Lebesgue measure theory overview (russian)},
  url   = {http://math.phys.msu.ru/data/172/fanI1.pdf}
}

@article{annila_intractable_2012,
  title    = {On intractable tracks},
  volume   = {25},
  issn     = {0836-1398},
  url      = {
              http://www.ingentaconnect.com/content/pe/pe/2012/00000025/00000002/art00013
              },
  doi      = {10.4006/0836-1398-25.2.233},
  language = {en},
  number   = {2},
  urldate  = {2019-06-25},
  journal  = {Physics Essays},
  author   = {Annila, Arto and Salthe, Stanley},
  month    = jun,
  year     = {2012},
  pages    = {233--238},
  file     = {Annila and Salthe - 2012 - On intractable
              tracks.pdf:/Users/apodusenko/Zotero/storage/MHHKSZ7N/Annila and Salthe
              - 2012 - On intractable tracks.pdf:application/pdf}
}

@misc{elidan_residual_2012,
  title         = {Residual Belief Propagation: Informed Scheduling for Asynchronous Message Passing},
  author        = {Gal Elidan and Ian McGraw and Daphne Koller},
  year          = {2012},
  eprint        = {1206.6837},
  archiveprefix = {arXiv},
  primaryclass  = {cs.AI}
}

@article{daunizeau_variational_2017,
  title    = {The variational {Laplace} approach to approximate {Bayesian}
              inference},
  url      = {http://arxiv.org/abs/1703.02089},
  abstract = {Variational approaches to approximate Bayesian inference provide
              very efficient means of performing parameter estimation and model
              selection. Among these, so-called variational-Laplace or VL schemes
              rely on Gaussian approximations to posterior densities on model
              parameters. In this note, we review the main variants of VL
              approaches, that follow from considering nonlinear models of
              continuous and/or categorical data. En passant, we also derive a
              few novel theoretical results that complete the portfolio of
              existing analyses of variational Bayesian approaches, including
              investigations of their asymptotic convergence. We also suggest
              practical ways of extending existing VL approaches to hierarchical
              generative models that include (e.g., precision) hyperparameters.},
  urldate  = {2018-01-18},
  journal  = {arXiv:1703.02089 [q-bio, stat]},
  author   = {Daunizeau, Jean},
  month    = mar,
  year     = {2017},
  note     = {arXiv: 1703.02089},
  keywords = {Statistics - Machine Learning, Quantitative Biology - Neurons and
              Cognition, Statistics - Methodology},
  file     = {arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/XT5T92PC/1703.html:text/html;Daunizeau
              - 2017 - The variational Laplace approach to approximate
              Ba.pdf:/Users/apodusenko/Zotero/storage/5Y2UQVNH/Daunizeau - 2017 - The
              variational Laplace approach to approximate Ba.pdf:application/pdf}
}

@book{cline_variational_2019,
  title      = {Variational {Principles} in {Classical} {Mechanics}: {Revised} {
                Second} {Edition}},
  isbn       = {978-0-9988372-9-1},
  shorttitle = {Variational {Principles} in {Classical} {Mechanics}},
  abstract   = {Two dramatically different philosophical approaches to classical
                mechanics were proposed during the 17th – 18th centuries. Newton
                developed his vectorial formulation that uses time-dependent
                differential equations of motion to relate vector observables like
                force and rate of change of momentum. Euler, Lagrange, Hamilton,
                and Jacobi, developed powerful alternative variational formulations
                based on the assumption that nature follows the principle of least
                action. These variational formulations now play a pivotal role in
                science and engineering. This book introduces variational
                principles and their application to classical mechanics. The
                relative merits of the intuitive Newtonian vectorial formulation,
                and the more powerful variational formulations are compared.
                Applications to a wide variety of topics illustrate the
                intellectual beauty, remarkable power, and broad scope provided by
                use of variational principles in physics. This second edition adds
                discussion of the use of variational principles applied to the
                following topics: 1) Systems subject to initial boundary conditions
                2) The hierarchy of the related formulations based on action,
                Lagrangian, Hamiltonian, and equations of motion, to systems that
                involve symmetries 3) Non-conservative systems. 4) Variable-mass
                systems. 5) The General Theory of Relativity.},
  language   = {English},
  publisher  = {River Campus Libraries},
  author     = {Cline, Douglas},
  month      = jan,
  year       = {2019},
  file       = {Cline - 2019 - Variational Principles in Classical Mechanics
                Rev.pdf:/Users/apodusenko/Zotero/storage/IQIQVLVN/Cline - 2019 -
                Variational Principles in Classical Mechanics Rev.pdf:application/pdf}
}

@book{gregory_classical_2006,
  address   = {Cambridge, UK ; New York},
  edition   = {1 edition},
  title     = {Classical {Mechanics}},
  isbn      = {978-0-521-53409-3},
  abstract  = {Gregory's Classical Mechanics is a major new textbook for
               undergraduates in mathematics and physics. It is a thorough,
               self-contained and highly readable account of a subject many
               students find difficult. The author's clear and systematic style
               promotes a good understanding of the subject; each concept is
               motivated and illustrated by worked examples, while problem sets
               provide plenty of practice for understanding and technique.
               Computer assisted problems, some suitable for projects, are also
               included. The book is structured to make learning the subject easy;
               there is a natural progression from core topics to more advanced
               ones and hard topics are treated with particular care. A theme of
               the book is the importance of conservation principles. These appear
               first in vectorial mechanics where they are proved and applied to
               problem solving. They reappear in analytical mechanics, where they
               are shown to be related to symmetries of the Lagrangian,
               culminating in Noether's theorem.},
  language  = {English},
  publisher = {Cambridge University Press},
  author    = {Gregory, R. Douglas},
  month     = apr,
  year      = {2006},
  file      = {Gregory - 2006 - Classical
               Mechanics.pdf:/Users/apodusenko/Zotero/storage/E2RVZPIZ/Gregory - 2006
               - Classical Mechanics.pdf:application/pdf;Gregory R.D. - Classical
               mechanics_ Solution manual-CUP
               (2006).pdf:/Users/apodusenko/Zotero/storage/GG7ZQ32K/Gregory R.D. -
               Classical mechanics_ Solution manual-CUP (2006).pdf:application/pdf}
}

@inproceedings{heskes_stable_2003,
  title     = {Stable fixed points of loopy belief propagation are local minima of
               the bethe free energy},
  booktitle = {Advances in neural information processing systems},
  author    = {Heskes, Tom},
  year      = {2003},
  pages     = {359--366},
  file      = {Heskes - 2003 - Stable fixed points of loopy belief propagation
               ar.pdf:/Users/apodusenko/Zotero/storage/TSUKJ78Z/Heskes - 2003 - Stable
               fixed points of loopy belief propagation ar.pdf:application/pdf}
}

@misc{kouw_schedule-free_2019,
  type   = {{BIASlab} {Seminar}},
  title  = {Schedule-free message-passing},
  author = {Kouw, Wouter},
  month  = nov,
  year   = {2019},
  file   = {Kouw - 2019 - Schedule-free
            message-passing.pdf:/Users/apodusenko/Zotero/storage/8EMNM75K/Kouw -
            2019 - Schedule-free message-passing.pdf:application/pdf}
}

@article{baltieri_active_nodate,
  title    = {Active inference: building a new bridge between control theory and
              embodied cognitive science},
  abstract = {The application of Bayesian techniques to the study and
              computational modelling of biological systems is one of the most
              remarkable advances in the natural and cognitive sciences over the
              last 50 years. More recently, it has been proposed that Bayesian
              frameworks are not only useful for building descriptive models of
              biological functions, but that living systems themselves can be
              seen as Bayesian (inference) machines. On this view, the
              statistical tools more traditionally used to account for data in
              biology, neuroscience and psychology, are now used to model the
              mechanisms underlying functions and properties of living systems as
              if the systems themselves were the ones “calculating” those
              probabilities following Bayesian inference schemes. The free energy
              principle (FEP) is a framework proposed in light of this paradigm
              shift, advocating the minimisation of variational free energy, a
              proxy for sensory surprisal, as a general computational principle
              for biological systems. More intuitively and under some simplifying
              assumptions, the minimisation of variational free energy reduces,
              for an agent, to the minimisation of prediction errors on sensory
              input. Initially proposed as a candidate unifying theory of brain
              functioning, the FEP was later extended to encompass hypotheses on
              the origins of life, and is nowadays discussed in the cognitive
              science community for its possible implications for theories of the
              mind. In particular, one of the most popular process theories
              derived from the FEP, active inference, describes a biologically
              plausible algorithmic implementation of this principle with several
              repercussions on our understanding of cognition.},
  language = {en},
  author   = {Baltieri, Manuel},
  pages    = {200},
  file     = {Baltieri - Active inference building a new bridge between
              co.pdf:/Users/apodusenko/Zotero/storage/4PB6A4HP/Baltieri - Active
              inference building a new bridge between co.pdf:application/pdf}
}

@inproceedings{baltieri_active_2019,
  address    = {Berlin, Germany},
  title      = {Active {Inference}: {Computational} {Models} of {Motor} {Control}
                without {Efference} {Copy}},
  shorttitle = {Active {Inference}},
  url        = {https://ccneuro.org/2019/Papers/ViewPapers.asp?PaperNum=1144},
  doi        = {10.32470/CCN.2019.1144-0},
  abstract   = {Computational frameworks for the study of motor systems in
                neuroscience often rely on a mathematical formulation based on
                optimal control theory, e.g., forward and inverse models and linear
                quadratic Gaussian (LQG) control architectures. A forward model
                maps actions to (predicted) consequences, while an inverse model is
                thought to deﬁne how motor commands are generated from
                observations. One of the central tenets of the forward/inverse
                architecture is the presence of a copy of motor commands produced
                by an inverse model and provided to the forward counterpart. Such
                copy, usually referred to as “efference copy”, is assumed to be
                necessary to model, and ultimately explain, motor control and
                behaviour. Over the years, different results have challenged the
                idea of an efference copy, suggesting that it may not be
                physiologically plausible, especially in humans. In this work we
                focus on a process theory that combines the mathematical richness
                of LQG models with efferencecopy-free architectures, active
                inference. We provide a minimal computational model discussing and
                comparing the forward/inverse and the active inference
                architectures on an idealised model of a single-joint control
                system.},
  language   = {en},
  urldate    = {2019-10-27},
  booktitle  = {2019 {Conference} on {Cognitive} {Computational} {Neuroscience}},
  publisher  = {Cognitive Computational Neuroscience},
  author     = {Baltieri, Manuel and Buckley, Christopher L.},
  year       = {2019},
  file       = {Baltieri en Buckley - 2019 - Active Inference Computational Models of
                Motor Co.pdf:/Users/apodusenko/Zotero/storage/XYQ49TQG/Baltieri en
                Buckley - 2019 - Active Inference Computational Models of Motor
                Co.pdf:application/pdf}
}

@article{cox_bayesian_2016,
  title    = {A {Bayesian} binary classification approach to pure tone audiometry},
  url      = {http://arxiv.org/abs/1511.08670},
  abstract = {The pure tone hearing threshold is usually estimated from
              responses to stimuli at a set of standard frequencies. This paper
              describes a probabilistic approach to the estimation problem in
              which the hearing threshold is modelled as a smooth continuous
              function of frequency using a Gaussian process. This allows
              sampling at any frequency and reduces the number of required
              measurements. The Gaussian process is combined with a probabilistic
              response model to account for uncertainty in the responses. The
              resulting full model can be interpreted as a twodimensional binary
              classiﬁer for stimuli, and provides uncertainty bands on the
              estimated threshold curve. The optimal next stimulus is determined
              based on an information theoretic criterion. This leads to a robust
              adaptive estimation method that can be applied to fully automate
              the hearing threshold estimation process.},
  language = {en},
  urldate  = {2019-10-24},
  journal  = {arXiv:1511.08670 [stat]},
  author   = {Cox, Marco and de Vries, Bert},
  month    = mar,
  year     = {2016},
  note     = {arXiv: 1511.08670},
  keywords = {Statistics - Applications},
  file     = {Cox and de Vries - 2016 - A Bayesian binary classification approach to
              pure .pdf:/Users/apodusenko/Zotero/storage/IGJSC7I9/Cox and de Vries -
              2016 - A Bayesian binary classification approach to pure
              .pdf:application/pdf}
}

@inproceedings{wijnings_robust_2019,
  title     = {Robust {Bayesian} {Beamforming} for {Sources} at {Different} {
               Distances} with {Applications} in {Urban} {Monitoring}},
  doi       = {10.1109/ICASSP.2019.8682835},
  abstract  = {Acoustic smart sensor networks can provide valuable actionable
               intelligence to authorities for managing safety in the urban
               environment. A spatial filter (beamformer) for localization and
               separation of acoustic sources is a key component of such a
               network. However, classical methods such as delay-and-sum
               beamforming fail, because sources are located at varying distances
               from the sensor array. This causes a regularization problem where
               either far-away sources are wrongly attenuated, or noise is wrongly
               amplified. We solve this by considering source strength and
               location as random variables. The posterior distributions are
               approximated using Gibbs sampling. Each marginal is computed by
               combining importance sampling and inverse transform sampling using
               Chebyshev polynomial approximation. This leads to an iterative
               algorithm with similarities to deconvolution beamforming. Our
               method is robust against deviations in manifold model, can deal
               with sources at different distances and power levels, and does not
               require an a priori known number of sources.},
  booktitle = {{ICASSP} 2019 - 2019 {IEEE} {International} {Conference} on {
               Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
  author    = {Wijnings, Patrick W.A. and Stuijk, Sander and de Vries, Bert and
               Corporaal, Henk},
  month     = may,
  year      = {2019},
  note      = {ISSN: 2379-190X, 1520-6149},
  keywords  = {Gibbs sampling, iterative algorithm, Bayes methods, iterative
               methods, Signal to noise ratio, Markov processes, acoustic signal
               processing, Acoustics, Array signal processing, sensor arrays,
               source separation, Robustness, array signal processing, spatial
               filters, polynomial approximation, Sensor arrays, importance
               sampling, Acoustic applications, acoustic smart sensor networks,
               acoustic source localization, acoustic source separation,
               Attenuation, beamformer, Chebyshev approximation, Chebyshev
               polynomial approximation, deconvolution, deconvolution beamforming,
               intelligent sensors, inverse transform sampling, inverse transforms
               , Laplace equations, Manifolds, posterior distributions, random
               variables, regularization problem, robust Bayesian beamforming,
               sensor array, source strength, spatial filter, statistical
               distributions, urban environment, urban monitoring, valuable
               actionable intelligence, varying distances},
  pages     = {4325--4329},
  file      = {IEEE Xplore Abstract
               Record:/Users/apodusenko/Zotero/storage/673YC6VQ/8682835.html:text/html;Wijnings
               et al. - 2019 - Robust Bayesian Beamforming for Sources at
               Differe.pdf:/Users/apodusenko/Zotero/storage/LL89QVEZ/Wijnings et al. -
               2019 - Robust Bayesian Beamforming for Sources at
               Differe.pdf:application/pdf}
}

@inproceedings{wijnings_approximate_2020,
  address  = {Barcelona},
  title    = {Approximate {Inference} by {Kullback}-{Leibler} {Tensor} {Belief} {
              Propagation}},
  abstract = {Probabilistic programming provides a structured approach to signal
              processing algorithm design. The design task is formulated as a
              generative model, and the algorithm is derived through automatic
              inference. Efficient inference is a major challenge; e.g., the
              Shafer-Shenoy algorithm (SS) performs badly on models with large
              treewidth, which arise from various real-world problems. We focus
              on reducing the size of discrete models with large treewidth by
              storing intermediate factors in compressed form, thereby decoupling
              the variables through conditioning on introduced weights. This work
              proposes pruning of these weights using Kullback-Leibler
              divergence. We adapt a strategy from the Gaussian mixture reduction
              literature, leading to Kullback-Leibler Tensor Belief Propagation
              (KL-TBP), in which we use agglomerative hierarchical clustering to
              subsequently merge pairs of weights. Experiments using benchmark
              problems show KL-TBP consistently achieves lower approximation
              error than existing methods with competitive runtime.},
  language = {en},
  author   = {Wijnings, Patrick W.A. and Stuijk, Sander and de Vries, Bert and
              Corporaal, Henk},
  year     = {2020},
  pages    = {5},
  file     = {Wijnings et al. - 2020 - Approximate Inference by Kullback-Leibler
              Tensor B.pdf:/Users/apodusenko/Zotero/storage/PIPGN8MX/Wijnings et al.
              - 2020 - Approximate Inference by Kullback-Leibler Tensor
              B.pdf:application/pdf}
}

@book{chow_analysis_1975,
  title     = {Analysis and control of dynamic economic systems},
  publisher = {Wiley},
  author    = {Chow, Gregory C.},
  year      = {1975},
  file      = {
               Snapshot:/Users/apodusenko/Zotero/storage/ZIXCJLSA/search.html:text/html
               }
}

@inproceedings{degris_model-free_2012,
  title     = {Model-free reinforcement learning with continuous action in practice},
  booktitle = {2012 {American} {Control} {Conference} ({ACC})},
  publisher = {IEEE},
  author    = {Degris, Thomas and Pilarski, Patrick M. and Sutton, Richard S.},
  year      = {2012},
  pages     = {2177--2182},
  file      = {Degris e.a. - 2012 - Model-free reinforcement learning with continuous
               .pdf:/Users/apodusenko/Zotero/storage/QCKYBACS/Degris e.a. - 2012 -
               Model-free reinforcement learning with continuous
               .pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/23Q4V82V/6315022.html:text/html
               }
}

@article{taylor_introduction_2011,
  title    = {An {Introduction} to {Intertask} {Transfer} for {Reinforcement} {
              Learning}},
  volume   = {32},
  issn     = {0738-4602, 0738-4602},
  url      = {https://aaai.org/ojs/index.php/aimagazine/article/view/2329},
  doi      = {10.1609/aimag.v32i1.2329},
  abstract = {Transfer learning has recently gained popularity due to the
              development of algorithms that can successfully generalize
              information across multiple tasks. This article focuses on transfer
              in the context of reinforcement learning domains, a general
              learning framework where an agent acts in an environment to
              maximize a reward signal. The goals of this article are to (1)
              familiarize readers with the transfer learning problem in
              reinforcement learning domains, (2) explain why the problem is both
              interesting and difﬁcult, (3) present a selection of existing
              techniques that demonstrate different solutions, and (4) provide
              representative open problems in the hope of encouraging additional
              research in this exciting area.},
  language = {en},
  number   = {1},
  urldate  = {2021-05-19},
  journal  = {AI Magazine},
  author   = {Taylor, Matthew E. and Stone, Peter},
  month    = mar,
  year     = {2011},
  pages    = {15},
  file     = {Taylor and Stone - 2011 - An Introduction to Intertask Transfer for
              Reinforc.pdf:/Users/apodusenko/Zotero/storage/UKI2QMRE/Taylor and Stone
              - 2011 - An Introduction to Intertask Transfer for
              Reinforc.pdf:application/pdf}
}

@article{boussinot_reactive_2015,
  title    = {Reactive {Programming} of {Simulations} in {Physics}},
  volume   = {26},
  issn     = {0129-1831, 1793-6586},
  url      = {http://arxiv.org/abs/1410.7763},
  doi      = {10.1142/S0129183115501326},
  abstract = {We consider the Reactive Programming (RP) approach to simulate
              physical systems. The choice of RP is motivated by the fact that RP
              genuinely offers logical parallelism, instantaneously broadcast
              events, and dynamic creation/destruction of parallel components and
              events. To illustrate our approach, we consider the implementation
              of a system of Molecular Dynamics, in the context of Java with the
              Java3D library for 3D visualisation.},
  number   = {12},
  urldate  = {2021-05-18},
  journal  = {International Journal of Modern Physics C},
  author   = {Boussinot, Frédéric and Monasse, Bernard and Susini, Jean-Ferdy},
  month    = dec,
  year     = {2015},
  note     = {arXiv: 1410.7763},
  keywords = {Condensed Matter - Statistical Mechanics, Physics - Computational
              Physics, 82-08},
  pages    = {1550132},
  file     = {arXiv Fulltext PDF:/Users/apodusenko/Zotero/storage/EB7TGP9N/Boussinot
              et al. - 2015 - Reactive Programming of Simulations in
              Physics.pdf:application/pdf;arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/QQJU7TAY/1410.html:text/html}
}

@article{lepper_baltasar_nodate,
  title    = {Baltasar {Trancón} y {Widemann} 1,2},
  abstract = {The Sig programming language is a total functional, clocked
              synchronous data-ﬂow language. Its core has been designed to admit
              concise coalgebraic semantics. Universal coalgebra is an expressive
              theoretical framework for behavioral semantics, but traditionally
              phrased in abstract categorical language, and generally considered
              inaccessible. In the present paper, we rephrase the coalgebraic
              concepts relevant for the Sig language semantics in basic
              mathematical notation. We demonstrate how the language features
              characteristic of its paradigms, namely sequential and parallel
              composition for applicative style, delay for data ﬂow, and apply
              for higher-order functional programming, are shaped naturally by
              the semantic structure. Thus the present paper serves two purposes,
              as a gentle, self-contained and applied introduction to coalgebraic
              semantics, and as an explication of the Sig core language
              denotational and operational design.},
  language = {en},
  author   = {Lepper, Markus},
  pages    = {15},
  file     = {Lepper - Baltasar Trancón y Widemann 1,
              2.pdf:/Users/apodusenko/Zotero/storage/ZWATSCS2/Lepper - Baltasar
              Trancón y Widemann 1,2.pdf:application/pdf}
}

@article{sallay_online_2021,
  title     = {Online {Learning} of {Finite} and {Infinite} {Gamma} {Mixture} {
               Models} for {COVID}-19 {Detection} in {Medical} {Images}},
  volume    = {10},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  url       = {https://www.mdpi.com/2073-431X/10/1/6},
  doi       = {10.3390/computers10010006},
  abstract  = {The accurate detection of abnormalities in medical images (like
               X-ray and CT scans) is a challenging problem due to images\&rsquo;
               blurred boundary contours, different sizes, variable shapes, and
               uneven density. In this paper, we tackle this problem via a new
               effective online variational learning model for both mixtures of
               finite and infinite Gamma distributions. The proposed approach
               takes advantage of the Gamma distribution flexibility, the online
               learning scalability, and the variational inference efficiency.
               Three different batch and online learning methods based on robust
               texture-based feature extraction are proposed. Our work is
               evaluated and validated on several real challenging data sets for
               different kinds of pneumonia infection detection. The obtained
               results are very promising given that we approach the
               classification problem in an unsupervised manner. They also confirm
               the superiority of the Gamma mixture model compared to the Gaussian
               mixture model for medical images\&rsquo; classification.},
  language  = {en},
  number    = {1},
  urldate   = {2021-05-11},
  journal   = {Computers},
  author    = {Sallay, Hassen and Bourouis, Sami and Bouguila, Nizar},
  month     = jan,
  year      = {2021},
  note      = {Number: 1 Publisher: Multidisciplinary Digital Publishing Institute},
  keywords  = {machine learning, variational inference, Gamma distribution,
               COVID-19, diagnoses and biomedical applications, finite and
               infinite mixture models, online learning},
  pages     = {6},
  file      = {Full Text PDF:/Users/apodusenko/Zotero/storage/D8A5NDJT/Sallay et al.
               - 2021 - Online Learning of Finite and Infinite Gamma
               Mixtu.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/QFDFTDNV/6.html:text/html
               }
}

@article{friston_dynamic_2003,
  title   = {Dynamic causal modelling},
  volume  = {19},
  number  = {4},
  journal = {Neuroimage},
  author  = {Friston, Karl J. and Harrison, Lee and Penny, Will},
  year    = {2003},
  note    = {Publisher: Elsevier},
  pages   = {1273--1302},
  file    = {Friston et al. - 2003 - Dynamic causal
             modelling.html:/Users/apodusenko/Zotero/storage/XWZWM6DU/Friston et al.
             - 2003 - Dynamic causal modelling.html:text/html}
}

@article{maaten_visualizing_2008,
  title   = {Visualizing {Data} using t-{SNE}},
  volume  = {9},
  issn    = {1533-7928},
  url     = {http://jmlr.org/papers/v9/vandermaaten08a.html},
  number  = {86},
  urldate = {2021-05-05},
  journal = {Journal of Machine Learning Research},
  author  = {Maaten, Laurens van der and Hinton, Geoffrey},
  year    = {2008},
  pages   = {2579--2605},
  file    = {Full Text PDF:/Users/apodusenko/Zotero/storage/W9U6EL8M/Maaten and
             Hinton - 2008 - Visualizing Data using
             t-SNE.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/67QZ3RH7/vandermaaten08a.html:text/html
             }
}

@article{zoeter_gaussian_nodate,
  title    = {Gaussian {Quadrature} {Based} {Expectation} {Propagation}},
  abstract = {We present a general approximation method for Bayesian inference
              problems. The method is based on Expectation Propagation (EP).
              Projection steps in the EP iteration that can­ not be done
              analytically are done using Gaus­ sian quadrature. By identifying a
              general form in the projections, the only quadrature rules that are
              required are for exponential family weight functions. The
              corresponding cumulant and moment generating functions can then be
              used to automatically derive the necessary quadrature rules. In
              this article the approach is restricted to approxim ating families
              that factorize to a product of one­ dimensional families.},
  language = {en},
  author   = {Zoeter, Onno and Heskes, Tom},
  pages    = {9},
  file     = {Zoeter and Heskes - Gaussian Quadrature Based Expectation
              Propagation.pdf:/Users/apodusenko/Zotero/storage/ALSCDP4I/Zoeter and
              Heskes - Gaussian Quadrature Based Expectation
              Propagation.pdf:application/pdf}
}

@article{wiper_mixtures_2001,
  title    = {Mixtures of {Gamma} {Distributions} with {Applications}},
  volume   = {10},
  issn     = {1061-8600},
  url      = {https://www.jstor.org/stable/1391098},
  abstract = {This article proposes a Bayesian density estimation method based
              upon mixtures of gamma distributions. It considers both the cases
              of known mixture size, using a Gibbs sampling scheme with a
              Metropolis step, and unknown mixture size, using a reversible jump
              technique that allows us to move from one mixture size to another.
              We illustrate our methods using a number of simulated datasets,
              generated from distributions covering a wide range of cases: single
              distributions, mixtures of distributions with equal means and
              different variances, mixtures of distributions with different means
              and small variances and, finally, a distribution contaminated by
              low-weighted distributions with different means and equal, small
              variances. An application to estimation of some quantities for a
              M/G/1 queue is given, using real E-mail data from CNR-IAMI.},
  number   = {3},
  urldate  = {2021-05-04},
  journal  = {Journal of Computational and Graphical Statistics},
  author   = {Wiper, Michael and Insua, David Rios and Ruggeri, Fabrizio},
  year     = {2001},
  note     = {Publisher: [American Statistical Association, Taylor \& Francis, Ltd.,
              Institute of Mathematical Statistics, Interface Foundation of America]},
  pages    = {440--454},
  file     = {Wiper et al. - 2001 - Mixtures of Gamma Distributions with
              Applications.pdf:/Users/apodusenko/Zotero/storage/3G9ECPA9/Wiper et al.
              - 2001 - Mixtures of Gamma Distributions with
              Applications.pdf:application/pdf}
}

@article{knuth_informed_2013,
  title      = {Informed {Source} {Separation}: {A} {Bayesian} {Tutorial}},
  shorttitle = {Informed {Source} {Separation}},
  url        = {http://arxiv.org/abs/1311.3001},
  abstract   = {Source separation problems are ubiquitous in the physical
                sciences; any situation where signals are superimposed calls for
                source separation to estimate the original signals. In this
                tutorial I will discuss the Bayesian approach to the source
                separation problem. This approach has a specific advantage in that
                it requires the designer to explicitly describe the signal model in
                addition to any other information or assumptions that go into the
                problem description. This leads naturally to the idea of informed
                source separation, where the algorithm design incorporates relevant
                information about the specific problem. This approach promises to
                enable researchers to design their own high-quality algorithms that
                are specifically tailored to the problem at hand.},
  urldate    = {2021-02-26},
  journal    = {arXiv:1311.3001 [cs, stat]},
  author     = {Knuth, Kevin H.},
  month      = nov,
  year       = {2013},
  note       = {arXiv: 1311.3001},
  keywords   = {Statistics - Machine Learning, Computer Science - Machine Learning
                },
  file       = {arXiv Fulltext PDF:/Users/apodusenko/Zotero/storage/H8EIXIFC/Knuth -
                2013 - Informed Source Separation A Bayesian
                Tutorial.pdf:application/pdf;arXiv Fulltext
                PDF:/Users/apodusenko/Zotero/storage/IV8SU3LX/Knuth - 2013 - Informed
                Source Separation A Bayesian Tutorial.pdf:application/pdf;arXiv.org
                Snapshot:/Users/apodusenko/Zotero/storage/LRATF33H/1311.html:text/html;arXiv.org
                Snapshot:/Users/apodusenko/Zotero/storage/DX3EGCP4/1311.html:text/html}
}

@article{grenier_time-dependent_1983,
  title    = {Time-dependent {ARMA} modeling of nonstationary signals},
  volume   = {31},
  issn     = {0096-3518},
  doi      = {10.1109/TASSP.1983.1164152},
  abstract = {Modeling of nonstationary signals can be achieved through
              time-dependent autoregressive moving-average models and lattices,
              by the use of a limited series expansion of the time-varying
              coefficients in the models. This method leads to an extension of
              several well-known techniques of stationary spectral estimation to
              the nonstationary case. Time-varying AR models are identified by
              means of a fast (Levinson) algorithm which is also suitable for the
              AR part of a mixed ARMA model. An alternative to this method is
              given by the extension of Cadzow's method. Lattices with
              time-dependent reflection coefficients are identified through an
              algorithm which is similar to Burg's. Finally, the Prony-Pisarenko
              estimator is adapted to this nonstationary context, the signal
              considered in this case being the output of a zero-input
              time-varying system corrupted by an additive white noise. In all
              these methods the estimation is global in the sense that the
              parameters are estimated over a time interval [0, T], given the
              observations [y0... yT]. The maximum likelihood method which falls
              within the same framework is also briefly studied in this paper.
              Simulations of these algorithms on chirp signals and on transitions
              between phonemes in speech conclude the paper.},
  number   = {4},
  journal  = {IEEE Transactions on Acoustics, Speech, and Signal Processing},
  author   = {Grenier, Y.},
  month    = aug,
  year     = {1983},
  note     = {Conference Name: IEEE Transactions on Acoustics, Speech, and Signal
              Processing},
  keywords = {Parameter estimation, Signal processing algorithms, Speech
              analysis, Brain modeling, Signal analysis, Additive white noise,
              Speech synthesis, Acoustic reflection, Lattices, Signal synthesis},
  pages    = {899--911},
  file     = {IEEE Xplore Abstract
              Record:/Users/apodusenko/Zotero/storage/9N2EQD3D/1164152.html:text/html
              }
}

@article{nielsen_perception-based_2015,
  title    = {Perception-{Based} {Personalization} of {Hearing} {Aids} {Using} {
              Gaussian} {Processes} and {Active} {Learning}},
  volume   = {23},
  issn     = {2329-9290},
  doi      = {10.1109/TASLP.2014.2377581},
  abstract = {Personalization of multi-parameter hearing aids involves an
              initial fitting followed by a manual knowledge-based
              trial-and-error fine-tuning from ambiguous verbal user feedback.
              The result is an often suboptimal HA setting whereby the full
              potential of modern hearing aids is not utilized. This article
              proposes an interactive hearing-aid personalization system that
              obtains an optimal individual setting of the hearing aids from
              direct perceptual user feedback. Results obtained with ten
              hearing-impaired subjects show that ten to twenty pairwise user
              assessments between different settings—equivalent to 5-10 min—is
              sufficient for personalization of up to four hearing-aid
              parameters. A setting obtained by the system was significantly
              preferred by the subject over the initial fitting, and the obtained
              setting could be reproduced with reasonable precision. The system
              may have potential for clinical usage to assist both the
              hearing-care professional and the user.},
  number   = {1},
  journal  = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  author   = {Nielsen, J.B.B. and Nielsen, J. and Larsen, J.},
  month    = jan,
  year     = {2015},
  keywords = {hearing aids, Hearing aids, Signal processing algorithms, Speech,
              Gaussian processes, Speech processing, Gain, Approximation methods,
              Active learning, Gaussian process (GP), individualization, pairwise
              comparisons, personalization},
  pages    = {162--173},
  file     = {IEEE Xplore Abstract
              Record:/Users/apodusenko/Zotero/storage/ZWZJ832W/articleDetails.html:text/html;IEEE
              Xplore Abstract
              Record:/Users/apodusenko/Zotero/storage/9ZELKESL/6975061.html:text/html;IEEE
              Xplore Full Text PDF:/Users/apodusenko/Zotero/storage/9X6IKFBR/Nielsen
              et al. - 2015 - Perception-Based Personalization of Hearing Aids
              U.pdf:application/pdf;Nielsen et al. - 2015 - Perception-Based
              Personalization of Hearing Aids
              U.pdf:/Users/apodusenko/Zotero/storage/MA8V33K8/Nielsen et al. - 2015 -
              Perception-Based Personalization of Hearing Aids U.pdf:application/pdf}
}

@article{cournapeau_online_2010,
  title    = {Online {Unsupervised} {Classification} {With} {Model} {Comparison} in
              the {Variational} {Bayes} {Framework} for {Voice} {Activity} {
              Detection}},
  volume   = {4},
  issn     = {1932-4553},
  doi      = {10.1109/JSTSP.2010.2080821},
  abstract = {A new online, unsupervised method for Voice Activity Detection
              (VAD) is proposed. The conventional VAD methods often rely on
              heuristics to adapt the decision threshold to the estimated SNR.
              The proposed VAD method is based on the Variational Bayes (VB)
              approach to the online Expectation Maximization (EM), so that it
              can automatically adapt the decision level and the statistical
              model at the same time. We consider two parallel classifiers, one
              for the noise-only case, and the other for speech-and-noise case.
              Both models are trained concurrently and online using the VB
              framework. The VB framework also provides an explicit approximation
              of the log evidence called free energy. It is used to assess the
              reliability of the classifier in an online fashion, and to decide
              which model is more appropriate at a given time frame. Experimental
              evaluations were conducted on the CENSREC-1-C database designed for
              VAD evaluations. With the effect of the model comparison, the
              proposed scheme outperforms the conventional VAD algorithms,
              especially in the remote recording condition. It is also shown to
              be more robust with respect to changes of the noise type.},
  number   = {6},
  journal  = {IEEE Journal of Selected Topics in Signal Processing},
  author   = {Cournapeau, D. and Watanabe, S. and Nakamura, A. and Kawahara, T.},
  month    = dec,
  year     = {2010},
  keywords = {Noise, expectation-maximisation algorithm, Bayes methods, Bayesian
              methods, Hidden Markov models, Speech analysis, EM Algorithm, SE,
              SNR, sequential estimation, signal detection, Automatic speech
              recognition, CENSREC-1-C database, variational Bayes framework,
              Voice Activity Detection, speech processing, Speech coding,
              Approximation methods, Adaptation model, online expectation
              maximization, online unsupervised classification, pattern
              classification, remote recording condition, Sequential estimation,
              speech analysis, statistical model, VAD method, variational Bayes
              (VB), VB approach, voice activity detection, voice activity
              detection (VAD)},
  pages    = {1071--1083},
  file     = {Cournapeau - 2011 - Online Unsupervised Classiﬁcation With Model
              Comparison in the Variational Bayes
              Framework.pdf:/Users/apodusenko/Zotero/storage/462947BH/Cournapeau -
              2011 - Online Unsupervised Classiﬁcation With Model Comparison in the
              Variational Bayes Framework.pdf:application/pdf;Full
              Text:/Users/apodusenko/Zotero/storage/KPH4MG2J/Cournapeau et al. - 2010
              - Online Unsupervised Classification With Model
              Comp.pdf:application/pdf;IEEE Xplore Abstract
              Record:/Users/apodusenko/Zotero/storage/H2G7VV4W/login.html:text/html;IEEE
              Xplore Abstract
              Record:/Users/apodusenko/Zotero/storage/98BQSN98/5586640.html:text/html
              }
}

@article{olsson_linear_nodate,
  title    = {Linear {State}-{Space} {Models} for {Blind} {Source} {Separation}},
  abstract = {We apply a type of generative modelling to the problem of blind
              source separation in which prior knowledge about the latent source
              signals, such as time-varying auto-correlation and quasiperiodicity
              , are incorporated into a linear state-space model. In simulations,
              we show that in terms of signal-to-error ratio, the sources are
              inferred more accurately as a result of the inclusion of strong
              prior knowledge. We explore different schemes of maximum-likelihood
              optimization for the purpose of learning the model parameters. The
              Expectation Maximization algorithm, which is often considered the
              standard optimization method in this context, results in slow
              convergence when the noise variance is small. In such scenarios,
              quasi-Newton optimization yields substantial improvements in a
              range of signal to noise ratios. We analyze the performance of the
              methods on convolutive mixtures of speech signals.},
  language = {en},
  author   = {Olsson, Rasmus Kongsgaard and Hansen, Lars Kai},
  pages    = {18},
  file     = {Olsson and Hansen - Linear State-Space Models for Blind Source
              Separat.pdf:/Users/apodusenko/Zotero/storage/T6T83TDH/Olsson and Hansen
              - Linear State-Space Models for Blind Source
              Separat.pdf:application/pdf}
}

@article{ignatenko_sequential_2021,
  title    = {On {Sequential} {Bayesian} {Optimization} with {Pairwise} {Comparison
              }},
  url      = {http://arxiv.org/abs/2103.13192},
  abstract = {In this work, we study the problem of user preference learning on
              the example of parameter setting for a hearing aid (HA). We propose
              to use an agent that interacts with a HA user, in order to collect
              the most informative data, and learns user preferences for HA
              parameter settings, based on these data. We model the HA system as
              two interacting sub-systems, one representing a user with his/her
              preferences and another one representing an agent. In this system,
              the user responses to HA settings, proposed by the agent. In our
              user model, the responses are driven by a parametric user
              preference function. The agent comprises the sequential mechanisms
              for user model inference and HA parameter proposal generation. To
              infer the user model (preference function), Bayesian approximate
              inference is used in the agent. Here we propose the normalized
              weighted Kullback-Leibler (KL) divergence between true and
              agent-assigned predictive user response distributions as a metric
              to assess the quality of learned preferences. Moreover, our agent
              strategy for generating HA parameter proposals is to generate HA
              settings, responses to which help resolving uncertainty associated
              with prediction of the user responses the most. The resulting data,
              consequently, allows for efﬁcient user model learning. The
              normalized weighted KL-divergence plays an important role here as
              well, since it characterizes the informativeness of the data to be
              used for probing the user. The efﬁciency of our approach is
              validated by numerical simulations.},
  language = {en},
  urldate  = {2021-04-19},
  journal  = {arXiv:2103.13192 [cs, math, stat]},
  author   = {Ignatenko, Tanya and Kondrashov, Kirill and Cox, Marco and de Vries,
              Bert},
  month    = mar,
  year     = {2021},
  note     = {arXiv: 2103.13192},
  keywords = {Statistics - Machine Learning, Computer Science - Artificial
              Intelligence, Computer Science - Information Theory, Computer
              Science - Machine Learning},
  file     = {Ignatenko et al. - 2021 - On Sequential Bayesian Optimization with
              Pairwise .pdf:/Users/apodusenko/Zotero/storage/YLZUIDCG/Ignatenko et
              al. - 2021 - On Sequential Bayesian Optimization with Pairwise
              .pdf:application/pdf}
}

@article{bouvrie_entropy_2011,
  title    = {Entropy and complexity analysis of {Dirac}-delta-like quantum
              potentials},
  volume   = {390},
  issn     = {03784371},
  url      = {https://linkinghub.elsevier.com/retrieve/pii/S0378437111001464},
  doi      = {10.1016/j.physa.2011.02.020},
  abstract = {The Dirac-delta-like quantum-mechanical potentials are frequently
              used to describe and interpret numerous phenomena in many
              scientific fields including atomic and molecular physics, condensed
              matter and quantum computation. The entropy and complexity
              properties of potentials with one and two Dirac-delta functions are
              here analytically calculated and numerically discussed in both
              position and momentum spaces. We have studied the
              information-theoretic lengths of Fisher, Rényi and Shannon types as
              well as the Cramér–Rao, Fisher–Shannon and LMC shape complexities
              of the lowest-lying stationary states of one-delta and twin-delta.
              They allow us to grasp and quantify different facets of the
              spreading of the charge and momentum of the system far beyond the
              celebrated standard deviation.},
  language = {en},
  number   = {11},
  urldate  = {2021-04-18},
  journal  = {Physica A: Statistical Mechanics and its Applications},
  author   = {Bouvrie, P.A. and Angulo, J.C. and Dehesa, J.S.},
  month    = jun,
  year     = {2011},
  pages    = {2215--2228},
  file     = {Bouvrie et al. - 2011 - Entropy and complexity analysis of
              Dirac-delta-lik.pdf:/Users/apodusenko/Zotero/storage/UK53U7XQ/Bouvrie
              et al. - 2011 - Entropy and complexity analysis of
              Dirac-delta-lik.pdf:application/pdf}
}

@article{xu_convergence_nodate,
  title    = {On {Convergence} {Properties} of the {EM} {Algorithm} for {Gaussian}
              {Mixtures}},
  language = {en},
  author   = {Xu, Lei},
  pages    = {23},
  file     = {Xu - On Convergence Properties of the EM Algorithm for
              .pdf:/Users/apodusenko/Zotero/storage/EWYSNWCZ/Xu - On Convergence
              Properties of the EM Algorithm for .pdf:application/pdf}
}

@article{friston_bayesian_2018,
  title    = {Bayesian model reduction},
  url      = {http://arxiv.org/abs/1805.07092},
  abstract = {This paper reviews recent developments in statistical structure
              learning; namely, Bayesian model reduction. Bayesian model
              reduction is a special but ubiquitous case of Bayesian model
              comparison that, in the setting of variational Bayes, furnishes an
              analytic solution for (a lower bound on) model evidence induced by
              a change in priors. This analytic solution finesses the problem of
              scoring large model spaces in model comparison or structure
              learning. This is because each new model can be cast in terms of an
              alternative set of priors over model parameters. Furthermore, the
              reduced free energy (i.e. evidence bound on the reduced model)
              finds an expedient application in hierarchical models, where it
              plays the role of a summary statistic. In other words, it contains
              all the necessary information contained in the posterior
              distributions over parameters of lower levels. In this technical
              note, we review Bayesian model reduction - in terms of common forms
              of reduced free energy - and illustrate recent applications in
              structure learning, hierarchical or empirical Bayes and as a
              metaphor for neurobiological processes like abductive reasoning and
              sleep.},
  urldate  = {2018-05-28},
  journal  = {arXiv:1805.07092 [stat]},
  author   = {Friston, Karl and Parr, Thomas and Zeidman, Peter},
  month    = may,
  year     = {2018},
  note     = {arXiv: 1805.07092},
  keywords = {Statistics - Methodology},
  file     = {arXiv Fulltext PDF:/Users/apodusenko/Zotero/storage/2RQAMGMA/Friston
              et al. - 2019 - Bayesian model reduction.pdf:application/pdf;arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/4VVJ88WE/1805.html:text/html;arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/EQE7XDUF/1805.html:text/html;Friston
              et al. - 2018 - Bayesian model
              reduction.pdf:/Users/apodusenko/Zotero/storage/GE87NWF2/Friston et al.
              - 2018 - Bayesian model reduction.pdf:application/pdf}
}

@book{sarkka_bayesian_2013,
  address   = {London ; New York},
  title     = {Bayesian {Filtering} and {Smoothing}},
  isbn      = {978-0-415-55809-9},
  abstract  = {Filtering and smoothing methods are used to produce an accurate
               estimate of the state of a time-varying system based on multiple
               observational inputs (data). Interest in these methods has exploded
               in recent years, with numerous applications emerging in fields such
               as navigation, aerospace engineering, telecommunications and
               medicine. This compact, informal introduction for graduate students
               and advanced undergraduates presents the current state-of-the-art
               filtering and smoothing methods in a unified Bayesian framework.
               Readers learn what non-linear Kalman filters and particle filters
               are, how they are related, and their relative advantages and
               disadvantages. They also discover how state-of-the-art Bayesian
               parameter estimation methods can be combined with state-of-the-art
               filtering and smoothing algorithms. The book's practical and
               algorithmic approach assumes only modest mathematical
               prerequisites. Examples include MATLAB computations, and the
               numerous end-of-chapter exercises include computational
               assignments. MATLAB/GNU Octave source code is available for
               download at www.cambridge.org/sarkka, promoting hands-on work with
               the methods.},
  language  = {English},
  publisher = {Cambridge University Press},
  author    = {Särkkä, Simo},
  month     = oct,
  year      = {2013},
  file      = {Särkkä - 2013 - Bayesian Filtering and
               Smoothing.pdf:/Users/apodusenko/Zotero/storage/7TREJ8JV/Särkkä - 2013 -
               Bayesian Filtering and Smoothing.pdf:application/pdf;Särkkä - 2013 -
               Bayesian Filtering and
               Smoothing.pdf:/Users/apodusenko/Zotero/storage/Y3KKQKCV/Särkkä - 2013 -
               Bayesian Filtering and Smoothing.pdf:application/pdf;Särkkä - 2013 -
               Bayesian Filtering and
               Smoothing.pdf:/Users/apodusenko/Zotero/storage/MZEVBGYI/Särkkä - 2013 -
               Bayesian Filtering and Smoothing.pdf:application/pdf}
}

@inproceedings{yu_deep_2019,
  title     = {A {Deep} {Neural} {Network} {Based} {Kalman} {Filter} for {Time} {
               Domain} {Speech} {Enhancement}},
  doi       = {10.1109/ISCAS.2019.8702161},
  abstract  = {In this paper, we present a novel deep neural network (DNN) based
               Kalman filter (KF) algorithm for speech enhancement, where DNN is
               applied for estimating key parameters in the KF, namely, the linear
               prediction coefficients (LPCs). By training the DNN with a large
               database and making use of the powerful learning ability of DNN,
               our proposed DNN-KF algorithm is able to estimate LPCs from noisy
               speech more accurately and robustly, leading to an improved
               performance as compared to traditional KF based approaches in
               speech enhancement. Experimental results demonstrate that our
               DNN-KF method outperforms two existing KF based speech enhancement
               methods in terms of both speech quality and intelligibility.},
  booktitle = {2019 {IEEE} {International} {Symposium} on {Circuits} and {
               Systems} ({ISCAS})},
  author    = {Yu, Hongjiang and Ouyang, Zhiheng and Zhu, Wei-Ping and Champagne,
               Benoit and Ji, Yunyun},
  month     = may,
  year      = {2019},
  note      = {ISSN: 2158-1525},
  keywords  = {Kalman filters, Kalman filter, Speech enhancement, neural nets,
               Noise measurement, speech intelligibility, speech quality, Training
               , time-domain analysis, Covariance matrices, speech enhancement,
               parameter estimation, linear prediction coefficients, deep neural
               network, DNN-KF algorithm, Kalman filter algorithm, key parameter
               estimation, KF based speech enhancement methods, LPCs, noisy speech
               , time domain speech enhancement, Time-domain analysis},
  pages     = {1--5},
  file      = {IEEE Xplore Abstract
               Record:/Users/apodusenko/Zotero/storage/MFHHIEMT/8702161.html:text/html;IEEE
               Xplore Abstract
               Record:/Users/apodusenko/Zotero/storage/DYYYEA6F/8702161.html:text/html;IEEE
               Xplore Full Text PDF:/Users/apodusenko/Zotero/storage/ELQDIEAR/Yu et
               al. - 2019 - A Deep Neural Network Based Kalman Filter for
               Time.pdf:application/pdf;IEEE Xplore Full Text
               PDF:/Users/apodusenko/Zotero/storage/7BDQSTCZ/Yu et al. - 2019 - A Deep
               Neural Network Based Kalman Filter for Time.pdf:application/pdf}
}

@inproceedings{grundlehner_performance_2005,
  title     = {Performance assessment method for speech enhancement systems},
  abstract  = {A new method to assess noise reduction algorithms with respect to
               their ability to enhance the perceived quality of speech is
               pre-sented. Such algorithms consist of both single-microphone
               sys-tems and multiple microphone systems. Tests of the presented
               method show a higher correlation with subjective assessments than
               any other objective system known by the authors. It is be-lieved
               that this method is suitable to improve the comparability between
               noise reduction algorithms. Another area of applica-tion could be
               the optimization of parameters in a noise reduction algorithm, as
               well as the optimization of the geometric micro-phone positioning.
               1.},
  booktitle = {in {Proc}. 1st {Annu}. {IEEE} {BENELUX}/{DSP} {Valley} {Signal} {
               Process}. {Symp}},
  author    = {Grundlehner, Bernard and Lecocq, Johan and Balan, Radu and Rosca,
               Justinian},
  year      = {2005},
  file      = {Citeseer - Full Text
               PDF:/Users/apodusenko/Zotero/storage/89G3TJY9/Grundlehner et al. - 2005
               - Performance assessment method for speech
               enhanceme.pdf:application/pdf;Citeseer -
               Snapshot:/Users/apodusenko/Zotero/storage/XD83B26F/summary.html:text/html
               }
}

@article{hu_subjective_2007,
  title    = {Subjective comparison and evaluation of speech enhancement algorithms
              },
  volume   = {49},
  issn     = {0167-6393},
  url      = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2098693/},
  doi      = {10.1016/j.specom.2006.12.006},
  abstract = {Making meaningful comparisons between the performance of the
              various speech enhancement algorithms proposed over the years, has
              been elusive due to lack of a common speech database, differences
              in the types of noise used and differences in the testing
              methodology. To facilitate such comparisons, we report on the
              development of a noisy speech corpus suitable for evaluation of
              speech enhancement algorithms. This corpus is subsequently used for
              the subjective evaluation of 13 speech enhancement methods
              encompassing four classes of algorithms: spectral subtractive,
              subspace, statistical-model based and Wiener-type algorithms. The
              subjective evaluation was performed by Dynastat, Inc. using the
              ITU-T P.835 methodology designed to evaluate the speech quality
              along three dimensions: signal distortion, noise distortion and
              overall quality. This paper reports the results of the subjective
              tests.},
  number   = {7},
  urldate  = {2021-04-13},
  journal  = {Speech communication},
  author   = {Hu, Yi and Loizou, Philipos C.},
  month    = jul,
  year     = {2007},
  pmid     = {18046463},
  pmcid    = {PMC2098693},
  pages    = {588--601},
  file     = {Hu and Loizou - 2007 - Subjective comparison and evaluation of speech
              enh.pdf:/Users/apodusenko/Zotero/storage/MVAB6SVP/Hu and Loizou - 2007
              - Subjective comparison and evaluation of speech
              enh.pdf:application/pdf}
}

@misc{noauthor_noizeus_nodate,
  title   = {{NOIZEUS}: {Noisy} speech corpus - {Univ}. {Texas}-{Dallas}},
  url     = {https://ecs.utdallas.edu/loizou/speech/noizeus/},
  urldate = {2021-04-13},
  file    = {NOIZEUS\: Noisy speech corpus - Univ.
             Texas-Dallas:/Users/apodusenko/Zotero/storage/VMGN9ZZF/noizeus.html:text/html
             }
}

@inproceedings{yang_weighted_2019,
  title     = {A {Weighted} {Data} {Fusion} {Method} in {Distributed} {Multi}
               -sensors {Measurement} and {Control} {System}},
  doi       = {10.1109/ICISCE48695.2019.00135},
  abstract  = {In order to improve the precision of distributed multi-sensors
               measurement and control system, a data fusion method based on
               global state correction is proposed. The measurement data of single
               sensor is smoothed and the reliability of measurement data is
               improved based on statistic analysis of measurement data of single
               sensor in distributed multi-sensors measurement and control system.
               The data weight distribution model of multi-sensors is built and
               the rationality of data fusion weight is improved based on the
               analysis of weight distribution influence on data fusion. Typical
               example is used to validate the proposed fusion algorithm, and the
               result shows that the fusion result is satisfying, and the
               algorithm has theoretical and practical significance.},
  booktitle = {2019 6th {International} {Conference} on {Information} {Science}
               and {Control} {Engineering} ({ICISCE})},
  author    = {Yang, J. and Wang, S. and Na, X. and Yang, Y.},
  month     = dec,
  year      = {2019},
  keywords  = {data correction, data fusion, multi-sensors},
  pages     = {654--658},
  file      = {IEEE Xplore Abstract
               Record:/Users/apodusenko/Zotero/storage/ZJ6M6WAV/9107853.html:text/html;IEEE
               Xplore Full Text PDF:/Users/apodusenko/Zotero/storage/7RWPCUXN/Yang et
               al. - 2019 - A Weighted Data Fusion Method in Distributed
               Multi.pdf:application/pdf}
}

@inproceedings{goodfellow_generative_2014,
  title     = {Generative {Adversarial} {Nets}},
  volume    = {27},
  url       = {
               https://proceedings.neurips.cc/paper/2014/file/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf
               },
  booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
  publisher = {Curran Associates, Inc.},
  author    = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu,
               Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron
               and Bengio, Yoshua},
  editor    = {Ghahramani, Z. and Welling, M. and Cortes, C. and Lawrence, N. and
               Weinberger, K. Q.},
  year      = {2014}
}

@article{kavalekalam_model-based_2019,
  title    = {Model-{Based} {Speech} {Enhancement} for {Intelligibility} {
              Improvement} in {Binaural} {Hearing} {Aids}},
  volume   = {27},
  issn     = {2329-9304},
  doi      = {10.1109/TASLP.2018.2872128},
  abstract = {Speech intelligibility is often severely degraded among hearing
              impaired individuals in situations such as the cocktail party
              scenario. The performance of the current hearing aid technology has
              been observed to be limited in these scenarios. In this paper, we
              propose a binaural speech enhancement framework that takes into
              consideration the speech production model. The enhancement
              framework proposed here is based on the Kalman filter that allows
              us to take the speech production dynamics into account during the
              enhancement process. The usage of a Kalman filter requires the
              estimation of clean speech and noise short term predictor (STP)
              parameters, and the clean speech pitch parameters. In this work, a
              binaural codebook-based method is proposed for estimating the STP
              parameters, and a directional pitch estimator based on the harmonic
              model and maximum likelihood principle is used to estimate the
              pitch parameters. The proposed method for estimating the STP and
              pitch parameters jointly uses the information from left and right
              ears, leading to a more robust estimation of the filter parameters.
              Objective measures such as PESQ and STOI have been used to evaluate
              the enhancement framework in different acoustic scenarios
              representative of the cocktail party scenario. We have also
              conducted subjective listening tests on a set of nine normal
              hearing subjects, to evaluate the performance in terms of
              intelligibility and quality improvement. The listening tests show
              that the proposed algorithm, even with access to only a single
              channel noisy observation, significantly improves the overall
              speech quality, and the speech intelligibility by up to 15 \%.},
  number   = {1},
  journal  = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  author   = {Kavalekalam, M. S. and Nielsen, J. K. and Boldt, J. B. and
              Christensen, M. G.},
  month    = jan,
  year     = {2019},
  note     = {Conference Name: IEEE/ACM Transactions on Audio, Speech, and Language
              Processing},
  keywords = {Ear, Estimation, Kalman filters, Kalman filter, Speech enhancement
              , Noise measurement, Mathematical model, autoregressive model,
              binaural enhancement, pitch estimation},
  pages    = {99--113},
  file     = {IEEE Xplore Abstract
              Record:/Users/apodusenko/Zotero/storage/DB7AXY9X/8472176.html:text/html;IEEE
              Xplore Full Text
              PDF:/Users/apodusenko/Zotero/storage/3HXSX75S/Kavalekalam et al. - 2019
              - Model-Based Speech Enhancement for
              Intelligibility.pdf:application/pdf}
}

@article{nielsen_hearing_nodate,
  title    = {Hearing {Aid} {Personalization}},
  abstract = {Modern digital hearing aids require and offer a great level of
              personalization. Today, this personalization is not performed based
              directly on what the user actually perceives, but on a hearing-care
              professional’s interpretation of what the user explains about what
              is perceived. In this paper, an interactive personalization system
              based on Gaussian process regression and active learning is
              proposed, which personalize the hearing aids based directly on what
              the user perceives. Preliminary results demonstrate a signiﬁcant
              difference between a truly personalized setting obtained with the
              proposed system and a setting obtained by the current practice.},
  language = {en},
  author   = {Nielsen, Jens Brehm and Nielsen, Jakob and Jensen, Bjørn Sand and
              Larsen, Jan},
  pages    = {4},
  file     = {Nielsen et al. - Hearing Aid
              Personalization.pdf:/Users/apodusenko/Zotero/storage/5XQEXC5I/Nielsen
              et al. - Hearing Aid Personalization.pdf:application/pdf}
}

@article{charbonnier_results_1987,
  title    = {Results on {AR}-modelling of nonstationary signals},
  volume   = {12},
  issn     = {0165-1684},
  url      = {https://www.sciencedirect.com/science/article/pii/0165168487900028},
  doi      = {10.1016/0165-1684(87)90002-8},
  abstract = {Modelling of nonstationary signals can be performed using
              time-varying AR-models. The time-dependent AR-coefficients are
              assumed to be well represented by a linear combination of a small
              number of known time functions. This paper intends to compare two
              methods for the identification of such models. The first one is a
              blockwise method in which the parameters are estimated using the
              Morf-Dickinson-Kailath-Vieira algorithm for the resolution of
              covariance equations. In the second method, the identification is
              performed by a recursive least-squares algorithm. Finally, an
              extension of the second method for the detection of abrupt changes
              in AR-processes is presented. Zusammenfassung Nichtstationäre
              Signale können durch zeitvariante AR-Quellen modelliert werden. Die
              somit zeitabhängigen AR-Modell-Koeffizienten werden als
              Linearkombination einer beschränkten Anzahl bekannter
              Zeitfunktionen dargestellt. In diesem Beitrag sollen zwei Methoden
              zur Modellidentifizierung miteinander verglichen werden. Zunächst
              wird eine blockweise arbeitende Methode under Benutzung des
              Morf-Dickinson-Kailath-Vieira-Algorithmus angewandt, um die
              Modellparameter zu schätzen. Sodann wird mit einer zweiten (bezogen
              auf die Blocklänge) rekursiven Methode die Parameterschätzung
              vorgenommen, wobei ein modifizierter ‘least-square’-Algorithmus zur
              Anwendung kommt. Offensichtlich ist die zweite Methode besonders
              dazu geeignet, abrupte Änderungen in den AR-Parametern zu erkennen.
              Résumé La modélisation de signaux non stationnaires peut être
              réalisée au moyen de modèle AR variant dans le temps. On suppose
              que les coefficients du modèle AR sont une combinaison linéaire
              d'un nombre restreint des fonctions du temps. L'objet de cet
              article est de comparer deux méthodes d'identification de tels
              modèles. La première est une méthode globale dans laquelle les
              paramètres sont estimés en utilisant l'algorithme de
              Morf-Dickinson-Kailath-Vieira pur résoudre les équations de
              covariance. Dans la second méthode, l'identification est obtenue de
              manière récursive à partir d'un algorithme des moindres carrés
              recursifs. Finalement, on présente une utilsation de la deuxème
              méthode comme moyen de détection des ruptures de modèles AR.},
  language = {en},
  number   = {2},
  urldate  = {2021-03-30},
  journal  = {Signal Processing},
  author   = {Charbonnier, R and Barlaud, M and Alengrin, G and Menez, J},
  month    = mar,
  year     = {1987},
  keywords = {AR-modelling, identification, nonstationary signals, Parametric
              estimation},
  pages    = {143--151},
  file     = {ScienceDirect Full Text
              PDF:/Users/apodusenko/Zotero/storage/WU6I2C78/Charbonnier et al. - 1987
              - Results on AR-modelling of nonstationary
              signals.pdf:application/pdf;ScienceDirect
              Snapshot:/Users/apodusenko/Zotero/storage/B48QV8M7/0165168487900028.html:text/html
              }
}

@inproceedings{schmid_kalman_2010,
  title     = {A {Kalman} filter based noise suppression algorithm using speech and
               noise models derived from spatial information},
  abstract  = {In this paper, a novel Kalman filter based noise suppression
               algorithm for hearing aids, using spatial information for
               estimating the required noise and speech models, is proposed. The
               main assumption of the scheme is that the target (usually the
               speech signal) is directly in front of the hearing aid user while
               the interference (usually the noise signal) comes from the back
               hemisphere. While in an earlier paper [1], a related approach based
               on instantaneous Wiener filters using a Weighted Overlap Add (WOLA)
               decomposition has been presented, this paper focuses on a time
               domain approach employing a time varying Kalman filter. Clearly,
               with the proper noise and speech models, one would expect a better
               performance of a time varying Kalman filter than of a WOLA Wiener
               filter. Hearing tests as well as objective performance measures
               show the excellent performance of the Kalman filter based noise
               suppression algorithm.},
  booktitle = {2010 18th {European} {Signal} {Processing} {Conference}},
  author    = {Schmid, R. and Schuster, G. M.},
  month     = aug,
  year      = {2010},
  note      = {ISSN: 2219-5491},
  keywords  = {Noise, Kalman filters, Noise measurement, Speech, Distortion
               measurement, Acoustics, Interference},
  pages     = {1524--1528},
  file      = {IEEE Xplore Full Text
               PDF:/Users/apodusenko/Zotero/storage/UHD43VBK/Schmid and Schuster -
               2010 - A Kalman filter based noise suppression algorithm
               .pdf:application/pdf}
}

@inproceedings{smola_laplace_2004,
  title     = {Laplace propagation},
  booktitle = {{NIPS}},
  author    = {Smola, Alex J. and Vishwanathan, S. V. N. and Eskin, Eleazar},
  year      = {2004},
  pages     = {441--448},
  file      = {Smola et al. - 2004 - Laplace
               propagation.pdf:/Users/apodusenko/Zotero/storage/8FBRUQKP/Smola et al.
               - 2004 - Laplace
               propagation.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/YKN5G5CI/books.html:text/html
               }
}

@article{amin_graphical_2012,
  title    = {Graphical {Models} for {Bandit} {Problems}},
  url      = {http://arxiv.org/abs/1202.3782},
  abstract = {We introduce a rich class of graphical models for multi-armed
              bandit problems that permit both the state or context space and the
              action space to be very large, yet succinctly specify the payoffs
              for any context-action pair. Our main result is an algorithm for
              such models whose regret is bounded by the number of parameters and
              whose running time depends only on the treewidth of the graph
              substructure induced by the action space.},
  urldate  = {2021-03-20},
  journal  = {arXiv:1202.3782 [cs, stat]},
  author   = {Amin, Kareem and Kearns, Michael and Syed, Umar},
  month    = feb,
  year     = {2012},
  note     = {arXiv: 1202.3782},
  keywords = {Statistics - Machine Learning, Computer Science - Artificial
              Intelligence, Computer Science - Machine Learning},
  file     = {arXiv Fulltext PDF:/Users/apodusenko/Zotero/storage/ZA9GMHKY/Amin et
              al. - 2012 - Graphical Models for Bandit
              Problems.pdf:application/pdf;arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/QLQV2YKV/1202.html:text/html}
}

@article{yu_graphical_nodate,
  title    = {Graphical {Models} {Meet} {Bandits}: {A} {Variational} {Thompson} {
              Sampling} {Approach}},
  abstract = {We propose a novel framework for structured bandits, which we call
              an inﬂuence diagram bandit. Our framework uses a graphical model to
              capture complex statistical dependencies between actions, latent
              variables, and observations; and thus uniﬁes and extends many
              existing models, such as combinatorial semi-bandits, cascading
              bandits, and low-rank bandits. We develop novel online learning
              algorithms that learn to act efﬁciently in our models. The key idea
              is to track a structured posterior distribution of model parameters
              , either exactly or approximately. To act, we sample model
              parameters from their posterior and then use the structure of the
              inﬂuence diagram to ﬁnd the most optimistic action under the
              sampled parameters. We empirically evaluate our algorithms in three
              structured bandit problems, and show that they perform as well as
              or better than problem-speciﬁc state-of-the-art baselines.},
  language = {en},
  author   = {Yu, Tong and Kveton, Branislav and Wen, Zheng and Zhang, Ruiyi and
              Mengshoel, Ole J},
  pages    = {11},
  file     = {Yu et al. - Graphical Models Meet Bandits A Variational
              Thomp.pdf:/Users/apodusenko/Zotero/storage/I6TNGNK4/Yu et al. -
              Graphical Models Meet Bandits A Variational Thomp.pdf:application/pdf}
}

@article{zhu_adaptive_2019,
  title    = {Adaptive {Portfolio} by {Solving} {Multi}-armed {Bandit} via {
              Thompson} {Sampling}},
  url      = {http://arxiv.org/abs/1911.05309},
  abstract = {As the cornerstone of modern portfolio theory, Markowitz's
              mean-variance optimization is considered a major model adopted in
              portfolio management. However, due to the difficulty of estimating
              its parameters, it cannot be applied to all periods. In some cases,
              naive strategies such as Equally-weighted and Value-weighted
              portfolios can even get better performance. Under these
              circumstances, we can use multiple classic strategies as multiple
              strategic arms in multi-armed bandit to naturally establish a
              connection with the portfolio selection problem. This can also help
              to maximize the rewards in the bandit algorithm by the trade-off
              between exploration and exploitation. In this paper, we present a
              portfolio bandit strategy through Thompson sampling which aims to
              make online portfolio choices by effectively exploiting the
              performances among multiple arms. Also, by constructing multiple
              strategic arms, we can obtain the optimal investment portfolio to
              adapt different investment periods. Moreover, we devise a novel
              reward function based on users' different investment risk
              preferences, which can be adaptive to various investment styles.
              Our experimental results demonstrate that our proposed portfolio
              strategy has marked superiority across representative real-world
              market datasets in terms of extensive evaluation criteria.},
  urldate  = {2021-03-19},
  journal  = {arXiv:1911.05309 [cs, q-fin, stat]},
  author   = {Zhu, Mengying and Zheng, Xiaolin and Wang, Yan and Li, Yuyuan and
              Liang, Qianqiao},
  month    = nov,
  year     = {2019},
  note     = {arXiv: 1911.05309},
  keywords = {Statistics - Machine Learning, Computer Science - Machine Learning
              , Quantitative Finance - Portfolio Management},
  file     = {arXiv Fulltext PDF:/Users/apodusenko/Zotero/storage/6X44FV3D/Zhu et
              al. - 2019 - Adaptive Portfolio by Solving Multi-armed Bandit
              v.pdf:application/pdf;arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/BZF3ME6A/1911.html:text/html}
}

@inproceedings{shen_portfolio_2015,
  address   = {Buenos Aires, Argentina},
  series    = {{IJCAI}'15},
  title     = {Portfolio choices with orthogonal bandit learning},
  isbn      = {978-1-57735-738-4},
  abstract  = {The investigation and development of new methods from diverse
               perspectives to shed light on portfolio choice problems has never
               stagnated in financial research. Recently, multi-armed bandits have
               drawn intensive attention in various machine learning applications
               in online settings. The tradeoff between exploration and
               exploitation to maximize rewards in bandit algorithms naturally
               establishes a connection to portfolio choice problems. In this
               paper, we present a bandit algorithm for conducting online
               portfolio choices by effectually exploiting correlations among
               multiple arms. Through constructing orthogonal portfolios from
               multiple assets and integrating with the upper confidence bound
               bandit framework, we derive the optimal portfolio strategy that
               represents the combination of passive and active investments
               according to a risk-adjusted reward function. Compared with
               oft-quoted trading strategies in finance and machine learning
               fields across representative real-world market datasets, the
               proposed algorithm demonstrates superiority in both risk-adjusted
               return and cumulative wealth.},
  urldate   = {2021-03-19},
  booktitle = {Proceedings of the 24th {International} {Conference} on {
               Artificial} {Intelligence}},
  publisher = {AAAI Press},
  author    = {Shen, Weiwei and Wang, Jun and Jiang, Yu-Gang and Zha, Hongyuan},
  month     = jul,
  year      = {2015},
  pages     = {974--980}
}

@article{gaffney_translation-invariant_nodate,
  title    = {Translation-{Invariant} {Mixture} {Models} for {Curve} {Clustering}},
  abstract = {In this paper we present a family of algorithms that can
              simultaneously align and cluster sets of multidimensional curves
              deﬁned on a discrete time grid. Our approach uses the
              Expectation-Maximization (EM) algorithm to recover both the mean
              curve shapes for each cluster, and the most likely shifts, oﬀsets,
              and cluster memberships for each curve. We demonstrate how Bayesian
              estimation methods can improve the results for small sample sizes
              by enforcing smoothness in the cluster mean curves. We evaluate the
              methodology on two real-world data sets, time-course gene
              expression data and storm trajectory data. Experimental results
              show that models that incorporate curve alignment systematically
              provide improvements in predictive power and within-cluster
              variance on test data sets. The proposed approach provides a
              non-parametric, computationally eﬃcient, and robust methodology for
              clustering broad classes of curve data.},
  language = {en},
  author   = {Gaffney, Scott and Mjolsness, Eric and Smyth, Padhraic},
  pages    = {10},
  file     = {Gaffney et al. - Translation-Invariant Mixture Models for Curve
              Clu.pdf:/Users/apodusenko/Zotero/storage/TRQPFGMQ/Gaffney et al. -
              Translation-Invariant Mixture Models for Curve Clu.pdf:application/pdf}
}

@article{abraham_unsupervised_2003,
  title    = {Unsupervised {Curve} {Clustering} using {B}-{Splines}},
  volume   = {30},
  issn     = {0303-6898, 1467-9469},
  url      = {http://doi.wiley.com/10.1111/1467-9469.00350},
  doi      = {10.1111/1467-9469.00350},
  abstract = {Data in many different ﬁelds come to practitioners through a
              process naturally described as functional. Although data are
              gathered as ﬁnite vector and may contain measurement errors, the
              functional form have to be taken into account. We propose a
              clustering procedure of such data emphasizing the functional nature
              of the objects. The new clustering method consists of two stages:
              ﬁtting the functional data by B-splines and partitioning the
              estimated model coefﬁcients using a k-means algorithm. Strong
              consistency of the clustering method is proved and a real-world
              example from food industry is given.},
  language = {en},
  number   = {3},
  urldate  = {2021-03-16},
  journal  = {Scandinavian Journal of Statistics},
  author   = {Abraham, C. and Cornillon, P. A. and Matzner-Lober, E. and Molinari,
              N.},
  month    = sep,
  year     = {2003},
  pages    = {581--595},
  file     = {Abraham et al. - 2003 - Unsupervised Curve Clustering using
              B-Splines.pdf:/Users/apodusenko/Zotero/storage/STVP7FPW/Abraham et al.
              - 2003 - Unsupervised Curve Clustering using
              B-Splines.pdf:application/pdf}
}

@article{jacques_functional_2014,
  title      = {Functional data clustering: a survey},
  volume     = {8},
  issn       = {1862-5347, 1862-5355},
  shorttitle = {Functional data clustering},
  url        = {http://link.springer.com/10.1007/s11634-013-0158-y},
  doi        = {10.1007/s11634-013-0158-y},
  abstract   = {The main contributions to functional data clustering are reviewed.
                Most approaches used for clustering functional data are based on
                the following three methodologies: dimension reduction before
                clustering, nonparametric methods using speciﬁc distances or
                dissimilarities between curves and model-based clustering methods.
                These latter assume a probabilistic distribution on either the
                principal components or coeﬃcients of functional data expansion
                into a ﬁnite dimensional basis of functions. Numerical
                illustrations as well as a software review are presented.},
  language   = {en},
  number     = {3},
  urldate    = {2021-03-16},
  journal    = {Advances in Data Analysis and Classification},
  author     = {Jacques, Julien and Preda, Cristian},
  month      = sep,
  year       = {2014},
  pages      = {231--255},
  file       = {Jacques and Preda - 2014 - Functional data clustering a
                survey.pdf:/Users/apodusenko/Zotero/storage/5M956QIT/Jacques and Preda
                - 2014 - Functional data clustering a survey.pdf:application/pdf}
}

@article{sangalli_-mean_2010,
  title    = {-mean alignment for curve clustering},
  volume   = {54},
  issn     = {01679473},
  url      = {https://linkinghub.elsevier.com/retrieve/pii/S0167947309004605},
  doi      = {10.1016/j.csda.2009.12.008},
  abstract = {The problem of curve clustering when curves are misaligned is
              considered. A novel algorithm is described, which jointly clusters
              and aligns curves. The proposed procedure eﬃciently decouples
              amplitude and phase variability; in particular, it is able to
              detect amplitude clusters while simultaneously disclosing
              clustering structures in the phase, pointing out features that can
              neither be captured by simple curve clustering nor by simple curve
              alignment. The procedure is illustrated via simulation studies and
              applications to real data.},
  language = {en},
  number   = {5},
  urldate  = {2021-03-16},
  journal  = {Computational Statistics \& Data Analysis},
  author   = {Sangalli, Laura M. and Secchi, Piercesare and Vantini, Simone and
              Vitelli, Valeria},
  month    = may,
  year     = {2010},
  pages    = {1219--1233},
  file     = {Sangalli et al. - 2010 - -mean alignment for curve
              clustering.pdf:/Users/apodusenko/Zotero/storage/GDMUMGLZ/Sangalli et
              al. - 2010 - -mean alignment for curve clustering.pdf:application/pdf}
}

@article{gaffney_joint_nodate,
  title    = {Joint {Probabilistic} {Curve} {Clustering} and {Alignment}},
  abstract = {Clustering and prediction of sets of curves is an important
              problem in many areas of science and engineering. It is often the
              case that curves tend to be misaligned from each other in a
              continuous manner, either in space (across the measurements) or in
              time. We develop a probabilistic framework that allows for joint
              clustering and continuous alignment of sets of curves in curve
              space (as opposed to a ﬁxed-dimensional featurevector space). The
              proposed methodology integrates new probabilistic alignment models
              with model-based curve clustering algorithms. The probabilistic
              approach allows for the derivation of consistent EM learning
              algorithms for the joint clustering-alignment problem. Experimental
              results are shown for alignment of human growth data, and joint
              clustering and alignment of gene expression time-course data.},
  language = {en},
  author   = {Gaffney, Scott J and Smyth, Padhraic},
  pages    = {8},
  file     = {Gaffney and Smyth - Joint Probabilistic Curve Clustering and
              Alignment.pdf:/Users/apodusenko/Zotero/storage/JDINK9EZ/Gaffney and
              Smyth - Joint Probabilistic Curve Clustering and
              Alignment.pdf:application/pdf}
}

@article{struijs_curve_nodate,
  title    = {curve clustering},
  language = {en},
  author   = {Struijs, Martijn},
  pages    = {44},
  file     = {Struijs - curve
              clustering.pdf:/Users/apodusenko/Zotero/storage/F6Q99UF7/Struijs -
              curve clustering.pdf:application/pdf}
}

@article{zhang_bayesian_2015,
  title    = {Bayesian {Clustering} of {Shapes} of {Curves}},
  url      = {http://arxiv.org/abs/1504.00377},
  abstract = {Unsupervised clustering of curves according to their shapes is an
              important problem with broad scientiﬁc applications. The existing
              model-based clustering techniques either rely on simple probability
              models (e.g., Gaussian) that are not generally valid for shape
              analysis or assume the number of clusters. We develop an efﬁcient
              Bayesian method to cluster curve data using an elastic shape metric
              that is based on joint registration and comparison of shapes of
              curves. The elastic-inner product matrix obtained from the data is
              modeled using a Wishart distribution whose parameters are assigned
              carefully chosen prior distributions to allow for automatic
              inference on the number of clusters. Posterior is sampled through
              an efﬁcient Markov chain Monte Carlo procedure based on the Chinese
              restaurant process to infer (1) the posterior distribution on the
              number of clusters, and (2) clustering conﬁguration of shapes. This
              method is demonstrated on a variety of synthetic data and real data
              examples on protein structure analysis, cell shape analysis in
              microscopy images, and clustering of shaped from MPEG7 database.},
  language = {en},
  urldate  = {2021-03-16},
  journal  = {arXiv:1504.00377 [cs, stat]},
  author   = {Zhang, Zhengwu and Pati, Debdeep and Srivastava, Anuj},
  month    = apr,
  year     = {2015},
  note     = {arXiv: 1504.00377},
  keywords = {Statistics - Machine Learning, Computer Science - Machine Learning
              },
  file     = {Zhang et al. - 2015 - Bayesian Clustering of Shapes of
              Curves.pdf:/Users/apodusenko/Zotero/storage/T8MIALWY/Zhang et al. -
              2015 - Bayesian Clustering of Shapes of Curves.pdf:application/pdf}
}

@inproceedings{xu_iterated_2008,
  title     = {The iterated extended kalman particle filter for speech enhancement},
  doi       = {10.1109/ICOSP.2008.4697079},
  abstract  = {Particle filters have been proposed as a new form of state-space
               filtering for speech enhancement applications. A crucial issue in
               particle filtering is the selection of the importance proposal
               distribution. In this paper, the iterated extended Kalman filter
               (IEKF) is used to generate the proposal distribution. The proposal
               distribution integrates the latest measurements into state
               transition density, so it can match the posteriori density well. We
               apply time-varying autoregressive (TVAR) models with stochastically
               evolving parameters to the problem of speech modeling and
               enhancement, which is superior to conventional AR models. The
               experimental results indicate that the new particle filter
               superiors to the standard particle filter and the other filters
               such as the extended Kalman particle filter (PF-EKF) in low SNR.},
  booktitle = {2008 9th {International} {Conference} on {Signal} {Processing}},
  author    = {Xu, X. and Zhao, N. and Dong, H.},
  month     = oct,
  year      = {2008},
  note      = {ISSN: 2164-523X},
  keywords  = {Kalman filters, Particle filters, autoregressive processes, Speech
               enhancement, Speech analysis, Speech processing, particle filtering
               (numerical methods), speech enhancement, Signal processing, speech
               modeling, time-varying autoregressive models, Electronic mail,
               Information filtering, Information filters, Iterated extended
               kalman filter, iterated extended Kalman particle filter, Particle
               filter, Proposals, state transition density, state-space filtering,
               Time-varying autoregressive models},
  pages     = {104--107},
  file      = {IEEE Xplore Abstract
               Record:/Users/apodusenko/Zotero/storage/QEZ72VRQ/4697079.html:text/html;IEEE
               Xplore Full Text PDF:/Users/apodusenko/Zotero/storage/G23QG2NH/Xu et
               al. - 2008 - The iterated extended kalman particle filter for
               s.pdf:application/pdf}
}

@article{yu_influence_2020,
  title      = {Influence {Diagram} {Bandits}: {Variational} {Thompson} {Sampling}
                for {Structured} {Bandit} {Problems}},
  shorttitle = {Influence {Diagram} {Bandits}},
  url        = {http://arxiv.org/abs/2007.04915},
  abstract   = {We propose a novel framework for structured bandits, which we call
                an influence diagram bandit. Our framework captures complex
                statistical dependencies between actions, latent variables, and
                observations; and thus unifies and extends many existing models,
                such as combinatorial semi-bandits, cascading bandits, and low-rank
                bandits. We develop novel online learning algorithms that learn to
                act efficiently in our models. The key idea is to track a
                structured posterior distribution of model parameters, either
                exactly or approximately. To act, we sample model parameters from
                their posterior and then use the structure of the influence diagram
                to find the most optimistic action under the sampled parameters. We
                empirically evaluate our algorithms in three structured bandit
                problems, and show that they perform as well as or better than
                problem-specific state-of-the-art baselines.},
  urldate    = {2020-12-09},
  journal    = {arXiv:2007.04915 [cs, stat]},
  author     = {Yu, Tong and Kveton, Branislav and Wen, Zheng and Zhang, Ruiyi and
                Mengshoel, Ole J.},
  month      = jul,
  year       = {2020},
  note       = {arXiv: 2007.04915},
  keywords   = {Statistics - Machine Learning, Computer Science - Machine Learning
                },
  file       = {arXiv.org
                Snapshot:/Users/apodusenko/Zotero/storage/7FKXFJNX/2007.html:text/html;Yu
                et al. - 2020 - Influence Diagram Bandits Variational Thompson
                Sa.pdf:/Users/apodusenko/Zotero/storage/9G843AC3/Yu et al. - 2020 -
                Influence Diagram Bandits Variational Thompson Sa.pdf:application/pdf}
}

@article{urteaga_variational_2017,
  title    = {Variational inference for the multi-armed contextual bandit},
  url      = {http://arxiv.org/abs/1709.03163},
  abstract = {In many biomedical, science, and engineering problems, one must
              sequentially decide which action to take next so as to maximize
              rewards. Reinforcement learning is an area of machine learning that
              studies how this maximization balances exploration and exploitation
              , optimizing interactions with the world while simultaneously
              learning how the world operates. One general class of algorithms
              for this type of learning is the multi-armed bandit setting and, in
              particular, the contextual bandit case, in which observed rewards
              are dependent on each action as well as on given information or
              'context' available at each interaction with the world. The
              Thompson sampling algorithm has recently been shown to perform well
              in real-world settings and to enjoy provable optimality properties
              for this set of problems. It facilitates generative and
              interpretable modeling of the problem at hand, though complexity of
              the model limits its application, since one must both sample from
              the distributions modeled and calculate their expected rewards. We
              here show how these limitations can be overcome using variational
              approximations, applying to the reinforcement learning case
              advances developed for the inference case in the machine learning
              community over the past two decades. We consider bandit
              applications where the true reward distribution is unknown and
              approximate it with a mixture model, whose parameters are inferred
              via variational inference.},
  urldate  = {2018-03-20},
  journal  = {arXiv:1709.03163 [cs, stat]},
  author   = {Urteaga, Iñigo and Wiggins, Chris H.},
  month    = sep,
  year     = {2017},
  note     = {arXiv: 1709.03163},
  keywords = {Computer Science - Learning, Statistics - Machine Learning,
              Statistics - Computation, I.2.6},
  file     = {arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/EEMCBQZC/1709.html:text/html;Urteaga
              en Wiggins - 2017 - Variational inference for the multi-armed
              contextu.pdf:/Users/apodusenko/Zotero/storage/822PD67X/Urteaga en
              Wiggins - 2017 - Variational inference for the multi-armed
              contextu.pdf:application/pdf}
}

@article{goil_mafia_nodate,
  title    = {{MAFIA}: {E} cient and {LSacraglaebDleaStaubSseptasce} {Clustering}
              for {Very}},
  abstract = {Clustering techniques are used in database mining for nding
              interesting patterns in high dimensional data. These are useful in
              various applications of knowledge discovery in databases. Some
              challenges in clustering for large data sets in terms of
              scalability, data distribution, understanding end-results, and
              sensitivity to input order, have received attention in the recent
              past. Recent approaches attempt to nd clusters embedded in
              subspaces of high dimensional data. In this paper we propose the
              use of adaptive grids for e cient and scalable computation of
              clusters in subspaces for large data sets and large number of
              dimensions. The bottom-up algorithm for subspace clustering
              computes the dense units in all dimensions and combines these to
              generate the dense units in higher dimensions. Computation is
              heavily dependent on the choice of the partitioning parameter
              chosen to partition each dimension into intervals (bins) to be
              tested for density. The number of bins determines the computation
              requirements and the quality of the clustering results. Hence, it
              is important to determine the appropriate size and number of the
              bins. We present MAFIA, which 1) proposes adaptive grids for fast
              subspace clustering and 2) introduces a scalable parallel framework
              on a shared-nothing architecture to handle massive data sets.
              Performance results on very large data sets and a large number of
              dimensions show very good results, making an order of magnitude
              improvement in the computation time over current methods and
              providing much better quality of clustering.},
  language = {en},
  author   = {Goil, Sanjay and Nagesh, Harsha and Choudhary, Alok},
  pages    = {20},
  file     = {Goil et al. - MAFIA E cient and LSacraglaebDleaStaubSseptasce
              C.pdf:/Users/apodusenko/Zotero/storage/D9WSXR76/Goil et al. - MAFIA E
              cient and LSacraglaebDleaStaubSseptasce C.pdf:application/pdf}
}

@inproceedings{nagesh_adaptive_2001,
  title     = {Adaptive {Grids} for {Clustering} {Massive} {Data} {Sets}},
  isbn      = {978-0-89871-495-1 978-1-61197-271-9},
  url       = {https://epubs.siam.org/doi/10.1137/1.9781611972719.7},
  doi       = {10.1137/1.9781611972719.7},
  language  = {en},
  urldate   = {2021-03-03},
  booktitle = {Proceedings of the 2001 {SIAM} {International} {Conference} on {
               Data} {Mining}},
  publisher = {Society for Industrial and Applied Mathematics},
  author    = {Nagesh, Harsha and Goil, Sanjay and Choudhary, Alok},
  month     = apr,
  year      = {2001},
  pages     = {1--17},
  file      = {Nagesh et al. - 2001 - Adaptive Grids for Clustering Massive Data
               Sets.pdf:/Users/apodusenko/Zotero/storage/FETAFQFB/Nagesh et al. - 2001
               - Adaptive Grids for Clustering Massive Data Sets.pdf:application/pdf}
}

@article{bouveyron_high-dimensional_2007,
  title    = {High-dimensional data clustering},
  volume   = {52},
  issn     = {01679473},
  url      = {https://linkinghub.elsevier.com/retrieve/pii/S0167947307000692},
  doi      = {10.1016/j.csda.2007.02.009},
  abstract = {Clustering in high-dimensional spaces is a difﬁcult problem which
              is recurrent in many domains, for example in image analysis. The
              difﬁculty is due to the fact that high-dimensional data usually
              exist in different low-dimensional subspaces hidden in the original
              space. A family of Gaussian mixture models designed for
              high-dimensional data which combine the ideas of subspace
              clustering and parsimonious modeling are presented. These models
              give rise to a clustering method based on the
              expectation–maximization algorithm which is called high-dimensional
              data clustering (HDDC). In order to correctly ﬁt the data, HDDC
              estimates the speciﬁc subspace and the intrinsic dimension of each
              group. Experiments on artiﬁcial and real data sets show that HDDC
              outperforms existing methods for clustering high-dimensional data.},
  language = {en},
  number   = {1},
  urldate  = {2021-03-03},
  journal  = {Computational Statistics \& Data Analysis},
  author   = {Bouveyron, C. and Girard, S. and Schmid, C.},
  month    = sep,
  year     = {2007},
  pages    = {502--519},
  file     = {Bouveyron et al. - 2007 - High-dimensional data
              clustering.pdf:/Users/apodusenko/Zotero/storage/8MBSJWZQ/Bouveyron et
              al. - 2007 - High-dimensional data clustering.pdf:application/pdf}
}

@article{hao_clustering_2011,
  title      = {Clustering {16S} {rRNA} for {OTU} prediction: a method of
                unsupervised {Bayesian} clustering},
  volume     = {27},
  issn       = {1460-2059, 1367-4803},
  shorttitle = {Clustering {16S} {rRNA} for {OTU} prediction},
  url        = {
                https://academic.oup.com/bioinformatics/article-lookup/doi/10.1093/bioinformatics/btq725
                },
  doi        = {10.1093/bioinformatics/btq725},
  abstract   = {Motivation: With the advancements of next-generation sequencing
                technology, it is now possible to study samples directly obtained
                from the environment. Particularly, 16S rRNA gene sequences have
                been frequently used to proﬁle the diversity of organisms in a
                sample. However, such studies are still taxed to determine both the
                number of operational taxonomic units (OTUs) and their relative
                abundance in a sample.},
  language   = {en},
  number     = {5},
  urldate    = {2021-03-03},
  journal    = {Bioinformatics},
  author     = {Hao, Xiaolin and Jiang, Rui and Chen, Ting},
  month      = mar,
  year       = {2011},
  pages      = {611--618},
  file       = {Hao et al. - 2011 - Clustering 16S rRNA for OTU prediction a method
                o.pdf:/Users/apodusenko/Zotero/storage/JW27HD9L/Hao et al. - 2011 -
                Clustering 16S rRNA for OTU prediction a method o.pdf:application/pdf}
}

@article{mesot_switching_2007,
  title    = {Switching {Linear} {Dynamical} {Systems} for {Noise} {Robust} {Speech
              } {Recognition}},
  volume   = {15},
  issn     = {1558-7924},
  doi      = {10.1109/TASL.2007.901312},
  abstract = {Real world applications such as hands-free dialling in cars may
              have to deal with potentially very noisy environments. Existing
              state-of-the-art solutions to this problem use feature-based HMMs,
              with a preprocessing stage to clean the noisy signal. However, the
              effect that raw signal noise has on the induced HMM features is
              poorly understood, and limits the performance of the HMM system. An
              alternative to feature-based HMMs is to model the raw signal, which
              has the potential advantage that including an explicit noise model
              is straightforward. Here we jointly model the dynamics of both the
              raw speech signal and the noise, using a switching linear dynamical
              system (SLDS). The new model was tested on isolated digit
              utterances corrupted by Gaussian noise. Contrary to the
              autoregressive HMM and its derivatives, which provides a model of
              uncorrupted raw speech, the SLDS is comparatively noise robust and
              also significantly outperforms a state-of-the-art feature-based
              HMM. The computational complexity of the SLDS scales exponentially
              with the length of the time series. To counter this we use
              expectation correction which provides a stable and accurate
              linear-time approximation for this important class of models,
              aiding their further application in acoustic modeling.},
  number   = {6},
  journal  = {IEEE Transactions on Audio, Speech, and Language Processing},
  author   = {Mesot, B. and Barber, D.},
  month    = aug,
  year     = {2007},
  note     = {Conference Name: IEEE Transactions on Audio, Speech, and Language
              Processing},
  keywords = {inference mechanisms, computational complexity, hidden Markov
              model, Hidden Markov models, Speech enhancement, speech recognition
              , Working environment noise, Testing, linear dynamical system,
              approximation theory, Noise robustness, hidden Markov models,
              Gaussian noise, signal denoising, speech signal, Speech recognition
              , Approximate inference, Computational complexity, Counting
              circuits, expectation correction, feature-based HMM, isolated digit
              recognition, linear-time approximation, noise robust speech
              recognition, noise robustness, Superluminescent diodes, switching
              autoregressive process, switching linear dynamical systems},
  pages    = {1850--1858},
  file     = {IEEE Xplore Full Text
              PDF:/Users/apodusenko/Zotero/storage/USUDKX58/Mesot and Barber - 2007 -
              Switching Linear Dynamical Systems for Noise Robus.pdf:application/pdf}
}

@inproceedings{weller_understanding_2014,
  title      = {Understanding the {Bethe} approximation: {When} and how can it go
                wrong?},
  shorttitle = {Understanding the {Bethe} approximation},
  booktitle  = {{UAI}},
  author     = {Weller, Adrian and Tang, Kui and Jebara, Tony and Sontag, David A.},
  year       = {2014},
  pages      = {868--877},
  file       = {Weller et al. - 2014 - Understanding the Bethe approximation When and
                ho.pdf:/Users/apodusenko/Zotero/storage/LRQPHSK8/Weller et al. - 2014 -
                Understanding the Bethe approximation When and ho.pdf:application/pdf}
}

@article{beck_bayesian_2010,
  title     = {Bayesian system identification based on probability logic},
  volume    = {17},
  copyright = {Copyright © 2010 John Wiley \& Sons, Ltd.},
  issn      = {1545-2263},
  url       = {https://onlinelibrary.wiley.com/doi/abs/10.1002/stc.424},
  doi       = {https://doi.org/10.1002/stc.424},
  abstract  = {Probability logic with Bayesian updating provides a rigorous
               framework to quantify modeling uncertainty and perform system
               identification. It uses probability as a multi-valued propositional
               logic for plausible reasoning where the probability of a model is a
               measure of its relative plausibility within a set of models. System
               identification is thus viewed as inference about plausible system
               models and not as a quixotic quest for the true model. Instead of
               using system data to estimate the model parameters, Bayes' Theorem
               is used to update the relative plausibility of each model in a
               model class, which is a set of input–output probability models for
               the system and a probability distribution over this set that
               expresses the initial plausibility of each model. Robust predictive
               analyses informed by the system data use the entire model class
               with the probabilistic predictions of each model being weighed by
               its posterior probability. Additional robustness to modeling
               uncertainty comes from combining the robust predictions of each
               model class in a set of candidates for the system, where each
               contribution is weighed by the posterior probability of the model
               class. This application of Bayes' Theorem automatically applies a
               quantitative Ockham's razor that penalizes the data-fit of more
               complex model classes that extract more information from the data.
               Robust analyses involve integrals over parameter spaces that
               usually must be evaluated numerically by Laplace's method of
               asymptotic approximation or by Markov Chain Monte Carlo methods. An
               illustrative application is given using synthetic data
               corresponding to a structural health monitoring benchmark
               structure. Copyright © 2010 John Wiley \& Sons, Ltd.},
  language  = {en},
  number    = {7},
  urldate   = {2021-02-18},
  journal   = {Structural Control and Health Monitoring},
  author    = {Beck, James L.},
  year      = {2010},
  keywords  = {system identification, Bayesian updating, model assessment,
               probability logic, robust predictions},
  pages     = {825--847},
  file      = {Beck - 2010 - Bayesian system identification based on
               probabilit.html:/Users/apodusenko/Zotero/storage/LUU9UVIU/Beck - 2010 -
               Bayesian system identification based on probabilit.html:text/html;Beck
               - 2010 - Bayesian system identification based on
               probabilit.pdf:/Users/apodusenko/Zotero/storage/BTSQDRXJ/Beck - 2010 -
               Bayesian system identification based on probabilit.pdf:application/pdf}
}

@article{forney_codes_2001,
  title      = {Codes on graphs: normal realizations},
  volume     = {47},
  issn       = {0018-9448},
  shorttitle = {Codes on graphs},
  url        = {https://ieeexplore.ieee.org/abstract/document/910573},
  doi        = {10.1109/18.910573},
  abstract   = {A generalized state realization of the Wiberg (1996) type is
                called normal if symbol variables have degree 1 and state variables
                have degree 2. A natural graphical model of such a realization has
                leaf edges representing symbols, ordinary edges representing states
                , and vertices representing local constraints. Such a graph can be
                decoded by any version of the sum-product algorithm. Any state
                realization of a code can be put into normal form without essential
                change in the corresponding graph or in its decoding complexity.
                Group or linear codes are generated by group or linear state
                realizations. On a cycle-free graph, there exists a well-defined
                minimal canonical realization, and the sum-product algorithm is
                exact. However, the cut-set bound shows that graphs with cycles may
                have a superior performance-complexity tradeoff, although the
                sum-product algorithm is then inexact and iterative, and minimal
                realizations are not well-defined. Efficient cyclic and cycle-free
                realizations of Reed-Muller (RM) codes are given as examples. The
                dual of a normal group realization, appropriately defined,
                generates the dual group code. The dual realization has the same
                graph topology as the primal realization, replaces symbol and state
                variables by their character groups, and replaces primal local
                constraints by their duals. This fundamental result has many
                applications, including to dual state spaces, dual minimal
                trellises, duals to Tanner (1981) graphs, dual input/output (I/O)
                systems, and dual kernel and image representations. Finally a group
                code may be decoded using the dual graph, with appropriate Fourier
                transforms of the inputs and outputs; this can simplify decoding of
                high-rate codes},
  number     = {2},
  journal    = {IEEE Transactions on Information Theory},
  author     = {Forney, G.David},
  month      = feb,
  year       = {2001},
  keywords   = {Graphical models, graph theory, sum-product algorithm, character
                groups, codes on graphs, computational complexity, cut-set bound,
                cycle-free graph, cyclic codes, decoding, decoding complexity, dual
                codes, Dual codes, dual group code, dual input/output systems, dual
                kernel representation, dual minimal trellises, dual state spaces,
                Fourier transforms, generalized state realization, graph topology,
                graphical model, group codes, high-rate codes, Image representation
                , iterative algorithm, Kernel, leaf edges, Linear code, linear
                codes, local constraints, minimal canonical realization, normal
                group realization, normal realizations, ordinary edges, Parity
                check codes, performance-complexity tradeoff, primal realization,
                Reed-Muller codes, Reed-Solomon codes, state variables, Sum product
                algorithm, symbol variables, Tanner graphs, Topology, vertices},
  pages      = {520--548},
  file       = {Forney - 2001 - Codes on graphs normal
                realizations.pdf:/Users/apodusenko/Zotero/storage/HCMC86XM/Forney -
                2001 - Codes on graphs normal realizations.pdf:application/pdf;IEEE
                Xplore Abstract
                Record:/Users/apodusenko/Zotero/storage/TNBP24UJ/login.html:text/html;IEEE
                Xplore Abstract
                Record:/Users/apodusenko/Zotero/storage/TW9BWV89/login.html:text/html;IEEE
                Xplore Abstract
                Record:/Users/apodusenko/Zotero/storage/L6DV5IQF/910573.html:text/html;IEEE
                Xplore Full Text PDF:/Users/apodusenko/Zotero/storage/W7YGU84Y/Forney -
                2001 - Codes on graphs normal realizations.pdf:application/pdf}
}

@article{liu_bayesian_2019,
  title    = {Bayesian estimation of generalized {Gamma} mixture model based on
              variational {EM} algorithm},
  volume   = {87},
  issn     = {0031-3203},
  url      = {http://www.sciencedirect.com/science/article/pii/S0031320318303789},
  doi      = {10.1016/j.patcog.2018.10.025},
  abstract = {In this paper, we propose a Bayesian inference method for the
              generalized Gamma mixture model (GΓMM) based on variational
              expectation-maximization algorithm. Specifically, the shape
              parameters, the inverse scale parameters, and the mixing
              coefficients in the GΓMM are treated as random variables, while the
              power parameters are left as parameters without assigning prior
              distributions. The help function is designed to approximate the
              lower bound of the variational objective function, which
              facilitates the assignment of the conjugate prior distributions and
              leads to the closed-form update equations. On this basis, the
              variational E-step and the variational M-step are alternatively
              implemented to infer the posteriors of the variables and estimate
              the parameters. The computational demand is reduced by the proposed
              method. More importantly, the effective number of components of the
              GΓMM can be determined automatically. The experimental results
              demonstrate the effectiveness of the proposed method especially in
              modeling the asymmetric and heavy-tailed data.},
  language = {en},
  urldate  = {2021-02-02},
  journal  = {Pattern Recognition},
  author   = {Liu, Chi and Li, Heng-Chao and Fu, Kun and Zhang, Fan and Datcu,
              Mihai and Emery, William J.},
  month    = mar,
  year     = {2019},
  keywords = {Maximum likelihood estimation, Extended factorized approximation,
              Finite mixture models, Generalized Gamma distribution, Variational
              expectation-maximization (VEM)},
  pages    = {269--284},
  file     = {ScienceDirect Full Text
              PDF:/Users/apodusenko/Zotero/storage/AQ77S3T3/Liu et al. - 2019 -
              Bayesian estimation of generalized Gamma mixture
              m.pdf:application/pdf;ScienceDirect Full Text
              PDF:/Users/apodusenko/Zotero/storage/K3GYBNWN/Liu et al. - 2019 -
              Bayesian estimation of generalized Gamma mixture
              m.pdf:application/pdf;ScienceDirect
              Snapshot:/Users/apodusenko/Zotero/storage/Q84UQZU9/S0031320318303789.html:text/html;ScienceDirect
              Snapshot:/Users/apodusenko/Zotero/storage/V8I7SV2H/S0031320318303789.html:text/html
              }
}

@article{souza_time-varying_2019,
  title    = {A {Time}-{Varying} {Autoregressive} {Model} for {Characterizing} {
              Nonstationary} {Processes}},
  volume   = {26},
  issn     = {1558-2361},
  doi      = {10.1109/LSP.2018.2880086},
  abstract = {This letter presents a time-varying autoregressive (TVAR) model
              aiming to characterize nonstationary behaviors often observed in
              real-world processes, which cannot be properly described by
              autoregressive processes such as first-order Markov and random-walk
              models. Specifically, general model expressions for the mean vector
              and covariance matrix of the TVAR model are firstly derived. Then,
              such expressions are used to guide the design of two special setups
              for the TVAR model. The capability of the developed model to
              reproduce important nonstationary behaviors is verified
              mathematically and through simulations.},
  number   = {1},
  journal  = {IEEE Signal Processing Letters},
  author   = {Souza, D. Baptista de and Kuhn, E. V. and Seara, R.},
  month    = jan,
  year     = {2019},
  keywords = {Convergence, autoregressive processes, Markov processes,
              Mathematical model, Adaptation models, Brain modeling, Covariance
              matrices, signal processing, covariance matrices, covariance matrix
              , time-varying autoregressive model, TVAR model, random processes,
              mean vector, model expressions, nonstationary behaviors,
              nonstationary processes, Nonstationary processes, Steady-state,
              stochastic analysis, time-varying autoregressive (TVAR) model,
              vectors},
  pages    = {134--138},
  file     = {IEEE Xplore Abstract
              Record:/Users/apodusenko/Zotero/storage/FVNRCXSI/8526302.html:text/html;IEEE
              Xplore Full Text PDF:/Users/apodusenko/Zotero/storage/D987RPUG/Souza et
              al. - 2019 - A Time-Varying Autoregressive Model for
              Characteri.pdf:application/pdf;IEEE Xplore Full Text
              PDF:/Users/apodusenko/Zotero/storage/N3CYUMIX/Souza et al. - 2019 - A
              Time-Varying Autoregressive Model for Characteri.pdf:application/pdf}
}

@article{betancourt_conceptual_2018,
  title    = {A {Conceptual} {Introduction} to {Hamiltonian} {Monte} {Carlo}},
  url      = {http://arxiv.org/abs/1701.02434},
  abstract = {Hamiltonian Monte Carlo has proven a remarkable empirical success,
              but only recently have we begun to develop a rigorous understanding
              of why it performs so well on difficult problems and how it is best
              applied in practice. Unfortunately, that understanding is confined
              within the mathematics of differential geometry which has limited
              its dissemination, especially to the applied communities for which
              it is particularly important. In this review I provide a
              comprehensive conceptual account of these theoretical foundations,
              focusing on developing a principled intuition behind the method and
              its optimal implementations rather of any exhaustive rigor. Whether
              a practitioner or a statistician, the dedicated reader will acquire
              a solid grasp of how Hamiltonian Monte Carlo works, when it
              succeeds, and, perhaps most importantly, when it fails.},
  urldate  = {2020-05-06},
  journal  = {arXiv:1701.02434 [stat]},
  author   = {Betancourt, Michael},
  month    = jul,
  year     = {2018},
  note     = {arXiv: 1701.02434},
  keywords = {Statistics - Methodology},
  file     = {arXiv Fulltext
              PDF:/Users/apodusenko/Zotero/storage/RULVL6AH/Betancourt - 2018 - A
              Conceptual Introduction to Hamiltonian Monte
              Car.pdf:application/pdf;arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/AP4L77L7/1701.html:text/html;arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/NQHLMS3H/1701.html:text/html;Betancourt
              - 2018 - A Conceptual Introduction to Hamiltonian Monte
              Car.pdf:/Users/apodusenko/Zotero/storage/WI7X8TSE/Betancourt - 2018 - A
              Conceptual Introduction to Hamiltonian Monte Car.pdf:application/pdf}
}

@article{kalchbrenner_efficient_2018,
  title    = {Efficient {Neural} {Audio} {Synthesis}},
  url      = {http://arxiv.org/abs/1802.08435},
  abstract = {Sequential models achieve state-of-the-art results in audio,
              visual and textual domains with respect to both estimating the data
              distribution and generating high-quality samples. Efficient
              sampling for this class of models has however remained an elusive
              problem. With a focus on text-to-speech synthesis, we describe a
              set of general techniques for reducing sampling time while
              maintaining high output quality. We first describe a single-layer
              recurrent neural network, the WaveRNN, with a dual softmax layer
              that matches the quality of the state-of-the-art WaveNet model. The
              compact form of the network makes it possible to generate 24kHz
              16-bit audio 4x faster than real time on a GPU. Second, we apply a
              weight pruning technique to reduce the number of weights in the
              WaveRNN. We find that, for a constant number of parameters, large
              sparse networks perform better than small dense networks and this
              relationship holds for sparsity levels beyond 96\%. The small
              number of weights in a Sparse WaveRNN makes it possible to sample
              high-fidelity audio on a mobile CPU in real time. Finally, we
              propose a new generation scheme based on subscaling that folds a
              long sequence into a batch of shorter sequences and allows one to
              generate multiple samples at once. The Subscale WaveRNN produces 16
              samples per step without loss of quality and offers an orthogonal
              method for increasing sampling efficiency.},
  urldate  = {2018-02-26},
  journal  = {arXiv:1802.08435 [cs, eess]},
  author   = {Kalchbrenner, Nal and Elsen, Erich and Simonyan, Karen and Noury,
              Seb and Casagrande, Norman and Lockhart, Edward and Stimberg, Florian
              and Oord, Aaron van den and Dieleman, Sander and Kavukcuoglu, Koray},
  month    = feb,
  year     = {2018},
  note     = {arXiv: 1802.08435},
  keywords = {Computer Science - Learning, Computer Science - Sound, Electrical
              Engineering and Systems Science - Audio and Speech Processing,
              Computer Science - Machine Learning},
  file     = {arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/XHKIYGZS/1802.html:text/html;arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/K75IB6IC/1802.html:text/html;Kalchbrenner
              e.a. - 2018 - Efficient Neural Audio
              Synthesis.pdf:/Users/apodusenko/Zotero/storage/GAHV8JBG/Kalchbrenner
              e.a. - 2018 - Efficient Neural Audio
              Synthesis.pdf:application/pdf;Kalchbrenner et al. - 2018 - Efficient
              Neural Audio
              Synthesis.pdf:/Users/apodusenko/Zotero/storage/FA349DF5/Kalchbrenner et
              al. - 2018 - Efficient Neural Audio Synthesis.pdf:application/pdf}
}

@article{cournapeau_evaluation_nodate,
  title    = {Evaluation of {Real}-{Time} {Voice} {Activity} {Detection} {Based} on
              {High} {Order} {Statistics}},
  abstract = {We have proposed a method for real-time, unsupervised voice
              activity detection (VAD). In this paper, problems of feature
              selection and classiﬁcation scheme are addressed. The feature is
              based on High Order Statistics (HOS) to discriminate close and
              far-ﬁeld talk, enhanced by a feature derived from the normalized
              autocorrelation. Comparative effectiveness on several HOS is shown.
              The classiﬁcation is done in real-time with a recursive, online EM
              algorithm. The algorithm is evaluated on the CENSREC-1-C database,
              which is used for VAD evaluation for automatic speech recognition
              (ASR) [1], and the proposed method is conﬁrmed to signiﬁcantly
              outperform the baseline energy-based method.},
  language = {en},
  author   = {Cournapeau, David and Kawahara, Tatsuya},
  pages    = {4},
  file     = {Cournapeau and Kawahara - Evaluation of Real-Time Voice Activity
              Detection B.pdf:/Users/apodusenko/Zotero/storage/DRP53J8U/Cournapeau
              and Kawahara - Evaluation of Real-Time Voice Activity Detection
              B.pdf:application/pdf;Cournapeau and Kawahara - Evaluation of Real-Time
              Voice Activity Detection
              B.pdf:/Users/apodusenko/Zotero/storage/7HGUUPVJ/Cournapeau and Kawahara
              - Evaluation of Real-Time Voice Activity Detection
              B.pdf:application/pdf}
}

@article{kingma_auto-encoding_2013,
  title    = {Auto-{Encoding} {Variational} {Bayes}},
  url      = {http://arxiv.org/abs/1312.6114},
  abstract = {How can we perform efficient inference and learning in directed
              probabilistic models, in the presence of continuous latent
              variables with intractable posterior distributions, and large
              datasets? We introduce a stochastic variational inference and
              learning algorithm that scales to large datasets and, under some
              mild differentiability conditions, even works in the intractable
              case. Our contributions is two-fold. First, we show that a
              reparameterization of the variational lower bound yields a lower
              bound estimator that can be straightforwardly optimized using
              standard stochastic gradient methods. Second, we show that for
              i.i.d. datasets with continuous latent variables per datapoint,
              posterior inference can be made especially efficient by fitting an
              approximate inference model (also called a recognition model) to
              the intractable posterior using the proposed lower bound estimator.
              Theoretical advantages are reflected in experimental results.},
  urldate  = {2015-03-11},
  journal  = {arXiv:1312.6114 [cs, stat]},
  author   = {Kingma, Diederik P. and Welling, Max},
  month    = dec,
  year     = {2013},
  note     = {arXiv: 1312.6114},
  keywords = {Computer Science - Learning, Statistics - Machine Learning,
              Computer Science - Machine Learning},
  file     = {arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/W3EUX8CV/1312.html:text/html;Kingma
              and Welling - 2013 - Auto-Encoding Variational
              Bayes.pdf:/Users/apodusenko/Zotero/storage/8PQ3N6NM/Kingma and Welling
              - 2013 - Auto-Encoding Variational Bayes.pdf:application/pdf;Kingma and
              Welling - 2013 - Auto-Encoding Variational
              Bayes.pdf:/Users/apodusenko/Zotero/storage/5WKDL4IV/Kingma and Welling
              - 2013 - Auto-Encoding Variational
              Bayes.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/LC9KL6TE/forum.html:text/html
              }
}

@inproceedings{dauwels_expectation_2005,
  title     = {Expectation maximization as message passing},
  doi       = {10.1109/ISIT.2005.1523402},
  abstract  = {Based on prior work by Eckford, it is shown how expectation
               maximization (EM) may be viewed, and used, as a message passing
               algorithm in factor graphs},
  booktitle = {International {Symposium} on {Information} {Theory}, 2005. {ISIT}
               2005. {Proceedings}},
  author    = {Dauwels, J. and Korl, S. and Loeliger, H.-A.},
  month     = sep,
  year      = {2005},
  keywords  = {Kalman filters, Message passing, Graphical models, factor graphs,
               graph theory, Convergence, expectation maximization,
               expectation-maximisation algorithm, Filtering algorithms, graphical
               models, Information technology, message passing, message passing
               algorithm, modeling, Modeling, Parameter estimation, Particle
               filters, Signal processing algorithms},
  pages     = {583--586},
  file      = {Dauwels et al. - 2005 - Expectation maximization as message
               passing.pdf:/Users/apodusenko/Zotero/storage/2J4QW4ZR/Dauwels et al. -
               2005 - Expectation maximization as message
               passing.pdf:application/pdf;IEEE Xplore Abstract
               Record:/Users/apodusenko/Zotero/storage/DJWQ66FD/login.html:text/html;IEEE
               Xplore Abstract
               Record:/Users/apodusenko/Zotero/storage/37TBAS65/1523402.html:text/html;IEEE
               Xplore Full Text PDF:/Users/apodusenko/Zotero/storage/6DNFKEJH/Dauwels
               et al. - 2005 - Expectation maximization as message
               passing.pdf:application/pdf}
}

@article{chen_factor_2018,
  title    = {Factor graph fragmentization of expectation propagation},
  url      = {http://arxiv.org/abs/1801.05108},
  abstract = {Expectation propagation is a general approach to fast approximate
              inference for graphical models. The existing literature treats
              models separately when it comes to deriving and coding expectation
              propagation inference algorithms. This comes at the cost of similar
              , long-winded algebraic steps being repeated and slowing down
              algorithmic development. We demonstrate how factor graph
              fragmentization can overcome this impediment. This involves
              adoption of the message passing on a factor graph approach to
              expectation propagation and identification of factor graph
              sub-graphs, which we call fragments, that are common to wide
              classes of models. Key fragments and their corresponding messages
              are catalogued which means that their algebra does not need to be
              repeated. This allows compartmentalization of coding and efficient
              software development.},
  urldate  = {2018-03-12},
  journal  = {arXiv:1801.05108 [stat]},
  author   = {Chen, Wilson Y. and Wand, Matt P.},
  month    = jan,
  year     = {2018},
  note     = {arXiv: 1801.05108},
  keywords = {62F15, Statistics - Methodology},
  file     = {arXiv Fulltext PDF:/Users/apodusenko/Zotero/storage/S2BCA2CR/Chen and
              Wand - 2018 - Factor graph fragmentization of expectation
              propag.pdf:application/pdf;arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/6IUY9TIQ/1801.html:text/html;arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/YIGRT4F5/1801.html:text/html;Chen
              and Wand - 2018 - Factor graph fragmentization of expectation
              propag.pdf:/Users/apodusenko/Zotero/storage/3XC6E6FW/Chen and Wand -
              2018 - Factor graph fragmentization of expectation
              propag.pdf:application/pdf}
}

@article{wainwright_graphical_2008,
  title    = {Graphical {Models}, {Exponential} {Families}, and {Variational} {
              Inference}},
  volume   = {1},
  issn     = {1935-8237, 1935-8245},
  url      = {https://www.nowpublishers.com/article/Details/MAL-001},
  doi      = {10.1561/2200000001},
  abstract = {Graphical Models, Exponential Families, and Variational Inference},
  language = {English},
  number   = {1–2},
  urldate  = {2018-11-26},
  journal  = {Foundations and Trends® in Machine Learning},
  author   = {Wainwright, Martin J. and Jordan, Michael I.},
  month    = nov,
  year     = {2008},
  pages    = {1--305},
  file     = {
              Snapshot:/Users/apodusenko/Zotero/storage/7QCDMC5Y/MAL-001.html:text/html;Wainwright
              and Jordan - 2007 - Graphical Models, Exponential Families, and
              Variat.pdf:/Users/apodusenko/Zotero/storage/NHXUBNCY/Wainwright and
              Jordan - 2007 - Graphical Models, Exponential Families, and
              Variat.pdf:application/pdf;Wainwright and Jordan - 2008 - Graphical
              Models, Exponential Families, and
              Variat.pdf:/Users/apodusenko/Zotero/storage/B8PRFJQ9/Wainwright and
              Jordan - 2008 - Graphical Models, Exponential Families, and
              Variat.pdf:application/pdf}
}

@inproceedings{lu_variational_2014,
  title     = {A variational {Bayesian} approach to identification of switched {ARX}
               models},
  doi       = {10.1109/CDC.2014.7039777},
  abstract  = {In the identification of switched Auto-Regressive eXogenous (SARX)
               models, the number of local models is often assumed to be known a
               priori. However, in many industrial applications the prior process
               knowledge or the available information about the plant operation
               might not be sufficient to determine the number of local models. In
               such cases, the optimal number of local models needs to be inferred
               from collected operational data. The switching mechanism of the
               process is also often unknown. Therefore, classical SARX
               identification methods assuming a piecewise affine system fail to
               accurately identify randomly switched models. Furthermore,
               classical identification methods result in single-point estimates
               of unknown parameters, thereby ignoring the parameter uncertainty.
               The main objective of this work is to formulate and solve the
               problem of SARX model identification under the variational Bayesian
               framework through which the aforementioned challenging issues can
               be addressed. As a full Bayesian system identification approach,
               the proposed method not only provides a posterior distribution over
               model parameters to reveal the level of uncertainty of the
               estimated values, but also determines the optimal number of local
               models automatically. Since the identification pair identity at
               each sampling instant can be inferred from the data set, the
               switching mechanism will not influence the identification results.
               The effectiveness of the proposed Bayesian approach is demonstrated
               through a simulation case study.},
  booktitle = {53rd {IEEE} {Conference} on {Decision} and {Control}},
  author    = {Lu, Y. and Khatibisepehr, S. and Huang, B.},
  month     = dec,
  year      = {2014},
  note      = {ISSN: 0191-2216},
  keywords  = {Bayes methods, autoregressive processes, Equations, posterior
               distribution, variational techniques, Mathematical model, Switches,
               Data models, statistical distributions, chemical engineering,
               chemical processes, Chemical processes, full-Bayesian system
               identification approach, identification pair identity, industrial
               applications, industrial plants, Integrated circuit modeling,
               operational data, optimal local models, parameter uncertainty,
               plant operation, randomly switched models, sampling instant,
               sampling methods, SARX model identification, single-point estimates
               , switched ARX model identification, switched autoregressive
               exogenous model identification, uncertainty level, unknown
               parameters, unknown switching mechanism process, variational
               Bayesian approach},
  pages     = {2542--2547},
  file      = {IEEE Xplore Abstract
               Record:/Users/apodusenko/Zotero/storage/PR6JR6FD/7039777.html:text/html;IEEE
               Xplore Abstract
               Record:/Users/apodusenko/Zotero/storage/RRJGRA8I/7039777.html:text/html;IEEE
               Xplore Full Text PDF:/Users/apodusenko/Zotero/storage/9RMHXCKS/Lu et
               al. - 2014 - A variational Bayesian approach to identification
               .pdf:application/pdf}
}

@incollection{ting_locally_2010,
  address   = {Boston, MA},
  title     = {Locally {Weighted} {Regression} for {Control}},
  isbn      = {978-0-387-30164-8},
  url       = {https://doi.org/10.1007/978-0-387-30164-8_488},
  language  = {en},
  urldate   = {2019-10-25},
  booktitle = {Encyclopedia of {Machine} {Learning}},
  publisher = {Springer US},
  author    = {Ting, Jo-Anne and Vijayakumar, Sethu and Schaal, Stefan},
  editor    = {Sammut, Claude and Webb, Geoffrey I.},
  year      = {2010},
  doi       = {10.1007/978-0-387-30164-8_488},
  pages     = {613--624},
  file      = {Ting et al. - 2010 - Locally Weighted Regression for
               Control.pdf:/Users/apodusenko/Zotero/storage/55ZIWQU5/Ting et al. -
               2010 - Locally Weighted Regression for Control.pdf:application/pdf}
}

@article{paliwal_role_2011,
  title    = {Role of modulation magnitude and phase spectrum towards speech
              intelligibility},
  volume   = {53},
  issn     = {0167-6393},
  url      = {http://www.sciencedirect.com/science/article/pii/S0167639310001639},
  doi      = {10.1016/j.specom.2010.10.004},
  abstract = {In this paper our aim is to investigate the properties of the
              modulation domain and more specifically, to evaluate the relative
              contributions of the modulation magnitude and phase spectra towards
              speech intelligibility. For this purpose, we extend the traditional
              (acoustic domain) analysis–modification–synthesis framework to
              include modulation domain processing. We use this framework to
              construct stimuli that retain only selected spectral components,
              for the purpose of objective and subjective intelligibility tests.
              We conduct three experiments. In the first, we investigate the
              relative contributions to intelligibility of the modulation
              magnitude, modulation phase, and acoustic phase spectra. In the
              second experiment, the effect of modulation frame duration on
              intelligibility for processing of the modulation magnitude spectrum
              is investigated. In the third experiment, the effect of modulation
              frame duration on intelligibility for processing of the modulation
              phase spectrum is investigated. Results of these experiments show
              that both the modulation magnitude and phase spectra are important
              for speech intelligibility, and that significant improvement is
              gained by the inclusion of acoustic phase information. They also
              show that smaller modulation frame durations improve
              intelligibility when processing the modulation magnitude spectrum,
              while longer frame durations improve intelligibility when
              processing the modulation phase spectrum.},
  language = {en},
  number   = {3},
  urldate  = {2020-03-03},
  journal  = {Speech Communication},
  author   = {Paliwal, Kuldip and Schwerin, Belinda and Wójcicki, Kamil},
  month    = mar,
  year     = {2011},
  keywords = {Modulation domain, Analysis frame duration,
              Analysis–modification–synthesis (AMS), Modulation frame duration,
              Modulation magnitude spectrum, Modulation phase spectrum, Speech
              intelligibility, Speech transmission index (STI)},
  pages    = {327--339},
  file     = {ScienceDirect Full Text
              PDF:/Users/apodusenko/Zotero/storage/KQN35IPX/Paliwal et al. - 2011 -
              Role of modulation magnitude and phase spectrum
              to.pdf:application/pdf;ScienceDirect Full Text
              PDF:/Users/apodusenko/Zotero/storage/DUNE4ZQQ/Paliwal et al. - 2011 -
              Role of modulation magnitude and phase spectrum
              to.pdf:application/pdf;ScienceDirect
              Snapshot:/Users/apodusenko/Zotero/storage/P6ABUE8X/S0167639310001639.html:text/html;ScienceDirect
              Snapshot:/Users/apodusenko/Zotero/storage/4M7UN8LZ/S0167639310001639.html:text/html
              }
}

@article{del_moral_sequential_2006,
  title    = {Sequential {Monte} {Carlo} samplers},
  volume   = {68},
  issn     = {1369-7412, 1467-9868},
  url      = {http://doi.wiley.com/10.1111/j.1467-9868.2006.00553.x},
  doi      = {10.1111/j.1467-9868.2006.00553.x},
  abstract = {We propose a methodology to sample sequentially from a sequence of
              probability distributions that are deﬁned on a common space, each
              distribution being known up to a normalizing constant. These
              probability distributions are approximated by a cloud of weighted
              random samples which are propagated over time by using sequential
              Monte Carlo methods. This methodology allows us to derive simple
              algorithms to make parallel Markov chain Monte Carlo algorithms
              interact to perform global optimization and sequential Bayesian
              estimation and to compute ratios of normalizing constants. We
              illustrate these algorithms for various integration tasks arising
              in the context of Bayesian inference.},
  language = {en},
  number   = {3},
  urldate  = {2020-03-24},
  journal  = {Journal of the Royal Statistical Society: Series B (Statistical
              Methodology)},
  author   = {Del Moral, Pierre and Doucet, Arnaud and Jasra, Ajay},
  month    = jun,
  year     = {2006},
  pages    = {411--436},
  file     = {Del Moral et al. - 2006 - Sequential Monte Carlo
              samplers.pdf:/Users/apodusenko/Zotero/storage/HMGDF2UD/Del Moral et al.
              - 2006 - Sequential Monte Carlo samplers.pdf:application/pdf}
}

@article{gales_cepstral_1993,
  title    = {Cepstral parameter compensation for {HMM} recognition in noise},
  volume   = {12},
  url      = {http://www.sciencedirect.com/science/article/pii/016763939390093Z},
  number   = {3},
  urldate  = {2015-12-16},
  journal  = {Speech communication},
  author   = {Gales, Mark JF and Young, Steve J.},
  year     = {1993},
  keywords = {Speech recognition, AMN, noise compensation, PMC},
  pages    = {231--239},
  file     = {Gales and Young - 1993 - Cepstral parameter compensation for HMM
              recognitio.pdf:/Users/apodusenko/Zotero/storage/SBH4VZJ5/Gales and
              Young - 1993 - Cepstral parameter compensation for HMM
              recognitio.pdf:application/pdf;ScienceDirect Full Text
              PDF:/Users/apodusenko/Zotero/storage/4TYAXTB6/Gales and Young - 1993 -
              Cepstral parameter compensation for HMM
              recognitio.pdf:application/pdf;ScienceDirect
              Snapshot:/Users/apodusenko/Zotero/storage/Q8N6NPMM/016763939390093Z.html:text/html;Snapshot:/Users/apodusenko/Zotero/storage/B597MSIQ/016763939390093Z.html:text/html
              }
}

@article{penny_bayesian_2002,
  title    = {Bayesian multivariate autoregressive models with structured priors},
  volume   = {149},
  issn     = {1350-245X},
  doi      = {10.1049/ip-vis:20020149},
  abstract = {A variational Bayesian (VB) learning algorithm for parameter
              estimation and model-order selection in multivariate autoregressive
              (MAR) models is described. The use of structured priors in which
              subsets of coefficients are grouped together and constrained to be
              of a similar magnitude is explored. This allows MAR models to be
              more readily applied to high-dimensional data and to data with
              greater temporal complexity. The VB model order selection criterion
              is compared with the minimum description length approach. Results
              are presented on synthetic and electroencephalogram data.},
  number   = {1},
  journal  = {IEE Proceedings - Vision, Image and Signal Processing},
  author   = {Penny, Will D. and Roberts, Stephen J.},
  month    = feb,
  year     = {2002},
  keywords = {computational complexity, Bayes methods, medical signal processing
              , autoregressive processes, electroencephalography, synthetic data,
              parameter estimation, Bayesian multivariate AR models, Bayesian
              multivariate autoregressive models, cognitive-EEG data,
              electroencephalogram data, high-dimensional data, learning systems,
              minimum description length, model order selection, multiple time
              series data, sleep-EEG data, structured priors, temporal complexity
              , variational Bayesian learning algorithm},
  pages    = {33--41},
  file     = {IEEE Xplore Abstract
              Record:/Users/apodusenko/Zotero/storage/M6DK8LI9/999168.html:text/html;Penny
              and Roberts - 2002 - Bayesian multivariate autoregressive models with
              s.pdf:/Users/apodusenko/Zotero/storage/AMEXXP6S/Penny and Roberts -
              2002 - Bayesian multivariate autoregressive models with
              s.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/RDY8PH2P/ip-vis_20020149.html:text/html
              }
}

@article{cox_factor_2019,
  title    = {A factor graph approach to automated design of {Bayesian} signal
              processing algorithms},
  volume   = {104},
  issn     = {0888-613X},
  url      = {http://www.sciencedirect.com/science/article/pii/S0888613X18304298},
  doi      = {10.1016/j.ijar.2018.11.002},
  abstract = {The benefits of automating design cycles for Bayesian
              inference-based algorithms are becoming increasingly recognized by
              the machine learning community. As a result, interest in
              probabilistic programming frameworks has much increased over the
              past few years. This paper explores a specific probabilistic
              programming paradigm, namely message passing in Forney-style factor
              graphs (FFGs), in the context of automated design of efficient
              Bayesian signal processing algorithms. To this end, we developed
              “ForneyLab”2 as a Julia toolbox for message passing-based inference
              in FFGs. We show by example how ForneyLab enables automatic
              derivation of Bayesian signal processing algorithms, including
              algorithms for parameter estimation and model comparison. Crucially
              , due to the modular makeup of the FFG framework, both the model
              specification and inference methods are readily extensible in
              ForneyLab. In order to test this framework, we compared variational
              message passing as implemented by ForneyLab with automatic
              differentiation variational inference (ADVI) and Monte Carlo
              methods as implemented by state-of-the-art tools “Edward” and
              “Stan”. In terms of performance, extensibility and stability issues
              , ForneyLab appears to enjoy an edge relative to its competitors
              for automated inference in state-space models.},
  urldate  = {2018-11-16},
  journal  = {International Journal of Approximate Reasoning},
  author   = {Cox, Marco and van de Laar, Thijs and de Vries, Bert},
  month    = jan,
  year     = {2019},
  keywords = {Message passing, Bayesian inference, Factor graphs, Julia,
              Probabilistic programming},
  pages    = {185--204},
  file     = {Cox & van de Laar e.a. - 2019 - A factor graph approach to automated
              design of Bay.pdf:/Users/apodusenko/Zotero/storage/NAUIR5DJ/Cox & van
              de Laar e.a. - 2019 - A factor graph approach to automated design of
              Bay.pdf:application/pdf;ScienceDirect Full Text
              PDF:/Users/apodusenko/Zotero/storage/PL6FS972/Cox et al. - 2019 - A
              factor graph approach to automated design of Bay.pdf:application/pdf}
}

@article{de_vries_factor_2017,
  title    = {A {Factor} {Graph} {Description} of {Deep} {Temporal} {Active} {
              Inference}},
  volume   = {11},
  issn     = {1662-5188},
  url      = {https://www.frontiersin.org/articles/10.3389/fncom.2017.00095/full},
  doi      = {10.3389/fncom.2017.00095},
  abstract = {Active inference is a corollary of the Free Energy Principle that
              prescribes how self-organizing biological agents interact with
              their environment. The study of active inference processes relies
              on the definition of a generative probabilistic model and a
              description of how a free energy functional is minimized by
              neuronal message passing under that model. This paper presents a
              tutorial introduction to specifying active inference processes by
              Forney-style factor graphs (FFG). The FFG framework provides both
              an insightful representation of the probabilistic model and a
              biologically plausible inference scheme that, in principle, can be
              automatically executed in a computer simulation. As an illustrative
              example, we present an FFG for a deep temporal active inference
              process. The graph clearly shows how policy selection by expected
              free energy minimization results from free energy minimization per
              se, in an appropriate generative policy model.},
  language = {English},
  urldate  = {2018-12-05},
  journal  = {Frontiers in Computational Neuroscience},
  author   = {de Vries, Bert and Friston, Karl J.},
  year     = {2017},
  keywords = {Message passing, Belief propagation, message passing, Active
              inference, active inference, Factor Graphs, free-energy principle,
              multi-scale dynamical systems},
  file     = {de Vries and Friston - 2017 - A Factor Graph Description of Deep
              Temporal Active.pdf:/Users/apodusenko/Zotero/storage/HQPE5VMM/de Vries
              and Friston - 2017 - A Factor Graph Description of Deep Temporal
              Active.pdf:application/pdf;de Vries and Friston - 2017 - A Factor Graph
              Description of Deep Temporal
              Active.pdf:/Users/apodusenko/Zotero/storage/572NSRBL/de Vries and
              Friston - 2017 - A Factor Graph Description of Deep Temporal
              Active.pdf:application/pdf;Full Text
              PDF:/Users/apodusenko/Zotero/storage/7YQB9I5M/de Vries and Friston -
              2017 - A Factor Graph Description of Deep Temporal
              Active.pdf:application/pdf}
}

@phdthesis{korl_factor_2005,
  address  = {Zurich},
  title    = {A factor graph approach to signal modelling, system identification
              and filtering},
  language = {English},
  school   = {Swiss Federal Institute of Technology},
  author   = {Korl, Sascha},
  year     = {2005},
  keywords = {Electric engineering, info:eu-repo/classification/ddc/621.3,
              SIGNAL PROCESSING (TELECOMMUNICATIONS), SIGNAL THEORY + ELECTRICAL
              PULSE TECHNIQUE (TELECOMMUNICATIONS), SIGNALTHEORIE + ELEKTRISCHE
              IMPULSTECHNIK (NACHRICHTENTECHNIK), SIGNALVERARBEITUNG
              (NACHRICHTENTECHNIK)},
  file     = {Korl - 2005 - A Factor Graph Approach to Signal Modelling , System
              Identification and
              Filtering.pdf:/Users/apodusenko/Zotero/storage/CE2QTVXP/Korl - 2005 - A
              Factor Graph Approach to Signal Modelling , System Identification and
              Filtering.pdf:application/pdf;Korl - 2005 - A factor graph approach to
              signal modelling,
              syste.pdf:/Users/apodusenko/Zotero/storage/7VMPHYQH/Korl - 2005 - A
              factor graph approach to signal modelling, syste.pdf:application/pdf}
}

@article{le_data-driven_2017,
  title    = {Data-{Driven} {Ghosting} using {Deep} {Imitation} {Learning}},
  language = {en},
  author   = {Le, Hoang M and Carr, Peter and Yue, Yisong and Lucey, Patrick},
  year     = {2017},
  pages    = {15},
  file     = {Le et al. - 2017 - Data-Driven Ghosting using Deep Imitation
              Learning.pdf:/Users/apodusenko/Zotero/storage/TVGWSUSX/Le et al. - 2017
              - Data-Driven Ghosting using Deep Imitation
              Learning.pdf:application/pdf;Le et al. - 2017 - Data-Driven Ghosting
              using Deep Imitation
              Learning.pdf:/Users/apodusenko/Zotero/storage/AHBYW4A7/Le et al. - 2017
              - Data-Driven Ghosting using Deep Imitation
              Learning.pdf:application/pdf}
}

@article{wen-rong_wu_subband_1998,
  title    = {Subband {Kalman} filtering for speech enhancement},
  volume   = {45},
  issn     = {1558-125X},
  doi      = {10.1109/82.718814},
  abstract = {Kalman filtering is an effective speech-enhancement technique, in
              which speech signals are usually modeled as autoregressive (AR)
              processes and represented in the state-space domain. Since AR
              coefficients identification and Kalman filtering require extensive
              computations, real-time implementation of this approach is
              difficult. This paper proposes a simple and practical scheme that
              overcomes these obstacles. Speech signals are first decomposed into
              subbands. Subband speech signals are then modeled as low-order AR
              processes, such that low-order Kalman filters can be applied.
              Enhanced fullband speech signals are finally obtained by combining
              the enhanced subband speech signals. To identify AR coefficients,
              prediction-error filters adapted by the LMS algorithm are applied.
              Due to noisy inputs, the LMS algorithm converges to biased
              solutions. The performance of the Kalman filter with biased
              parameters is analyzed. It is shown that accurate estimates of AR
              coefficients are not required when the driving-noise variance is
              properly estimated. New methods for making such estimates are
              proposed. Thus, we can tolerate biased AR coefficients and take
              advantage of the LMS algorithm's simple structure. Simulation
              results show that speech enhancement in the subband domain not only
              greatly reduces the computational complexity, but also achieves
              better performance compared to that in the fullband domain.},
  number   = {8},
  journal  = {IEEE Transactions on Circuits and Systems II: Analog and Digital
              Signal Processing},
  author   = {{Wen-Rong Wu} and {Po-Cheng Chen}},
  month    = aug,
  year     = {1998},
  note     = {Conference Name: IEEE Transactions on Circuits and Systems II: Analog
              and Digital Signal Processing},
  keywords = {Kalman filters, computational complexity, Filtering algorithms,
              Signal processing algorithms, autoregressive processes, Hidden
              Markov models, Speech enhancement, least mean squares methods,
              filtering theory, Speech processing, speech enhancement, Signal
              processing, Wiener filter, parameter estimation, Computational
              complexity, AR coefficients identification, computational
              complexity reduction, Least squares approximation, LMS algorithm,
              prediction-error filters, real-time implementation, state-space
              domain, subband Kalman filtering},
  pages    = {1072--1083},
  file     = {IEEE Xplore Abstract
              Record:/Users/apodusenko/Zotero/storage/5N7JKMX4/718814.html:text/html;IEEE
              Xplore Full Text PDF:/Users/apodusenko/Zotero/storage/ET7FZGMM/Wen-Rong
              Wu and Po-Cheng Chen - 1998 - Subband Kalman filtering for speech
              enhancement.pdf:application/pdf;IEEE Xplore Full Text
              PDF:/Users/apodusenko/Zotero/storage/T34WFFXH/Wen-Rong Wu and Po-Cheng
              Chen - 1998 - Subband Kalman filtering for speech
              enhancement.pdf:application/pdf}
}

@article{khan_fast_2018,
  title    = {Fast yet {Simple} {Natural}-{Gradient} {Descent} for {Variational} {
              Inference} in {Complex} {Models}},
  url      = {http://arxiv.org/abs/1807.04489},
  abstract = {Bayesian inference plays an important role in advancing machine
              learning, but faces computational challenges when applied to
              complex models such as deep neural networks. Variational inference
              circumvents these challenges by formulating Bayesian inference as
              an optimization problem and solving it using gradient-based
              optimization. In this paper, we argue in favor of natural-gradient
              approaches which, unlike their gradient-based counterparts, can
              improve convergence by exploiting the information geometry of the
              solutions. We show how to derive fast yet simple natural-gradient
              updates by using a duality associated with exponential-family
              distributions. An attractive feature of these methods is that, by
              using natural-gradients, they are able to extract accurate local
              approximations for individual model components. We summarize recent
              results for Bayesian deep learning showing the superiority of
              natural-gradient approaches over their gradient counterparts.},
  urldate  = {2018-11-29},
  journal  = {arXiv:1807.04489 [cs, math, stat]},
  author   = {Khan, Mohammad Emtiyaz and Nielsen, Didrik},
  month    = jul,
  year     = {2018},
  note     = {arXiv: 1807.04489},
  keywords = {Statistics - Machine Learning, Computer Science - Information
              Theory, Statistics - Computation, Computer Science - Machine
              Learning},
  file     = {arXiv\:1807.04489 PDF:/Users/apodusenko/Zotero/storage/YISV63CM/Khan
              and Nielsen - 2018 - Fast yet Simple Natural-Gradient Descent for
              Varia.pdf:application/pdf;arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/MT47EBLV/1807.html:text/html;arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/6LLQLFSC/1807.html:text/html;Khan
              and Nielsen - 2018 - Fast yet Simple Natural-Gradient Descent for
              Varia.pdf:/Users/apodusenko/Zotero/storage/UHCXUY8Z/Khan and Nielsen -
              2018 - Fast yet Simple Natural-Gradient Descent for
              Varia.pdf:application/pdf}
}

@article{wand_fast_2017,
  title    = {Fast {Approximate} {Inference} for {Arbitrarily} {Large} {
              Semiparametric} {Regression} {Models} via {Message} {Passing}},
  volume   = {112},
  issn     = {0162-1459, 1537-274X},
  url      = {https://www.tandfonline.com/doi/full/10.1080/01621459.2016.1197833},
  doi      = {10.1080/01621459.2016.1197833},
  abstract = {We show how the notion of message passing can be used to
              streamline the algebra and computer coding for fast approximate
              inference in large Bayesian semiparametric regression models. In
              particular, this approach is amenable to handling arbitrarily large
              models of particular types once a set of primitive operations is
              established. The approach is founded upon a message passing
              formulation of mean field variational Bayes that utilizes factor
              graph representations of statistical models. The underlying
              principles apply to general Bayesian hierarchical models although
              we focus on semiparametric regression. The notion of factor graph
              fragments is introduced and is shown to facilitate
              compartmentalization of the required algebra and coding. The
              resultant algorithms have ready-to-implement closed form
              expressions and allow a broad class of arbitrarily large
              semiparametric regression models to be handled. Ongoing software
              projects such as Infer.NET and Stan support variational-type
              inference for particular model classes. This article is not
              concerned with software packages per se and focuses on the
              underlying tenets of scalable variational inference algorithms.
              Supplementary materials for this article are available online.},
  language = {en},
  number   = {517},
  urldate  = {2018-05-22},
  journal  = {Journal of the American Statistical Association},
  author   = {Wand, M. P.},
  month    = jan,
  year     = {2017},
  keywords = {Statistics - Methodology, Factor graphs, Generalized additive
              models, Generalized linear mixed models, Low-rank smoothing splines
              , Mean field variational Bayes, Scalable statistical methodology,
              Variational message passing},
  pages    = {137--168},
  file     = {arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/MIHCS7ML/1602.html:text/html;Wand
              - 2016 - Fast Approximate Inference for Arbitrarily Large
              S.pdf:/Users/apodusenko/Zotero/storage/QZFEZNLR/Wand - 2016 - Fast
              Approximate Inference for Arbitrarily Large S.pdf:application/pdf;Wand
              - 2017 - Fast Approximate Inference for Arbitrarily Large
              S.pdf:/Users/apodusenko/Zotero/storage/TJ9RI8IY/Wand - 2017 - Fast
              Approximate Inference for Arbitrarily Large S.pdf:application/pdf}
}

@article{hoffman_stochastic_2012,
  title    = {Stochastic {Variational} {Inference}},
  url      = {http://arxiv.org/abs/1206.7051},
  abstract = {We develop stochastic variational inference, a scalable algorithm
              for approximating posterior distributions. We develop this
              technique for a large class of probabilistic models and we
              demonstrate it with two probabilistic topic models, latent
              Dirichlet allocation and the hierarchical Dirichlet process topic
              model. Using stochastic variational inference, we analyze several
              large collections of documents: 300K articles from Nature, 1.8M
              articles from The New York Times, and 3.8M articles from Wikipedia.
              Stochastic inference can easily handle data sets of this size and
              outperforms traditional variational inference, which can only
              handle a smaller subset. (We also show that the Bayesian
              nonparametric topic model outperforms its parametric counterpart.)
              Stochastic variational inference lets us apply complex Bayesian
              models to massive data sets.},
  urldate  = {2017-08-27},
  journal  = {arXiv:1206.7051 [cs, stat]},
  author   = {Hoffman, Matt and Blei, David M. and Wang, Chong and Paisley, John},
  month    = jun,
  year     = {2012},
  note     = {arXiv: 1206.7051},
  keywords = {Statistics - Machine Learning, Computer Science - Artificial
              Intelligence, Statistics - Computation, Statistics - Methodology},
  file     = {arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/7HHPMKKJ/1206.html:text/html;Hoffman
              et al. - 2012 - Stochastic Variational
              Inference.pdf:/Users/apodusenko/Zotero/storage/FXPD6VPJ/Hoffman et al.
              - 2012 - Stochastic Variational Inference.pdf:application/pdf;Hoffman
              et al. - 2013 - Stochastic Variational
              Inference.pdf:/Users/apodusenko/Zotero/storage/HHYYW5A5/Hoffman et al.
              - 2013 - Stochastic Variational
              Inference.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/JMASB4FE/hoffman13a.html:text/html
              }
}

@inproceedings{grancharov_improved_2005,
  title     = {Improved {Kalman} filtering for speech enhancement},
  volume    = {1},
  doi       = {10.1109/ICASSP.2005.1415312},
  abstract  = {The Kalman recursion is a powerful technique for reconstruction of
               the speech signal observed in additive background noise. In
               contrast to Wiener filtering and spectral subtraction schemes, the
               Kalman algorithm can be easily implemented in both causal and
               noncausal form. After studying the perceptual differences between
               these two implementations we propose a novel algorithm that
               combines the low complexity and the robustness of the Kalman filter
               and the proper noise shaping of the Kalman smoother.},
  booktitle = {Proceedings. ({ICASSP} '05). {IEEE} {International} {Conference}
               on {Acoustics}, {Speech}, and {Signal} {Processing}, 2005.},
  author    = {Grancharov, V. and Samuelsson, J. and Kleijn, W. B.},
  month     = mar,
  year      = {2005},
  keywords  = {Kalman filters, smoothing methods, Kalman filter, Background noise
               , Speech enhancement, Kalman smoother, Additive noise, Speech
               processing, Kalman filtering, speech enhancement, Filtering, Speech
               coding, Wiener filter, acoustic noise, additive background noise,
               signal reconstruction, causal form, complexity, Kalman recursion,
               noise shaping, Noise shaping, noncausal form, perceptual
               differences, robustness, Sensor systems, speech signal
               reconstruction},
  pages     = {I/1109--I/1112 Vol. 1},
  file      = {Grancharov et al. - 2005 - Improved Kalman filtering for speech
               enhancement.html:/Users/apodusenko/Zotero/storage/APGK9CIJ/Grancharov
               et al. - 2005 - Improved Kalman filtering for speech
               enhancement.html:text/html;Grancharov et al. - 2005 - Improved Kalman
               filtering for speech
               enhancement.pdf:/Users/apodusenko/Zotero/storage/66TMHKER/Grancharov et
               al. - 2005 - Improved Kalman filtering for speech
               enhancement.pdf:application/pdf;IEEE Xplore Abstract
               Record:/Users/apodusenko/Zotero/storage/8Y25257T/1415312.html:text/html;IEEE
               Xplore Full Text
               PDF:/Users/apodusenko/Zotero/storage/MKW8WZ3S/Grancharov et al. - 2005
               - Improved Kalman filtering for speech enhancement.pdf:application/pdf}
}

@inproceedings{dionelis_modulation-domain_2017,
  title     = {Modulation-domain speech enhancement using a {Kalman} filter with a {
               Bayesian} update of speech and noise in the log-spectral domain},
  doi       = {10.1109/HSCMA.2017.7895572},
  abstract  = {We present a Bayesian estimator that performs log-spectrum
               estimation of both speech and noise, and is used as a Bayesian
               Kalman filter update step for single-channel speech enhancement in
               the modulation domain. We use Kalman filtering in the log-power
               spectral domain rather than in the amplitude or power spectral
               domains. In the Bayesian Kalman filter update step, we define the
               posterior distribution of the clean speech and noise log-power
               spectra as a two-dimensional multivariate Gaussian distribution. We
               utilize a Kalman filter observation constraint surface in the
               three-dimensional space, where the third dimension is the phase
               factor. We evaluate the results of the phase-sensitive log-spectrum
               Kalman filter by comparing them with the results obtained by
               traditional noise suppression techniques and by an alternative
               Kalman filtering technique that assumes additivity of speech and
               noise in the power spectral domain.},
  booktitle = {2017 {Hands}-free {Speech} {Communications} and {Microphone} {
               Arrays} ({HSCMA})},
  author    = {Dionelis, Nikolaos and Brookes, Mike},
  month     = mar,
  year      = {2017},
  note      = {ISSN: null},
  keywords  = {Kalman filters, Bayes methods, Gaussian distribution, noise
               suppression, Speech enhancement, Noise measurement, Speech,
               Mathematical model, speech enhancement, signal denoising, Spectral
               analysis, Bayesian estimator, Bayesian Kalman filter update step,
               Bayesian noise update, Bayesian speech update, clean speech
               posterior distribution, Kalman filter observation constraint
               surface, log-power spectral domain, log-spectrum estimation,
               Modulation, modulation-domain speech enhancement, noise log-power
               spectra, single-channel speech enhancement, three-dimensional space
               , two-dimensional multivariate Gaussian distribution},
  pages     = {111--115},
  file      = {IEEE Xplore Abstract
               Record:/Users/apodusenko/Zotero/storage/AM6XQ9TR/7895572.html:text/html;IEEE
               Xplore Abstract
               Record:/Users/apodusenko/Zotero/storage/CXUEESS9/7895572.html:text/html;IEEE
               Xplore Full Text PDF:/Users/apodusenko/Zotero/storage/T8UJXXSI/Dionelis
               and Brookes - 2017 - Modulation-domain speech enhancement using a
               Kalma.pdf:application/pdf;IEEE Xplore Full Text
               PDF:/Users/apodusenko/Zotero/storage/2DGRK9QZ/Dionelis and Brookes -
               2017 - Modulation-domain speech enhancement using a
               Kalma.pdf:application/pdf}
}

@inproceedings{sarafnia_noise_2013,
  title     = {Noise reduction of speech signal using bayesian state-space {Kalman}
               filter},
  doi       = {10.1109/APCC.2013.6766008},
  abstract  = {The noise exists in almost all environments such as cellular
               mobile telephone systems. Various types of noise can be introduced
               such as speech additive noise which is the main factor of
               degradation in perceived speech quality. At some applications for
               example at the receiver of a telecommunication system, the direct
               value of interfering noise is not available and there is just
               access to noisy speech. In these cases the noise cannot be
               cancelled totally but it may be possible to reduce the noise in a
               sensible way by utilizing the statistics of the noise and speech
               signal. In this paper the proposed method for noise reduction is
               Bayesian recursive state-space Kalman filter, which is a method for
               estimation of a speech signal from its noisy version. It utilizes
               the prior probability distributions of the signal and noise
               processes, which are assumed to be zero-mean Gaussian processes, to
               implement the noise reduction. The function of Kalman filter is
               assessed for SNR values of -5 dB and 5 dB respectively. Evaluation
               results indicate that this method of noise reduction yields better
               speech perceived quality and efficient results.},
  booktitle = {2013 19th {Asia}-{Pacific} {Conference} on {Communications} ({
               APCC})},
  author    = {Sarafnia, Ali and Ghorshi, Seyed},
  month     = aug,
  year      = {2013},
  note      = {ISSN: 2163-0771},
  keywords  = {Estimation, Kalman filters, Bayes methods, Equations, Noise
               measurement, Signal to noise ratio, Speech, state-space model,
               Gaussian processes, noise reduction, interference suppression,
               speech enhancement, Noise reduction, state-space methods, recursive
               filters, noisy speech, Bayesian Method, Bayesian recursive
               state-space Kalman filter, cellular mobile telephone system, Kalman
               filte, noise figure -5 dB, noise figure 5 dB, perceived speech
               quality degradation, probability distribution, speech additive
               noise, speech signal estimation, zero-mean Gaussian process},
  pages     = {545--549},
  file      = {IEEE Xplore Abstract
               Record:/Users/apodusenko/Zotero/storage/XNU5SDZZ/6766008.html:text/html;IEEE
               Xplore Abstract
               Record:/Users/apodusenko/Zotero/storage/W56UZZJR/6766008.html:text/html;IEEE
               Xplore Full Text PDF:/Users/apodusenko/Zotero/storage/B5F98YQP/Sarafnia
               and Ghorshi - 2013 - Noise reduction of speech signal using bayesian
               st.pdf:application/pdf;Sarafnia and Ghorshi - 2013 - Noise reduction of
               speech signal using bayesian
               st.pdf:/Users/apodusenko/Zotero/storage/VNKPUXKF/Sarafnia and Ghorshi -
               2013 - Noise reduction of speech signal using bayesian
               st.pdf:application/pdf}
}

@article{kamary_testing_2014,
  title    = {Testing hypotheses via a mixture estimation model},
  url      = {http://arxiv.org/abs/1412.2044},
  abstract = {We consider a novel paradigm for Bayesian testing of hypotheses
              and Bayesian model comparison. Our alternative to the traditional
              construction of posterior probabilities that a given hypothesis is
              true or that the data originates from a specific model is to
              consider the models under comparison as components of a mixture
              model. We therefore replace the original testing problem with an
              estimation one that focus on the probability weight of a given
              model within a mixture model. We analyze the sensitivity on the
              resulting posterior distribution on the weights of various prior
              modeling on the weights. We stress that a major appeal in using
              this novel perspective is that generic improper priors are
              acceptable, while not putting convergence in jeopardy. Among other
              features, this allows for a resolution of the Lindley-Jeffreys
              paradox. When using a reference Beta B(a,a) prior on the mixture
              weights, we note that the sensitivity of the posterior estimations
              of the weights to the choice of a vanishes with the sample size
              increasing and avocate the default choice a=0.5, derived from
              Rousseau and Mengersen (2011). Another feature of this easily
              implemented alternative to the classical Bayesian solution is that
              the speeds of convergence of the posterior mean of the weight and
              of the corresponding posterior probability are quite similar.},
  urldate  = {2016-11-09},
  journal  = {arXiv:1412.2044 [stat]},
  author   = {Kamary, Kaniav and Mengersen, Kerrie and Robert, Christian P. and
              Rousseau, Judith},
  month    = dec,
  year     = {2014},
  note     = {arXiv: 1412.2044},
  keywords = {Statistics - Methodology},
  file     = {arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/YFKINZ4B/1412.html:text/html;arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/MV2AUYNC/1412.html:text/html;Kamary
              et al. - 2014 - Testing hypotheses via a mixture estimation
              model.pdf:/Users/apodusenko/Zotero/storage/BAI2E4RB/Kamary et al. -
              2014 - Testing hypotheses via a mixture estimation
              model.pdf:application/pdf;Kamary et al. - 2014 - Testing hypotheses via
              a mixture estimation
              model.pdf:/Users/apodusenko/Zotero/storage/84SVN77H/Kamary et al. -
              2014 - Testing hypotheses via a mixture estimation
              model.pdf:application/pdf}
}

@techreport{petersen_matrix_2012,
  title  = {The matrix cookbook},
  url    = {
            http://www2.imm.dtu.dk/pubdb/views/edoc_download.php/3274/pdf/imm3274.pdf
            },
  author = {Petersen, Kaare Brandt and Pedersen, Michael Syskind},
  year   = {2012},
  file   = {Citeseer -
            Snapshot:/Users/apodusenko/Zotero/storage/ENE4HE9B/summary.html:text/html;Petersen
            and Pedersen - 2012 - The Matrix
            Cookbook.pdf:/Users/apodusenko/Zotero/storage/XBJ4MCX2/Petersen and
            Pedersen - 2012 - The Matrix Cookbook.pdf:application/pdf;Petersen and
            Pedersen - 2012 - The matrix
            cookbook.pdf:/Users/apodusenko/Zotero/storage/BBU3PNBZ/Petersen and
            Pedersen - 2012 - The matrix cookbook.pdf:application/pdf;The Matrix
            Cookbook:/Users/apodusenko/Zotero/storage/7NL5WAUR/3274-full.html:text/html
            }
}

@inproceedings{sharman_time-varying_1984,
  title     = {Time-varying autoregressive modeling of a class of nonstationary
               signals},
  volume    = {9},
  doi       = {10.1109/ICASSP.1984.1172536},
  abstract  = {The problem of estimating sinusoidal or narrowband signals with a
               time-varying center frequency is considered. The signal parameters
               are estimated by fitting an autoregressive model with time-varying
               coefficients to the data. The overdetermined modified Yule-Walker
               equations are used to estimate a set of constant model parameters.
               Some numerical examples illustrating the behavior of the estimator
               are presented, and its accuracy aspects are briefly discussed.},
  booktitle = {{ICASSP} '84. {IEEE} {International} {Conference} on {Acoustics},
               {Speech}, and {Signal} {Processing}},
  author    = {Sharman, K. and Friedlander, B.},
  month     = mar,
  year      = {1984},
  keywords  = {Parameter estimation, Equations, Acoustic noise, Narrowband,
               Frequency estimation, Signal processing, Parametric statistics,
               Autoregressive processes, Radar, Sonar},
  pages     = {227--230},
  file      = {IEEE Xplore Abstract
               Record:/Users/apodusenko/Zotero/storage/FXBTBK77/1172536.html:text/html;IEEE
               Xplore Abstract
               Record:/Users/apodusenko/Zotero/storage/CZLKMN53/1172536.html:text/html;IEEE
               Xplore Full Text PDF:/Users/apodusenko/Zotero/storage/PFBYL2A2/Sharman
               and Friedlander - 1984 - Time-varying autoregressive modeling of a
               class of.pdf:application/pdf;IEEE Xplore Full Text
               PDF:/Users/apodusenko/Zotero/storage/6NL8Z82I/Sharman and Friedlander -
               1984 - Time-varying autoregressive modeling of a class
               of.pdf:application/pdf}
}

@article{welling_structured_2012,
  title      = {Structured {Region} {Graphs}: {Morphing} {EP} into {GBP}},
  shorttitle = {Structured {Region} {Graphs}},
  url        = {http://arxiv.org/abs/1207.1426},
  abstract   = {GBP and EP are two successful algorithms for approximate
                probabilistic inference, which are based on different approximation
                strategies. An open problem in both algorithms has been how to
                choose an appropriate approximation structure. We introduce
                'structured region graphs', a formalism which marries these two
                strategies, reveals a deep connection between them, and suggests
                how to choose good approximation structures. In this formalism,
                each region has an internal structure which defines an exponential
                family, whose sufficient statistics must be matched by the parent
                region. Reduction operators on these structures allow conversion
                between EP and GBP free energies. Thus it is revealed that all EP
                approximations on discrete variables are special cases of GBP, and
                conversely that some wellknown GBP approximations, such as
                overlapping squares, are special cases of EP. Furthermore, region
                graphs derived from EP have a number of good structural properties,
                including maxent-normality and overall counting number of one. The
                result is a convenient framework for producing high-quality
                approximations with a user-adjustable level of complexity},
  urldate    = {2020-10-25},
  journal    = {arXiv:1207.1426 [cs]},
  author     = {Welling, Max and Minka, Thomas P. and Teh, Yee Whye},
  month      = jul,
  year       = {2012},
  note       = {arXiv: 1207.1426},
  keywords   = {Computer Science - Artificial Intelligence},
  file       = {arXiv Fulltext PDF:/Users/apodusenko/Zotero/storage/RLTBUKA6/Welling
                et al. - 2012 - Structured Region Graphs Morphing EP into
                GBP.pdf:application/pdf;arXiv.org
                Snapshot:/Users/apodusenko/Zotero/storage/IGJUGR9C/1207.html:text/html;Welling
                et al. - Structured Region Graphs Morphing EP into
                GBP.pdf:/Users/apodusenko/Zotero/storage/XEFJWC3T/Welling et al. -
                Structured Region Graphs Morphing EP into GBP.pdf:application/pdf}
}

@article{hao_speech_2010,
  title    = {Speech {Enhancement} {Using} {Gaussian} {Scale} {Mixture} {Models}},
  volume   = {18},
  issn     = {1558-7916},
  url      = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3045111/},
  doi      = {10.1109/TASL.2009.2030012},
  abstract = {This paper presents a novel probabilistic approach to speech
              enhancement. Instead of a deterministic logarithmic relationship,
              we assume a probabilistic relationship between the frequency
              coefficients and the log-spectra. The speech model in the
              log-spectral domain is a Gaussian mixture model (GMM). The
              frequency coefficients obey a zero-mean Gaussian whose covariance
              equals to the exponential of the log-spectra. This results in a
              Gaussian scale mixture model (GSMM) for the speech signal in the
              frequency domain, since the log-spectra can be regarded as scaling
              factors. The probabilistic relation between frequency coefficients
              and log-spectra allows these to be treated as two random variables,
              both to be estimated from the noisy signals.
              Expectation-maximization (EM) was used to train the GSMM and
              Bayesian inference was used to compute the posterior signal
              distribution. Because exact inference of this full probabilistic
              model is computationally intractable, we developed two approaches
              to enhance the efficiency: the Laplace method and a variational
              approximation. The proposed methods were applied to enhance speech
              corrupted by Gaussian noise and speech-shaped noise (SSN). For both
              approximations, signals reconstructed from the estimated frequency
              coefficients provided higher signal-to-noise ratio (SNR) and those
              reconstructed from the estimated log-spectra produced lower word
              recognition error rate because the log-spectra fit the inputs to
              the recognizer better. Our algorithms effectively reduced the SSN,
              which algorithms based on spectral analysis were not able to
              suppress.},
  number   = {6},
  urldate  = {2015-12-28},
  journal  = {IEEE transactions on audio, speech, and language processing},
  author   = {Hao, Jiucang and Lee, Te-Won and Sejnowski, Terrence J.},
  month    = aug,
  year     = {2010},
  pmid     = {21359139},
  pmcid    = {PMC3045111},
  keywords = {Bayesian inference, expectation-maximisation algorithm, Bayes
              methods, Bayesian methods, Random variables, variational techniques
              , Speech enhancement, speech recognition, Signal to noise ratio,
              frequency-domain analysis, spectral analysis, Gaussian noise,
              Frequency estimation, speech enhancement, Distributed computing,
              signal-to-noise ratio, variational approximation, Computational
              modeling, speech signal, random processes, signal reconstruction,
              Laplace equations, covariance, covariance analysis, Error analysis,
              expectation-maximization algorithm, frequency coefficient,
              frequency domain, Frequency domain analysis, Gaussian scale mixture
              model, Gaussian scale mixture model (GSMM), Laplace method,
              log-spectral domain, noisy signal, posterior signal distribution,
              probabilistic relationship, random variable, speech model,
              speech-shaped noise, word recognition error rate, zero-mean
              Gaussian},
  pages    = {1127--1136},
  file     = {Hao et al. - 2010 - Speech Enhancement Using Gaussian Scale Mixture
              Mo.pdf:/Users/apodusenko/Zotero/storage/RWFM3TYS/Hao et al. - 2010 -
              Speech Enhancement Using Gaussian Scale Mixture
              Mo.pdf:application/pdf;Hao et al. - 2010 - Speech Enhancement Using
              Gaussian Scale Mixture
              Mo.pdf:/Users/apodusenko/Zotero/storage/WVJQEFR9/Hao et al. - 2010 -
              Speech Enhancement Using Gaussian Scale Mixture
              Mo.pdf:application/pdf;IEEE Xplore Abstract
              Record:/Users/apodusenko/Zotero/storage/6JR58HYW/5200538.html:text/html
              }
}

@article{defossez_real_2020,
  title    = {Real {Time} {Speech} {Enhancement} in the {Waveform} {Domain}},
  url      = {http://arxiv.org/abs/2006.12847},
  abstract = {We present a causal speech enhancement model working on the raw
              waveform that runs in real-time on a laptop CPU. The proposed model
              is based on an encoder-decoder architecture with skip-connections.
              It is optimized on both time and frequency domains, using multiple
              loss functions. Empirical evidence shows that it is capable of
              removing various kinds of background noise including stationary and
              non-stationary noises, as well as room reverb. Additionally, we
              suggest a set of data augmentation techniques applied directly on
              the raw waveform which further improve model performance and its
              generalization abilities. We perform evaluations on several
              standard benchmarks, both using objective metrics and human
              judgements. The proposed model matches state-of-the-art performance
              of both causal and non causal methods while working directly on the
              raw waveform.},
  urldate  = {2020-09-07},
  journal  = {arXiv:2006.12847 [cs, eess, stat]},
  author   = {Defossez, Alexandre and Synnaeve, Gabriel and Adi, Yossi},
  month    = sep,
  year     = {2020},
  note     = {arXiv: 2006.12847},
  keywords = {Computer Science - Sound, Statistics - Machine Learning,
              Electrical Engineering and Systems Science - Audio and Speech
              Processing, Computer Science - Machine Learning},
  file     = {arXiv Fulltext PDF:/Users/apodusenko/Zotero/storage/Z6FQHEMY/Defossez
              et al. - 2020 - Real Time Speech Enhancement in the Waveform
              Domai.pdf:application/pdf;arXiv Fulltext
              PDF:/Users/apodusenko/Zotero/storage/FX8PW2T4/Defossez et al. - 2020 -
              Real Time Speech Enhancement in the Waveform
              Domai.pdf:application/pdf;arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/WMRPSCAZ/2006.html:text/html;arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/F828R46U/2006.html:text/html;arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/EW9JVGG2/2006.html:text/html;Defossez
              et al. - 2020 - Real Time Speech Enhancement in the Waveform
              Domai.pdf:/Users/apodusenko/Zotero/storage/2X5PJCJF/Defossez et al. -
              2020 - Real Time Speech Enhancement in the Waveform
              Domai.pdf:application/pdf}
}

@article{hensman_variational_2016,
  title    = {Variational {Fourier} features for {Gaussian} processes},
  url      = {http://arxiv.org/abs/1611.06740},
  abstract = {This work brings together two powerful concepts in Gaussian
              processes: the variational approach to sparse approximation and the
              spectral representation of Gaussian processes. This gives rise to
              an approximation that inherits the benefits of the variational
              approach but with the representational power and computational
              scalability of spectral representations. The work hinges on a key
              result that there exist spectral features related to a finite
              domain of the Gaussian process which exhibit almost-independent
              covariances. We derive these expressions for Matern kernels in one
              dimension, and generalize to more dimensions using kernels with
              specific structures. Under the assumption of additive Gaussian
              noise, our method requires only a single pass through the dataset,
              making for very fast and accurate computation. We fit a model to 4
              million training points in just a few minutes on a standard laptop.
              With non-conjugate likelihoods, our MCMC scheme reduces the cost of
              computation from O(NM2) (for a sparse Gaussian process) to O(NM)
              per iteration, where N is the number of data and M is the number of
              features.},
  urldate  = {2017-05-01},
  journal  = {arXiv:1611.06740 [stat]},
  author   = {Hensman, James and Durrande, Nicolas and Solin, Arno},
  month    = nov,
  year     = {2016},
  note     = {arXiv: 1611.06740},
  keywords = {Statistics - Machine Learning},
  file     = {arXiv\:1611.06740
              PDF:/Users/apodusenko/Zotero/storage/M2UPME4R/Hensman et al. - 2016 -
              Variational Fourier features for Gaussian
              processe.pdf:application/pdf;arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/GZK4XG5J/1611.html:text/html;Hensman
              et al. - Variational Fourier Features for Gaussian
              Processe.pdf:/Users/apodusenko/Zotero/storage/FHDU5WKH/Hensman et al. -
              Variational Fourier Features for Gaussian Processe.pdf:application/pdf}
}

@article{blei_variational_2017,
  title      = {Variational {Inference}: {A} {Review} for {Statisticians}},
  volume     = {112},
  issn       = {0162-1459, 1537-274X},
  shorttitle = {Variational {Inference}},
  url        = {https://www.tandfonline.com/doi/full/10.1080/01621459.2017.1285773},
  doi        = {10.1080/01621459.2017.1285773},
  language   = {en},
  number     = {518},
  urldate    = {2018-06-22},
  journal    = {Journal of the American Statistical Association},
  author     = {Blei, David M. and Kucukelbir, Alp and McAuliffe, Jon D.},
  month      = apr,
  year       = {2017},
  keywords   = {Algorithms, Computationally intensive methods, Statistical
                computing},
  pages      = {859--877},
  file       = {Blei et al. - 2017 - Variational Inference A Review for
                Statisticians.pdf:/Users/apodusenko/Zotero/storage/9P6W2JF6/Blei et al.
                - 2017 - Variational Inference A Review for
                Statisticians.pdf:application/pdf;Full Text
                PDF:/Users/apodusenko/Zotero/storage/Z9UY3NBS/Blei et al. - 2017 -
                Variational Inference A Review for Statisticians.pdf:application/pdf}
}

@article{oord_wavenet:_2016,
  title      = {{WaveNet}: {A} {Generative} {Model} for {Raw} {Audio}},
  shorttitle = {{WaveNet}},
  url        = {http://arxiv.org/abs/1609.03499},
  abstract   = {This paper introduces WaveNet, a deep neural network for
                generating raw audio waveforms. The model is fully probabilistic
                and autoregressive, with the predictive distribution for each audio
                sample conditioned on all previous ones; nonetheless we show that
                it can be efficiently trained on data with tens of thousands of
                samples per second of audio. When applied to text-to-speech, it
                yields state-of-the-art performance, with human listeners rating it
                as significantly more natural sounding than the best parametric and
                concatenative systems for both English and Mandarin. A single
                WaveNet can capture the characteristics of many different speakers
                with equal fidelity, and can switch between them by conditioning on
                the speaker identity. When trained to model music, we find that it
                generates novel and often highly realistic musical fragments. We
                also show that it can be employed as a discriminative model,
                returning promising results for phoneme recognition.},
  urldate    = {2016-10-28},
  journal    = {arXiv:1609.03499 [cs]},
  author     = {Oord, Aaron van den and Dieleman, Sander and Zen, Heiga and Simonyan
                , Karen and Vinyals, Oriol and Graves, Alex and Kalchbrenner, Nal and
                Senior, Andrew and Kavukcuoglu, Koray},
  month      = sep,
  year       = {2016},
  note       = {arXiv: 1609.03499},
  keywords   = {Computer Science - Learning, Computer Science - Sound, Computer
                Science - Machine Learning},
  file       = {arXiv.org
                Snapshot:/Users/apodusenko/Zotero/storage/NUT3RJ2X/1609.html:text/html;Oord
                et al. - 2016 - WaveNet A Generative Model for Raw
                Audio.pdf:/Users/apodusenko/Zotero/storage/FKAYKXCV/Oord et al. - 2016
                - WaveNet A Generative Model for Raw Audio.pdf:application/pdf;Oord et
                al. - 2016 - WaveNet A Generative Model for Raw
                Audio.pdf:/Users/apodusenko/Zotero/storage/V9WKJ6BP/Oord et al. - 2016
                - WaveNet A Generative Model for Raw Audio.pdf:application/pdf}
}

@article{millidge_whence_2020,
  title    = {Whence the {Expected} {Free} {Energy}?},
  journal  = {arXiv preprint arXiv:2004.08128},
  author   = {Millidge, Beren and Tschantz, Alexander and Buckley, Christopher L.},
  year     = {2020},
  keywords = {Computer Science - Artificial Intelligence},
  file     = {arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/2KIGV5CX/2004.html:text/html;Millidge
              et al. - 2020 - Whence the Expected Free
              Energy.pdf:/Users/apodusenko/Zotero/storage/LWSB9AH7/Millidge et al. -
              2020 - Whence the Expected Free Energy.pdf:application/pdf;Millidge et
              al. - 2020 - Whence the Expected Free
              Energy.pdf:/Users/apodusenko/Zotero/storage/7DH8IX9T/Millidge et al. -
              2020 - Whence the Expected Free
              Energy.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/4X5XBUIX/2004.html:text/html
              }
}

@article{wheeler_dirac_nodate,
  title    = {{DIRAC} {DELTA} {FUNCTION} {IDENTITIES}},
  language = {en},
  author   = {Wheeler, Nicholas},
  pages    = {18},
  file     = {Wheeler - DIRAC DELTA FUNCTION
              IDENTITIES.pdf:/Users/apodusenko/Zotero/storage/T23TK6B5/Wheeler -
              DIRAC DELTA FUNCTION IDENTITIES.pdf:application/pdf;Wheeler - DIRAC
              DELTA FUNCTION
              IDENTITIES.pdf:/Users/apodusenko/Zotero/storage/FPN6N9ZA/Wheeler -
              DIRAC DELTA FUNCTION IDENTITIES.pdf:application/pdf}
}

@inproceedings{you_autoregressive_2007,
  title     = {Autoregressive {Parameter} {Estimation} for {Kalman} {Filtering} {
               Speech} {Enhancement}},
  volume    = {4},
  doi       = {10.1109/ICASSP.2007.367219},
  abstract  = {In this paper, autoregressive parameter estimation for Kalman
               filtering speech enhancement is studied. In conventional Kalman
               filtering speech enhancement, spectral subtraction is usually used
               for speech autoregressive (AR) parameter estimation. We propose log
               spectral amplitude (LSA) minimum mean-square error (MMSE) instead
               of spectral subtraction for the estimation of speech AR parameters.
               Based on an observation that full-band Kalman filtering speech
               enhancement often causes an unbalanced noise reduction between
               speech and non-speech segments, a spectral solution is proposed to
               overcome the unbalanced reduction of noise. This is done by shaping
               the spectral envelopes of the noise through likelihood ratio. Our
               simulation results show the effectiveness of the proposed method.},
  booktitle = {2007 {IEEE} {International} {Conference} on {Acoustics}, {Speech}
               and {Signal} {Processing} - {ICASSP} '07},
  author    = {You, Chang Huai and Rahardja, Susanto and Koh, Soo Ngee},
  month     = apr,
  year      = {2007},
  note      = {ISSN: 2379-190X},
  keywords  = {Kalman filters, Parameter estimation, autoregressive processes,
               Hidden Markov models, Speech enhancement, least mean squares
               methods, Signal to noise ratio, Speech processing, Speech
               Enhancement, speech enhancement, Signal processing, minimum
               mean-square error, Filtering, spectral subtraction, Noise reduction
               , parameter estimation, Noise shaping, Autoregressive Model,
               autoregressive parameter estimation, Kalman Filtering, Kalman
               filtering enhancement, log spectral amplitude, spectral envelopes,
               spectral solution, unbalanced noise reduction},
  pages     = {IV--913--IV--916},
  file      = {IEEE Xplore Abstract
               Record:/Users/apodusenko/Zotero/storage/KF6WW5S6/4218250.html:text/html;IEEE
               Xplore Full Text PDF:/Users/apodusenko/Zotero/storage/6AILD3S7/You et
               al. - 2007 - Autoregressive Parameter Estimation for Kalman
               Fil.pdf:application/pdf;IEEE Xplore Full Text
               PDF:/Users/apodusenko/Zotero/storage/6CGN56NA/You et al. - 2007 -
               Autoregressive Parameter Estimation for Kalman Fil.pdf:application/pdf}
}

@inproceedings{yuanjin_zheng_time-varying_2000,
  title     = {Time-varying autoregressive system identification using wavelets},
  volume    = {1},
  doi       = {10.1109/ICASSP.2000.862046},
  abstract  = {In this paper, the problem of time-varying parametric
               autoregressive (AR) model identification by wavelets is discussed.
               Firstly, we derive multiresolution least squares (MLS) algorithm
               Gaussian time-varying AR model identification employing wavelet
               operator matrix representation. This method can optimally balance
               between the over-fitted solution and the poorly represented
               estimation. Utilizing multiresolution analysis techniques, the
               smooth trends and the rapidly changing components of time-varying
               AR model parameters can both be estimated accurately. Then, the
               proposed MLS algorithm is combined with the total least squares
               algorithm for noisy time-varying AR model identification.
               Simulation results verify the effectiveness of our algorithms.},
  booktitle = {2000 {IEEE} {International} {Conference} on {Acoustics}, {Speech}
               , and {Signal} {Processing}. {Proceedings} ({Cat}. {No}.{00CH37100
               })},
  author    = {{Yuanjin Zheng} and {Zhiping Lin}},
  month     = jun,
  year      = {2000},
  note      = {ISSN: 1520-6149},
  keywords  = {Parameter estimation, autoregressive processes, Testing, Gaussian
               processes, Time varying systems, signal representation, Least
               squares methods, Signal resolution, matrix algebra, parameter
               estimation, Discrete wavelet transforms, Gaussian time-varying AR
               model identification, least squares approximations, Low pass
               filters, Matrix decomposition, Multilevel systems, multiresolution
               analysis, multiresolution least squares algorithm, noisy
               time-varying AR model identification, over-fitted solution, poorly
               represented estimation, rapidly changing components, signal
               resolution, smooth trends, System identification, time-varying AR
               model parameters, time-varying autoregressive system identification
               , time-varying filters, total least squares algorithm, wavelet
               operator matrix representation, wavelet transforms, wavelets},
  pages     = {572--575 vol.1},
  file      = {IEEE Xplore Abstract
               Record:/Users/apodusenko/Zotero/storage/RS5ETUJN/862046.html:text/html;IEEE
               Xplore Full Text PDF:/Users/apodusenko/Zotero/storage/L2VSXX3X/Yuanjin
               Zheng and Zhiping Lin - 2000 - Time-varying autoregressive system
               identification .pdf:application/pdf;IEEE Xplore Full Text
               PDF:/Users/apodusenko/Zotero/storage/FWF92HJK/Yuanjin Zheng and Zhiping
               Lin - 2000 - Time-varying autoregressive system identification
               .pdf:application/pdf}
}

@article{smith_active_2020,
  title      = {An {Active} {Inference} {Approach} to {Modeling} {Structure} {
                Learning}: {Concept} {Learning} as an {Example} {Case}},
  volume     = {14},
  issn       = {1662-5188},
  shorttitle = {An {Active} {Inference} {Approach} to {Modeling} {Structure} {
                Learning}},
  url        = {https://www.frontiersin.org/articles/10.3389/fncom.2020.00041/full},
  doi        = {10.3389/fncom.2020.00041},
  abstract   = {Within computational neuroscience, the algorithmic and neural
                basis of structure learning remains poorly understood. Concept
                learning is one primary example, which requires both a type of
                internal model expansion process (adding novel hidden states that
                explain new observations), and a model reduction process (merging
                different states into one underlying cause and thus reducing model
                complexity via meta-learning). Although various algorithmic models
                of concept learning have been proposed within machine learning and
                cognitive science, many are limited to various degrees by an
                inability to generalize, the need for very large amounts of
                training data, and/or insufficiently established biological
                plausibility. Using concept learning as an example case, we
                introduce a novel approach for modeling structure learning – and
                specifically state-space expansion and reduction – within the
                active inference framework and its accompanying neural process
                theory. Our aim is to demonstrate its potential to facilitate a
                novel line of active inference research in this area. The approach
                we lay out is based on the idea that a generative model can be
                equipped with extra (hidden state or cause) ‘slots’ that can be
                engaged when an agent learns about novel concepts. This can be
                combined with a Bayesian model reduction process, in which any
                concept learning – associated with these slots – can be reset in
                favor of a simpler model with higher model evidence. We use
                simulations to illustrate this model’s ability to add new concepts
                to its state space (with relatively few observations) and increase
                the granularity of the concepts it currently possesses. We also
                simulate the predicted neural basis of these processes. We further
                show that it can accomplish a simple form of ‘one-shot’
                generalization to new stimuli. Although deliberately simple, these
                simulation results highlight ways in which active inference could
                offer useful resources in developing neurocomputational models of
                structure learning. They provide a template for how future active
                inference research could apply this approach to real-world
                structure learning problems and assess the added utility it may
                offer.},
  language   = {English},
  urldate    = {2021-02-18},
  journal    = {Frontiers in Computational Neuroscience},
  author     = {Smith, Ryan and Schwartenbeck, Philipp and Parr, Thomas and Friston,
                Karl J.},
  year       = {2020},
  note       = {Publisher: Frontiers},
  keywords   = {Bayesian model reduction, active inference, Bayesian Model
                Expansion, computational neuroscience, concept learning, concepts,
                structure learning},
  file       = {Full Text PDF:/Users/apodusenko/Zotero/storage/TNZT2YD7/Smith et al. -
                2020 - An Active Inference Approach to Modeling
                Structure.pdf:application/pdf}
}

@inproceedings{huan_wang_eeg_2015,
  title     = {{EEG} recognition through {Time}-varying {Vector} {Autoregressive} {
               Model}},
  doi       = {10.1109/FSKD.2015.7381956},
  abstract  = {In this paper, the multi-task motor imagery
               EEG(electroencephalogram) signals are pretreated by principal
               component analysis and Fourier transform. By use of the methods of
               time series analysis and mathematic statistics, pretreated EEG
               signal series are separated into the deterministic part and
               stochastic part. Then, the stochastic part is analyzed by use of
               TVVAR (Time Varying Vector Auto-regressive) model to obtain the
               residuals. Therefore, the EEG signals are studied on the stochastic
               parts and the residuals of TVVAR model. EEG signals of 3 types of
               actions, 60 signals per action type, are sampled, from which a
               signal is in turn analyzed to be recognized. Experiments in this
               study indicate that the recognition rates of left, right, hold
               still are 93.33\%, 98.33\%, 96.67\% respectively, and the average
               recognition rate is 96.11\% through both the stochastic parts and
               the residuals of TVVAR model. It verifies the TVVAR model be useful
               to analyze autocovariance nonstationary vector process.},
  booktitle = {2015 12th {International} {Conference} on {Fuzzy} {Systems} and {
               Knowledge} {Discovery} ({FSKD})},
  author    = {{Huan Wang} and Bai, L. and {Jianmei Xu} and Fei, W.},
  month     = aug,
  year      = {2015},
  keywords  = {Stochastic processes, Fourier transforms, medical signal
               processing, autoregressive processes, Mathematical model, Brain
               modeling, electroencephalography, time series, Time series analysis
               , Analytical models, time-varying systems, vectors, autocovariance
               nonstationary vector process, EEG recognition, EEG signal
               recognition, Electrodes, electroencephalogram signals,
               Electroencephalography, Fourier transform, Mahalanobis distance,
               mathematic statistics, multitask motor imagery, nonstationary
               process, principal component analysis, time series analysis,
               time-varying vector autoregressive model, TVVAR, TVVAR model},
  pages     = {292--296},
  file      = {IEEE Xplore Abstract
               Record:/Users/apodusenko/Zotero/storage/WBZ7MZL7/7381956.html:text/html;IEEE
               Xplore Full Text PDF:/Users/apodusenko/Zotero/storage/K4JNJBDQ/Huan
               Wang et al. - 2015 - EEG recognition through Time-varying Vector
               Autore.pdf:application/pdf}
}

@article{zhang_local_2011,
  title    = {Local {Polynomial} {Modeling} of {Time}-{Varying} {Autoregressive} {
              Models} {With} {Application} to {Time}–{Frequency} {Analysis} of {
              Event}-{Related} {EEG}},
  volume   = {58},
  issn     = {1558-2531},
  doi      = {10.1109/TBME.2010.2089686},
  abstract = {This paper proposes a new local polynomial modeling (LPM) method
              for identification of time-varying autoregressive (TVAR) models and
              applies it to time-frequency analysis (TFA) of event-related
              electroencephalogram (ER-EEG). The LPM method models the TVAR
              coefficients locally by polynomials and estimates the polynomial
              coefficients using weighted least-squares with a window having a
              certain bandwidth. A data-driven variable bandwidth selection
              method is developed to determine the optimal bandwidth that
              minimizes the mean squared error. The resultant time-varying power
              spectral density estimation of the signal is capable of achieving
              both high time resolution and high frequency resolution in the
              time-frequency domain, making it a powerful TFA technique for
              nonstationary biomedical signals like ER-EEG. Experimental results
              on synthesized signals and real EEG data show that the LPM method
              can achieve a more accurate and complete time-frequency
              representation of the signal.},
  number   = {3},
  journal  = {IEEE Transactions on Biomedical Engineering},
  author   = {Zhang, Z. G. and Hung, Y. S. and Chan, S. C.},
  month    = mar,
  year     = {2011},
  note     = {Conference Name: IEEE Transactions on Biomedical Engineering},
  keywords = {Humans, Regression Analysis, medical signal processing,
              Electroencephalogram, Signal Processing, Computer-Assisted,
              electroencephalography, Models, Theoretical, Bandwidth, Analytical
              models, TVAR model, regression analysis, time-varying
              autoregressive (TVAR) model, Electroencephalography, Brain models,
              ER-EEG signal, event related EEG, event related
              electroencephalogram, event-related potential, Evoked Potentials,
              local polynomial modeling, local polynomial modeling (LPM),
              polynomials, Polynomials, Time frequency analysis, time varying
              autoregressive model, time-frequency analysis, time–frequency
              analysis (TFA), trime frequency analysis, weighted least squares},
  pages    = {557--566},
  file     = {IEEE Xplore Full Text
              PDF:/Users/apodusenko/Zotero/storage/AHN9RA4R/Zhang et al. - 2011 -
              Local Polynomial Modeling of Time-Varying Autoregr.pdf:application/pdf}
}

@article{kostoglou_novel_2019,
  title    = {A {Novel} {Framework} for {Estimating} {Time}-{Varying} {Multivariate
              } {Autoregressive} {Models} and {Application} to {Cardiovascular} {
              Responses} to {Acute} {Exercise}},
  volume   = {66},
  issn     = {1558-2531},
  doi      = {10.1109/TBME.2019.2903012},
  abstract = {Objective: We present a novel modeling framework for identifying
              time-varying (TV) couplings between time-series of biomedical
              relevance. Methods: The proposed methodology is based on
              multivariate autoregressive (MVAR) models, which have been
              extensively used to study couplings between biosignals. Contrary to
              the standard estimation methods that assume time-invariant
              relationships, we propose a modified recursive Kalman filter (KF)
              to track changes in the model parameters. We perform model order
              selection and hyperparameter optimization simultaneously using
              Genetic Algorithms, greatly improving accuracy and computation
              time. In addition, we address the effect of residual
              heteroscedasticity, possibly associated with event-related changes
              or phase transitions during a given experimental protocol, on the
              TV-MVAR coupling measures by using Generalized Autoregressive
              Conditional Heteroskedasticity (GARCH) models to fit the TV-MVAR
              residuals. Results: Using simulated data, we show that the proposed
              framework yields more accurate parameter estimates compared to the
              conventional KF, particularly when the true system parameters
              exhibit different rate of variations over time. Furthermore, by
              accounting for heteroskedasticity, we obtain more accurate
              estimates of the strength and directionality of the underlying
              couplings. We also use our approach to investigate TV hemodynamic
              interactions during exercise in young and old healthy adults, as
              well as individuals with chronic stroke. We extract TV coupling
              patterns that reflect well known exercise-induced effects on the
              underlying regulatory mechanisms with excellent time resolution.
              Conclusion and Significance: The proposed modeling framework can be
              used to efficiently quantify TV couplings between biosignals. It is
              fully automated and does not require prior knowledge of the system
              TV characteristics.},
  number   = {11},
  journal  = {IEEE Transactions on Biomedical Engineering},
  author   = {Kostoglou, K. and Robertson, A. D. and MacIntosh, B. J. and Mitsis,
              G. D.},
  month    = nov,
  year     = {2019},
  note     = {Conference Name: IEEE Transactions on Biomedical Engineering},
  keywords = {Humans, Kalman filters, Algorithms, Kalman filter, medical signal
              processing, Computer Simulation, autoregressive processes, Models,
              Statistical, Brain modeling, time series, Adult, Exercise,
              Biological system modeling, Covariance matrices, parameter
              estimation, Computational modeling, model parameters, model order
              selection, biomechanics, biomedical relevance, biosignals,
              cardiovascular responses, cardiovascular system, computation time,
              Couplings, estimation methods, exercise, exercise-induced effects,
              generalized autoregressive conditional heteroskedasticity models,
              genetic algorithms, Genetic algorithms, haemodynamics, Hemodynamics
              , heteroskedasticity, hyperparameter optimization, KF, modified
              recursive Kalman filter, Multivariate Analysis, MVAR, partial
              directed coherence, residual heteroscedasticity, Stroke, system
              parameters, time resolution, time varying multivariate
              autoregressive models, time-varying, time-varying couplings, TV, TV
              coupling patterns, TV hemodynamic interactions, TV-MVAR coupling
              measures, TV-MVAR residuals},
  pages    = {3257--3266},
  file     = {IEEE Xplore Abstract
              Record:/Users/apodusenko/Zotero/storage/LHEMM4T4/8660477.html:text/html;IEEE
              Xplore Full Text
              PDF:/Users/apodusenko/Zotero/storage/PMWAUKFR/Kostoglou et al. - 2019 -
              A Novel Framework for Estimating Time-Varying Mult.pdf:application/pdf}
}

@inproceedings{paulik_time_1994,
  title     = {A time varying vector autoregressive model for signature verification
               },
  volume    = {2},
  doi       = {10.1109/MWSCAS.1994.519068},
  abstract  = {This paper examines a new approach for verification of on-line
               handwritten signatures. A signature is treated as a vector random
               process whose components are the x and y Cartesian coordinates and
               the instantaneous velocity of the recording stylus. This
               multivariate process is then represented by a time varying p/sup
               th/ order vector autoregressive (VAR) model which approximates the
               changes in the complex contours typical in signature analysis. The
               vector structure attempts to model the correlation between the
               signature sequence variables to allow the extraction of superior
               distinguishing features. The model's matrix coefficients are used
               to generate feature vectors which permit the verification of a
               writer's identity. An experimental study is presented which
               compares performance with a 1-D counterpart.},
  booktitle = {Proceedings of 1994 37th {Midwest} {Symposium} on {Circuits} and
               {Systems}},
  author    = {Paulik, M. J. and Mohankrishnan, N. and Nikiforuk, M.},
  month     = aug,
  year      = {1994},
  keywords  = {autoregressive processes, Switches, Databases, Random processes,
               Reactive power, time-varying systems, Cartesian coordinates, Credit
               cards, feature vectors, Fingerprint recognition, handwriting
               recognition, Handwriting recognition, instantaneous velocity,
               Marketing and sales, matrix coefficients, multivariate process,
               online handwritten signatures, pattern recognition, Personal
               digital assistants, recording stylus, Registers, signature sequence
               variables, signature verification, time varying vector AR model,
               vector autoregressive model, vector random process},
  pages     = {1395--1398 vol.2},
  file      = {IEEE Xplore Full Text
               PDF:/Users/apodusenko/Zotero/storage/FYW2PC4T/Paulik et al. - 1994 - A
               time varying vector autoregressive model for sig.pdf:application/pdf}
}

@inproceedings{chu_new_2012,
  title     = {A new regularized {TVAR}-based algorithm for recursive detection of
               nonstationarity and its application to speech signals},
  doi       = {10.1109/SSP.2012.6319704},
  abstract  = {This paper develops a new recursive nonstationarity detection
               method based on time-varying autoregressive (TVAR) modeling. A
               local likelihood estimation approach is introduced which gives more
               weights to observations near the current time instant but less to
               those distance apart. It thus allows the Wald test to be computed
               based on RLS-type algorithms with low computational cost. A
               reliable and efficient state regularized variable forgetting factor
               (VFF) QR decomposition (QRD)-based RLS (SR-VFF-QRRLS) algorithm is
               adopted for estimation for its asymptotically unbiased property and
               immunity to lacking of excitation. Advantages of the proposed
               approach over conventional approaches are 1) it provides continuous
               parameter estimates and the corresponding stationary intervals with
               low complexity, 2) it mitigates low excitation problems using state
               regularization, and 3) stationarity at different scales can be
               detected by appropriately choosing a certain window size. The
               effectiveness of the proposed algorithm is evaluated by testing
               vocal tract changes in real speech signals.},
  booktitle = {2012 {IEEE} {Statistical} {Signal} {Processing} {Workshop} ({SSP}
               )},
  author    = {Chu, Y. J. and Chan, S. C. and Zhang, Z. G. and Tsui, K. M.},
  month     = aug,
  year      = {2012},
  note      = {ISSN: 2373-0803},
  keywords  = {Signal processing algorithms, Speech, Vectors, Algorithm design
               and analysis, speech signals, Maximum likelihood estimation,
               maximum likelihood estimation, speech processing, parameter
               estimation, Computational modeling, regression analysis, recursive
               estimation, computational cost, continuous parameter estimation,
               efficient state regularized variable forgetting factor QRD-based
               RLS, efficient state regularized VFF QRD-based RLS, local
               likelihood, local likelihood estimation approach, Nonstationarity
               detection, recursive nonstationarity detection method, regularized
               TVAR-based algorithm, reliable state regularized variable
               forgetting factor QRD-based RLS, reliable state regularized VFF
               QRD-based RLS, RLS, SR-VFF-QRRLS algorithm, state regularization,
               stationary intervals, time-varying autoregressive modeling, TVAR,
               Wald test},
  pages     = {361--364},
  file      = {IEEE Xplore Full Text
               PDF:/Users/apodusenko/Zotero/storage/8YYGYRQS/Chu et al. - 2012 - A new
               regularized TVAR-based algorithm for recursi.pdf:application/pdf}
}

@inproceedings{tahir_time-varying_2001,
  title     = {Time-varying autoregressive modeling approach for speech segmentation
               },
  volume    = {2},
  doi       = {10.1109/ISSPA.2001.950248},
  abstract  = {Speech is considered as a nonstationary signal since the
               parameters such as amplitude, frequency and phase vary with time.
               Traditional speech segmentation is done based on a fixed frame
               length. However, speech characteristics can change within the fixed
               length or can be similar to the adjacent frames. Thus, it would be
               of interest to vary the length of the segment to accommodate the
               changes in the speech characteristics. The developed segmentation
               algorithm is based on a time-varying autoregressive model and the
               segmentation rules are developed based on the instantaneous energy
               and frequency estimate.},
  booktitle = {Proceedings of the {Sixth} {International} {Symposium} on {Signal
               } {Processing} and its {Applications} ({Cat}.{No}.{01EX467})},
  author    = {Tahir, S. M. and Shaameri, A. Z. and Salleh, S. H. S.},
  month     = aug,
  year      = {2001},
  keywords  = {autoregressive processes, Speech analysis, Signal analysis, Speech
               processing, Frequency estimation, speech processing, Signal
               processing, Parametric statistics, time-varying autoregressive
               model, time-varying systems, Speech recognition, nonstationary
               signals, Computational complexity, Energy storage, frequency
               estimation, instantaneous energy estimation, speech segmentation,
               Vocabulary},
  pages     = {715--718 vol.2},
  file      = {IEEE Xplore Abstract
               Record:/Users/apodusenko/Zotero/storage/VNRFNG79/950248.html:text/html;IEEE
               Xplore Full Text PDF:/Users/apodusenko/Zotero/storage/AF7FUK5Y/Tahir et
               al. - 2001 - Time-varying autoregressive modeling approach for
               .pdf:application/pdf}
}

@inproceedings{zhong_xionghu_time-varying_2005,
  title     = {Time-varying {Parameters} {Estimation} based on {Kalman} {Particle} {
               Filter} with {Forgetting} {Factors}},
  volume    = {2},
  doi       = {10.1109/EURCON.2005.1630264},
  abstract  = {Parameter estimation of time-varying nonGaussian auto regressive
               processes is a highly nonlinear problem, which is more difficult if
               the functional form of the time variation of the parameters is
               unknown. In this paper, an efficient particle filter is presented.
               By integrating Kalman particle filter and the concept of forgetting
               factors in RLS filter theory, the parameter evolution through time
               is affected by old and current observations both, and the choice of
               the unknown parameter distribution is then broadened. Computer
               simulations proof that the estimation result of this method is more
               accurate than present approaches in nonlinear and nonGaussian
               environments, and also the method can suppress the degeneracy of
               the particles effectively},
  booktitle = {{EUROCON} 2005 - {The} {International} {Conference} on "{Computer
               } as a {Tool}"},
  author    = {{Zhong Xionghu} and {Song Shubiao} and {Pei Chengming}},
  month     = nov,
  year      = {2005},
  keywords  = {Kalman filters, Parameter estimation, Particle filters, Signal
               processing algorithms, Kalman filter, Recursive estimation,
               forgetting factor, particle filtering (numerical methods), Signal
               processing, recursive filters, time-varying autoregressive model,
               recursive estimation, Particle filter, Least squares approximation,
               Adaptive algorithm, Engines, Forgetting factor, Kalman particle
               filter, nonGaussian autoregressive process, nonlinear problem,
               parameter evolution, parameter time variation, Resonance light
               scattering, RLS filter theory, Time-varying Autoregressive model,
               time-varying parameter estimation, unknown parameter distribution},
  pages     = {1558--1561},
  file      = {IEEE Xplore Full Text
               PDF:/Users/apodusenko/Zotero/storage/NQRGLDFN/Zhong Xionghu et al. -
               2005 - Time-varying Parameters Estimation based on
               Kalman.pdf:application/pdf}
}

@misc{noauthor_david_nodate,
  title   = {David {Barber} : {Brml} - {Home} {Page} browse},
  url     = {
             http://web4.cs.ucl.ac.uk/staff/D.Barber/pmwiki/pmwiki.php?n=Brml.HomePage
             },
  urldate = {2021-02-18},
  file    = {David Barber \: Brml - Home Page
             browse:/Users/apodusenko/Zotero/storage/JFI77TIF/pmwiki.html:text/html}
}

@inproceedings{reddy_non_2014,
  title     = {Non stationary signal prediction using {TVAR} model},
  doi       = {10.1109/ICCSP.2014.6950136},
  abstract  = {In this paper Time-varying Auto Regressive (TVAR) model based
               approach for non stationary signal prediction in noisy environment
               is presented. Covariance method is applied for least square
               estimation of time-varying autoregressive parameters. In TVAR
               modeling approach, the time-varying parameters are expressed as a
               linear combination of constants multiplied by basis functions. In
               this paper, the TVAR parameters are expanded by a low-order
               discrete cosine basis. The order determination of TVAR model is
               addressed by means of the maximum likelihood estimation (MLE)
               algorithm. The experimental results are presented for prediction of
               Discrete AM, Discrete FM, Discrete AM-FM signals.},
  booktitle = {2014 {International} {Conference} on {Communication} and {Signal}
               {Processing}},
  author    = {Reddy, G. Ravi Shankar and Rao, R.},
  month     = apr,
  year      = {2014},
  keywords  = {Abstracts, autoregressive processes, Noise measurement, maximum
               likelihood estimation, noisy environment, signal processing,
               Predictive models, TVAR model, Chebyshev approximation, Electronic
               mail, covariance analysis, least squares approximations, TV, Basis
               function, covariance method, Discrete Amplitude and Frequency
               Modulation, Discrete Amplitude Modulation, Discrete Frequency
               Modulation, least square estimation, linear combination, maximum
               likelihood estimation algorithm, non stationary signal prediction,
               time varying auto regressive model, Time-Varying Autoregressive
               model},
  pages     = {1692--1697},
  file      = {IEEE Xplore Full Text
               PDF:/Users/apodusenko/Zotero/storage/9DDBV6D3/Reddy and Rao - 2014 -
               Non stationary signal prediction using TVAR model.pdf:application/pdf}
}

@article{nossier_experimental_2021,
  title     = {An {Experimental} {Analysis} of {Deep} {Learning} {Architectures} for
               {Supervised} {Speech} {Enhancement}},
  volume    = {10},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  url       = {https://www.mdpi.com/2079-9292/10/1/17},
  doi       = {10.3390/electronics10010017},
  abstract  = {Recent speech enhancement research has shown that deep learning
               techniques are very effective in removing background noise. Many
               deep neural networks are being proposed, showing promising results
               for improving overall speech perception. The Deep Multilayer
               Perceptron, Convolutional Neural Networks, and the Denoising
               Autoencoder are well-established architectures for speech
               enhancement; however, choosing between different deep learning
               models has been mainly empirical. Consequently, a comparative
               analysis is needed between these three architecture types in order
               to show the factors affecting their performance. In this paper,
               this analysis is presented by comparing seven deep learning models
               that belong to these three categories. The comparison includes
               evaluating the performance in terms of the overall quality of the
               output speech using five objective evaluation metrics and a
               subjective evaluation with 23 listeners; the ability to deal with
               challenging noise conditions; generalization ability; complexity;
               and, processing time. Further analysis is then provided while using
               two different approaches. The first approach investigates how the
               performance is affected by changing network hyperparameters and the
               structure of the data, including the Lombard effect. While the
               second approach interprets the results by visualizing the
               spectrogram of the output layer of all the investigated models, and
               the spectrograms of the hidden layers of the convolutional neural
               network architecture. Finally, a general evaluation is performed
               for supervised deep learning-based speech enhancement while using
               SWOC analysis, to discuss the technique\&rsquo;s Strengths,
               Weaknesses, Opportunities, and Challenges. The results of this
               paper contribute to the understanding of how different deep neural
               networks perform the speech enhancement task, highlight the
               strengths and weaknesses of each architecture, and provide
               recommendations for achieving better performance. This work
               facilitates the development of better deep neural networks for
               speech enhancement in the future.},
  language  = {en},
  number    = {1},
  urldate   = {2021-02-18},
  journal   = {Electronics},
  author    = {Nossier, Soha A. and Wall, Julie and Moniri, Mansour and Glackin,
               Cornelius and Cannings, Nigel},
  month     = jan,
  year      = {2021},
  note      = {Number: 1 Publisher: Multidisciplinary Digital Publishing Institute},
  keywords  = {noise reduction, speech enhancement, speech processing, deep
               learning, deep neural networks},
  pages     = {17},
  file      = {Full Text PDF:/Users/apodusenko/Zotero/storage/JVSAMAA9/Nossier et al.
               - 2021 - An Experimental Analysis of Deep Learning
               Architec.pdf:application/pdf}
}

@article{cui_exact_2016,
  title    = {Exact {Distribution} for the {Product} of {Two} {Correlated} {
              Gaussian} {Random} {Variables}},
  volume   = {23},
  issn     = {1558-2361},
  doi      = {10.1109/LSP.2016.2614539},
  abstract = {This letter considers the distribution of product for two
              correlated real Gaussian random variables with nonzero means and
              arbitrary variances, which arises widely in radar and communication
              societies. We determine the exact probability density function
              (PDF) in terms of an infinite sum of modified Bessel functions of
              second kind, which includes some existent results, i.e., zero-means
              and/or independent variables, as special cases. Then, we study the
              approximation error and convergence rate when finite summations are
              exploited in practice. Finally, we evaluate the PDF behaviors of
              the derived expression as well as the Monte Carlo simulations.},
  number   = {11},
  journal  = {IEEE Signal Processing Letters},
  author   = {Cui, G. and Yu, X. and Iommelli, S. and Kong, L.},
  month    = nov,
  year     = {2016},
  note     = {Conference Name: IEEE Signal Processing Letters},
  keywords = {Convergence, Random variables, signal processing, Monte Carlo
              methods, Radar, Approximation error, correlated Gaussian random
              variables, Distribution, exact probability density function (PDF),
              finite summations, modified Bessel functions, Monte Carlo
              simulations, nonzero means and arbitrary variances, PDF behaviors,
              probability, probability density function, Probability density
              function, product distribution, Radar signal processing, two
              correlated real Gaussian random variables},
  pages    = {1662--1666},
  file     = {IEEE Xplore Abstract
              Record:/Users/apodusenko/Zotero/storage/I4J6QI6A/7579552.html:text/html;IEEE
              Xplore Full Text PDF:/Users/apodusenko/Zotero/storage/WFGFFM3B/Cui et
              al. - 2016 - Exact Distribution for the Product of Two
              Correlat.pdf:application/pdf}
}

@misc{noauthor_full_nodate,
  title   = {Full article: {Variational} {Inference}: {A} {Review} for {
             Statisticians}},
  url     = {https://www.tandfonline.com/doi/full/10.1080/01621459.2017.1285773},
  urldate = {2021-02-18},
  file    = {Full article\: Variational Inference\: A Review for
             Statisticians:/Users/apodusenko/Zotero/storage/KWFN4Y8G/01621459.2017.html:text/html
             }
}

@article{nakajima_bayesian_2011,
  title    = {Bayesian analysis of time-varying parameter vector autoregressive
              model for the {Japanese} economy and monetary policy},
  volume   = {25},
  issn     = {0889-1583},
  url      = {https://www.sciencedirect.com/science/article/pii/S0889158311000384},
  doi      = {10.1016/j.jjie.2011.07.004},
  abstract = {This paper analyzes the time-varying parameter vector
              autoregressive (TVP–VAR) model for the Japanese economy and
              monetary policy. The parameters are allowed to follow a random walk
              process and estimated using the Markov chain Monte Carlo method.
              The empirical result reveals the time-varying structure of the
              Japanese economy and monetary policy during the period from 1981 to
              2008. The marginal likelihoods of the TVP–VAR model and other fixed
              parameter VAR models are estimated for model comparison. The
              estimated marginal likelihoods indicate that the TVP–VAR model best
              fits the Japanese economic data.},
  language = {en},
  number   = {3},
  urldate  = {2021-02-18},
  journal  = {Journal of the Japanese and International Economies},
  author   = {Nakajima, Jouchi and Kasuya, Munehisa and Watanabe, Toshiaki},
  month    = sep,
  year     = {2011},
  keywords = {Bayesian inference, Markov chain Monte Carlo, Monetary policy,
              State space model, Stochastic volatility, Time-varying parameter
              vector autoregressive model},
  pages    = {225--245},
  file     = {ScienceDirect Full Text
              PDF:/Users/apodusenko/Zotero/storage/JVYWNV8K/Nakajima et al. - 2011 -
              Bayesian analysis of time-varying parameter vector.pdf:application/pdf}
}

@inproceedings{prado_bayesian_2000,
  title      = {Bayesian time-varying autoregressions: {Theory}, methods and {
                Applications}},
  shorttitle = {Bayesian time-varying autoregressions},
  abstract   = {We review the class of time-varying autoregressive (TVAR) models
                and a range of related recent developments of Bayesian time series
                modelling. Beginning with TVAR models in a Bayesian dynamic linear
                modelling framework, we review aspects of latent structure analysis
                , including time-domain decomposition methods that provide
                inferences on the structure underlying non-stationary time series,
                and that are now central tools in the time series analyst's
                toolkit. Recent model extensions that deal with model order
                uncertainty, and are enabled using efficient Markov Chain Monte
                Carlo simulation methods, are discussed, as are novel approaches to
                sequential filtering and smoothing using particulate filtering
                methods. We emphasize the relevance of TVAR modelling in a range of
                applied contexts, including biomedical signal processing and
                communications, and highlight some of the central developments via
                examples arising in studies of multiple electroencephalographic
                (EEG) traces in neurophysiology. We conclude with comments about
                current research frontiers.},
  booktitle  = {University of {Sao} {Paolo}},
  author     = {Prado, Raquel and Huerta, Gabriel and West, Mike},
  year       = {2000},
  pages      = {2000},
  file       = {Citeseer - Full Text
                PDF:/Users/apodusenko/Zotero/storage/DGFG4GYP/Prado et al. - 2000 -
                Bayesian time-varying autoregressions Theory, met.pdf:application/pdf}
}

@article{rajan_bayesian_1997,
  title    = {Bayesian approach to parameter estimation and interpolation of
              time-varying autoregressive processes using the {Gibbs} sampler},
  volume   = {144},
  issn     = {1359-7108},
  url      = {
              https://digital-library.theiet.org/content/journals/10.1049/ip-vis_19971305
              },
  doi      = {10.1049/ip-vis:19971305},
  abstract = {A nonstationary time series is one in which the statistics of the
              process are a function of time; this time dependency makes it
              impossible to utilise standard analytically defined statistical
              estimators to parameterise the process. To overcome this difficulty
              , the time series is considered within a finite time interval and
              is modelled as a time-varying autoregressive (AR) process. The AR
              coefficients that characterise this process are functions of time,
              represented by a family of basis vectors. The corresponding basis
              coefficients are invariant over the time window and have stationary
              statistical properties. A method is described for applying a Markov
              Chain Monte Carlo method known as the Gibbs sampler to the problem
              of estimating the parameters of such a time-varying autoregressive
              (TVAR) model, whose time dependent coefficients are modelled by
              basis functions. The Gibbs sampling scheme is then extended to
              include a stage which may be used for interpolation. Results on
              synthetic and real audio signals show that the model is flexible,
              and that a Gibbs sampling framework is a reasonable scheme for
              estimating and characterising a time-varying AR process.},
  language = {en},
  number   = {4},
  urldate  = {2021-02-18},
  journal  = {IEE Proceedings - Vision, Image and Signal Processing},
  author   = {Rajan, J. J. and Rayner, P. J. W. and Godsill, S. J.},
  month    = aug,
  year     = {1997},
  note     = {Publisher: IET Digital Library},
  pages    = {249--256},
  file     = {
              Snapshot:/Users/apodusenko/Zotero/storage/VMAKYDLS/ip-vis_19971305.html:text/html;Submitted
              Version:/Users/apodusenko/Zotero/storage/YUKBAU2X/Rajan et al. - 1997 -
              Bayesian approach to parameter estimation and inte.pdf:application/pdf}
}

@misc{noauthor_ieee_nodate,
  title   = {{IEEE} {Xplore}},
  url     = {https://ieeexplore.ieee.org/Xplore/home.jsp},
  urldate = {2021-02-18},
  file    = {IEEE
             Xplore:/Users/apodusenko/Zotero/storage/88R53QIS/home.html:text/html}
}

@article{eom_analysis_1999,
  title    = {Analysis of {Acoustic} {Signatures} from {Moving} {Vehicles} {Using}
              {Time}-{Varying} {Autoregressive} {Models}},
  volume   = {10},
  issn     = {1573-0824},
  url      = {https://doi.org/10.1023/A:1008475713345},
  doi      = {10.1023/A:1008475713345},
  abstract = {Time-varying autoregressive (TVAR) modeling approach for the
              analysis of acoustic signatures from moving vehicles is presented
              in this paper. Acoustic signatures from moving vehicles are
              nonstationary, and features extracted under the stationary
              assumption often result unsatisfactory performance. In TVAR
              modeling approach, the time-varying parameters are expanded as a
              linear combination of deterministic time functions. In this paper,
              the TVAR parameters are expanded by a low-order discrete cosine
              transform (DCT), since DCT is known to be close to the optimal
              Kahrunen-Loève transform when the signal is Markov. The maximum
              likelihood estimation and order selection in TVAR models are also
              discussed. Many attributes of vehicle activities, such as vehicle
              type, engine speed, loading, road condition, etc., may be inferred
              from the estimated model parameters. The performance of the TVAR
              modeling approach is tested with both synthetic and real acoustic
              signatures. A synthetic signal containing multiple time-varying
              sinusoids are used to compare the performances in the estimation of
              time-frequency distribution with other approaches. In the
              experiment with acoustic signatures from moving vehicles, it is
              shown that the TVAR models can be effectively used to determine
              vehicle activities and types at close range and cruising speed.},
  language = {en},
  number   = {4},
  urldate  = {2021-02-18},
  journal  = {Multidimensional Systems and Signal Processing},
  author   = {Eom, Kie B.},
  month    = oct,
  year     = {1999},
  pages    = {357--378},
  file     = {Springer Full Text PDF:/Users/apodusenko/Zotero/storage/RRFGI9A9/Eom -
              1999 - Analysis of Acoustic Signatures from Moving
              Vehicl.pdf:application/pdf}
}

@article{abramovich_time-varying_2007,
  title    = {Time-{Varying} {Autoregressive} ({TVAR}) {Models} for {Multiple} {
              Radar} {Observations}},
  volume   = {55},
  issn     = {1941-0476},
  doi      = {10.1109/TSP.2006.888064},
  abstract = {We consider the adaptive radar problem where the properties of the
              (nonstationary) clutter signals can be estimated using multiple
              observations of radar returns from a number of sufficiently
              homogeneous range/azimuth resolution cells. We derive a method for
              approximating an arbitrary Hermitian covariance matrix by a
              time-varying autoregressive model of order m, TVAR(m), that is
              based on the Dym-Gohberg band-matrix extension technique which
              gives the unique TVAR(m) model for any nondegenerate covariance
              matrix. We demonstrate that the Dym-Gohberg transformation of the
              sample covariance matrix gives the maximum-likelihood (ML) estimate
              of the TVAR(m) covariance matrix. We introduce an example of
              TVAR(m) clutter modeling for high-frequency over-the-horizon radar
              that demonstrates its practical importance},
  number   = {4},
  journal  = {IEEE Transactions on Signal Processing},
  author   = {Abramovich, Y. I. and Spencer, N. K. and Turley, M. D. E.},
  month    = apr,
  year     = {2007},
  note     = {Conference Name: IEEE Transactions on Signal Processing},
  keywords = {autoregressive processes, Covariance matrix, Maximum likelihood
              estimation, Signal resolution, Frequency modulation, maximum
              likelihood estimation, Radar antennas, autoregressive models,
              covariance matrices, time-varying autoregressive models,
              time-varying, Radar signal processing, Adaptive processing,
              adaptive radar, arbitrary Hermitian covariance matrix, Australia,
              Azimuth, clutter signals, Dym-Gohberg band-matrix, homogeneous
              range-azimuth resolution cells, maximum-likelihood estimate,
              multiple radar observations, nonstationary clutter, nonstationary
              interference, radar clutter, Radar clutter, radar observations,
              radar signal processing, Sea surface},
  pages    = {1298--1311},
  file     = {Abramovich et al. - 2007 - Time-Varying Autoregressive (TVAR) Models
              for Mult.pdf:/Users/apodusenko/Zotero/storage/7MKFGESK/Abramovich et
              al. - 2007 - Time-Varying Autoregressive (TVAR) Models for
              Mult.pdf:application/pdf}
}

@article{akaike_fitting_1969,
  title    = {Fitting autoregressive models for prediction},
  volume   = {21},
  issn     = {1572-9052},
  url      = {https://doi.org/10.1007/BF02532251},
  doi      = {10.1007/BF02532251},
  language = {en},
  number   = {1},
  urldate  = {2021-02-18},
  journal  = {Annals of the Institute of Statistical Mathematics},
  author   = {Akaike, Hirotugu},
  month    = dec,
  year     = {1969},
  pages    = {243--247},
  file     = {Akaike - 1969 - Fitting autoregressive models for
              prediction.pdf:/Users/apodusenko/Zotero/storage/482T4ZP7/Akaike - 1969
              - Fitting autoregressive models for prediction.pdf:application/pdf}
}

@incollection{levchuk_active_2019,
  title      = {Active inference in multiagent systems: context-driven collaboration
                and decentralized purpose-driven team adaptation},
  shorttitle = {Active inference in multiagent systems},
  booktitle  = {Artificial {Intelligence} for the {Internet} of {Everything}},
  publisher  = {Elsevier},
  author     = {Levchuk, Georgiy and Pattipati, Krishna and Serfaty, Daniel and
                Fouse, Adam and McCormack, Robert},
  year       = {2019},
  pages      = {67--85},
  file       = {
                Snapshot:/Users/apodusenko/Zotero/storage/GAABGMQV/B9780128176368000041.html:text/html
                }
}

@article{giffin_maximum_2009,
  title      = {Maximum {Entropy}: {The} {Universal} {Method} for {Inference}},
  shorttitle = {Maximum {Entropy}},
  url        = {http://arxiv.org/abs/0901.2987},
  abstract   = {In this thesis we start by providing some detail regarding how we
                arrived at our present understanding of probabilities and how we
                manipulate them - the product and addition rules by Cox. We also
                discuss the modern view of entropy and how it relates to known
                entropies such as the thermodynamic entropy and the information
                entropy. Next, we show that Skilling's method of induction leads us
                to a unique general theory of inductive inference, the ME method
                and precisely how it is that other entropies such as those of Renyi
                or Tsallis are ruled out for problems of inference. We then explore
                the compatibility of Bayes and ME updating. We show that ME is
                capable of producing every aspect of orthodox Bayesian inference
                and proves the complete compatibility of Bayesian and entropy
                methods. The realization that the ME method incorporates Bayes'
                rule as a special case allows us to go beyond Bayes' rule and to
                process both data and expected value constraints simultaneously. We
                discuss the general problem of non-commuting constraints, when they
                should be processed sequentially and when simultaneously. The
                generic "canonical" form of the posterior distribution for the
                problem of simultaneous updating with data and moments is obtained.
                This is a major achievement since it shows that ME is not only
                capable of processing information in the form of constraints, like
                MaxEnt and information in the form of data, as in Bayes' Theorem,
                but also can process both forms simultaneously, which Bayes and
                MaxEnt cannot do alone. Finally, we illustrate some potential
                applications for this new method by applying ME to potential
                problems of interest.},
  urldate    = {2019-12-23},
  journal    = {arXiv:0901.2987 [physics]},
  author     = {Giffin, Adom},
  month      = jan,
  year       = {2009},
  note       = {arXiv: 0901.2987},
  keywords   = {Physics - Data Analysis, Statistics and Probability},
  file       = {arXiv.org
                Snapshot:/Users/apodusenko/Zotero/storage/ABICC8VA/0901.html:text/html;Giffin
                - 2009 - Maximum Entropy The Universal Method for
                Inferenc.pdf:/Users/apodusenko/Zotero/storage/TCPRL7JK/Giffin - 2009 -
                Maximum Entropy The Universal Method for Inferenc.pdf:application/pdf}
}

@article{shore_axiomatic_1980,
  title    = {Axiomatic derivation of the principle of maximum entropy and the
              principle of minimum cross-entropy},
  volume   = {26},
  issn     = {1557-9654},
  doi      = {10.1109/TIT.1980.1056144},
  abstract = {Jaynes's principle of maximum entropy and Kullbacks principle of
              minimum cross-entropy (minimum directed divergence) are shown to be
              uniquely correct methods for inductive inference when new
              information is given in the form of expected values. Previous
              justifications use intuitive arguments and rely on the properties
              of entropy and cross-entropy as information measures. The approach
              here assumes that reasonable methods of inductive inference should
              lead to consistent results when there are different ways of taking
              the same information into account (for example, in different
              coordinate system). This requirement is formalized as four
              consistency axioms. These are stated in terms of an abstract
              information operator and make no reference to information measures.
              It is proved that the principle of maximum entropy is correct in
              the following sense: maximizing any function but entropy will lead
              to inconsistency unless that function and entropy have identical
              maxima. In other words given information in the form of constraints
              on expected values, there is only one (distribution satisfying the
              constraints that can be chosen by a procedure that satisfies the
              consistency axioms; this unique distribution can be obtained by
              maximizing entropy. This result is established both directly and as
              a special case (uniform priors) of an analogous result for the
              principle of minimum cross-entropy. Results are obtained both for
              continuous probability densities and for discrete distributions.},
  number   = {1},
  journal  = {IEEE Transactions on Information Theory},
  author   = {Shore, J. and Johnson, R.},
  month    = jan,
  year     = {1980},
  note     = {Conference Name: IEEE Transactions on Information Theory},
  keywords = {Entropy functions},
  pages    = {26--37},
  file     = {IEEE Xplore Abstract
              Record:/Users/apodusenko/Zotero/storage/V2EFTRLC/1056144.html:text/html;IEEE
              Xplore Abstract
              Record:/Users/apodusenko/Zotero/storage/T3AMSMP4/1056144.html:text/html;Submitted
              Version:/Users/apodusenko/Zotero/storage/FLAIZT8C/Shore and Johnson -
              1980 - Axiomatic derivation of the principle of maximum
              e.pdf:application/pdf}
}

@article{khan_fast_2018-1,
  title    = {Fast and {Scalable} {Bayesian} {Deep} {Learning} by {Weight}-{
              Perturbation} in {Adam}},
  url      = {http://arxiv.org/abs/1806.04854},
  abstract = {Uncertainty computation in deep learning is essential to design
              robust and reliable systems. Variational inference (VI) is a
              promising approach for such computation, but requires more effort
              to implement and execute compared to maximum-likelihood methods. In
              this paper, we propose new natural-gradient algorithms to reduce
              such efforts for Gaussian mean-field VI. Our algorithms can be
              implemented within the Adam optimizer by perturbing the network
              weights during gradient evaluations, and uncertainty estimates can
              be cheaply obtained by using the vector that adapts the learning
              rate. This requires lower memory, computation, and implementation
              effort than existing VI methods, while obtaining uncertainty
              estimates of comparable quality. Our empirical results confirm this
              and further suggest that the weight-perturbation in our algorithm
              could be useful for exploration in reinforcement learning and
              stochastic optimization.},
  urldate  = {2020-05-23},
  journal  = {arXiv:1806.04854 [cs, stat]},
  author   = {Khan, Mohammad Emtiyaz and Nielsen, Didrik and Tangkaratt, Voot and
              Lin, Wu and Gal, Yarin and Srivastava, Akash},
  month    = aug,
  year     = {2018},
  note     = {arXiv: 1806.04854},
  keywords = {Statistics - Machine Learning, Computer Science - Artificial
              Intelligence, Statistics - Computation, Computer Science - Machine
              Learning},
  file     = {arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/6LQML2A3/1806.html:text/html;Khan
              et al. - 2018 - Fast and Scalable Bayesian Deep Learning by
              Weight.pdf:/Users/apodusenko/Zotero/storage/JA2TXPYW/Khan et al. - 2018
              - Fast and Scalable Bayesian Deep Learning by
              Weight.pdf:application/pdf}
}

@inproceedings{loeliger_factor_2018,
  title     = {Factor {Graphs} with {NUV} {Priors} and {Iteratively} {Reweighted} {
               Descent} for {Sparse} {Least} {Squares} and {More}},
  doi       = {10.1109/ISTC.2018.8625332},
  abstract  = {Normal priors with unknown variance (NUV) are well known to
               include a large class of sparsity promoting priors and to blend
               well with Gaussian message passing. Essentially equivalently,
               sparsifying norms (including the L1 norm) as well as the Huber cost
               function from robust statistics have variational representations
               that lead to algorithms based on iteratively reweighted
               L2-regularization. In this paper, we rephrase these well-known
               facts in terms of factor graphs. In particular, we propose a
               smoothed-NUV representation of the Huber function and of a related
               nonconvex cost function, and we illustrate their use for sparse
               least-squares with outliers and in a natural (piecewise smooth)
               prior for imaging. We also point out pertinent iterative algorithms
               including variations of gradient descent and coordinate descent.},
  booktitle = {2018 {IEEE} 10th {International} {Symposium} on {Turbo} {Codes} {
               Iterative} {Information} {Processing} ({ISTC})},
  author    = {Loeliger, H. and Ma, B. and Malmberg, H. and Wadehn, F.},
  month     = dec,
  year      = {2018},
  keywords  = {Imaging, Message passing, Standards, Approximation algorithms,
               Information processing, Cost function, Turbo codes},
  pages     = {1--5},
  file      = {IEEE Xplore Abstract
               Record:/Users/apodusenko/Zotero/storage/3PZ5MRE2/8625332.html:text/html;Loeliger
               et al. - 2018 - Factor Graphs with NUV Priors and Iteratively
               Rewe.pdf:/Users/apodusenko/Zotero/storage/UENQNCN9/Loeliger et al. -
               2018 - Factor Graphs with NUV Priors and Iteratively
               Rewe.pdf:application/pdf}
}

@article{friston_generative_2020,
  title    = {Generative models, linguistic communication and active inference},
  issn     = {0149-7634},
  url      = {http://www.sciencedirect.com/science/article/pii/S0149763420304668},
  doi      = {10.1016/j.neubiorev.2020.07.005},
  abstract = {This paper presents a biologically plausible generative model and
              inference scheme that is capable of simulating communication
              between synthetic subjects who talk to each other. Building on
              active inference formulations of dyadic interactions, we simulate
              linguistic exchange to explore generative models that support
              dialogues. These models employ high-order interactions among
              abstract (discrete) states in deep (hierarchical) models. The
              sequential nature of language processing mandates generative models
              with a particular factorial structure—necessary to accommodate the
              rich combinatorics of language. We illustrate linguistic
              communication by simulating a synthetic subject who can play the
              ‘Twenty Questions’ game. In this game, synthetic subjects take the
              role of the questioner or answerer, using the same generative
              model. This simulation setup is used to illustrate some key
              architectural points and demonstrate that many behavioural and
              neurophysiological correlates of linguistic communication emerge
              under variational (marginal) message passing, given the right kind
              of generative model. For example, we show that theta-gamma coupling
              is an emergent property of belief updating, when listening to
              another.},
  language = {en},
  urldate  = {2020-07-22},
  journal  = {Neuroscience \& Biobehavioral Reviews},
  author   = {Friston, Karl J. and Parr, Thomas and Yufik, Yan and Sajid, Noor and
              Price, Catherine J. and Holmes, Emma},
  month    = jul,
  year     = {2020},
  keywords = {message passing, Bayesian, inference, free energy, Language,
              connectivity, hierarchical, neuronal},
  file     = {Friston et al. - 2020 - Generative models, linguistic communication
              and ac.pdf:/Users/apodusenko/Zotero/storage/UTN7CI9L/Friston et al. -
              2020 - Generative models, linguistic communication and
              ac.pdf:application/pdf;ScienceDirect
              Snapshot:/Users/apodusenko/Zotero/storage/6PYRJWZQ/S0149763420304668.html:text/html
              }
}

@article{parak_ecg_nodate,
  title    = {{ECG} {SIGNAL} {PROCESSING} {AND} {HEART} {RATE} {FREQUENCY} {
              DETECTION} {METHODS}},
  abstract = {Digital signal processing and data analysis are very often used
              methods in a biomedical engineering research. This paper describes
              utilization of digital signal filtering on electrocardiogram (ECG).
              Designed filters are focused on removing supply network 50 Hz
              frequency and breathing muscle artefacts. Moreover, this paper
              contains description of three heart rate frequency detection
              algorithms from ECG. Algorithms are based on statistical and
              differential mathematical methods. All of the methods are compared
              on stress test measurements. All described methods are suitable for
              next simple implementation to a microprocessor for real-time signal
              processing and analysing.},
  language = {en},
  author   = {Parak, J and Havlik, J},
  pages    = {6},
  file     = {Parak and Havlik - ECG SIGNAL PROCESSING AND HEART RATE FREQUENCY
              DET.pdf:/Users/apodusenko/Zotero/storage/IXVQY7UN/Parak and Havlik -
              ECG SIGNAL PROCESSING AND HEART RATE FREQUENCY DET.pdf:application/pdf}
}

@incollection{pearl_probabilistic_1994,
  title     = {A probabilistic calculus of actions},
  booktitle = {Uncertainty {Proceedings} 1994},
  publisher = {Elsevier},
  author    = {Pearl, Judea},
  year      = {1994},
  pages     = {454--462},
  file      = {Pearl - 1994 - A probabilistic calculus of
               actions.pdf:/Users/apodusenko/Zotero/storage/TNQ9L7ZP/Pearl - 1994 - A
               probabilistic calculus of
               actions.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/U3SNKBYK/B9781558603325500626.html:text/html
               }
}

@book{luenberger_linear_2008,
  address   = {New York, NY},
  edition   = {3rd ed},
  series    = {International series in operations research and management science},
  title     = {Linear and nonlinear programming},
  isbn      = {978-0-387-74502-2},
  language  = {en},
  publisher = {Springer},
  author    = {Luenberger, David G. and Ye, Yinyu},
  year      = {2008},
  keywords  = {Linear programming, Nonlinear programming},
  file      = {Luenberger and Ye - 2008 - Linear and nonlinear
               programming.pdf:/Users/apodusenko/Zotero/storage/WJ88YPIQ/Luenberger
               and Ye - 2008 - Linear and nonlinear programming.pdf:application/pdf}
}

@article{meshi_convexifying_2012,
  title    = {Convexifying the {Bethe} {Free} {Energy}},
  url      = {http://arxiv.org/abs/1205.2624},
  abstract = {The introduction of loopy belief propagation (LBP) revitalized the
              application of graphical models in many domains. Many recent works
              present improvements on the basic LBP algorithm in an attempt to
              overcome convergence and local optima problems. Notable among these
              are convexified free energy approximations that lead to inference
              procedures with provable convergence and quality properties.
              However, empirically LBP still outperforms most of its convex
              variants in a variety of settings, as we also demonstrate here.
              Motivated by this fact we seek convexified free energies that
              directly approximate the Bethe free energy. We show that the
              proposed approximations compare favorably with state-of-the art
              convex free energy approximations.},
  urldate  = {2021-02-08},
  journal  = {arXiv:1205.2624 [cs]},
  author   = {Meshi, Ofer and Jaimovich, Ariel and Globerson, Amir and Friedman,
              Nir},
  month    = may,
  year     = {2012},
  note     = {arXiv: 1205.2624},
  keywords = {Computer Science - Artificial Intelligence, Computer Science -
              Machine Learning},
  file     = {arXiv Fulltext PDF:/Users/apodusenko/Zotero/storage/H4M7FVSM/Meshi et
              al. - 2012 - Convexifying the Bethe Free
              Energy.pdf:application/pdf;arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/4ZYECAVB/1205.html:text/html}
}

@article{briers_smoothing_2009,
  title    = {Smoothing algorithms for state–space models},
  volume   = {62},
  issn     = {1572-9052},
  url      = {https://doi.org/10.1007/s10463-009-0236-2},
  doi      = {10.1007/s10463-009-0236-2},
  abstract = {Two-filter smoothing is a principled approach for performing
              optimal smoothing in non-linear non-Gaussian state–space models
              where the smoothing distributions are computed through the
              combination of ‘forward’ and ‘backward’ time filters. The ‘forward’
              filter is the standard Bayesian filter but the ‘backward’ filter,
              generally referred to as the backward information filter, is not a
              probability measure on the space of the hidden Markov process. In
              cases where the backward information filter can be computed in
              closed form, this technical point is not important. However, for
              general state–space models where there is no closed form expression
              , this prohibits the use of flexible numerical techniques such as
              Sequential Monte Carlo (SMC) to approximate the two-filter
              smoothing formula. We propose here a generalised two-filter
              smoothing formula which only requires approximating probability
              distributions and applies to any state–space model, removing the
              need to make restrictive assumptions used in previous approaches to
              this problem. SMC algorithms are developed to implement this
              generalised recursion and we illustrate their performance on
              various problems.},
  number   = {1},
  journal  = {Annals of the Institute of Statistical Mathematics},
  author   = {Briers, Mark and Doucet, Arnaud and Maskell, Simon},
  month    = jun,
  year     = {2009},
  pages    = {61},
  file     = {Briers et al. - 2009 - Smoothing algorithms for state–space
              models.pdf:/Users/apodusenko/Zotero/storage/YYILSWYM/Briers et al. -
              2009 - Smoothing algorithms for state–space models.pdf:application/pdf}
}

@article{stroud_bayesian_2016,
  title    = {A {Bayesian} adaptive ensemble {Kalman} filter for sequential state
              and parameter estimation},
  url      = {http://arxiv.org/abs/1611.03835},
  abstract = {This paper proposes new methodology for sequential state and
              parameter estimation within the ensemble Kalman filter. The method
              is fully Bayesian and propagates the joint posterior density of
              states and parameters over time. In order to implement the method
              we consider two representations of the marginal posterior
              distribution of the parameters: a grid-based approach and a
              Gaussian approximation. Contrary to existing algorithms, the new
              method explicitly accounts for parameter uncertainty and provides a
              formal way to combine information about the parameters from data at
              different time periods. The method is illustrated and compared to
              existing approaches using simulated and real data.},
  urldate  = {2021-01-22},
  journal  = {arXiv:1611.03835 [stat]},
  author   = {Stroud, Jonathan R. and Katzfuss, Matthias and Wikle, Christopher K.
              },
  month    = nov,
  year     = {2016},
  note     = {arXiv: 1611.03835},
  keywords = {Statistics - Computation, Statistics - Methodology},
  file     = {arXiv Fulltext PDF:/Users/apodusenko/Zotero/storage/6EGYK97V/Stroud et
              al. - 2016 - A Bayesian adaptive ensemble Kalman filter for
              seq.pdf:application/pdf;arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/A2CZP5PQ/1611.html:text/html}
}

@article{wainwright_new_2012,
  title    = {A {New} {Class} of {Upper} {Bounds} on the {Log} {Partition} {
              Function}},
  url      = {http://arxiv.org/abs/1301.0610},
  abstract = {Bounds on the log partition function are important in a variety of
              contexts, including approximate inference, model fitting, decision
              theory, and large deviations analysis. We introduce a new class of
              upper bounds on the log partition function, based on convex
              combinations of distributions in the exponential domain, that is
              applicable to an arbitrary undirected graphical model. In the
              special case of convex combinations of tree-structured
              distributions, we obtain a family of variational problems, similar
              to the Bethe free energy, but distinguished by the following
              desirable properties: i. they are cnvex, and have a unique global
              minimum; and ii. the global minimum gives an upper bound on the log
              partition function. The global minimum is defined by stationary
              conditions very similar to those defining fixed points of belief
              propagation or tree-based reparameterization Wainwright et al.,
              2001. As with BP fixed points, the elements of the minimizing
              argument can be used as approximations to the marginals of the
              original model. The analysis described here can be extended to
              structures of higher treewidth e.g., hypertrees, thereby making
              connections with more advanced approximations e.g., Kikuchi and
              variants Yedidia et al., 2001; Minka, 2001.},
  urldate  = {2021-01-19},
  journal  = {arXiv:1301.0610 [cs, stat]},
  author   = {Wainwright, Martin and Jaakkola, Tommi S. and Willsky, Alan},
  month    = dec,
  year     = {2012},
  note     = {arXiv: 1301.0610},
  keywords = {Statistics - Machine Learning, Computer Science - Machine Learning
              },
  file     = {arXiv Fulltext
              PDF:/Users/apodusenko/Zotero/storage/B74VKEA4/Wainwright et al. - 2012
              - A New Class of Upper Bounds on the Log Partition
              F.pdf:application/pdf;arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/XRTVXFMP/1301.html:text/html}
}

@article{ahn_gauging_2019,
  title    = {Gauging variational inference},
  abstract = {Computing of partition function is the most important statistical
              inference task arising in applications of graphical models (GM).
              Since it is computationally intractable, approximate methods have
              been used in practice, where mean-ﬁeld (MF) and belief propagation
              (BP) are arguably the most popular and successful approaches of a
              variational type. In this paper, we propose two new variational
              schemes, coined Gauged-MF (G-MF) and Gauged-BP (G-BP), improving MF
              and BP, respectively. Both provide lower bounds for the partition
              function by utilizing the so-called gauge transformation which
              modiﬁes factors of GM while keeping the partition function
              invariant. Moreover, we prove that both G-MF and G-BP are exact for
              GMs with a single loop of a special structure, even though the bare
              MF and BP perform badly in this case. Our extensive experiments
              indeed conﬁrm that the proposed algorithms outperform and
              generalize MF and BP.},
  language = {en},
  author   = {Ahn, Sungsoo and Chertkov, Michael and Shin, Jinwoo},
  year     = {2019},
  pages    = {14},
  file     = {Ahn et al. - 2019 - Gauging variational
              inference.pdf:/Users/apodusenko/Zotero/storage/A7TRB366/Ahn et al. -
              2019 - Gauging variational inference.pdf:application/pdf}
}

@article{presse_principles_2013,
  title    = {Principles of maximum entropy and maximum caliber in statistical
              physics},
  volume   = {85},
  url      = {https://link.aps.org/doi/10.1103/RevModPhys.85.1115},
  doi      = {10.1103/RevModPhys.85.1115},
  abstract = {The variational principles called maximum entropy (MaxEnt) and
              maximum caliber (MaxCal) are reviewed. MaxEnt originated in the
              statistical physics of Boltzmann and Gibbs, as a theoretical tool
              for predicting the equilibrium states of thermal systems. Later,
              entropy maximization was also applied to matters of information,
              signal transmission, and image reconstruction. Recently, since the
              work of Shore and Johnson, MaxEnt has been regarded as a principle
              that is broader than either physics or information alone. MaxEnt is
              a procedure that ensures that inferences drawn from stochastic data
              satisfy basic self-consistency requirements. The different
              historical justifications for the entropy S=−∑ipilog pi and its
              corresponding variational principles are reviewed. As an
              illustration of the broadening purview of maximum entropy
              principles, maximum caliber, which is path entropy maximization
              applied to the trajectories of dynamical systems, is also reviewed.
              Examples are given in which maximum caliber is used to interpret
              dynamical fluctuations in biology and on the nanoscale, in
              single-molecule and few-particle systems such as molecular motors,
              chemical reactions, biological feedback circuits, and diffusion in
              microfluidics devices.},
  number   = {3},
  urldate  = {2020-07-07},
  journal  = {Reviews of Modern Physics},
  author   = {Pressé, Steve and Ghosh, Kingshuk and Lee, Julian and Dill, Ken A.},
  month    = jul,
  year     = {2013},
  note     = {Publisher: American Physical Society},
  pages    = {1115--1141},
  file     = {APS
              Snapshot:/Users/apodusenko/Zotero/storage/WCSY5UN6/RevModPhys.85.html:text/html;Pressé
              et al. - 2013 - Principles of maximum entropy and maximum caliber
              .pdf:/Users/apodusenko/Zotero/storage/V6328L3F/Pressé et al. - 2013 -
              Principles of maximum entropy and maximum caliber .pdf:application/pdf}
}

@mastersthesis{walther_non-equilibrium_2014,
  title  = {Non-{Equilibrium} {Physics} from an {Inference} {Perspective}},
  school = {Stony Brook University},
  author = {Walther, Valentin},
  year   = {2014},
  file   = {Walther - 2014 - Non-Equilibrium Physics from an Inference
            Perspect.pdf:/Users/apodusenko/Zotero/storage/IILQ66UA/Walther - 2014 -
            Non-Equilibrium Physics from an Inference Perspect.pdf:application/pdf}
}

@misc{noauthor_deriving_2020,
  title    = {Deriving {PLA} from {MaxCal}},
  url      = {https://shanhelab.com/2020/03/04/deriving-pla-from-maxcal/},
  abstract = {The principle of least action (PLA), or more accurately, the
              principle of stationary action, is one first principle in physics.
              PLA offers the deepest explanatory power of our external reality.
              In …},
  language = {en},
  urldate  = {2021-01-09},
  journal  = {Shan He Lab},
  month    = mar,
  year     = {2020},
  file     = {
              Snapshot:/Users/apodusenko/Zotero/storage/U86X52ZW/deriving-pla-from-maxcal.html:text/html
              }
}

@article{ghosh_maximum_2020,
  title    = {The {Maximum} {Caliber} {Variational} {Principle} for {Nonequilibria}
              },
  volume   = {71},
  url      = {https://doi.org/10.1146/annurev-physchem-071119-040206},
  doi      = {10.1146/annurev-physchem-071119-040206},
  abstract = {AbstractEver since Clausius in 1865 and Boltzmann in 1877, the
              concepts of entropy and of its maximization have been the
              foundations for predicting how material equilibria derive from
              microscopic properties. But, despite much work, there has been no
              equally satisfactory general variational principle for
              nonequilibrium situations. However, in 1980, a new avenue was
              opened by E.T. Jaynes and by Shore and Johnson. We review here
              maximum caliber, which is a maximum-entropy-like principle that can
              infer distributions of flows over pathways, given dynamical
              constraints. This approach is providing new insights, particularly
              into few-particle complex systems, such as gene circuits, protein
              conformational reaction coordinates, network traffic, bird flocking
              , cell motility, and neuronal firing.},
  number   = {1},
  urldate  = {2020-07-07},
  journal  = {Annual Review of Physical Chemistry},
  author   = {Ghosh, Kingshuk and Dixit, Purushottam D. and Agozzino, Luca and
              Dill, Ken A.},
  year     = {2020},
  pmid     = {32075515},
  note     = {\_eprint: https://doi.org/10.1146/annurev-physchem-071119-040206},
  pages    = {213--238},
  file     = {Ghosh et al. - 2020 - The Maximum Caliber Variational Principle for
              None.pdf:/Users/apodusenko/Zotero/storage/FPAN2G52/Ghosh et al. - 2020
              - The Maximum Caliber Variational Principle for
              None.pdf:application/pdf}
}

@article{davis_probabilistic_2018,
  title     = {Probabilistic {Inference} for {Dynamical} {Systems}},
  volume    = {20},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  url       = {https://www.mdpi.com/1099-4300/20/9/696},
  doi       = {10.3390/e20090696},
  abstract  = {A general framework for inference in dynamical systems is
               described, based on the language of Bayesian probability theory and
               making use of the maximum entropy principle. Taking the concept of
               a path as fundamental, the continuity equation and Cauchy\&rsquo;s
               equation for fluid dynamics arise naturally, while the specific
               information about the system can be included using the maximum
               caliber (or maximum path entropy) principle.},
  language  = {en},
  number    = {9},
  urldate   = {2020-12-30},
  journal   = {Entropy},
  author    = {Davis, Sergio and González, Diego and Gutiérrez, Gonzalo},
  month     = sep,
  year      = {2018},
  note      = {Number: 9 Publisher: Multidisciplinary Digital Publishing Institute},
  keywords  = {bayesian inference, dynamical systems, fluid equations},
  pages     = {696},
  file      = {Davis et al. - 2018 - Probabilistic Inference for Dynamical
               Systems.pdf:/Users/apodusenko/Zotero/storage/4N2MF9YE/Davis et al. -
               2018 - Probabilistic Inference for Dynamical
               Systems.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/Y9UJIUX6/696.html:text/html
               }
}

@article{thiesson_arma_2012,
  title    = {{ARMA} {Time}-{Series} {Modeling} with {Graphical} {Models}},
  url      = {http://arxiv.org/abs/1207.4162},
  abstract = {We express the classic ARMA time-series model as a directed
              graphical model. In doing so, we find that the deterministic
              relationships in the model make it effectively impossible to use
              the EM algorithm for learning model parameters. To remedy this
              problem, we replace the deterministic relationships with Gaussian
              distributions having a small variance, yielding the stochastic ARMA
              (ARMA) model. This modification allows us to use the EM algorithm
              to learn parmeters and to forecast,even in situations where some
              data is missing. This modification, in conjunction with the
              graphicalmodel approach, also allows us to include cross predictors
              in situations where there are multiple times series and/or
              additional nontemporal covariates. More surprising,experiments
              suggest that the move to stochastic ARMA yields improved accuracy
              through better smoothing. We demonstrate improvements afforded by
              cross prediction and better smoothing on real data.},
  urldate  = {2021-01-13},
  journal  = {arXiv:1207.4162 [cs, stat]},
  author   = {Thiesson, Bo and Chickering, David Maxwell and Heckerman, David and
              Meek, Christopher},
  month    = aug,
  year     = {2012},
  note     = {arXiv: 1207.4162},
  keywords = {Statistics - Applications, Statistics - Methodology, Computer
              Science - Machine Learning},
  file     = {arXiv Fulltext PDF:/Users/apodusenko/Zotero/storage/GUXRWTL7/Thiesson
              et al. - 2012 - ARMA Time-Series Modeling with Graphical
              Models.pdf:application/pdf;arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/2EPMNUHE/1207.html:text/html}
}

@article{atitey_variational_2019,
  title    = {Variational {Bayesian} inference of hidden stochastic processes with
              unknown parameters},
  url      = {http://arxiv.org/abs/1911.00757},
  abstract = {Estimating hidden processes from non-linear noisy observations is
              particularly difficult when the parameters of these processes are
              not known. This paper adopts a machine learning approach to devise
              variational Bayesian inference for such scenarios. In particular, a
              random process generated by the autoregressive moving average
              (ARMA) linear model is inferred from non-linearity noise
              observations. The posterior distribution of hidden states are
              approximated by a set of weighted particles generated by the
              sequential Monte carlo (SMC) algorithm involving sampling with
              importance sampling resampling (SISR). Numerical efficiency and
              estimation accuracy of the proposed inference method are evaluated
              by computer simulations. Furthermore, the proposed inference method
              is demonstrated on a practical problem of estimating the missing
              values in the gene expression time series assuming vector
              autoregressive (VAR) data model.},
  urldate  = {2021-01-13},
  journal  = {arXiv:1911.00757 [cs, stat]},
  author   = {Atitey, Komlan and Loskot, Pavel and Mihaylova, Lyudmila},
  month    = nov,
  year     = {2019},
  note     = {arXiv: 1911.00757},
  keywords = {Statistics - Machine Learning, Statistics - Computation, Computer
              Science - Machine Learning},
  file     = {arXiv Fulltext PDF:/Users/apodusenko/Zotero/storage/88FH7LIG/Atitey et
              al. - 2019 - Variational Bayesian inference of hidden
              stochasti.pdf:application/pdf;arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/ZWPTGAAH/1911.html:text/html}
}

@article{ihler_loopy_nodate,
  title    = {Loopy {Belief} {Propagation}: {Convergence} and {Effects} of {Message
              } {Errors}},
  abstract = {Belief propagation (BP) is an increasingly popular method of
              performing approximate inference on arbitrary graphical models. At
              times, even further approximations are required, whether due to
              quantization of the messages or model parameters, from other
              simpliﬁed message or model representations, or from stochastic
              approximation methods. The introduction of such errors into the BP
              message computations has the potential to affect the solution
              obtained adversely. We analyze the effect resulting from message
              approximation under two particular measures of error, and show
              bounds on the accumulation of errors in the system. This analysis
              leads to convergence conditions for traditional BP message passing,
              and both strict bounds and estimates of the resulting error in
              systems of approximate BP message passing.},
  language = {en},
  author   = {Ihler, Alexander T and Iii, John W Fisher and Willsky, Alan S},
  pages    = {32},
  file     = {Ihler et al. - Loopy Belief Propagation Convergence and Effects
              .pdf:/Users/apodusenko/Zotero/storage/UQZKJELB/Ihler et al. - Loopy
              Belief Propagation Convergence and Effects .pdf:application/pdf}
}

@article{knowles_non-conjugate_nodate,
  title    = {Non-conjugate {Variational} {Message} {Passing} for {Multinomial} and
              {Binary} {Regression}},
  abstract = {Variational Message Passing (VMP) is an algorithmic implementation
              of the Variational Bayes (VB) method which applies only in the
              special case of conjugate exponential family models. We propose an
              extension to VMP, which we refer to as Non-conjugate Variational
              Message Passing (NCVMP) which aims to alleviate this restriction
              while maintaining modularity, allowing choice in how expectations
              are calculated, and integrating into an existing message-passing
              framework: Infer.NET. We demonstrate NCVMP on logistic binary and
              multinomial regression. In the multinomial case we introduce a
              novel variational bound for the softmax factor which is tighter
              than other commonly used bounds whilst maintaining computational
              tractability.},
  language = {en},
  author   = {Knowles, David A and Minka, Thomas P},
  pages    = {9},
  file     = {Knowles and Minka - Non-conjugate Variational Message Passing for
              Mult.pdf:/Users/apodusenko/Zotero/storage/QPI65J62/Knowles and Minka -
              Non-conjugate Variational Message Passing for Mult.pdf:application/pdf}
}

@book{schiessel_biophysics_2014,
  address    = {Singapore},
  title      = {Biophysics for {Beginners} - {A} {Journey} {Through} {The} {Cell} {
                Nucleus}},
  isbn       = {981-4303-94-1},
  shorttitle = {Biophysics for {Beginners}},
  language   = {English},
  publisher  = {Pan Stanford Publishing},
  author     = {Schiessel, Helmut},
  year       = {2014}
}

@inproceedings{walther_unified_1971,
  address   = {New York, NY, USA},
  series    = {{AFIPS} '71 ({Spring})},
  title     = {A unified algorithm for elementary functions},
  isbn      = {978-1-4503-7907-6},
  url       = {https://doi.org/10.1145/1478786.1478840},
  doi       = {10.1145/1478786.1478840},
  abstract  = {This paper describes a single unified algorithm for the
               calculation of elementary functions including multiplication,
               division, sin, cos, tan, arctan, sinh, cosh, tanh, arctanh, In, exp
               and square-root. The basis for the algorithm is coordinate rotation
               in a linear, circular, or hyperbolic coordinate system depending on
               which function is to be calculated. The only operations required
               are shifting, adding, subtracting and the recall of prestored
               constants. The limited domain of convergence of the algorithm is
               calculated, leading to a discussion of the modifications required
               to extend the domain for floating point calculations.},
  urldate   = {2020-12-09},
  booktitle = {Proceedings of the {May} 18-20, 1971, spring joint computer
               conference},
  publisher = {Association for Computing Machinery},
  author    = {Walther, J. S.},
  month     = may,
  year      = {1971},
  pages     = {379--385}
}

@inproceedings{meera_free_2020,
  title     = {Free {Energy} {Principle} {Based} {State} and {Input} {Observer} {
               Design} for {Linear} {Systems} with {Colored} {Noise}},
  doi       = {10.23919/ACC45564.2020.9147581},
  abstract  = {The free energy principle from neuroscience provides a
               biologically plausible solution to the brain's inference mechanism.
               This paper reformulates this theory to design a brain-inspired
               state and input estimator for a linear time-invariant state space
               system with colored noise. This reformulation for linear systems
               bridges the gap between the neuroscientific theory and control
               theory, therefore opening up the possibility of evaluating it under
               the hood of standard control approaches. Through rigorous
               simulations under colored noises, the observer is shown to
               outperform Kalman Filter and Unknown Input Observer with minimal
               error in state and input estimation. It is tested against a wide
               range of scenarios and the proof of concept is demonstrated by
               applying it on a real system.},
  booktitle = {2020 {American} {Control} {Conference} ({ACC})},
  author    = {Meera, Ajith Anil and Wisse, Martijn},
  month     = jul,
  year      = {2020},
  note      = {ISSN: 2378-5861},
  keywords  = {Kalman filters, inference mechanisms, Linear systems, Mathematical
               model, state-space methods, biologically plausible solution,
               brain-inspired state, Bridges, colored noise, Colored noise,
               control theory, free energy principle, input estimator, input
               observer design, linear systems, linear systems bridges, linear
               time-invariant state space system, neuroscientific theory,
               observers, Observers, standard control, unknown input observer},
  pages     = {5052--5058},
  file      = {IEEE Xplore Abstract
               Record:/Users/apodusenko/Zotero/storage/66UIQDMV/9147581.html:text/html;Meera
               and Wisse - 2020 - Free Energy Principle Based State and Input
               Observ.pdf:/Users/apodusenko/Zotero/storage/NZN5QZSY/Meera and Wisse -
               2020 - Free Energy Principle Based State and Input
               Observ.pdf:application/pdf}
}

@article{jordanov_exponential_2012,
  title    = {Exponential signal synthesis in digital pulse processing},
  volume   = {670},
  issn     = {0168-9002},
  url      = {http://www.sciencedirect.com/science/article/pii/S0168900211022510},
  doi      = {10.1016/j.nima.2011.12.042},
  abstract = {Digital pulse processing allows the synthesis of exponential
              signals that can be used in pulse shaping and baseline restoration.
              A recursive algorithm for the synthesis of high-pass filters is
              presented and discussed in view of its application as a baseline
              restorer. The high-pass filter can be arranged in a gated baseline
              restorer configuration similar to widely used analog
              implementations. Two techniques to synthesize time-invariant,
              finite impulse response (FIR) cusp shapers are presented. The first
              technique synthesizes a true cusp shape in the discrete-time
              domain. This algorithm may be sensitive to round-off errors and may
              require a large amount of computational resources. The second
              method for synthesis of cusp shapes is suitable for implementation
              using integer arithmetic, particularly in hardware. This algorithm
              uses linear interpolation to synthesize close approximations of
              true cusp shapes. The algorithm does not introduce round-off errors
              and has been tested in hardware.},
  language = {en},
  urldate  = {2020-12-08},
  journal  = {Nuclear Instruments and Methods in Physics Research Section A:
              Accelerators, Spectrometers, Detectors and Associated Equipment},
  author   = {Jordanov, Valentin T.},
  month    = apr,
  year     = {2012},
  keywords = {Digital signal processing, Signal synthesis, Cusp shape, Digital
              pulse processing, Exponential signal, Pulse shaping, Radiation},
  pages    = {18--24},
  file     = {ScienceDirect Full Text
              PDF:/Users/apodusenko/Zotero/storage/EZWDXVJ8/Jordanov - 2012 -
              Exponential signal synthesis in digital pulse
              proc.pdf:application/pdf;ScienceDirect
              Snapshot:/Users/apodusenko/Zotero/storage/XZK6378U/S0168900211022510.html:text/html
              }
}

@article{jordanov_digital_1994,
  title    = {Digital synthesis of pulse shapes in real time for high resolution
              radiation spectroscopy},
  volume   = {345},
  issn     = {0168-9002},
  url      = {http://www.sciencedirect.com/science/article/pii/0168900294910111},
  doi      = {10.1016/0168-9002(94)91011-1},
  abstract = {Techniques have been developed for the synthesis of pulse shapes
              using fast digital schemes in place of the traditional analog
              methods of pulse shaping. Efficient recursive algorithms have been
              developed that allow real time implementation of a shaper that can
              produce either trapezoidal or triangular pulse shapes. Other
              recursive techniques are presented which allow a synthesis of
              finite cusp-like shapes. Preliminary experimental tests show
              potential advantages of using these techniques in high resolution,
              high count rate pulse spectroscopy.},
  language = {en},
  number   = {2},
  urldate  = {2020-12-07},
  journal  = {Nuclear Instruments and Methods in Physics Research Section A:
              Accelerators, Spectrometers, Detectors and Associated Equipment},
  author   = {Jordanov, Valentin T. and Knoll, Glenn F.},
  month    = jun,
  year     = {1994},
  pages    = {337--345},
  file     = {ScienceDirect Full Text
              PDF:/Users/apodusenko/Zotero/storage/L9HUBQCV/Jordanov and Knoll - 1994
              - Digital synthesis of pulse shapes in real time
              for.pdf:application/pdf;ScienceDirect
              Snapshot:/Users/apodusenko/Zotero/storage/GB95Q9UP/0168900294910111.html:text/html
              }
}

@article{zhang_matrix-variate_nodate,
  title    = {Matrix-{Variate} {Dirichlet} {Process} {Mixture} {Models}},
  abstract = {We are concerned with a multivariate response regression problem
              where the interest is in considering correlations both across
              response variates and across response samples. In this paper we
              develop a new Bayesian nonparametric model for such a setting based
              on Dirichlet process priors. Building on an additive kernel model,
              we allow each sample to have its own regression matrix. Although
              this overcomplete representation could in principle suﬀer from
              severe overﬁtting problems, we are able to provide eﬀective control
              over the model via a matrix-variate Dirichlet process prior on the
              regression matrices. Our model is able to share statistical
              strength among regression matrices due to the clustering property
              of the Dirichlet process. We make use of a Markov chain Monte Carlo
              algorithm for inference and prediction. Compared with other
              Bayesian kernel models, our model has advantages in both
              computational and statistical eﬃciency.},
  language = {en},
  author   = {Zhang, Zhihua and Dai, Guang and Jordan, Michael I},
  pages    = {8},
  file     = {Zhang et al. - Matrix-Variate Dirichlet Process Mixture
              Models.pdf:/Users/apodusenko/Zotero/storage/QLCNJVWS/Zhang et al. -
              Matrix-Variate Dirichlet Process Mixture Models.pdf:application/pdf}
}

@article{spearman_physics-based_2017,
  title    = {Physics-{Based} {Modeling} of {Pass} {Probabilities} in {Soccer}},
  abstract = {In this paper, we present a model for ball control in soccer based
              on the concepts of how long it takes a player to reach the ball
              (time-to-intercept) and how long it takes a player to control the
              ball (time-to-control). We use this model to quantify the
              likelihood that a given pass will succeed we determine the free
              parameters of the model using tracking and event data from the
              2015-2016 Premier League season. On a reserved test set, the model
              correctly predicts the receiving team with an accuracy of 81\% and
              the specific receiving player with an accuracy of 68\%. Though
              based on simple mathematical concepts, various phenomena are
              emergent such as the effect of pressure on receiving a pass. Using
              the pass probability model, we derive a number of innovative new
              metrics around passing that can be used to quantify the value of
              passes and the skill of receivers and defenders. Computed per-team
              over a 38-game dataset, these metrics are found to correlate
              strongly with league standing at the end of the season. We believe
              that this model and derived metrics will be useful for both
              post-match analysis and player scouting. Lastly, we apply the
              approach used to compute passing probabilities to calculate a pitch
              control function that can be used to quantify and visualize regions
              of the pitch controlled by each team.},
  language = {en},
  author   = {Spearman, William and Basye, Austin and Dick, Greg and Hotovy, Ryan
              and Pop, Paul},
  year     = {2017},
  pages    = {15},
  file     = {Spearman et al. - 2017 - Physics-Based Modeling of Pass Probabilities
              in So.pdf:/Users/apodusenko/Zotero/storage/P23YQQV2/Spearman et al. -
              2017 - Physics-Based Modeling of Pass Probabilities in
              So.pdf:application/pdf}
}

@article{fernandez_wide_2018,
  title    = {Wide {Open} {Spaces}: {A} statistical technique for measuring space
              creation in professional soccer},
  language = {en},
  author   = {Fernandez, Javier and Barcelona, F C and Bornn, Luke},
  year     = {2018},
  pages    = {19},
  file     = {Fernandez et al. - 2018 - Wide Open Spaces A statistical technique for
              meas.pdf:/Users/apodusenko/Zotero/storage/JTXIFXVT/Fernandez et al. -
              2018 - Wide Open Spaces A statistical technique for
              meas.pdf:application/pdf}
}

@article{link_real_2016,
  title    = {Real {Time} {Quantification} of {Dangerousity} in {Football} {Using}
              {Spatiotemporal} {Tracking} {Data}},
  volume   = {11},
  issn     = {1932-6203},
  url      = {https://dx.plos.org/10.1371/journal.pone.0168768},
  doi      = {10.1371/journal.pone.0168768},
  abstract = {This study describes an approach to quantification of attacking
              performance in football. Our procedure determines a quantitative
              representation of the probability of a goal being scored for every
              point in time at which a player is in possession of the ball–we
              refer to this as dangerousity. The calculation is based on the
              spatial constellation of the player and the ball, and comprises
              four components: (1) Zone describes the danger of a goal being
              scored from the position of the player on the ball, (2) Control
              stands for the extent to which the player can implement his
              tactical intention on the basis of the ball dynamics, (3) Pressure
              represents the possibility that the defending team prevent the
              player from completing an action with the ball and (4) Density is
              the chance of being able to defend the ball after the action. Other
              metrics can be derived from dangerousity by means of which
              questions relating to analysis of the play can be answered. Action
              Value represents the extent to which the player can make a
              situation more dangerous through his possession of the ball.
              Performance quantifies the number and quality of the attacks by a
              team over a period of time, while Dominance describes the
              difference in performance between teams. The evaluation uses the
              correlation between probability of winning the match (derived from
              betting odds) and performance indicators, and indicates that among
              Goal difference (r = .55), difference in Shots on Goal (r = .58),
              difference in Passing Accuracy (r = .56), Tackling Rate (r = .24)
              Ball Possession (r = .71) and Dominance (r = .82), the latter makes
              the largest contribution to explaining the skill of teams. We use
              these metrics to analyse individual actions in a match, to describe
              passages of play, and to characterise the performance and
              efficiency of teams over the season. For future studies, they
              provide a criterion that does not depend on chance or results to
              investigate the influence of central events in a match, various
              playing systems or tactical group concepts on success.},
  language = {en},
  number   = {12},
  urldate  = {2020-11-20},
  journal  = {PLOS ONE},
  author   = {Link, Daniel and Lang, Steffen and Seidenschwarz, Philipp},
  editor   = {Amaral, Luís A. Nunes},
  month    = dec,
  year     = {2016},
  pages    = {e0168768},
  file     = {Link et al. - 2016 - Real Time Quantification of Dangerousity in
              Footba.pdf:/Users/apodusenko/Zotero/storage/6GH2RXH9/Link et al. - 2016
              - Real Time Quantification of Dangerousity in
              Footba.pdf:application/pdf}
}

@article{oord_parallel_2017,
  title      = {Parallel {WaveNet}: {Fast} {High}-{Fidelity} {Speech} {Synthesis}},
  shorttitle = {Parallel {WaveNet}},
  url        = {http://arxiv.org/abs/1711.10433},
  abstract   = {The recently-developed WaveNet architecture is the current state
                of the art in realistic speech synthesis, consistently rated as
                more natural sounding for many different languages than any
                previous system. However, because WaveNet relies on sequential
                generation of one audio sample at a time, it is poorly suited to
                today's massively parallel computers, and therefore hard to deploy
                in a real-time production setting. This paper introduces
                Probability Density Distillation, a new method for training a
                parallel feed-forward network from a trained WaveNet with no
                significant difference in quality. The resulting system is capable
                of generating high-fidelity speech samples at more than 20 times
                faster than real-time, and is deployed online by Google Assistant,
                including serving multiple English and Japanese voices.},
  urldate    = {2020-11-16},
  journal    = {arXiv:1711.10433 [cs]},
  author     = {Oord, Aaron van den and Li, Yazhe and Babuschkin, Igor and Simonyan,
                Karen and Vinyals, Oriol and Kavukcuoglu, Koray and Driessche, George
                van den and Lockhart, Edward and Cobo, Luis C. and Stimberg, Florian
                and Casagrande, Norman and Grewe, Dominik and Noury, Seb and Dieleman
                , Sander and Elsen, Erich and Kalchbrenner, Nal and Zen, Heiga and
                Graves, Alex and King, Helen and Walters, Tom and Belov, Dan and
                Hassabis, Demis},
  month      = nov,
  year       = {2017},
  note       = {arXiv: 1711.10433},
  keywords   = {Computer Science - Machine Learning},
  file       = {arXiv Fulltext PDF:/Users/apodusenko/Zotero/storage/5962ANUQ/Oord et
                al. - 2017 - Parallel WaveNet Fast High-Fidelity Speech
                Synthe.pdf:application/pdf;arXiv.org
                Snapshot:/Users/apodusenko/Zotero/storage/WL8FFAKF/1711.html:text/html}
}

@article{dieleman_challenge_2018,
  title      = {The challenge of realistic music generation: modelling raw audio at
                scale},
  shorttitle = {The challenge of realistic music generation},
  url        = {http://arxiv.org/abs/1806.10474},
  abstract   = {Realistic music generation is a challenging task. When building
                generative models of music that are learnt from data, typically
                high-level representations such as scores or MIDI are used that
                abstract away the idiosyncrasies of a particular performance. But
                these nuances are very important for our perception of musicality
                and realism, so in this work we embark on modelling music in the
                raw audio domain. It has been shown that autoregressive models
                excel at generating raw audio waveforms of speech, but when applied
                to music, we find them biased towards capturing local signal
                structure at the expense of modelling long-range correlations. This
                is problematic because music exhibits structure at many different
                timescales. In this work, we explore autoregressive discrete
                autoencoders (ADAs) as a means to enable autoregressive models to
                capture long-range correlations in waveforms. We find that they
                allow us to unconditionally generate piano music directly in the
                raw audio domain, which shows stylistic consistency across tens of
                seconds.},
  urldate    = {2020-11-16},
  journal    = {arXiv:1806.10474 [cs, eess, stat]},
  author     = {Dieleman, Sander and Oord, Aäron van den and Simonyan, Karen},
  month      = jun,
  year       = {2018},
  note       = {arXiv: 1806.10474},
  keywords   = {Computer Science - Sound, Statistics - Machine Learning,
                Electrical Engineering and Systems Science - Audio and Speech
                Processing, Computer Science - Machine Learning},
  file       = {arXiv.org
                Snapshot:/Users/apodusenko/Zotero/storage/UVSIDTY4/1806.html:text/html;Dieleman
                et al. - 2018 - The challenge of realistic music generation
                model.pdf:/Users/apodusenko/Zotero/storage/9DQNXHSY/Dieleman et al. -
                2018 - The challenge of realistic music generation
                model.pdf:application/pdf}
}

@article{purwins_deep_2019,
  title    = {Deep {Learning} for {Audio} {Signal} {Processing}},
  volume   = {13},
  issn     = {1932-4553, 1941-0484},
  url      = {http://arxiv.org/abs/1905.00078},
  doi      = {10.1109/JSTSP.2019.2908700},
  abstract = {Given the recent surge in developments of deep learning, this
              article provides a review of the state-of-the-art deep learning
              techniques for audio signal processing. Speech, music, and
              environmental sound processing are considered side-by-side, in
              order to point out similarities and differences between the domains
              , highlighting general methods, problems, key references, and
              potential for cross-fertilization between areas. The dominant
              feature representations (in particular, log-mel spectra and raw
              waveform) and deep learning models are reviewed, including
              convolutional neural networks, variants of the long short-term
              memory architecture, as well as more audio-specific neural network
              models. Subsequently, prominent deep learning application areas are
              covered, i.e. audio recognition (automatic speech recognition,
              music information retrieval, environmental sound detection,
              localization and tracking) and synthesis and transformation (source
              separation, audio enhancement, generative models for speech, sound,
              and music synthesis). Finally, key issues and future questions
              regarding deep learning applied to audio signal processing are
              identified.},
  number   = {2},
  urldate  = {2020-11-16},
  journal  = {IEEE Journal of Selected Topics in Signal Processing},
  author   = {Purwins, Hendrik and Li, Bo and Virtanen, Tuomas and Schlüter, Jan
              and Chang, Shuo-yiin and Sainath, Tara},
  month    = may,
  year     = {2019},
  note     = {arXiv: 1905.00078},
  keywords = {Computer Science - Sound, Statistics - Machine Learning,
              Electrical Engineering and Systems Science - Audio and Speech
              Processing, I.2.6, H.5.1},
  pages    = {206--219},
  file     = {arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/MD3ZN6AQ/1905.html:text/html;Purwins
              et al. - 2019 - Deep Learning for Audio Signal
              Processing.pdf:/Users/apodusenko/Zotero/storage/IG59HPMR/Purwins et al.
              - 2019 - Deep Learning for Audio Signal Processing.pdf:application/pdf}
}

@article{huzaifah_deep_2020,
  title    = {Deep generative models for musical audio synthesis},
  url      = {http://arxiv.org/abs/2006.06426},
  abstract = {Sound modelling is the process of developing algorithms that
              generate sound under parametric control. There are a few distinct
              approaches that have been developed historically including
              modelling the physics of sound production and propagation,
              assembling signal generating and processing elements to capture
              acoustic features, and manipulating collections of recorded audio
              samples. While each of these approaches has been able to achieve
              high-quality synthesis and interaction for specific applications,
              they are all labour-intensive and each comes with its own
              challenges for designing arbitrary control strategies. Recent
              generative deep learning systems for audio synthesis are able to
              learn models that can traverse arbitrary spaces of sound defined by
              the data they train on. Furthermore, machine learning systems are
              providing new techniques for designing control and navigation
              strategies for these models. This paper is a review of developments
              in deep learning that are changing the practice of sound modelling.
              },
  urldate  = {2020-11-16},
  journal  = {arXiv:2006.06426 [cs, eess, stat]},
  author   = {Huzaifah, M. and Wyse, L.},
  month    = jun,
  year     = {2020},
  note     = {arXiv: 2006.06426},
  keywords = {Computer Science - Sound, Statistics - Machine Learning,
              Electrical Engineering and Systems Science - Audio and Speech
              Processing, I.2.6, Computer Science - Machine Learning, J.5},
  file     = {arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/539XJW7H/2006.html:text/html;Huzaifah
              and Wyse - 2020 - Deep generative models for musical audio
              synthesis.pdf:/Users/apodusenko/Zotero/storage/FMKNY9Z6/Huzaifah and
              Wyse - 2020 - Deep generative models for musical audio
              synthesis.pdf:application/pdf}
}

@article{wang_too_2020,
  title      = {Too many cooks: {Bayesian} inference for coordinating multi-agent
                collaboration},
  shorttitle = {Too many cooks},
  url        = {http://arxiv.org/abs/2003.11778},
  abstract   = {Collaboration requires agents to coordinate their behavior on the
                fly, sometimes cooperating to solve a single task together and
                other times dividing it up into sub-tasks to work on in parallel.
                Underlying the human ability to collaborate is theory-of-mind, the
                ability to infer the hidden mental states that drive others to act.
                Here, we develop Bayesian Delegation, a decentralized multi-agent
                learning mechanism with these abilities. Bayesian Delegation
                enables agents to rapidly infer the hidden intentions of others by
                inverse planning. We test Bayesian Delegation in a suite of
                multi-agent Markov decision processes inspired by cooking problems.
                On these tasks, agents with Bayesian Delegation coordinate both
                their high-level plans (e.g. what sub-task they should work on) and
                their low-level actions (e.g. avoiding getting in each other's
                way). In a self-play evaluation, Bayesian Delegation outperforms
                alternative algorithms. Bayesian Delegation is also a capable
                ad-hoc collaborator and successfully coordinates with other agent
                types even in the absence of prior experience. Finally, in a
                behavioral experiment, we show that Bayesian Delegation makes
                inferences similar to human observers about the intent of others.
                Together, these results demonstrate the power of Bayesian
                Delegation for decentralized multi-agent collaboration.},
  urldate    = {2020-11-16},
  journal    = {arXiv:2003.11778 [cs]},
  author     = {Wang, Rose E. and Wu, Sarah A. and Evans, James A. and Tenenbaum,
                Joshua B. and Parkes, David C. and Kleiman-Weiner, Max},
  month      = jul,
  year       = {2020},
  note       = {arXiv: 2003.11778},
  keywords   = {Computer Science - Artificial Intelligence, Computer Science -
                Machine Learning, Computer Science - Multiagent Systems},
  file       = {arXiv.org
                Snapshot:/Users/apodusenko/Zotero/storage/9F9UKI7W/2003.html:text/html;Wang
                et al. - 2020 - Too many cooks Bayesian inference for
                coordinatin.pdf:/Users/apodusenko/Zotero/storage/2YPJSF6S/Wang et al. -
                2020 - Too many cooks Bayesian inference for
                coordinatin.pdf:application/pdf}
}

@article{fevotte_bayesian_2006,
  title    = {A {Bayesian} {Approach} for {Blind} {Separation} of {Sparse} {Sources
              }},
  volume   = {14},
  issn     = {1558-7924},
  doi      = {10.1109/TSA.2005.858523},
  abstract = {We present a Bayesian approach for blind separation of linear
              instantaneous mixtures of sources having a sparse representation in
              a given basis. The distributions of the coefficients of the sources
              in the basis are modeled by a Student t distribution, which can be
              expressed as a scale mixture of Gaussians, and a Gibbs sampler is
              derived to estimate the sources, the mixing matrix, the input noise
              variance and also the hyperparameters of the Student t
              distributions. The method allows for separation of underdetermined
              (more sources than sensors) noisy mixtures. Results are presented
              with audio signals using a modified discrete cosine transform basis
              and compared with a finite mixture of Gaussians prior approach.
              These results show the improved sound quality obtained with the
              Student t prior and the better robustness to mixing matrices close
              to singularity of the Markov chain Monte Carlo approach},
  number   = {6},
  journal  = {IEEE Transactions on Audio, Speech, and Language Processing},
  author   = {Fevotte, C. and Godsill, S. J.},
  month    = nov,
  year     = {2006},
  note     = {Conference Name: IEEE Transactions on Audio, Speech, and Language
              Processing},
  keywords = {Bayes methods, Bayesian methods, Gaussian distribution, Acoustic
              noise, Markov processes, Bayesian approach, Gaussian processes,
              signal representation, Bayesian estimation, Gaussian noise,
              Robustness, matrix algebra, signal sampling, Monte Carlo methods,
              audio signals, Acoustic sensors, blind separation, blind source
              separation, blind source separation (BSS), discrete cosine
              transform, discrete cosine transforms, Discrete cosine transforms,
              Gaussians prior approach, Gibbs sampler, independent component
              analysis, linear instantaneous mixtures, Markov chain Monte Carlo
              (MCMC) methods, Markov chain Monte Carlo approach, mixing matrix,
              Sparse matrices, sparse representation, sparse representations,
              sparse sources},
  pages    = {2174--2188},
  file     = {IEEE Xplore Abstract
              Record:/Users/apodusenko/Zotero/storage/IXNCDQ4S/1709905.html:text/html;IEEE
              Xplore Full Text PDF:/Users/apodusenko/Zotero/storage/BGRMFTZV/Fevotte
              and Godsill - 2006 - A Bayesian Approach for Blind Separation of
              Sparse.pdf:application/pdf}
}

@article{amari_new_nodate,
  title    = {A {New} {Learning} {Algorithm} for {Blind} {Signal} {Separation}},
  abstract = {A new on-line learning algorithm which minimizes a statistical
              dependency among outputs is derived for blind separation of mixed
              signals. The dependency is measured by the average mutual
              information (MI) of the outputs. The source signals and the mixing
              matrix are unknown except for the number of the sources. The
              Gram-Charlier expansion instead of the Edgeworth expansion is used
              in evaluating the MI. The natural gradient approach is used to
              minimize the MI. A novel activation function is proposed for the
              on-line learning algorithm which has an equivariant property and is
              easily implemented on a neural network like model. The validity of
              the new learning algorithm are verified by computer simulations.},
  language = {en},
  author   = {Amari, Shun-ichi and Cichocki, Andrzej and Yang, Howard Hua},
  pages    = {7},
  file     = {Amari et al. - A New Learning Algorithm for Blind Signal
              Separati.pdf:/Users/apodusenko/Zotero/storage/HHANCMEP/Amari et al. - A
              New Learning Algorithm for Blind Signal Separati.pdf:application/pdf}
}

@article{lacey_tutorial_nodate,
  title    = {Tutorial: {The} {Kalman} {Filter}},
  language = {en},
  author   = {Lacey, Tony},
  pages    = {8},
  file     = {Lacey - Tutorial The Kalman
              Filter.pdf:/Users/apodusenko/Zotero/storage/GBQB3EAN/Lacey - Tutorial
              The Kalman Filter.pdf:application/pdf}
}

@article{welling_choice_2012,
  title    = {On the {Choice} of {Regions} for {Generalized} {Belief} {Propagation}
              },
  url      = {http://arxiv.org/abs/1207.4158},
  abstract = {Generalized belief propagation (GBP) has proven to be a promising
              technique for approximate inference tasks in AI and machine
              learning. However, the choice of a good set of clusters to be used
              in GBP has remained more of an art then a science until this day.
              This paper proposes a sequential approach to adding new clusters of
              nodes and their interactions (i.e. "regions") to the approximation.
              We first review and analyze the recently introduced region graphs
              and find that three kinds of operations ("split", "merge" and "
              death") leave the free energy and (under some conditions) the fixed
              points of GBP invariant. This leads to the notion of "weakly
              irreducible" regions as the natural candidates to be added to the
              approximation. Computational complexity of the GBP algorithm is
              controlled by restricting attention to regions with small "
              region-width". Combining the above with an efficient (i.e. local in
              the graph) measure to predict the improved accuracy of GBP leads to
              the sequential "region pursuit" algorithm for adding new regions
              bottom-up to the region graph. Experiments show that this algorithm
              can indeed perform close to optimally.},
  urldate  = {2020-10-25},
  journal  = {arXiv:1207.4158 [cs]},
  author   = {Welling, Max},
  month    = jul,
  year     = {2012},
  note     = {arXiv: 1207.4158},
  keywords = {Computer Science - Artificial Intelligence, Computer Science -
              Machine Learning},
  file     = {arXiv Fulltext PDF:/Users/apodusenko/Zotero/storage/UEW8JRQ5/Welling -
              2012 - On the Choice of Regions for Generalized Belief
              Pr.pdf:application/pdf;arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/RSRMMJKK/1207.html:text/html}
}

@article{uhler_exact_2016,
  title    = {Exact formulas for the normalizing constants of {Wishart}
              distributions for graphical models},
  url      = {http://arxiv.org/abs/1406.4901},
  abstract = {Gaussian graphical models have received considerable attention
              during the past four decades from the statistical and machine
              learning communities. In Bayesian treatments of this model, the
              G-Wishart distribution serves as the conjugate prior for inverse
              covariance matrices satisfying graphical constraints. While it is
              straightforward to posit the unnormalized densities, the
              normalizing constants of these distributions have been known only
              for graphs that are chordal, or decomposable. Up until now, it was
              unknown whether the normalizing constant for a general graph could
              be represented explicitly, and a considerable body of computational
              literature emerged that attempted to avoid this apparent
              intractability. We close this question by providing an explicit
              representation of the G-Wishart normalizing constant for general
              graphs.},
  urldate  = {2020-10-10},
  journal  = {arXiv:1406.4901 [math, stat]},
  author   = {Uhler, Caroline and Lenkoski, Alex and Richards, Donald},
  month    = jun,
  year     = {2016},
  note     = {arXiv: 1406.4901},
  keywords = {Mathematics - Statistics Theory, 62H05, 60E05, 62E15},
  file     = {arXiv Fulltext PDF:/Users/apodusenko/Zotero/storage/27ML8WRU/Uhler et
              al. - 2016 - Exact formulas for the normalizing constants of
              Wi.pdf:application/pdf;arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/T25CD5XP/1406.html:text/html}
}

@article{watson_active_2020,
  title      = {Active {Inference} or {Control} as {Inference}? {A} {Unifying} {View}
                },
  shorttitle = {Active {Inference} or {Control} as {Inference}?},
  url        = {http://arxiv.org/abs/2010.00262},
  abstract   = {Active inference (AI) is a persuasive theoretical framework from
                computational neuroscience that seeks to describe action and
                perception as inference-based computation. However, this framework
                has yet to provide practical sensorimotor control algorithms that
                are competitive with alternative approaches. In this work, we frame
                active inference through the lens of control as inference (CaI), a
                body of work that presents trajectory optimization as inference.
                From the wider view of `probabilistic numerics', CaI offers
                principled, numerically robust optimal control solvers that provide
                uncertainty quantification, and can scale to nonlinear problems
                with approximate inference. We show that AI may be framed as
                partially-observed CaI when the cost function is defined
                specifically in the observation states.},
  urldate    = {2020-10-05},
  journal    = {arXiv:2010.00262 [cs, stat]},
  author     = {Watson, Joe and Imohiosen, Abraham and Peters, Jan},
  month      = oct,
  year       = {2020},
  note       = {arXiv: 2010.00262},
  keywords   = {Statistics - Machine Learning, Computer Science - Machine Learning
                },
  file       = {arXiv Fulltext PDF:/Users/apodusenko/Zotero/storage/GZIJ3PMD/Watson et
                al. - 2020 - Active Inference or Control as Inference A
                Unifyi.pdf:application/pdf;arXiv.org
                Snapshot:/Users/apodusenko/Zotero/storage/9BVGTJE4/2010.html:text/html}
}

@article{chen_wavegrad_2020,
  title      = {{WaveGrad}: {Estimating} {Gradients} for {Waveform} {Generation}},
  shorttitle = {{WaveGrad}},
  url        = {http://arxiv.org/abs/2009.00713},
  abstract   = {This paper introduces WaveGrad, a conditional model for waveform
                generation through estimating gradients of the data density. This
                model is built on the prior work on score matching and diffusion
                probabilistic models. It starts from Gaussian white noise and
                iteratively refines the signal via a gradient-based sampler
                conditioned on the mel-spectrogram. WaveGrad is non-autoregressive,
                and requires only a constant number of generation steps during
                inference. It can use as few as 6 iterations to generate high
                fidelity audio samples. WaveGrad is simple to train, and implicitly
                optimizes for the weighted variational lower-bound of the
                log-likelihood. Empirical experiments reveal WaveGrad to generate
                high fidelity audio samples matching a strong likelihood-based
                autoregressive baseline with less sequential operations.},
  urldate    = {2020-09-03},
  journal    = {arXiv:2009.00713 [cs, eess, stat]},
  author     = {Chen, Nanxin and Zhang, Yu and Zen, Heiga and Weiss, Ron J. and
                Norouzi, Mohammad and Chan, William},
  month      = sep,
  year       = {2020},
  note       = {arXiv: 2009.00713},
  keywords   = {Computer Science - Sound, Statistics - Machine Learning,
                Electrical Engineering and Systems Science - Audio and Speech
                Processing, Computer Science - Machine Learning},
  file       = {arXiv.org
                Snapshot:/Users/apodusenko/Zotero/storage/FMMJC7RV/2009.html:text/html;Chen
                et al. - 2020 - WaveGrad Estimating Gradients for Waveform
                Genera.pdf:/Users/apodusenko/Zotero/storage/K8G5GE7W/Chen et al. - 2020
                - WaveGrad Estimating Gradients for Waveform Genera.pdf:application/pdf
                }
}

@inproceedings{qi_bayesian_2002,
  title     = {Bayesian spectrum estimation of unevenly sampled nonstationary data},
  volume    = {2},
  doi       = {10.1109/ICASSP.2002.5744891},
  abstract  = {Spectral estimation methods typically assume stationarity and
               uniform spacing between samples of data. The non-stationarity of
               real data is usually accommodated by windowing methods, while the
               lack of uniformly-spaced samples is typically addressed by methods
               that “fill in” the data in some way. This paper presents a new
               approach to both of these problems: We use a non-stationary Kalman
               filter within a Bayesian framework to jointly estimate all spectral
               coefficients instantaneously. The new method works regardless of
               how the signal samples are spaced. We illustrate the method on
               several data sets, showing that it provides more accurate
               estimation than the Lomb-Scargle method and several classical
               spectral estimation methods.},
  booktitle = {2002 {IEEE} {International} {Conference} on {Acoustics}, {Speech}
               , and {Signal} {Processing}},
  author    = {Qi, Yuan and Minka, Thomas P. and Picara, Rosalind W.},
  month     = may,
  year      = {2002},
  note      = {ISSN: 1520-6149},
  keywords  = {Estimation, Bayesian methods, Entropy, Spectral analysis},
  pages     = {II--1473--II--1476},
  file      = {IEEE Xplore Abstract
               Record:/Users/apodusenko/Zotero/storage/B4LJLE47/5744891.html:text/html;Qi
               et al. - 2002 - Bayesian spectrum estimation of unevenly sampled
               n.pdf:/Users/apodusenko/Zotero/storage/EA59AM8Z/Qi et al. - 2002 -
               Bayesian spectrum estimation of unevenly sampled n.pdf:application/pdf}
}

@techreport{schwartenbeck_computational_2018,
  type        = {preprint},
  title       = {Computational mechanisms of curiosity and goal-directed exploration},
  url         = {http://biorxiv.org/lookup/doi/10.1101/411272},
  abstract    = {Abstract Successful behaviour depends on the right balance between
                 maximising reward and soliciting information about the world. Here,
                 we show how different types of information-gain emerge when casting
                 behaviour as surprise minimisation. We present two distinct
                 mechanisms for goal-directed exploration that express separable
                 profiles of active sampling to reduce uncertainty. ‘Hidden state’
                 exploration motivates agents to sample unambiguous observations to
                 accurately infer the (hidden) state of the world. Conversely,
                 ‘model parameter’ exploration, compels agents to sample outcomes
                 associated with high uncertainty, if they are informative for their
                 representation of the task structure. We illustrate the emergence
                 of these types of information-gain, termed active inference and
                 active learning, and show how these forms of exploration induce
                 distinct patterns of ‘Bayes-optimal’ behaviour. Our findings
                 provide a computational framework to understand how distinct levels
                 of uncertainty induce different modes of information-gain in
                 decision-making.},
  language    = {en},
  urldate     = {2020-07-06},
  institution = {Neuroscience},
  author      = {Schwartenbeck, Philipp and Passecker, Johannes and Hauser, Tobias U
                 and FitzGerald, Thomas H B and Kronbichler, Martin and Friston, Karl},
  month       = sep,
  year        = {2018},
  doi         = {10.1101/411272},
  file        = {Schwartenbeck et al. - 2018 - Computational mechanisms of curiosity
                 and
                 goal-dir.pdf:/Users/apodusenko/Zotero/storage/RBVPMCEN/Schwartenbeck et
                 al. - 2018 - Computational mechanisms of curiosity and
                 goal-dir.pdf:application/pdf}
}

@article{hafner_action_2020,
  title    = {Action and {Perception} as {Divergence} {Minimization}},
  url      = {http://arxiv.org/abs/2009.01791},
  abstract = {We introduce a unified objective for action and perception of
              intelligent agents. Extending representation learning and control,
              we minimize the joint divergence between the world and a target
              distribution. Intuitively, such agents use perception to align
              their beliefs with the world, and use actions to align the world
              with their beliefs. Minimizing the joint divergence to an
              expressive target maximizes the mutual information between the
              agent's representations and inputs, thus inferring representations
              that are informative of past inputs and exploring future inputs
              that are informative of the representations. This lets us derive
              intrinsic objectives, such as representation learning, information
              gain, empowerment, and skill discovery from minimal assumptions.
              Moreover, interpreting the target distribution as a latent variable
              model suggests expressive world models as a path toward highly
              adaptive agents that seek large niches in their environments, while
              rendering task rewards optional. The presented framework provides a
              common language for comparing a wide range of objectives,
              facilitates understanding of latent variables for decision making,
              and offers a recipe for designing novel objectives. We recommend
              deriving future agent objectives from the joint divergence to
              facilitate comparison, to point out the agent's target distribution
              , and to identify the intrinsic objective terms needed to reach
              that distribution.},
  urldate  = {2020-09-18},
  journal  = {arXiv:2009.01791 [cs, math, stat]},
  author   = {Hafner, Danijar and Ortega, Pedro A. and Ba, Jimmy and Parr, Thomas
              and Friston, Karl and Heess, Nicolas},
  month    = sep,
  year     = {2020},
  note     = {arXiv: 2009.01791},
  keywords = {Statistics - Machine Learning, Computer Science - Artificial
              Intelligence, Computer Science - Information Theory, Computer
              Science - Machine Learning},
  file     = {arXiv Fulltext PDF:/Users/apodusenko/Zotero/storage/78LYXKX9/Hafner et
              al. - 2020 - Action and Perception as Divergence
              Minimization.pdf:application/pdf;arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/C23QSMHI/2009.html:text/html}
}

@article{bisgaard_quality_2005,
  title      = {Quality {Quandaries}: {Interpretation} of {Time} {Series} {Models}*},
  volume     = {17},
  issn       = {0898-2112},
  shorttitle = {Quality {Quandaries}},
  url        = {
                http://www.informaworld.com/openurl?genre=article&doi=10.1081/QEN-200059890&magic=crossref||D404A21C5BB053405B1A640AFFD44AE3
                },
  doi        = {10.1081/QEN-200059890},
  language   = {en},
  number     = {4},
  urldate    = {2020-09-07},
  journal    = {Quality Engineering},
  author     = {Bisgaard, Søren and Kulahci, Murat},
  month      = oct,
  year       = {2005},
  pages      = {653--658},
  file       = {Bisgaard and Kulahci - 2005 - Quality Quandaries Interpretation of
                Time Series .pdf:/Users/apodusenko/Zotero/storage/58D5ZAQV/Bisgaard and
                Kulahci - 2005 - Quality Quandaries Interpretation of Time Series
                .pdf:application/pdf}
}

@article{defossez_music_2019,
  title    = {Music {Source} {Separation} in the {Waveform} {Domain}},
  url      = {https://openreview.net/forum?id=HJx7uJStPH},
  abstract = {We match the performance of spectrogram based model with a model
              trained end-to-end in the waveform domain},
  urldate  = {2020-09-07},
  author   = {Defossez, Alexandre and Usunier, Nicolas and Bottou, Leon and Bach,
              Francis},
  month    = sep,
  year     = {2019},
  file     = {Defossez et al. - 2019 - Music Source Separation in the Waveform
              Domain.pdf:/Users/apodusenko/Zotero/storage/Z4DPVPLT/Defossez et al. -
              2019 - Music Source Separation in the Waveform
              Domain.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/JVDLGM9E/forum.html:text/html
              }
}

@article{alguacil_seeing_2019,
  title    = {Seeing in to the future: using self-propelled particle models to aid
              player decision-making in soccer},
  abstract = {Soccer has some of the most complex team movement patterns of any
              team sport. Recently, several measurements have been proposed for
              evaluating state of play and for identifying the expected value of
              dribbles, passes or shots [1-6]. The next step is to automatically
              identify the alternative actions available to players both on and
              off the ball. We address this challenge by defining three
              optimization criteria that drives the movement of players during
              attack. (1) Pass probability: A player moves to maximize the
              probability of pass success to either himself or to another player,
              e. g. by opening up a passing lane. (2) Pitch Impact: Occupy point
              on the field which is maximally dangerous. For example, a striker
              moves to a point directly in front of goal. (3) Pitch Control:
              Maximize the amount of space controlled by the team. Soccer players
              often rate their teammates in terms of their ability to anticipate
              the movement of the other players on the pitch a few seconds in to
              the future. To account for this, and building on studies of
              pedestrian movement, we assume players maximize their future value
              position on a weighted combination of these three criteria.},
  language = {en},
  author   = {Alguacil, Francisco Peralta and Fernandez, Javier and Arce, Pablo
              Piñones},
  year     = {2019},
  pages    = {23},
  file     = {Alguacil et al. - Seeing in to the future using self-propelled
              part.pdf:/Users/apodusenko/Zotero/storage/FQXGBS4C/Alguacil et al. -
              Seeing in to the future using self-propelled part.pdf:application/pdf}
}

@article{sumpter_soccermatics_nodate,
  title    = {Soccermatics: {Mathematical} {Adventures} in the {Beautiful} {Game}},
  language = {en},
  author   = {Sumpter, David},
  pages    = {281},
  file     = {Sumpter - Soccermatics Mathematical Adventures in the
              Beaut.pdf:/Users/apodusenko/Zotero/storage/5IUV3F6T/Sumpter -
              Soccermatics Mathematical Adventures in the Beaut.pdf:application/pdf}
}

@article{afonso_learning_2019,
  title    = {Learning state representations and {Markov} models in football
              analytics},
  language = {en},
  author   = {Afonso, Marielby Mercedes Soares},
  year     = {2019},
  pages    = {67},
  file     = {Afonso - 2019 - Learning state representations and Markov models
              i.pdf:/Users/apodusenko/Zotero/storage/QJSEX2ST/Afonso - 2019 -
              Learning state representations and Markov models i.pdf:application/pdf}
}

@article{klinkachorn_evaluating_nodate,
  title    = {Evaluating {Current} {Machine} {Learning} {Techniques} {On} {
              Predicting} {Chaotic} {Systems}},
  abstract = {A chaotic system is one that shows large deviations in its
              behavior with small changes to its initial conditions. In
              particular we see this arise in the case of the kinematic motion of
              a double pendulum which we seek to model in order to understand
              more about predicting chaotic situations. We ﬁnd that linear
              regression with a polynomial feature map is able to accurately
              model a double pendulum at small initial angles where chaotic
              motion is not yet apparent. For large angles, where the motion
              becomes chaotic, we discover that a Long Short-Term Memory Network
              can accurately predict the future trajectory of the system.},
  language = {en},
  author   = {Klinkachorn, Sirapop and Parmar, Jupinder},
  pages    = {6},
  file     = {Klinkachorn and Parmar - Evaluating Current Machine Learning
              Techniques On
              .pdf:/Users/apodusenko/Zotero/storage/LYVZ22DM/Klinkachorn and Parmar -
              Evaluating Current Machine Learning Techniques On .pdf:application/pdf}
}

@article{akaike_autoregressive_1971,
  title    = {Autoregressive model fitting for control},
  volume   = {23},
  issn     = {1572-9052},
  url      = {https://doi.org/10.1007/BF02479221},
  doi      = {10.1007/BF02479221},
  abstract = {The use of a multidimensional extension of the minimum final
              prediction error (FPE) criterion which was originally developed for
              the decision of the order of one-dimensional autoregressive process
              [1] is discussed from the standpoint of controller design. It is
              shown by numerical examples that the criterion will also be useful
              for the decision of inclusion or exclusion of a variable into the
              model. Practical utility of the procedure was verified in the real
              controller design process of cement rotary kilns.},
  language = {en},
  number   = {1},
  urldate  = {2020-09-02},
  journal  = {Annals of the Institute of Statistical Mathematics},
  author   = {Akaike, Hirotugu},
  month    = dec,
  year     = {1971},
  pages    = {163--180},
  file     = {Springer Full Text
              PDF:/Users/apodusenko/Zotero/storage/FGGRRBFA/Akaike - 1971 -
              Autoregressive model fitting for control.pdf:application/pdf}
}

@inproceedings{spearman_beyond_2018,
  title     = {Beyond {Expected} {Goals}},
  url       = {
               https://www.researchgate.net/publication/327139841_Beyond_Expected_Goals
               },
  abstract  = {PDF {\textbar} Abstract: Many models have been constructed to
               quantify the quality of shots in soccer. In this paper, we evaluate
               the quality of off-ball... {\textbar} Find, read and cite all the
               research you need on ResearchGate},
  language  = {en},
  urldate   = {2020-09-02},
  booktitle = {{ResearchGate}},
  author    = {Spearman, William},
  year      = {2018},
  file      = {
               Snapshot:/Users/apodusenko/Zotero/storage/8QP657V9/327139841_Beyond_Expected_Goals.html:text/html;Spearman
               - 2018 - Beyond Expected
               Goals.pdf:/Users/apodusenko/Zotero/storage/BFP6SHDB/Spearman - 2018 -
               Beyond Expected Goals.pdf:application/pdf}
}

@inproceedings{garrett_whence_1998,
  address   = {Dordrecht},
  series    = {Fundamental {Theories} of {Physics}},
  title     = {Whence the {Laws} of {Probability}?},
  isbn      = {978-94-011-5028-6},
  doi       = {10.1007/978-94-011-5028-6_6},
  abstract  = {A new derivation is given of the sum and product rules of
               probability. Probability is treated as a number associated with one
               binary proposition conditioned on another, so that the Boolean
               calculus of the propositions induces a calculus for the
               probabilities. This is the strategy of R. T. Cox (1946), with a
               refinement: a formula is derived for the probability of the NAND of
               two propositions in terms of the probabilities of those
               propositions. Because NAND is a primitive logic operation from
               which any other can be synthesised, there are no further
               probabilities that the NAND can depend on. A functional equation is
               then set up for the relation between the probabilities and is
               solved. By synthesising the non-primitive operations NOT and AND
               from NAND the sum and product rules are derived from this one
               formula, the fundamental ‘law of probability’.},
  language  = {en},
  booktitle = {Maximum {Entropy} and {Bayesian} {Methods}},
  publisher = {Springer Netherlands},
  author    = {Garrett, Anthony J. M.},
  editor    = {Erickson, Gary J. and Rychert, Joshua T. and Smith, C. Ray},
  year      = {1998},
  keywords  = {probability, Boolean algebra, laws of probability, product rule,
               sum rule},
  pages     = {71--86},
  file      = {Garrett - 1998 - Whence the Laws of
               Probability.pdf:/Users/apodusenko/Zotero/storage/X9BTBJPI/Garrett -
               1998 - Whence the Laws of Probability.pdf:application/pdf}
}

@inproceedings{cournapeau_using_2008,
  address   = {Las Vegas, NV, USA},
  title     = {Using variational bayes free energy for unsupervised voice activity
               detection},
  isbn      = {978-1-4244-1483-3 978-1-4244-1484-0},
  url       = {http://ieeexplore.ieee.org/document/4518638/},
  doi       = {10.1109/ICASSP.2008.4518638},
  abstract  = {This paper addresses the problem of Voice Active Detection (VAD)
               in noisy environments. We introduce Variational Bayes approach to
               EM for classiﬁcation to replace the heuristic state machines. The
               Variational Bayes approach provides an explicit approximation of
               the evidence called Free Energy. Free Energy is used to assess the
               reliability of the classiﬁcation model, and can be periodically
               updated with a small number of samples. We apply this scheme to the
               detection of invalid classiﬁcation caused in noise-only portions
               for more reliable VAD, avoiding some of the heuristics
               conventionally used in many VAD algorithms. An experimental
               evaluation is conducted on the CENSREC-1-C database for VAD
               evaluation, and the proposed method gives a signiﬁcant improvement.
               },
  language  = {en},
  urldate   = {2020-08-20},
  booktitle = {2008 {IEEE} {International} {Conference} on {Acoustics}, {Speech}
               and {Signal} {Processing}},
  publisher = {IEEE},
  author    = {Cournapeau, David and {Tatsuya Kawahara}},
  month     = mar,
  year      = {2008},
  pages     = {4429--4432},
  file      = {Cournapeau and Tatsuya Kawahara - 2008 - Using variational bayes free
               energy for
               unsupervis.pdf:/Users/apodusenko/Zotero/storage/4ZB49YWJ/Cournapeau and
               Tatsuya Kawahara - 2008 - Using variational bayes free energy for
               unsupervis.pdf:application/pdf}
}

@article{tschantz_reinforcement_2020,
  title    = {Reinforcement {Learning} through {Active} {Inference}},
  url      = {http://arxiv.org/abs/2002.12636},
  abstract = {The central tenet of reinforcement learning (RL) is that agents
              seek to maximize the sum of cumulative rewards. In contrast, active
              inference, an emerging framework within cognitive and computational
              neuroscience, proposes that agents act to maximize the evidence for
              a biased generative model. Here, we illustrate how ideas from
              active inference can augment traditional RL approaches by (i)
              furnishing an inherent balance of exploration and exploitation, and
              (ii) providing a more flexible conceptualization of reward.
              Inspired by active inference, we develop and implement a novel
              objective for decision making, which we term the free energy of the
              expected future. We demonstrate that the resulting algorithm
              successfully balances exploration and exploitation, simultaneously
              achieving robust performance on several challenging RL benchmarks
              with sparse, well-shaped, and no rewards.},
  urldate  = {2020-08-18},
  journal  = {arXiv:2002.12636 [cs, eess, math, stat]},
  author   = {Tschantz, Alexander and Millidge, Beren and Seth, Anil K. and
              Buckley, Christopher L.},
  month    = feb,
  year     = {2020},
  note     = {arXiv: 2002.12636},
  keywords = {Statistics - Machine Learning, Computer Science - Artificial
              Intelligence, Computer Science - Information Theory, Computer
              Science - Machine Learning, Electrical Engineering and Systems
              Science - Systems and Control},
  file     = {arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/9PXBPYRM/2002.html:text/html;Tschantz
              et al. - 2020 - Reinforcement Learning through Active
              Inference.pdf:/Users/apodusenko/Zotero/storage/T4DEI83N/Tschantz et al.
              - 2020 - Reinforcement Learning through Active
              Inference.pdf:application/pdf}
}

@article{borwein_variational_nodate,
  title    = {A {Variational} {Approach} to {Lagrange} {Multipliers}},
  abstract = {We discuss Lagrange multiplier rules from a variational
              perspective. This allows us to highlight many of the issues
              involved and also to illustrate how broadly an abstract version can
              be applied.},
  language = {en},
  author   = {Borwein, Jonathan M and Zhu, Qiji J},
  pages    = {24},
  file     = {Borwein and Zhu - A Variational Approach to Lagrange
              Multipliers.pdf:/Users/apodusenko/Zotero/storage/2KKGRKZI/Borwein and
              Zhu - A Variational Approach to Lagrange
              Multipliers.pdf:application/pdf}
}

@article{gritsenko_spectral_2020,
  title    = {A {Spectral} {Energy} {Distance} for {Parallel} {Speech} {Synthesis}},
  url      = {http://arxiv.org/abs/2008.01160},
  abstract = {Speech synthesis is an important practical generative modeling
              problem that has seen great progress over the last few years, with
              likelihood-based autoregressive neural models now outperforming
              traditional concatenative systems. A downside of such
              autoregressive models is that they require executing tens of
              thousands of sequential operations per second of generated audio,
              making them ill-suited for deployment on specialized deep learning
              hardware. Here, we propose a new learning method that allows us to
              train highly parallel models of speech, without requiring access to
              an analytical likelihood function. Our approach is based on a
              generalized energy distance between the distributions of the
              generated and real audio. This spectral energy distance is a proper
              scoring rule with respect to the distribution over
              magnitude-spectrograms of the generated waveform audio and offers
              statistical consistency guarantees. The distance can be calculated
              from minibatches without bias, and does not involve adversarial
              learning, yielding a stable and consistent method for training
              implicit generative models. Empirically, we achieve
              state-of-the-art generation quality among implicit generative
              models, as judged by the recently-proposed cFDSD metric. When
              combining our method with adversarial techniques, we also improve
              upon the recently-proposed GAN-TTS model in terms of Mean Opinion
              Score as judged by trained human evaluators.},
  urldate  = {2020-08-10},
  journal  = {arXiv:2008.01160 [cs, eess, stat]},
  author   = {Gritsenko, Alexey A. and Salimans, Tim and Berg, Rianne van den and
              Snoek, Jasper and Kalchbrenner, Nal},
  month    = aug,
  year     = {2020},
  note     = {arXiv: 2008.01160},
  keywords = {Computer Science - Sound, Statistics - Machine Learning,
              Electrical Engineering and Systems Science - Audio and Speech
              Processing, Computer Science - Machine Learning},
  file     = {arXiv Fulltext PDF:/Users/apodusenko/Zotero/storage/I3QNYXNG/Gritsenko
              et al. - 2020 - A Spectral Energy Distance for Parallel Speech
              Syn.pdf:application/pdf;arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/K886FQHD/2008.html:text/html}
}

@article{van_doorn_-class_2020,
  title    = {An {In}-{Class} {Demonstration} of {Bayesian} {Inference}},
  volume   = {19},
  issn     = {1475-7257},
  url      = {https://doi.org/10.1177/1475725719848574},
  doi      = {10.1177/1475725719848574},
  abstract = {Sir Ronald Fisher’s venerable experiment “The Lady Tasting Tea” is
              revisited from a Bayesian perspective. We demonstrate how a similar
              tasting experiment, conducted in a classroom setting, can
              familiarize students with several key concepts of Bayesian
              inference, such as the prior distribution, the posterior
              distribution, the Bayes factor, and sequential analysis.},
  language = {en},
  number   = {1},
  urldate  = {2020-08-10},
  journal  = {Psychology Learning \& Teaching},
  author   = {van Doorn, Johnny and Matzke, Dora and Wagenmakers, Eric-Jan},
  month    = mar,
  year     = {2020},
  note     = {Publisher: SAGE Publications},
  pages    = {36--45},
  file     = {van Doorn et al. - 2020 - An In-Class Demonstration of Bayesian
              Inference.pdf:/Users/apodusenko/Zotero/storage/KKR8U67X/van Doorn et
              al. - 2020 - An In-Class Demonstration of Bayesian
              Inference.pdf:application/pdf}
}

@book{dirac_principles_nodate,
  edition   = {4},
  title     = {Principles of {Quantum} {Mechanics}},
  url       = {
               https://www.equipes.lps.u-psud.fr/Montambaux/histoire-physique/Dirac-Principles%20of%20Quantum%20Mechanics%20.pdf
               },
  urldate   = {2020-08-04},
  publisher = {The University Press, Oxford},
  author    = {Dirac, Paul},
  file      = {Dirac-Principles of Quantum Mechanics
               .pdf:/Users/apodusenko/Zotero/storage/UKGAUA24/Dirac-Principles of
               Quantum Mechanics .pdf:application/pdf}
}

@article{xue_neural_2020,
  title    = {Neural {Kalman} {Filtering} for {Speech} {Enhancement}},
  url      = {http://arxiv.org/abs/2007.13962},
  abstract = {Statistical signal processing based speech enhancement methods
              adopt expert knowledge to design the statistical models and linear
              filters, which is complementary to the deep neural network (DNN)
              based methods which are data-driven. In this paper, by using expert
              knowledge from statistical signal processing for network design and
              optimization, we extend the conventional Kalman filtering (KF) to
              the supervised learning scheme, and propose the neural Kalman
              filtering (NKF) for speech enhancement. Two intermediate clean
              speech estimates are first produced from recurrent neural networks
              (RNN) and linear Wiener filtering (WF) separately and are then
              linearly combined by a learned NKF gain to yield the NKF output.
              Supervised joint training is applied to NKF to learn to
              automatically trade-off between the instantaneous linear estimation
              made by the WF and the long-term non-linear estimation made by the
              RNN. The NKF method can be seen as using expert knowledge from WF
              to regularize the RNN estimations to improve its generalization
              ability to the noise conditions unseen in the training. Experiments
              in different noisy conditions show that the proposed method
              outperforms the baseline methods both in terms of objective
              evaluation metrics and automatic speech recognition (ASR) word
              error rates (WERs).},
  urldate  = {2020-07-31},
  journal  = {arXiv:2007.13962 [cs, eess]},
  author   = {Xue, Wei and Quan, Gang and Zhang, Chao and Ding, Guohong and He,
              Xiaodong and Zhou, Bowen},
  month    = jul,
  year     = {2020},
  note     = {arXiv: 2007.13962},
  keywords = {Computer Science - Sound, Electrical Engineering and Systems
              Science - Audio and Speech Processing},
  file     = {arXiv Fulltext PDF:/Users/apodusenko/Zotero/storage/FJ9L5MF2/Xue et
              al. - 2020 - Neural Kalman Filtering for Speech
              Enhancement.pdf:application/pdf;arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/ACUVI4LT/2007.html:text/html}
}

@article{berninger_bayesian_2020,
  title    = {A {Bayesian} {Time}-{Varying} {Autoregressive} {Model} for {Improved}
              {Short}- and {Long}-{Term} {Prediction}},
  url      = {http://arxiv.org/abs/2006.05750},
  abstract = {Motivated by the application to German interest rates, we propose
              a timevarying autoregressive model for short and long term
              prediction of time series that exhibit a temporary non-stationary
              behavior but are assumed to mean revert in the long run. We use a
              Bayesian formulation to incorporate prior assumptions on the mean
              reverting process in the model and thereby regularize predictions
              in the far future. We use MCMC-based inference by deriving relevant
              full conditional distributions and employ a Metropolis-Hastings
              within Gibbs Sampler approach to sample from the posterior
              (predictive) distribution. In combining data-driven short term
              predictions with long term distribution assumptions our model is
              competitive to the existing methods in the short horizon while
              yielding reasonable predictions in the long run. We apply our model
              to interest rate data and contrast the forecasting performance to
              the one of a 2-Additive-Factor Gaussian model as well as to the
              predictions of a dynamic Nelson-Siegel model.},
  urldate  = {2020-07-31},
  journal  = {arXiv:2006.05750 [q-fin, stat]},
  author   = {Berninger, Christoph and Stöcker, Almond and Rügamer, David},
  month    = jun,
  year     = {2020},
  note     = {arXiv: 2006.05750},
  keywords = {Statistics - Applications, Statistics - Methodology, G.3, 47N30,
              62M10, 62P20, Quantitative Finance - Risk Management},
  file     = {arXiv Fulltext PDF:/Users/apodusenko/Zotero/storage/2BFDLDCT/Berninger
              et al. - 2020 - A Bayesian Time-Varying Autoregressive Model for
              I.pdf:application/pdf;arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/JKTAR3L2/2006.html:text/html}
}

@book{caticha_entropic_2012,
  title     = {Entropic {Inference} and the {Foundations} of {Physics}},
  url       = {https://www.twirpx.com/file/1706750/},
  abstract  = {N.-Y.: International Society for Bayesian Analysis, 2012. - 293p.
               Science consists in using information about the world for the
               purpose of predicting, explaining, understanding, and or
               controlling phenomena of interest. The basic dfficulty is that the
               available information is usually insufficient...},
  language  = {ru},
  urldate   = {2019-11-07},
  publisher = {EBEB-2012, the 11th Brazilian Meeting on Bayesian Statistics},
  author    = {Caticha, Ariel},
  year      = {2012},
  file      = {Caticha - 2012 - Entropic Inference and the Foundations of
               Physics.pdf:/Users/apodusenko/Zotero/storage/NHY3WQG2/Caticha - 2012 -
               Entropic Inference and the Foundations of
               Physics.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/H3LR464X/1706750.html:text/html
               }
}

@article{nguyen_variational_2018,
  title   = {Variational continual learning},
  journal = {ICLR},
  author  = {Nguyen, Cuong V. and Li, Yingzhen and Bui, Thang D. and Turner,
             Richard E.},
  year    = {2018},
  file    = {Nguyen et al. - 2017 - Variational continual
             learning.pdf:/Users/apodusenko/Zotero/storage/SVHEDPHK/Nguyen et al. -
             2017 - Variational continual
             learning.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/4Z5DPWRC/1710.html:text/html
             }
}

@inproceedings{parr_density_1980,
  address   = {Dordrecht},
  series    = {Académie {Internationale} {Des} {Sciences} {Moléculaires} {
               Quantiques} / {International} {Academy} of {Quantum} {Molecular} {
               Science}},
  title     = {Density {Functional} {Theory} of {Atoms} and {Molecules}},
  isbn      = {978-94-009-9027-2},
  doi       = {10.1007/978-94-009-9027-2_2},
  abstract  = {Current studies in density functional theory and density matrix
               functional theory are reviewed, with special attention to the
               possible applications within chemistry. Topics discussed include
               the concept of electronegativity, the concept of an atom in a
               molecule, calculation of electronegativities from the Xα method,
               the concept of pressure, Gibbs-Duhem equation, Maxwell relations,
               stability conditions, and local density functional theory.},
  language  = {en},
  booktitle = {Horizons of {Quantum} {Chemistry}},
  publisher = {Springer Netherlands},
  author    = {Parr, Robert G.},
  editor    = {Fukui, Kenichi and Pullman, Bernard},
  year      = {1980},
  keywords  = {Density Functional Theory, Electron Potential Energy, Gradient
               Expansion, Maxwell Relation, Neutral Atom},
  pages     = {5--15}
}

@book{engel_density_2011,
  address    = {Berlin, Heidelberg},
  series     = {Theoretical and {Mathematical} {Physics}},
  title      = {Density {Functional} {Theory}: {An} {Advanced} {Course}},
  isbn       = {978-3-642-14089-1 978-3-642-14090-7},
  shorttitle = {Density {Functional} {Theory}},
  url        = {http://link.springer.com/10.1007/978-3-642-14090-7},
  language   = {en},
  urldate    = {2018-09-13},
  publisher  = {Springer Berlin Heidelberg},
  author     = {Engel, Eberhard and Dreizler, Reiner M.},
  year       = {2011},
  doi        = {10.1007/978-3-642-14090-7},
  file       = {Engel and Dreizler - 2011 - Density Functional Theory An Advanced
                Course.pdf:/Users/apodusenko/Zotero/storage/B3CE9ZVH/Engel and Dreizler
                - 2011 - Density Functional Theory An Advanced
                Course.pdf:application/pdf}
}

@article{kokkala_sigma-point_2015,
  title    = {Sigma-{Point} {Filtering} and {Smoothing} {Based} {Parameter} {
              Estimation} in {Nonlinear} {Dynamic} {Systems}},
  url      = {http://arxiv.org/abs/1504.06173},
  abstract = {We consider approximate maximum likelihood parameter estimation in
              nonlinear state-space models. We discuss both direct optimization
              of the likelihood and expectation--maximization (EM). For EM, we
              also give closed-form expressions for the maximization step in a
              class of models that are linear in parameters and have additive
              noise. To obtain approximations to the filtering and smoothing
              distributions needed in the likelihood-maximization methods, we
              focus on using Gaussian filtering and smoothing algorithms that
              employ sigma-points to approximate the required integrals. We
              discuss different sigma-point schemes based on the third, fifth,
              seventh, and ninth order unscented transforms and the
              Gauss--Hermite quadrature rule. We compare the performance of the
              methods in two simulated experiments: a univariate nonlinear growth
              model as well as tracking of a maneuvering target. In the
              experiments, we also compare against approximate likelihood
              estimates obtained by particle filtering and extended Kalman
              filtering based methods. The experiments suggest that the
              higher-order unscented transforms may in some cases provide more
              accurate estimates},
  urldate  = {2020-07-13},
  journal  = {arXiv:1504.06173 [math, stat]},
  author   = {Kokkala, Juho and Solin, Arno and Särkkä, Simo},
  month    = nov,
  year     = {2015},
  note     = {arXiv: 1504.06173},
  keywords = {Statistics - Computation, Statistics - Methodology, Mathematics -
              Optimization and Control, Mathematics - Dynamical Systems},
  file     = {arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/UWPSJUYQ/1504.html:text/html;Kokkala
              et al. - 2015 - Sigma-Point Filtering and Smoothing Based
              Paramete.pdf:/Users/apodusenko/Zotero/storage/ITCFERDS/Kokkala et al. -
              2015 - Sigma-Point Filtering and Smoothing Based
              Paramete.pdf:application/pdf}
}

@article{solin_hilbert_nodate,
  title    = {Hilbert {Space} {Methods} in {Infinite}-{Dimensional} {Kalman} {
              Filtering}},
  language = {en},
  author   = {Solin, Arno},
  pages    = {71},
  file     = {Solin - Hilbert Space Methods in Infinite-Dimensional
              Kalm.pdf:/Users/apodusenko/Zotero/storage/HXDB8UY9/Solin - Hilbert
              Space Methods in Infinite-Dimensional Kalm.pdf:application/pdf}
}

@article{solin_hilbert_2020,
  title    = {Hilbert {Space} {Methods} for {Reduced}-{Rank} {Gaussian} {Process} {
              Regression}},
  volume   = {30},
  issn     = {0960-3174, 1573-1375},
  url      = {http://arxiv.org/abs/1401.5508},
  doi      = {10.1007/s11222-019-09886-w},
  abstract = {This paper proposes a novel scheme for reduced-rank Gaussian
              process regression. The method is based on an approximate series
              expansion of the covariance function in terms of an eigenfunction
              expansion of the Laplace operator in a compact subset of \${
              \textbackslash}mathbb\{R\}{\textasciicircum}d\$. On this
              approximate eigenbasis the eigenvalues of the covariance function
              can be expressed as simple functions of the spectral density of the
              Gaussian process, which allows the GP inference to be solved under
              a computational cost scaling as \${\textbackslash}mathcal\{O\}(nm{
              \textasciicircum}2)\$ (initial) and \${\textbackslash}mathcal\{O\}
              (m{\textasciicircum}3)\$ (hyperparameter learning) with \$m\$ basis
              functions and \$n\$ data points. Furthermore, the basis functions
              are independent of the parameters of the covariance function, which
              allows for very fast hyperparameter learning. The approach also
              allows for rigorous error analysis with Hilbert space theory, and
              we show that the approximation becomes exact when the size of the
              compact subset and the number of eigenfunctions go to infinity. We
              also show that the convergence rate of the truncation error is
              independent of the input dimensionality provided that the
              differentiability order of the covariance function is increases
              appropriately, and for the squared exponential covariance function
              it is always bounded by \$\{{\textbackslash}sim\}1/m\$ regardless
              of the input dimensionality. The expansion generalizes to Hilbert
              spaces with an inner product which is defined as an integral over a
              specified input density. The method is compared to previously
              proposed methods theoretically and through empirical tests with
              simulated and real data.},
  number   = {2},
  urldate  = {2020-07-13},
  journal  = {Statistics and Computing},
  author   = {Solin, Arno and Särkkä, Simo},
  month    = mar,
  year     = {2020},
  note     = {arXiv: 1401.5508},
  keywords = {Statistics - Machine Learning},
  pages    = {419--446},
  file     = {arXiv Fulltext PDF:/Users/apodusenko/Zotero/storage/5K5DY88M/Solin and
              Särkkä - 2020 - Hilbert Space Methods for Reduced-Rank Gaussian
              Pr.pdf:application/pdf;arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/XSDZMUBH/1401.html:text/html}
}

@article{arasaratnam_cubature_2009,
  title    = {Cubature {Kalman} {Filters}},
  volume   = {54},
  issn     = {1558-2523},
  doi      = {10.1109/TAC.2009.2019800},
  abstract = {In this paper, we present a new nonlinear filter for
              high-dimensional state estimation, which we have named the cubature
              Kalman filter (CKF). The heart of the CKF is a spherical-radial
              cubature rule, which makes it possible to numerically compute
              multivariate moment integrals encountered in the nonlinear Bayesian
              filter. Specifically, we derive a third-degree spherical-radial
              cubature rule that provides a set of cubature points scaling
              linearly with the state-vector dimension. The CKF may therefore
              provide a systematic solution for high-dimensional nonlinear
              filtering problems. The paper also includes the derivation of a
              square-root version of the CKF for improved numerical stability.
              The CKF is tested experimentally in two nonlinear state estimation
              problems. In the first problem, the proposed cubature rule is used
              to compute the second-order statistics of a nonlinearly transformed
              Gaussian random variable. The second problem addresses the use of
              the CKF for tracking a maneuvering aircraft. The results of both
              experiments demonstrate the improved performance of the CKF over
              conventional nonlinear filters.},
  number   = {6},
  journal  = {IEEE Transactions on Automatic Control},
  author   = {Arasaratnam, Ienkaran and Haykin, Simon},
  month    = jun,
  year     = {2009},
  note     = {Conference Name: IEEE Transactions on Automatic Control},
  keywords = {Kalman filters, Kalman filter, Bayesian methods, Random variables,
              Statistics, Testing, Gaussian processes, State estimation,
              Filtering, nonlinear filters, Bayesian filters, cubature Kalman
              filters, cubature points, cubature rules, Gaussian quadrature rules
              , Gaussian random variable, Heart, high dimensional nonlinear
              filtering, high dimensional state estimation, invariant theory,
              maneuvering aircraft tracking, moment integrals, nonlinear Bayesian
              filter, nonlinear filtering, Nonlinear filters, nonlinear state
              estimation, numerical stability, Numerical stability, second-order
              statistics, spherical-radial cubature rule, state estimation, state
              vector dimension},
  pages    = {1254--1269},
  file     = {IEEE Xplore Abstract
              Record:/Users/apodusenko/Zotero/storage/2JKFNX9W/4982682.html:text/html;IEEE
              Xplore Full Text
              PDF:/Users/apodusenko/Zotero/storage/IIHL9V4H/Arasaratnam and Haykin -
              2009 - Cubature Kalman Filters.pdf:application/pdf}
}

@article{solin_cubature_nodate,
  title    = {Cubature {Integration} {Methods} in {Non}-{Linear} {Kalman} {
              Filtering} and {Smoothing}},
  language = {en},
  author   = {Solin, Arno},
  pages    = {51},
  file     = {Solin - Cubature Integration Methods in Non-Linear Kalman
              .pdf:/Users/apodusenko/Zotero/storage/EJH59A3P/Solin - Cubature
              Integration Methods in Non-Linear Kalman .pdf:application/pdf}
}

@article{millidge_deep_2019,
  title    = {Deep {Active} {Inference} as {Variational} {Policy} {Gradients}},
  url      = {http://arxiv.org/abs/1907.03876},
  abstract = {Active Inference is a theory of action arising from neuroscience
              which casts action and planning as a bayesian inference problem to
              be solved by minimizing a single quantity - the variational free
              energy. Active Inference promises a unifying account of action and
              perception coupled with a biologically plausible process theory.
              Despite these potential advantages, current implementations of
              Active Inference can only handle small, discrete policy and
              state-spaces and typically require the environmental dynamics to be
              known. In this paper we propose a novel deep Active Inference
              algorithm which approximates key densities using deep neural
              networks as flexible function approximators, which enables Active
              Inference to scale to significantly larger and more complex tasks.
              We demonstrate our approach on a suite of OpenAIGym benchmark tasks
              and obtain performance comparable with common reinforcement
              learning baselines. Moreover, our algorithm shows similarities with
              maximum entropy reinforcement learning and the policy gradients
              algorithm, which reveals interesting connections between the Active
              Inference framework and reinforcement learning.},
  urldate  = {2020-07-07},
  journal  = {arXiv:1907.03876 [cs]},
  author   = {Millidge, Beren},
  month    = jul,
  year     = {2019},
  note     = {arXiv: 1907.03876},
  keywords = {Computer Science - Neural and Evolutionary Computing, Computer
              Science - Machine Learning},
  file     = {arXiv Fulltext PDF:/Users/apodusenko/Zotero/storage/FQCGD8K7/Millidge
              - 2019 - Deep Active Inference as Variational Policy
              Gradie.pdf:application/pdf;arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/4FF86D4T/1907.html:text/html}
}

@misc{noauthor_mlss2020_causality_16x9pdf_nodate,
  title   = {{MLSS2020}\_causality\_16x9.pdf},
  url     = {
             https://drive.google.com/file/d/1qlUYuU7wfoD6C8Qo0x4Eyz5aT2k0B_jC/view?usp=sharing&usp=embed_facebook
             },
  urldate = {2020-06-29},
  journal = {Google Docs},
  note    = {Library Catalog: drive.google.com},
  file    = {Snapshot:/Users/apodusenko/Zotero/storage/VB7EYJ7N/view.html:text/html
             }
}

@article{fountas_deep_2020,
  title    = {Deep active inference agents using {Monte}-{Carlo} methods},
  url      = {http://arxiv.org/abs/2006.04176},
  abstract = {Active inference is a Bayesian framework for understanding
              biological intelligence. The underlying theory brings together
              perception and action under one single imperative: minimizing free
              energy. However, despite its theoretical utility in explaining
              intelligence, computational implementations have been restricted to
              low-dimensional and idealized situations. In this paper, we present
              a neural architecture for building deep active inference agents
              operating in complex, continuous state-spaces using multiple forms
              of Monte-Carlo (MC) sampling. For this, we introduce a number of
              techniques, novel to active inference. These include: i) selecting
              free-energy-optimal policies via MC tree search, ii) approximating
              this optimal policy distribution via a feed-forward `habitual'
              network, iii) predicting future parameter belief updates using MC
              dropouts and, finally, iv) optimizing state transition precision (a
              high-end form of attention). Our approach enables agents to learn
              environmental dynamics efficiently, while maintaining task
              performance, in relation to reward-based counterparts. We
              illustrate this in a new toy environment, based on the dSprites
              data-set, and demonstrate that active inference agents
              automatically create disentangled representations that are apt for
              modeling state transitions. In a more complex Animal-AI environment
              , our agents (using the same neural architecture) are able to
              simulate future state transitions and actions (i.e., plan), to
              evince reward-directed navigation - despite temporary suspension of
              visual input. These results show that deep active inference -
              equipped with MC methods - provides a flexible framework to develop
              biologically-inspired intelligent agents, with applications in both
              machine learning and cognitive science.},
  urldate  = {2020-06-22},
  journal  = {arXiv:2006.04176 [cs, q-bio, stat]},
  author   = {Fountas, Zafeirios and Sajid, Noor and Mediano, Pedro A. M. and
              Friston, Karl},
  month    = jun,
  year     = {2020},
  note     = {arXiv: 2006.04176},
  keywords = {Statistics - Machine Learning, Computer Science - Artificial
              Intelligence, Quantitative Biology - Neurons and Cognition},
  file     = {arXiv Fulltext PDF:/Users/apodusenko/Zotero/storage/HP68U8SV/Fountas
              et al. - 2020 - Deep active inference agents using Monte-Carlo
              met.pdf:application/pdf;arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/8Y56I6IE/2006.html:text/html}
}

@inproceedings{senoz_online_2018,
  title     = {Online {Variational} {Message} {Passing} in the {Hierarchical} {
               Gaussian} {Filter}},
  doi       = {10.1109/MLSP.2018.8517019},
  abstract  = {We address the problem of online state and parameter estimation in
               hierarchical Bayesian nonlinear dynamic systems. We focus on the
               Hierarchical Gaussian Filter (HGF), which is a popular model in the
               computational neuroscience literature. For this filter, explicit
               equations for online state estimation (and offline parameter
               estimation) have been derived before. We extend this work by
               casting the HGF as a probabilistic factor graph and present
               variational message passing update rules that facilitate both
               online state and parameter estimation as well as online tracking of
               the free energy (or ELBO), which can be used as a proxy for
               Bayesian evidence. Due to the locality and modularity of the factor
               graph framework, our approach supports application of HGF’s and
               variations as plug-in modules to a wide variety of dynamic
               modelling applications.},
  booktitle = {2018 {IEEE} 28th {International} {Workshop} on {Machine} {
               Learning} for {Signal} {Processing} ({MLSP})},
  author    = {Şenöz, I. and de Vries, B.},
  month     = sep,
  year      = {2018},
  keywords  = {Message passing, Dynamical systems, Parameter estimation, Bayes
               methods, Mathematical model, State estimation, free energy,
               Computational modeling, Hierarchical Gaussian Filter, Iron, Online
               state and parameter estimation, Variational Message Passing},
  pages     = {1--6},
  file      = {IEEE Xplore Abstract
               Record:/Users/apodusenko/Zotero/storage/MCEELHDE/8517019.html:text/html;IEEE
               Xplore Abstract
               Record:/Users/apodusenko/Zotero/storage/946VK63T/8517019.html:text/html;Şenöz
               and Vries - 2018 - ONLINE VARIATIONAL MESSAGE PASSING IN THE
               HIERARCH.pdf:/Users/apodusenko/Zotero/storage/3D3FA9I6/Şenöz and Vries
               - 2018 - ONLINE VARIATIONAL MESSAGE PASSING IN THE
               HIERARCH.pdf:application/pdf;Şenöz and Vries - 2018 - ONLINE
               VARIATIONAL MESSAGE PASSING IN THE
               HIERARCH.pdf:/Users/apodusenko/Zotero/storage/APSRD46I/Şenöz and Vries
               - 2018 - ONLINE VARIATIONAL MESSAGE PASSING IN THE
               HIERARCH.pdf:application/pdf}
}

@inproceedings{senoz_ismail_bayesian_2020,
  title     = {Bayesian joint state and parameter tracking in autoregressive models},
  url       = {https://proceedings.mlr.press/v120/senoz20a.html},
  abstract  = {We address the problem of online Bayesian state and parameter
               tracking in autoregressive (AR) models with time-varying process
               noise variance. The involved marginalization and expectation
               integrals cannot be analytically solved. Moreover, the online
               tracking constraint makes sampling and batch learning methods
               unsuitable for this problem. We propose a hybrid variational
               message passing algorithm that robustly tracks the time-varying
               dynamics of the latent states, AR coefﬁcients and process noise
               variance. Since message passing in a factor graph is a highly
               modular inference approach, the proposed methods easily extend to
               other non-stationary dynamic modeling problems.},
  language  = {en},
  booktitle = {Learning for {Dynamics} and {Control}},
  author    = {{Şenöz, İsmail} and Podusenko, Albert and Kouw, Wouter M and de
               Vries, Bert},
  year      = {2020},
  pages     = {9},
  file      = {Senoz et al. - 2020 - Bayesian joint state and parameter tracking in
               aut.pdf:/Users/apodusenko/Zotero/storage/5MNMM933/Senoz et al. - 2020 -
               Bayesian joint state and parameter tracking in aut.pdf:application/pdf}
}

@article{gustafsson_relations_2012,
  title    = {Some {Relations} {Between} {Extended} and {Unscented} {Kalman} {
              Filters}},
  volume   = {60},
  issn     = {1941-0476},
  doi      = {10.1109/TSP.2011.2172431},
  abstract = {The unscented Kalman filter (UKF) has become a popular alternative
              to the extended Kalman filter (EKF) during the last decade. UKF
              propagates the so called sigma points by function evaluations using
              the unscented transformation (UT), and this is at first glance very
              different from the standard EKF algorithm which is based on a
              linearized model. The claimed advantages with UKF are that it
              propagates the first two moments of the posterior distribution and
              that it does not require gradients of the system model. We point
              out several less known links between EKF and UKF in terms of two
              conceptually different implementations of the Kalman filter: the
              standard one based on the discrete Riccati equation, and one based
              on a formula on conditional expectations that does not involve an
              explicit Riccati equation. First, it is shown that the sigma point
              function evaluations can be used in the classical EKF rather than
              an explicitly linearized model. Second, a less cited version of the
              EKF based on a second-order Taylor expansion is shown to be quite
              closely related to UKF. The different algorithms and results are
              illustrated with examples inspired by core observation models in
              target tracking and sensor network applications.},
  number   = {2},
  journal  = {IEEE Transactions on Signal Processing},
  author   = {Gustafsson, Fredrik and Hendeby, Gustaf},
  year     = {2012},
  note     = {Conference Name: IEEE Transactions on Signal Processing},
  keywords = {Kalman filters, Covariance matrix, Transforms, Approximation
              methods, Extended Kalman filter (EKF), Jacobian matrices, Riccati
              equations, Taylor series, transformations, unscented Kalman filter
              (UKF)},
  pages    = {545--555}
}

@article{senoz_variational_2021,
  title     = {Variational {Message} {Passing} and {Local} {Constraint} {
               Manipulation} in {Factor} {Graphs}},
  volume    = {23},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  issn      = {1099-4300},
  url       = {https://www.mdpi.com/1099-4300/23/7/807},
  doi       = {10.3390/e23070807},
  abstract  = {Accurate evaluation of Bayesian model evidence for a given data
               set is a fundamental problem in model development. Since evidence
               evaluations are usually intractable, in practice variational free
               energy (VFE) minimization provides an attractive alternative, as
               the VFE is an upper bound on negative model log-evidence (NLE). In
               order to improve tractability of the VFE, it is common to
               manipulate the constraints in the search space for the posterior
               distribution of the latent variables. Unfortunately, constraint
               manipulation may also lead to a less accurate estimate of the NLE.
               Thus, constraint manipulation implies an engineering trade-off
               between tractability and accuracy of model evidence estimation. In
               this paper, we develop a unifying account of constraint
               manipulation for variational inference in models that can be
               represented by a (Forney-style) factor graph, for which we identify
               the Bethe Free Energy as an approximation to the VFE. We derive
               well-known message passing algorithms from first principles, as the
               result of minimizing the constrained Bethe Free Energy (BFE). The
               proposed method supports evaluation of the BFE in factor graphs for
               model scoring and development of new message passing-based
               inference algorithms that potentially improve evidence estimation
               accuracy.},
  language  = {en},
  number    = {7},
  urldate   = {2022-03-15},
  journal   = {Entropy},
  author    = {Şenöz, İsmail and van de Laar, Thijs and Bagaev, Dmitry and de Vries
               , Bert},
  month     = jul,
  year      = {2021},
  note      = {Number: 7 Publisher: Multidisciplinary Digital Publishing Institute},
  keywords  = {Bayesian inference, Bethe free energy, factor graphs, message
               passing, variational inference, variational message passing,
               variational free energy},
  pages     = {807},
  file      = {Full Text PDF:/Users/apodusenko/Zotero/storage/LA33ZE54/Şenöz et al. -
               2021 - Variational Message Passing and Local Constraint
               M.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/GHY4TMAQ/807.html:text/html
               }
}

@article{chen_robust_2016,
  title    = {Robust benchmarking in noisy environments},
  url      = {http://arxiv.org/abs/1608.04295},
  abstract = {We propose a benchmarking strategy that is robust in the presence
              of timer error, OS jitter and other environmental fluctuations, and
              is insensitive to the highly nonideal statistics produced by timing
              measurements. We construct a model that explains how these strongly
              nonideal statistics can arise from environmental fluctuations, and
              also justifies our proposed strategy. We implement this strategy in
              the BenchmarkTools Julia package, where it is used in production
              continuous integration (CI) pipelines for developing the Julia
              language and its ecosystem.},
  urldate  = {2022-03-15},
  journal  = {arXiv:1608.04295 [cs]},
  author   = {Chen, Jiahao and Revels, Jarrett},
  month    = aug,
  year     = {2016},
  note     = {arXiv: 1608.04295},
  keywords = {68N30, B.8.1, Computer Science - Performance, D.2.5},
  file     = {arXiv Fulltext PDF:/Users/apodusenko/Zotero/storage/7AFFIUNL/Chen and
              Revels - 2016 - Robust benchmarking in noisy
              environments.pdf:application/pdf;arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/WSF3VDE2/1608.html:text/html}
}

@inproceedings{monteith_turning_2011,
  address   = {San Jose, CA, USA},
  title     = {Turning {Bayesian} model averaging into {Bayesian} model combination},
  doi       = {10.1109/IJCNN.2011.6033566},
  abstract  = {Bayesian methods are theoretically optimal in many situations.
               Bayesian model averaging is generally considered the standard model
               for creating ensembles of learners using Bayesian methods, but this
               technique is often out-performed by more ad hoc methods in
               empirical studies. The reason for this failure has important
               theoretical implications for our understanding of why ensembles
               work. It has been proposed that Bayesian model averaging struggles
               in practice because it accounts for uncertainty about which model
               is correct but still operates under the assumption that only one of
               them is. In order to more effectively access the benefits inherent
               in ensembles, Bayesian strategies should therefore be directed more
               towards model combination rather than the model selection implicit
               in Bayesian model averaging. This work provides empirical
               verification for this hypothesis using several different Bayesian
               model combination approaches tested on a wide variety of
               classification problems. We show that even the most simplistic of
               Bayesian model combination strategies outperforms the traditional
               ad hoc techniques of bagging and boosting, as well as outperforming
               BMA over a wide variety of cases. This suggests that the power of
               ensembles does not come from their ability to account for model
               uncertainty, but instead comes from the changes in representational
               and preferential bias inherent in the process of combining several
               different models.},
  booktitle = {The 2011 {International} {Joint} {Conference} on {Neural} {
               Networks}},
  author    = {Monteith, Kristine and Carroll, James L. and Seppi, Kevin and
               Martinez, Tony},
  month     = jul,
  year      = {2011},
  note      = {ISSN: 2161-4407},
  keywords  = {Bayesian methods, Uncertainty, Mathematical model, Biological
               system modeling, Data models, Accuracy, Bagging},
  pages     = {2657--2663},
  file      = {IEEE Xplore Abstract
               Record:/Users/apodusenko/Zotero/storage/894539MX/6033566.html:text/html;IEEE
               Xplore Full Text PDF:/Users/apodusenko/Zotero/storage/HVVIWQSU/Monteith
               et al. - 2011 - Turning Bayesian model averaging into Bayesian
               mod.pdf:application/pdf}
}

@article{hoeting_bayesian_1999,
  title      = {Bayesian {Model} {Averaging}: {A} {Tutorial}},
  volume     = {14},
  issn       = {0883-4237},
  shorttitle = {Bayesian {Model} {Averaging}},
  url        = {https://www.jstor.org/stable/2676803},
  abstract   = {Standard statistical practice ignores model uncertainty. Data
                analysts typically select a model from some class of models and
                then proceed as if the selected model had generated the data. This
                approach ignores the uncertainty in model selection, leading to
                over-confident inferences and decisions that are more risky than
                one thinks they are. Bayesian model averaging (BMA) provides a
                coherent mechanism for accounting for this model uncertainty.
                Several methods for implementing BMA have recently emerged. We
                discuss these methods and present a number of examples. In these
                examples, BMA provides improved out-of-sample predictive
                performance. We also provide a catalogue of currently available BMA
                software.},
  number     = {4},
  urldate    = {2021-06-11},
  journal    = {Statistical Science},
  author     = {Hoeting, Jennifer A. and Madigan, David and Raftery, Adrian E. and
                Volinsky, Chris T.},
  year       = {1999},
  note       = {Publisher: Institute of Mathematical Statistics},
  pages      = {382--401},
  file       = {Hoeting et al. - 1999 - Bayesian Model Averaging A
                Tutorial.pdf:/Users/apodusenko/Zotero/storage/HKULW8RQ/Hoeting et al. -
                1999 - Bayesian Model Averaging A Tutorial.pdf:application/pdf}
}

@article{podusenko_aida_2022,
  title      = {{AIDA}: {An} {Active} {Inference}-{Based} {Design} {Agent} for {Audio
                } {Processing} {Algorithms}},
  volume     = {2},
  issn       = {2673-8198},
  shorttitle = {{AIDA}},
  url        = {https://www.frontiersin.org/article/10.3389/frsip.2022.842477},
  abstract   = {In this paper we present Active Inference-Based Design Agent
                (AIDA), which is an active inference-based agent that iteratively
                designs a personalized audio processing algorithm through situated
                interactions with a human client. The target application of AIDA is
                to propose on-the-spot the most interesting alternative values for
                the tuning parameters of a hearing aid (HA) algorithm, whenever a
                HA client is not satisfied with their HA performance. AIDA
                interprets searching for the “most interesting alternative” as an
                issue of optimal (acoustic) context-aware Bayesian trial design. In
                computational terms, AIDA is realized as an active inference-based
                agent with an Expected Free Energy criterion for trial design. This
                type of architecture is inspired by neuro-economic models on
                efficient (Bayesian) trial design in brains and implies that AIDA
                comprises generative probabilistic models for acoustic signals and
                user responses. We propose a novel generative model for acoustic
                signals as a sum of time-varying auto-regressive filters and a user
                response model based on a Gaussian Process Classifier. The full
                AIDA agent has been implemented in a factor graph for the
                generative model and all tasks (parameter learning, acoustic
                context classification, trial design, etc.) are realized by
                variational message passing on the factor graph. All verification
                and validation experiments and demonstrations are freely accessible
                at our GitHub repository.},
  urldate    = {2022-03-07},
  journal    = {Frontiers in Signal Processing},
  author     = {Podusenko, Albert and van Erp, Bart and Koudahl, Magnus and de Vries
                , Bert},
  year       = {2022},
  file       = {Full Text PDF:/Users/apodusenko/Zotero/storage/ZNXL3Q74/Podusenko et
                al. - 2022 - AIDA An Active Inference-Based Design Agent for
                A.pdf:application/pdf}
}

@article{farrow_mas3301_nodate,
  title    = {{MAS3301} {Bayesian} {Statistics}},
  language = {en},
  author   = {Farrow, M},
  pages    = {8},
  file     = {Farrow - MAS3301 Bayesian
              Statistics.pdf:/Users/apodusenko/Zotero/storage/L8J37Q4H/Farrow -
              MAS3301 Bayesian Statistics.pdf:application/pdf}
}

@article{ferguson_bayesian_1973,
  title    = {A {Bayesian} {Analysis} of {Some} {Nonparametric} {Problems}},
  volume   = {1},
  issn     = {0090-5364, 2168-8966},
  url      = {
              https://projecteuclid.org/journals/annals-of-statistics/volume-1/issue-2/A-Bayesian-Analysis-of-Some-Nonparametric-Problems/10.1214/aos/1176342360.full
              },
  doi      = {10.1214/aos/1176342360},
  abstract = {The Bayesian approach to statistical problems, though fruitful in
              many ways, has been rather unsuccessful in treating nonparametric
              problems. This is due primarily to the difficulty in finding
              workable prior distributions on the parameter space, which in
              nonparametric ploblems is taken to be a set of probability
              distributions on a given sample space. There are two desirable
              properties of a prior distribution for nonparametric problems. (I)
              The support of the prior distribution should be large--with respect
              to some suitable topology on the space of probability distributions
              on the sample space. (II) Posterior distributions given a sample of
              observations from the true probability distribution should be
              manageable analytically. These properties are antagonistic in the
              sense that one may be obtained at the expense of the other. This
              paper presents a class of prior distributions, called Dirichlet
              process priors, broad in the sense of (I), for which (II) is
              realized, and for which treatment of many nonparametric statistical
              problems may be carried out, yielding results that are comparable
              to the classical theory. In Section 2, we review the properties of
              the Dirichlet distribution needed for the description of the
              Dirichlet process given in Section 3. Briefly, this process may be
              described as follows. Let \${\textbackslash}mathscr\{X\}\$ be a
              space and \${\textbackslash}mathscr\{A\}\$ a \${\textbackslash}
              sigma\$-field of subsets, and let \${\textbackslash}alpha\$ be a
              finite non-null measure on \$({\textbackslash}mathscr\{X\}, {
              \textbackslash}mathscr\{A\})\$. Then a stochastic process \$P\$
              indexed by elements \$A\$ of \${\textbackslash}mathscr\{A\}\$, is
              said to be a Dirichlet process on \$({\textbackslash}mathscr\{X\},
              {\textbackslash}mathscr\{A\})\$ with parameter \${\textbackslash}
              alpha\$ if for any measurable partition \$(A\_1, {\textbackslash}
              cdots, A\_k)\$ of \${\textbackslash}mathscr\{X\}\$, the random
              vector \$(P(A\_1), {\textbackslash}cdots, P(A\_k))\$ has a
              Dirichlet distribution with parameter \$({\textbackslash}alpha(A\_
              1), {\textbackslash}cdots, {\textbackslash}alpha(A\_k)). P\$ may be
              considered a random probability measure on \$({\textbackslash}
              mathscr\{X\}, {\textbackslash}mathscr\{A\})\$, The main theorem
              states that if \$P\$ is a Dirichlet process on \$({\textbackslash}
              mathscr\{X\}, {\textbackslash}mathscr\{A\})\$ with parameter \${
              \textbackslash}alpha\$, and if \$X\_1, {\textbackslash}cdots, X\_n
              \$ is a sample from \$P\$, then the posterior distribution of \$P\$
              given \$X\_1, {\textbackslash}cdots, X\_n\$ is also a Dirichlet
              process on \$({\textbackslash}mathscr\{X\}, {\textbackslash}mathscr
              \{A\})\$ with a parameter \${\textbackslash}alpha + {\textbackslash
              }sum{\textasciicircum}n\_1 {\textbackslash}delta\_\{x\_i\}\$, where
              \${\textbackslash}delta\_x\$ denotes the measure giving mass one to
              the point \$x\$. In Section 4, an alternative definition of the
              Dirichlet process is given. This definition exhibits a version of
              the Dirichlet process that gives probability one to the set of
              discrete probability measures on \$({\textbackslash}mathscr\{X\}, {
              \textbackslash}mathscr\{A\})\$. This is in contrast to Dubins and
              Freedman [2], whose methods for choosing a distribution function on
              the interval [0, 1] lead with probability one to singular
              continuous distributions. Methods of choosing a distribution
              function on [0, 1] that with probability one is absolutely
              continuous have been described by Kraft [7]. The general method of
              choosing a distribution function on [0, 1], described in Section 2
              of Kraft and van Eeden [10], can of course be used to define the
              Dirichlet process on [0, 1]. Special mention must be made of the
              papers of Freedman and Fabius. Freedman [5] defines a notion of
              tailfree for a distribution on the set of all probability measures
              on a countable space \${\textbackslash}mathscr\{X\}\$. For a
              tailfree prior, posterior distribution given a sample from the true
              probability measure may be fairly easily computed. Fabius [3]
              extends the notion of tailfree to the case where \${\textbackslash}
              mathscr\{X\}\$ is the unit interval [0, 1], but it is clear his
              extension may be made to cover quite general \${\textbackslash}
              mathscr\{X\}\$. With such an extension, the Dirichlet process would
              be a special case of a tailfree distribution for which the
              posterior distribution has a particularly simple form. There are
              disadvantages to the fact that \$P\$ chosen by a Dirichlet process
              is discrete with probability one. These appear mainly because in
              sampling from a \$P\$ chosen by a Dirichlet process, we expect
              eventually to see one observation exactly equal to another. For
              example, consider the goodness-of-fit problem of testing the
              hypothesis \$H\_0\$ that a distribution on the interval [0, 1] is
              uniform. If on the alternative hypothesis we place a Dirichlet
              process prior with parameter \${\textbackslash}alpha\$ itself a
              uniform measure on [0, 1], and if we are given a sample of size \$n
              {\textbackslash}geqq 2\$, the only nontrivial nonrandomized Bayes
              rule is to reject \$H\_0\$ if and only if two or more of the
              observations are exactly equal. This is really a test of the
              hypothesis that a distribution is continuous against the hypothesis
              that it is discrete. Thus, there is still a need for a prior that
              chooses a continuous distribution with probability one and yet
              satisfies properties (I) and (II). Some applications in which the
              possible doubling up of the values of the observations plays no
              essential role are presented in Section 5. These include the
              estimation of a distribution function, of a mean, of quantiles, of
              a variance and of a covariance. A two-sample problem is considered
              in which the Mann-Whitney statistic, equivalent to the rank-sum
              statistic, appears naturally. A decision theoretic upper tolerance
              limit for a quantile is also treated. Finally, a hypothesis testing
              problem concerning a quantile is shown to yield the sign test. In
              each of these problems, useful ways of combining prior information
              with the statistical observations appear. Other applications exist.
              In his Ph. D. dissertation [1], Charles Antoniak finds a need to
              consider mixtures of Dirichlet processes. He treats several
              problems, including the estimation of a mixing distribution,
              bio-assay, empirical Bayes problems, and discrimination problems.},
  number   = {2},
  urldate  = {2022-02-28},
  journal  = {The Annals of Statistics},
  author   = {Ferguson, Thomas S.},
  month    = mar,
  year     = {1973},
  note     = {Publisher: Institute of Mathematical Statistics},
  pages    = {209--230},
  file     = {Full Text PDF:/Users/apodusenko/Zotero/storage/STN34FHI/Ferguson -
              1973 - A Bayesian Analysis of Some Nonparametric
              Problems.pdf:application/pdf}
}

@article{gorur_dirichlet_2010,
  title      = {Dirichlet {Process} {Gaussian} {Mixture} {Models}: {Choice} of the {
                Base} {Distribution}},
  volume     = {25},
  issn       = {1000-9000, 1860-4749},
  shorttitle = {Dirichlet {Process} {Gaussian} {Mixture} {Models}},
  url        = {http://link.springer.com/10.1007/s11390-010-9355-8},
  doi        = {10.1007/s11390-010-9355-8},
  abstract   = {In the Bayesian mixture modeling framework it is possible to infer
                the necessary number of components to model the data and therefore
                it is unnecessary to explicitly restrict the number of components.
                Nonparametric mixture models sidestep the problem of ﬁnding the
                “correct” number of mixture components by assuming inﬁnitely many
                components. In this paper Dirichlet process mixture (DPM) models
                are cast as inﬁnite mixture models and inference using Markov chain
                Monte Carlo is described. The speciﬁcation of the priors on the
                model parameters is often guided by mathematical and practical
                convenience. The primary goal of this paper is to compare the
                choice of conjugate and non-conjugate base distributions on a
                particular class of DPM models which is widely used in applications
                , the Dirichlet process Gaussian mixture model (DPGMM). We compare
                computational eﬃciency and modeling performance of DPGMM deﬁned
                using a conjugate and a conditionally conjugate base distribution.
                We show that better density models can result from using a wider
                class of priors with no or only a modest increase in computational
                eﬀort.},
  language   = {en},
  number     = {4},
  urldate    = {2022-02-28},
  journal    = {Journal of Computer Science and Technology},
  author     = {Görür, Dilan and Edward Rasmussen, Carl},
  month      = jul,
  year       = {2010},
  pages      = {653--664},
  file       = {Görür and Edward Rasmussen - 2010 - Dirichlet Process Gaussian Mixture
                Models Choice .pdf:/Users/apodusenko/Zotero/storage/NDGJM2WG/Görür and
                Edward Rasmussen - 2010 - Dirichlet Process Gaussian Mixture Models
                Choice .pdf:application/pdf}
}

@article{escobar_estimating_1994,
  title    = {Estimating {Normal} {Means} with a {Dirichlet} {Process} {Prior}},
  volume   = {89},
  issn     = {0162-1459},
  url      = {https://www.jstor.org/stable/2291223},
  doi      = {10.2307/2291223},
  abstract = {In this article, the Dirichlet process prior is used to provide a
              nonparametric Bayesian estimate of a vector of normal means. In the
              past there have been computational difficulties with this model.
              This article solves the computational difficulties by developing a
              "Gibbs sampler" algorithm. The estimator developed in this article
              is then compared to parametric empirical Bayes estimators (PEB) and
              nonparametric empirical Bayes estimators (NPEB) in a Monte Carlo
              study. The Monte Carlo study demonstrates that in some conditions
              the PEB is better than the NPEB and in other conditions the NPEB is
              better than the PEB. The Monte Carlo study also shows that the
              estimator developed in this article produces estimates that are
              about as good as the PEB when the PEB is better and produces
              estimates that are as good as the NPEB estimator when that method
              is better.},
  number   = {425},
  urldate  = {2022-02-28},
  journal  = {Journal of the American Statistical Association},
  author   = {Escobar, Michael D.},
  year     = {1994},
  note     = {Publisher: [American Statistical Association, Taylor \& Francis, Ltd.]
              },
  pages    = {268--277}
}

@inproceedings{podusenko_message_2021,
  address   = {Gold Coast, Australia},
  title     = {Message {Passing}-{Based} {Inference} in the {Gamma} {Mixture} {Model
               }},
  isbn      = {978-1-72816-338-3},
  url       = {https://ieeexplore.ieee.org/document/9596329/},
  doi       = {10.1109/MLSP52302.2021.9596329},
  urldate   = {2021-11-29},
  booktitle = {2021 {IEEE} 31st {International} {Workshop} on {Machine} {
               Learning} for {Signal} {Processing} ({MLSP})},
  publisher = {IEEE},
  author    = {Podusenko, Albert and van Erp, Bart and Bagaev, Dmitry and {Şenöz,
               İsmail} and de Vries, Bert},
  month     = oct,
  year      = {2021},
  keywords  = {Machine learning, Probability distribution, Conferences,
               Probabilistic logic, Signal processing, Mathematical models, Factor
               Graphs, Expectation-Maximization, Gamma Mixture Model, Message
               Passing, Mixture models, Moment Matching, Probabilistic Inference},
  pages     = {1--6},
  file      = {IEEE Xplore Abstract
               Record:/Users/apodusenko/Zotero/storage/YLX9ABX2/9596329.html:text/html;IEEE
               Xplore Full Text
               PDF:/Users/apodusenko/Zotero/storage/4IBDHHV7/Podusenko et al. - 2021 -
               Message Passing-Based Inference in the Gamma Mixtu.pdf:application/pdf}
}

@article{senoz_ismail_online_2018,
  title   = {Online {State} and {Parameter} {Estimation} in the {Hieararchical} {
             Gaussian} {Filter}},
  journal = {2018 IEEE International Workshop on Machine Learning for Signal
             Processing, MLSP 2018 - Proceedings},
  author  = {Şenöz, İsmail and De Vries, Bert},
  month   = sep,
  year    = {2018},
  file    = {Senoz - 2018 - Online State and Parameter Estimation in the
             Hiear.pdf:/Users/apodusenko/Zotero/storage/54IGTJF6/Senoz - 2018 -
             Online State and Parameter Estimation in the Hiear.pdf:application/pdf}
}

@article{martinez-beneito_bayesian_2008,
  title    = {Bayesian {Markov} switching models for the early detection of
              influenza epidemics},
  volume   = {27},
  issn     = {1097-0258},
  url      = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.3320},
  doi      = {10.1002/sim.3320},
  abstract = {The early detection of outbreaks of diseases is one of the most
              challenging objectives of epidemiological surveillance systems. In
              this paper, a Markov switching model is introduced to determine the
              epidemic and non-epidemic periods from influenza surveillance data:
              the process of differenced incidence rates is modelled either with
              a first-order autoregressive process or with a Gaussian white-noise
              process depending on whether the system is in an epidemic or in a
              non-epidemic phase. The transition between phases of the disease is
              modelled as a Markovian process. Bayesian inference is carried out
              on the former model to detect influenza epidemics at the very
              moment of their onset. Moreover, the proposal provides the
              probability of being in an epidemic state at any given moment. In
              order to validate the methodology, a comparison of its performance
              with other alternatives has been made using influenza illness data
              obtained from the Sanitary Sentinel Network of the Comunitat
              Valenciana, one of the 17 autonomous regions in Spain. Copyright ©
              2008 John Wiley \& Sons, Ltd.},
  language = {en},
  number   = {22},
  urldate  = {2022-02-24},
  journal  = {Statistics in Medicine},
  author   = {Martínez-Beneito, Miguel A. and Conesa, David and López-Quílez,
              Antonio and López-Maside, Aurora},
  year     = {2008},
  note     = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.3320},
  keywords = {hidden Markov models, autoregressive modelling, epidemiological
              surveillance},
  pages    = {4455--4468},
  file     = {Full Text
              PDF:/Users/apodusenko/Zotero/storage/UGKPACSQ/Martínez-Beneito et al. -
              2008 - Bayesian Markov switching models for the early
              det.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/5SZ74NDQ/sim.html:text/html
              }
}

@article{ailliot_markov-switching_2012,
  title    = {Markov-switching autoregressive models for wind time series},
  volume   = {30},
  issn     = {1364-8152},
  url      = {https://www.sciencedirect.com/science/article/pii/S1364815211002222},
  doi      = {10.1016/j.envsoft.2011.10.011},
  abstract = {In this paper, non-homogeneous Markov-Switching Autoregressive
              (MS-AR) models are proposed to describe wind time series. In these
              models, several autoregressive models are used to describe the time
              evolution of the wind speed and the switching between these
              different models is controlled by a hidden Markov chain which
              represents the weather types. We first block the data by month in
              order to remove seasonal components and propose a MS-AR model with
              non-homogeneous autoregressive models to describe daily components.
              Then we discuss extensions where the hidden Markov chain is also
              non-stationary to handle seasonal and interannual fluctuations. The
              different models are fitted using the EM algorithm to a long time
              series of wind speed measurement on the Island of Ouessant
              (France). It is shown that the fitted models are interpretable and
              provide a good description of important properties of the data such
              as the marginal distributions, the second-order structure or the
              length of the stormy and calm periods.},
  language = {en},
  urldate  = {2022-02-24},
  journal  = {Environmental Modelling \& Software},
  author   = {Ailliot, Pierre and Monbet, Valérie},
  month    = apr,
  year     = {2012},
  keywords = {Markov-switching autoregressive model, Multiscale model,
              Overdispersion, Stochastic weather generators, Wind time series},
  pages    = {92--101},
  file     = {ScienceDirect
              Snapshot:/Users/apodusenko/Zotero/storage/2NIAWR3F/S1364815211002222.html:text/html;Submitted
              Version:/Users/apodusenko/Zotero/storage/YCNPQGUR/Ailliot and Monbet -
              2012 - Markov-switching autoregressive models for wind
              ti.pdf:application/pdf}
}

@article{noman_markov-switching_2020,
  title    = {A {Markov}-{Switching} {Model} {Approach} to {Heart} {Sound} {
              Segmentation} and {Classification}},
  volume   = {24},
  issn     = {2168-2208},
  doi      = {10.1109/JBHI.2019.2925036},
  abstract = {Objective: We consider challenges in accurate segmentation of
              heart sound signals recorded under noisy clinical environments for
              subsequent classification of pathological events. Existing
              state-of-the-art solutions to heart sound segmentation use
              probabilistic models such as hidden Markov models (HMMs), which,
              however, are limited by its observation independence assumption and
              rely on pre-extraction of noise-robust features. Methods: We
              propose a Markov-switching autoregressive (MSAR) process to model
              the raw heart sound signals directly, which allows efficient
              segmentation of the cyclical heart sound states according to the
              distinct dependence structure in each state. To enhance robustness,
              we extend the MSAR model to a switching linear dynamic system
              (SLDS) that jointly model both the switching AR dynamics of
              underlying heart sound signals and the noise effects. We introduce
              a novel algorithm via fusion of switching Kalman filter and the
              duration-dependent Viterbi algorithm, which incorporates the
              duration of heart sound states to improve state decoding. Results:
              Evaluated on Physionet/CinC Challenge 2016 dataset, the proposed
              MSAR-SLDS approach significantly outperforms the hidden semi-Markov
              model (HSMM) in heart sound segmentation based on raw signals and
              comparable to a feature-based HSMM. The segmented labels were then
              used to train Gaussian-mixture HMM classifier for identification of
              abnormal beats, achieving high average precision of 86.1\% on the
              same dataset including very noisy recordings. Conclusion: The
              proposed approach shows noticeable performance in heart sound
              segmentation and classification on a large noisy dataset.
              Significance: It is potentially useful in developing automated
              heart monitoring systems for pre-screening of heart pathologies.},
  number   = {3},
  journal  = {IEEE Journal of Biomedical and Health Informatics},
  author   = {Noman, Fuad and Salleh, Sh-Hussain and Ting, Chee-Ming and Samdin,
              S. Balqis and Ombao, Hernando and Hussain, Hadri},
  month    = mar,
  year     = {2020},
  note     = {Conference Name: IEEE Journal of Biomedical and Health Informatics},
  keywords = {Hidden Markov models, Noise measurement, Switches, state-space
              models, Informatics, autoregressive models, Superluminescent diodes
              , Heart, Dynamic clustering, Pathology, regime-switching models,
              Viterbi algorithm},
  pages    = {705--716},
  file     = {IEEE Xplore Abstract
              Record:/Users/apodusenko/Zotero/storage/YMSAT5Y3/8746548.html:text/html;IEEE
              Xplore Full Text PDF:/Users/apodusenko/Zotero/storage/GHP9KICG/Noman et
              al. - 2020 - A Markov-Switching Model Approach to Heart Sound
              S.pdf:application/pdf}
}

@article{ephraim_revisiting_2005,
  title    = {Revisiting autoregressive hidden {Markov} modeling of speech signals},
  volume   = {12},
  issn     = {1558-2361},
  doi      = {10.1109/LSP.2004.840914},
  abstract = {Linear predictive hidden Markov modeling is compared with a simple
              form of the switching autoregressive process. The latter process
              captures existing signal correlation during transitions of the
              Markov chain. Parameter estimation is described using naturally
              stable forward-backward recursions. The switching autoregressive
              model outperformed the linear predictive model in a digit
              recognition task and provided comparable performance to a
              cepstral-based recognizer.},
  number   = {2},
  journal  = {IEEE Signal Processing Letters},
  author   = {Ephraim, Y. and Roberts, W.J.J.},
  month    = feb,
  year     = {2005},
  note     = {Conference Name: IEEE Signal Processing Letters},
  keywords = {Probability, Parameter estimation, Hidden Markov models, Vectors,
              Speech processing, Signal processing, Shape, Autoregressive
              processes, Predictive models, Speech recognition, Prediction
              algorithms, switching autoregressive processes},
  pages    = {166--169},
  file     = {IEEE Xplore Abstract
              Record:/Users/apodusenko/Zotero/storage/VRIZCI9C/1381477.html:text/html;IEEE
              Xplore Full Text PDF:/Users/apodusenko/Zotero/storage/SE5CJVZP/Ephraim
              and Roberts - 2005 - Revisiting autoregressive hidden Markov modeling
              o.pdf:application/pdf}
}

@article{sharpe_mutual_1966,
  title    = {Mutual {Fund} {Performance}},
  volume   = {39},
  url      = {http://www.jstor.org/stable/2351741},
  language = {en},
  number   = {1,},
  journal  = {The Journal of Business},
  author   = {Sharpe, William F.},
  year     = {1966},
  pages    = {119--138},
  file     = {Sharpe - 1966 - Mutual Fund
              Performance.pdf:/Users/apodusenko/Zotero/storage/7N37SXI6/Sharpe - 1966
              - Mutual Fund Performance.pdf:application/pdf}
}

@article{cooper_computational_1990,
  title    = {The computational complexity of probabilistic inference using
              bayesian belief networks},
  volume   = {42},
  issn     = {0004-3702},
  url      = {https://www.sciencedirect.com/science/article/pii/000437029090060D},
  doi      = {https://doi.org/10.1016/0004-3702(90)90060-D},
  abstract = {Bayesian belief networks provide a natural, efficient method for
              representing probabilistic dependencies among a set of variables.
              For these reasons, numerous researchers are exploring the use of
              belief networks as a knowledge representation in artificial
              intelligence. Algorithms have been developed previously for
              efficient probabilistic inference using special classes of belief
              networks. More general classes of belief networks, however, have
              eluded efforts to develop efficient inference algorithms. We show
              that probabilistic inference using belief networks is NP-hard.
              Therefore, it seems unlikely that an exact algorithm can be
              developed to perform probabilistic inference efficiently over all
              classes of belief networks. This result suggests that research
              should be directed away from the search for a general, efficient
              probabilistic inference algorithm, and toward the design of
              efficient special-case, average-case, and approximation algorithms.
              },
  number   = {2},
  journal  = {Artificial Intelligence},
  author   = {Cooper, Gregory F.},
  year     = {1990},
  pages    = {393--405}
}

@book{jansen_machine_2020,
  edition  = {2},
  title    = {Machine learning for algorithmic trading predictive models to extract
              signals from market and alternative data for systematic trading
              strategies with {Python}, second edition},
  isbn     = {978-1-83921-678-7 1-83921-678-6 978-1-83921-771-5 1-83921-771-5},
  abstract = {The explosive growth of digital data has boosted the demand for
              expertise in trading strategies that use machine learning (ML).
              This thoroughly revised and expanded second edition enables you to
              build and evaluate sophisticated supervised, unsupervised, and
              reinforcement learning models. This edition introduces end-to-end
              machine learning for the trading workflow, from the idea and
              feature engineering to model optimization, strategy design, and
              backtesting. It illustrates this workflow using examples that range
              from linear models and tree-based ensembles to deep-learning
              techniques from the cutting edge of the research frontier. This
              revised version shows how to work with market, fundamental, and
              alternative data, such as tick data, minute and daily bars, SEC
              filings, earnings call transcripts, financial news, or satellite
              images to generate tradeable signals. It illustrates how to
              engineer financial features or alpha factors that enable a machine
              learning model to predict returns from price data for US and
              international stocks and ETFs. It also demonstrates how to assess
              the signal content of new features using Alphalens and SHAP values
              and includes a new appendix with over one hundred alpha factor
              examples. By the end of the book, you will be proficient in
              translating machine learning model predictions into a trading
              strategy that operates at daily or intraday horizons, and in
              evaluating its performance.},
  language = {English},
  author   = {Jansen, , Stefan.},
  year     = {2020}
}

@inproceedings{fotiadou_deep_2020,
  title     = {Deep {Convolutional} {Long} {Short}-{Term} {Memory} {Network} for {
               Fetal} {Heart} {Rate} {Extraction}},
  doi       = {10.1109/EMBC44109.2020.9175442},
  abstract  = {Fetal electrocardiography is a valuable alternative to standard
               fetal monitoring. Suppression of the maternal electrocardiogram
               (ECG) in the abdominal measurements, results in fetal ECG signals,
               from which the fetal heart rate (HR) can be determined. This HR
               detection typically requires fetal R-peak detection, which is
               challenging, especially during low signal-to-noise ratio periods,
               caused for example by uterine activity. In this paper, we propose
               the combination of a convolutional neural network and a long
               short-term memory network that directly predicts the fetal HR from
               multichannel fetal ECG. The network is trained on a dataset,
               recorded during labor, while the performance of the method is
               evaluated both on a test dataset and on set-A of the 2013 Physionet
               /Computing in Cardiology Challenge. The algorithm achieved a
               positive percent agreement of 92.1\% and 98.1\% for the two
               datasets respectively, outperforming a top-performing
               state-of-the-art signal processing algorithm.},
  booktitle = {2020 42nd {Annual} {International} {Conference} of the {IEEE} {
               Engineering} in {Medicine} {Biology} {Society} ({EMBC})},
  author    = {Fotiadou, E. and Xu, M. and van Erp, B. and van Sloun, R. J. G. and
               Vullings, R.},
  month     = jul,
  year      = {2020},
  note      = {ISSN: 2694-0604},
  keywords  = {Signal processing algorithms, Signal to noise ratio,
               Electrocardiography, Feature extraction, Fetal heart rate, Scalp},
  pages     = {1--4},
  file      = {IEEE Xplore Abstract
               Record:/Users/apodusenko/Zotero/storage/DQYSKZTD/9175442.html:text/html
               }
}

@article{keller_bayesian_2017,
  title    = {Bayesian {Model} {Averaging} {By} {Mixture} {Modeling}},
  url      = {http://arxiv.org/abs/1711.10016},
  abstract = {A new and numerically efficient method for Bayes factor
              computation and Bayesian model averaging, seen as a special case of
              the mixture model approach for Bayesian model selection in the
              seminal work of Kamari, 2014. Inheriting from the good properties
              of this approach, it allows to extend classical Bayesian model
              selection/averaging to cases where improper priors are chosen for
              the common parameter of the candidate models.},
  urldate  = {2017-11-29},
  journal  = {arXiv:1711.10016 [stat]},
  author   = {Keller, Merlin and Kamary, Kaniav},
  month    = nov,
  year     = {2017},
  note     = {arXiv: 1711.10016},
  keywords = {Statistics - Methodology},
  file     = {arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/UKZWH2DV/1711.html:text/html;Keller
              en Kamary - 2017 - Bayesian Model Averaging By Mixture
              Modeling.pdf:/Users/apodusenko/Zotero/storage/522AC33R/Keller en Kamary
              - 2017 - Bayesian Model Averaging By Mixture
              Modeling.pdf:application/pdf}
}

@article{kamary_testing_2014-1,
  title    = {Testing hypotheses via a mixture estimation model},
  url      = {http://arxiv.org/abs/1412.2044},
  abstract = {We consider a novel paradigm for Bayesian testing of hypotheses
              and Bayesian model comparison. Our alternative to the traditional
              construction of posterior probabilities that a given hypothesis is
              true or that the data originates from a specific model is to
              consider the models under comparison as components of a mixture
              model. We therefore replace the original testing problem with an
              estimation one that focus on the probability weight of a given
              model within a mixture model. We analyze the sensitivity on the
              resulting posterior distribution on the weights of various prior
              modeling on the weights. We stress that a major appeal in using
              this novel perspective is that generic improper priors are
              acceptable, while not putting convergence in jeopardy. Among other
              features, this allows for a resolution of the Lindley-Jeffreys
              paradox. When using a reference Beta B(a,a) prior on the mixture
              weights, we note that the sensitivity of the posterior estimations
              of the weights to the choice of a vanishes with the sample size
              increasing and avocate the default choice a=0.5, derived from
              Rousseau and Mengersen (2011). Another feature of this easily
              implemented alternative to the classical Bayesian solution is that
              the speeds of convergence of the posterior mean of the weight and
              of the corresponding posterior probability are quite similar.},
  urldate  = {2019-07-21},
  journal  = {arXiv:1412.2044 [stat]},
  author   = {Kamary, Kaniav and Mengersen, Kerrie and Robert, Christian P. and
              Rousseau, Judith},
  month    = dec,
  year     = {2014},
  note     = {arXiv: 1412.2044},
  keywords = {Statistics - Methodology},
  file     = {arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/J9LSHG6E/1412.html:text/html;Kamary
              et al. - 2014 - Testing hypotheses via a mixture estimation
              model.pdf:/Users/apodusenko/Zotero/storage/3CYSXVY6/Kamary et al. -
              2014 - Testing hypotheses via a mixture estimation
              model.pdf:application/pdf}
}

@techreport{kagan_vitro_2021,
  title       = {In vitro neurons learn and exhibit sentience when embodied in a
                 simulated game-world},
  copyright   = {© 2021, Posted by Cold Spring Harbor Laboratory. This pre-print
                 is available under a Creative Commons License
                 (Attribution-NonCommercial-NoDerivs 4.0 International), CC
                 BY-NC-ND 4.0, as described at
                 http://creativecommons.org/licenses/by-nc-nd/4.0/},
  url         = {https://www.biorxiv.org/content/10.1101/2021.12.02.471005v2},
  abstract    = {Integrating neurons into digital systems to leverage their innate
                 intelligence may enable performance infeasible with silicon alone,
                 along with providing insight into the cellular origin of
                 intelligence. We developed DishBrain, a system which exhibits
                 natural intelligence by harnessing the inherent adaptive
                 computation of neurons in a structured environment. In vitro neural
                 networks from human or rodent origins, are integrated with in
                 silico computing via high-density multielectrode array. Through
                 electrophysiological stimulation and recording, cultures were
                 embedded in a simulated game-world, mimicking the arcade game
                 ‘Pong’. Applying a previously untestable theory of active inference
                 via the Free Energy Principle, we found that learning was apparent
                 within five minutes of real-time gameplay, not observed in control
                 conditions. Further experiments demonstrate the importance of
                 closed-loop structured feedback in eliciting learning over time.
                 Cultures display the ability to self-organise in a goal-directed
                 manner in response to sparse sensory information about the
                 consequences of their actions.},
  language    = {en},
  urldate     = {2022-01-31},
  institution = {bioRxiv},
  author      = {Kagan, Brett J. and Kitchen, Andy C. and Tran, Nhi T. and Parker,
                 Bradyn J. and Bhat, Anjali and Rollo, Ben and Razi, Adeel and Friston
                 , Karl J.},
  month       = dec,
  year        = {2021},
  doi         = {10.1101/2021.12.02.471005},
  note        = {Section: New Results Type: article},
  pages       = {2021.12.02.471005},
  file        = {Kagan et al. - 2021 - In vitro neurons learn and exhibit sentience
                 when .pdf:/Users/apodusenko/Zotero/storage/M5P49UK5/Kagan et al. - 2021
                 - In vitro neurons learn and exhibit sentience when
                 .pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/ND35T47R/2021.12.02.html:text/html
                 }
}

@article{vadlamani_physics_2020,
  title     = {Physics successfully implements {Lagrange} multiplier optimization},
  volume    = {117},
  copyright = {Copyright © 2020 the Author(s). Published by PNAS..
               https://creativecommons.org/licenses/by-nc-nd/4.0/This open access
               article is distributed under Creative Commons
               Attribution-NonCommercial-NoDerivatives License 4.0 (CC BY-NC-ND).
               },
  issn      = {0027-8424, 1091-6490},
  url       = {https://www.pnas.org/content/117/43/26639},
  doi       = {10.1073/pnas.2015192117},
  abstract  = {Optimization is a major part of human effort. While being
               mathematical, optimization is also built into physics. For example,
               physics has the Principle of Least Action; the Principle of Minimum
               Power Dissipation, also called Minimum Entropy Generation; and the
               Variational Principle. Physics also has Physical Annealing, which,
               of course, preceded computational Simulated Annealing. Physics has
               the Adiabatic Principle, which, in its quantum form, is called
               Quantum Annealing. Thus, physical machines can solve the
               mathematical problem of optimization, including constraints. Binary
               constraints can be built into the physical optimization. In that
               case, the machines are digital in the same sense that a flip–flop
               is digital. A wide variety of machines have had recent success at
               optimizing the Ising magnetic energy. We demonstrate in this paper
               that almost all those machines perform optimization according to
               the Principle of Minimum Power Dissipation as put forth by Onsager.
               Further, we show that this optimization is in fact equivalent to
               Lagrange multiplier optimization for constrained problems. We find
               that the physical gain coefficients that drive those systems
               actually play the role of the corresponding Lagrange multipliers.},
  language  = {en},
  number    = {43},
  urldate   = {2021-01-13},
  journal   = {Proceedings of the National Academy of Sciences},
  author    = {Vadlamani, Sri Krishna and Xiao, Tianyao Patrick and Yablonovitch,
               Eli},
  month     = oct,
  year      = {2020},
  pmid      = {33046659},
  note      = {Publisher: National Academy of Sciences Section: Physical Sciences},
  keywords  = {hardware accelerators, Ising solvers, physical optimization},
  pages     = {26639--26650},
  file      = {
               pnas.2015192117.sapp.pdf:/Users/apodusenko/Zotero/storage/WGWUM7J2/pnas.2015192117.sapp.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/V2T3W6TH/26639.html:text/html;Vadlamani
               et al. - 2020 - Physics successfully implements Lagrange
               multiplie.pdf:/Users/apodusenko/Zotero/storage/3FHRXLP4/Vadlamani et
               al. - 2020 - Physics successfully implements Lagrange
               multiplie.pdf:application/pdf}
}

@article{isomura_vitro_2018,
  title     = {In vitro neural networks minimise variational free energy},
  volume    = {8},
  copyright = {2018 The Author(s)},
  issn      = {2045-2322},
  url       = {https://www.nature.com/articles/s41598-018-35221-w},
  doi       = {10.1038/s41598-018-35221-w},
  abstract  = {In this work, we address the neuronal encoding problem from a
               Bayesian perspective. Specifically, we ask whether neuronal
               responses in an in vitro neuronal network are consistent with ideal
               Bayesian observer responses under the free energy principle. In
               brief, we stimulated an in vitro cortical cell culture with
               stimulus trains that had a known statistical structure. We then
               asked whether recorded neuronal responses were consistent with
               variational message passing based upon free energy minimisation
               (i.e., evidence maximisation). Effectively, this required us to
               solve two problems: first, we had to formulate the Bayes-optimal
               encoding of the causes or sources of sensory stimulation, and then
               show that these idealised responses could account for observed
               electrophysiological responses. We describe a simulation of an
               optimal neural network (i.e., the ideal Bayesian neural code) and
               then consider the mapping from idealised in silico responses to
               recorded in vitro responses. Our objective was to find evidence for
               functional specialisation and segregation in the in vitro neural
               network that reproduced in silico learning via free energy
               minimisation. Finally, we combined the in vitro and in silico
               results to characterise learning in terms of trajectories in a
               variational information plane of accuracy and complexity.},
  language  = {en},
  number    = {1},
  urldate   = {2022-01-31},
  journal   = {Scientific Reports},
  author    = {Isomura, Takuya and Friston, Karl},
  month     = nov,
  year      = {2018},
  note      = {Number: 1 Publisher: Nature Publishing Group},
  keywords  = {Learning algorithms, Neural encoding},
  pages     = {16926},
  file      = {Isomura and Friston - 2018 - In vitro neural networks minimise
               variational free.pdf:/Users/apodusenko/Zotero/storage/EI4TJ2QB/Isomura
               and Friston - 2018 - In vitro neural networks minimise variational
               free.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/UYNDNT6D/s41598-018-35221-w.html:text/html
               }
}

@article{opper_variational_2008,
  title    = {The {Variational} {Gaussian} {Approximation} {Revisited}},
  volume   = {21},
  doi      = {10.1162/neco.2008.08-07-592},
  abstract = {The variational approximation of posterior distributions by
              multivariate gaussians has been much less popular in the machine
              learning community compared to the corresponding approximation by
              factorizing distributions. This is for a good reason: the gaussian
              approximation is in general plagued by an Omicron(N)(2) number of
              variational parameters to be optimized, N being the number of
              random variables. In this letter, we discuss the relationship
              between the Laplace and the variational approximation, and we show
              that for models with gaussian priors and factorizing likelihoods,
              the number of variational parameters is actually Omicron(N). The
              approach is applied to gaussian process regression with nongaussian
              likelihoods.},
  journal  = {Neural computation},
  author   = {Opper, Manfred and Archambeau, Cedric},
  month    = oct,
  year     = {2008},
  pages    = {786--92},
  file     = {Full Text PDF:/Users/apodusenko/Zotero/storage/TDF99X9Q/Opper and
              Archambeau - 2008 - The Variational Gaussian Approximation
              Revisited.pdf:application/pdf}
}

@article{mandt_stochastic_2018,
  title    = {Stochastic {Gradient} {Descent} as {Approximate} {Bayesian} {
              Inference}},
  url      = {http://arxiv.org/abs/1704.04289},
  abstract = {Stochastic Gradient Descent with a constant learning rate
              (constant SGD) simulates a Markov chain with a stationary
              distribution. With this perspective, we derive several new results.
              (1) We show that constant SGD can be used as an approximate
              Bayesian posterior inference algorithm. Specifically, we show how
              to adjust the tuning parameters of constant SGD to best match the
              stationary distribution to a posterior, minimizing the
              Kullback-Leibler divergence between these two distributions. (2) We
              demonstrate that constant SGD gives rise to a new variational EM
              algorithm that optimizes hyperparameters in complex probabilistic
              models. (3) We also propose SGD with momentum for sampling and show
              how to adjust the damping coefficient accordingly. (4) We analyze
              MCMC algorithms. For Langevin Dynamics and Stochastic Gradient
              Fisher Scoring, we quantify the approximation errors due to finite
              learning rates. Finally (5), we use the stochastic process
              perspective to give a short proof of why Polyak averaging is
              optimal. Based on this idea, we propose a scalable approximate MCMC
              algorithm, the Averaged Stochastic Gradient Sampler.},
  urldate  = {2021-10-13},
  journal  = {arXiv:1704.04289 [cs, stat]},
  author   = {Mandt, Stephan and Hoffman, Matthew D. and Blei, David M.},
  month    = jan,
  year     = {2018},
  note     = {arXiv: 1704.04289},
  keywords = {Statistics - Machine Learning, Computer Science - Machine Learning
              },
  file     = {arXiv Fulltext PDF:/Users/apodusenko/Zotero/storage/NL3TH99P/Mandt et
              al. - 2018 - Stochastic Gradient Descent as Approximate
              Bayesia.pdf:application/pdf;arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/LK9R4H4R/1704.html:text/html}
}

@article{vehtari_rank-normalization_2021,
  title      = {Rank-normalization, folding, and localization: {An} improved \${
                \textbackslash}widehat\{{R}\}\$ for assessing convergence of {MCMC}},
  volume     = {16},
  issn       = {1936-0975},
  shorttitle = {Rank-normalization, folding, and localization},
  url        = {http://arxiv.org/abs/1903.08008},
  doi        = {10.1214/20-BA1221},
  abstract   = {Markov chain Monte Carlo is a key computational tool in Bayesian
                statistics, but it can be challenging to monitor the convergence of
                an iterative stochastic algorithm. In this paper we show that the
                convergence diagnostic \${\textbackslash}widehat\{R\}\$ of Gelman
                and Rubin (1992) has serious flaws. Traditional \${\textbackslash}
                widehat\{R\}\$ will fail to correctly diagnose convergence failures
                when the chain has a heavy tail or when the variance varies across
                the chains. In this paper we propose an alternative rank-based
                diagnostic that fixes these problems. We also introduce a
                collection of quantile-based local efficiency measures, along with
                a practical approach for computing Monte Carlo error estimates for
                quantiles. We suggest that common trace plots should be replaced
                with rank plots from multiple chains. Finally, we give
                recommendations for how these methods should be used in practice.},
  number     = {2},
  urldate    = {2021-10-22},
  journal    = {Bayesian Analysis},
  author     = {Vehtari, Aki and Gelman, Andrew and Simpson, Daniel and Carpenter,
                Bob and Bürkner, Paul-Christian},
  month      = jun,
  year       = {2021},
  note       = {arXiv: 1903.08008},
  keywords   = {Statistics - Computation, Statistics - Methodology},
  file       = {arXiv Fulltext PDF:/Users/apodusenko/Zotero/storage/7VADH7RX/Vehtari
                et al. - 2021 - Rank-normalization, folding, and localization An
                .pdf:application/pdf;arXiv.org
                Snapshot:/Users/apodusenko/Zotero/storage/Y5HQ64B5/1903.html:text/html}
}

@article{vehtari_pareto_2021,
  title    = {Pareto {Smoothed} {Importance} {Sampling}},
  url      = {http://arxiv.org/abs/1507.02646},
  abstract = {Importance weighting is a general way to adjust Monte Carlo
              integration to account for draws from the wrong distribution, but
              the resulting estimate can be noisy when the importance ratios have
              a heavy right tail. This routinely occurs when there are aspects of
              the target distribution that are not well captured by the
              approximating distribution, in which case more stable estimates can
              be obtained by modifying extreme importance ratios. We present a
              new method for stabilizing importance weights using a generalized
              Pareto distribution fit to the upper tail of the distribution of
              the simulated importance ratios. The method, which empirically
              performs better than existing methods for stabilizing importance
              sampling estimates, includes stabilized effective sample size
              estimates, Monte Carlo error estimates and convergence diagnostics.
              },
  urldate  = {2021-10-13},
  journal  = {arXiv:1507.02646 [stat]},
  author   = {Vehtari, Aki and Simpson, Daniel and Gelman, Andrew and Yao, Yuling
              and Gabry, Jonah},
  month    = feb,
  year     = {2021},
  note     = {arXiv: 1507.02646},
  keywords = {Statistics - Machine Learning, Statistics - Computation,
              Statistics - Methodology},
  file     = {arXiv Fulltext PDF:/Users/apodusenko/Zotero/storage/GMCM33IX/Vehtari
              et al. - 2021 - Pareto Smoothed Importance
              Sampling.pdf:application/pdf;arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/F9EB394W/1507.html:text/html}
}

@article{fisher_mathematical_1922,
  title    = {On the mathematical foundations of theoretical statistics},
  volume   = {222},
  url      = {https://royalsocietypublishing.org/doi/10.1098/rsta.1922.0009},
  doi      = {10.1098/rsta.1922.0009},
  abstract = {Several reasons have contributed to the prolonged neglect into
              which the study of statistics, in its theoretical aspects, has
              fallen. In spite of the immense amount of fruitful labour which has
              been expended in its practical applications, the basic principles
              of this organ of science are still in a state of obscurity, and it
              cannot be denied that, during the recent rapid development of
              practical methods, fundamental problems have been ignored and
              fundamental paradoxes left unresolved. This anomalous state of
              statistical science is strikingly exemplified by a recent paper
              entitled "The Fundamental Problem of Practical Statistics," in
              which one of the most eminent of modern statisticians presents what
              purports to be a general proof of BAYES' postulate, a proof which,
              in the opinion of a second statistician of equal eminence, "seems
              to rest upon a very peculiar -- not to say hardly supposable --
              relation."},
  number   = {594-604},
  urldate  = {2021-09-09},
  journal  = {Philosophical Transactions of the Royal Society of London. Series A
              , Containing Papers of a Mathematical or Physical Character},
  author   = {Fisher, R. A. and Russell, Edward John},
  month    = jan,
  year     = {1922},
  note     = {Publisher: Royal Society},
  pages    = {309--368},
  file     = {Full Text PDF:/Users/apodusenko/Zotero/storage/V7RJWSEA/Fisher and
              Russell - 1922 - On the mathematical foundations of theoretical
              sta.pdf:application/pdf}
}

@article{winn_variational_2005,
  title    = {Variational {Message} {Passing}},
  volume   = {6},
  issn     = {1533-7928},
  url      = {http://jmlr.org/papers/v6/winn05a.html},
  abstract = {Bayesian inference is now widely established as one of the
              principal foundations for machine learning. In practice, exact
              inference is rarely possible, and so a variety of approximation
              techniques have been developed, one of the most widely used being a
              deterministic framework called variational inference. In this paper
              we introduce Variational Message Passing (VMP), a general purpose
              algorithm for applying variational inference to Bayesian Networks.
              Like belief propagation, VMP proceeds by sending messages between
              nodes in the network and updating posterior beliefs using local
              operations at each node. Each such update increases a lower bound
              on the log evidence (unless already at a local maximum). In
              contrast to belief propagation, VMP can be applied to a very
              general class of conjugate-exponential models because it uses a
              factorised variational approximation. Furthermore, by introducing
              additional variational parameters, VMP can be applied to models
              containing non-conjugate distributions. The VMP framework also
              allows the lower bound to be evaluated, and this can be used both
              for model comparison and for detection of convergence. Variational
              message passing has been implemented in the form of a general
              purpose inference engine called VIBES ('Variational Inference for
              BayEsian networkS') which allows models to be specified graphically
              and then solved variationally without recourse to coding.},
  number   = {23},
  urldate  = {2021-10-14},
  journal  = {Journal of Machine Learning Research},
  author   = {Winn, John and Bishop, Christopher M.},
  year     = {2005},
  pages    = {661--694},
  file     = {Full Text PDF:/Users/apodusenko/Zotero/storage/Q7NFHIE4/Winn and
              Bishop - 2005 - Variational Message Passing.pdf:application/pdf}
}

@article{mandt_stochastic_2018-1,
  title    = {Stochastic {Gradient} {Descent} as {Approximate} {Bayesian} {
              Inference}},
  url      = {http://arxiv.org/abs/1704.04289},
  abstract = {Stochastic Gradient Descent with a constant learning rate
              (constant SGD) simulates a Markov chain with a stationary
              distribution. With this perspective, we derive several new results.
              (1) We show that constant SGD can be used as an approximate
              Bayesian posterior inference algorithm. Specifically, we show how
              to adjust the tuning parameters of constant SGD to best match the
              stationary distribution to a posterior, minimizing the
              Kullback-Leibler divergence between these two distributions. (2) We
              demonstrate that constant SGD gives rise to a new variational EM
              algorithm that optimizes hyperparameters in complex probabilistic
              models. (3) We also propose SGD with momentum for sampling and show
              how to adjust the damping coefficient accordingly. (4) We analyze
              MCMC algorithms. For Langevin Dynamics and Stochastic Gradient
              Fisher Scoring, we quantify the approximation errors due to finite
              learning rates. Finally (5), we use the stochastic process
              perspective to give a short proof of why Polyak averaging is
              optimal. Based on this idea, we propose a scalable approximate MCMC
              algorithm, the Averaged Stochastic Gradient Sampler.},
  urldate  = {2021-10-22},
  journal  = {arXiv:1704.04289 [cs, stat]},
  author   = {Mandt, Stephan and Hoffman, Matthew D. and Blei, David M.},
  month    = jan,
  year     = {2018},
  note     = {arXiv: 1704.04289},
  keywords = {Statistics - Machine Learning, Computer Science - Machine Learning
              },
  file     = {arXiv Fulltext PDF:/Users/apodusenko/Zotero/storage/XLIJFUPD/Mandt et
              al. - 2018 - Stochastic Gradient Descent as Approximate
              Bayesia.pdf:application/pdf;arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/EMC6857D/1704.html:text/html}
}

@article{mandt_stochastic_2018-2,
  title    = {Stochastic {Gradient} {Descent} as {Approximate} {Bayesian} {
              Inference}},
  url      = {http://arxiv.org/abs/1704.04289},
  abstract = {Stochastic Gradient Descent with a constant learning rate
              (constant SGD) simulates a Markov chain with a stationary
              distribution. With this perspective, we derive several new results.
              (1) We show that constant SGD can be used as an approximate
              Bayesian posterior inference algorithm. Specifically, we show how
              to adjust the tuning parameters of constant SGD to best match the
              stationary distribution to a posterior, minimizing the
              Kullback-Leibler divergence between these two distributions. (2) We
              demonstrate that constant SGD gives rise to a new variational EM
              algorithm that optimizes hyperparameters in complex probabilistic
              models. (3) We also propose SGD with momentum for sampling and show
              how to adjust the damping coefficient accordingly. (4) We analyze
              MCMC algorithms. For Langevin Dynamics and Stochastic Gradient
              Fisher Scoring, we quantify the approximation errors due to finite
              learning rates. Finally (5), we use the stochastic process
              perspective to give a short proof of why Polyak averaging is
              optimal. Based on this idea, we propose a scalable approximate MCMC
              algorithm, the Averaged Stochastic Gradient Sampler.},
  urldate  = {2021-10-22},
  journal  = {arXiv:1704.04289 [cs, stat]},
  author   = {Mandt, Stephan and Hoffman, Matthew D. and Blei, David M.},
  month    = jan,
  year     = {2018},
  note     = {arXiv: 1704.04289},
  keywords = {Statistics - Machine Learning, Computer Science - Machine Learning
              },
  file     = {arXiv Fulltext PDF:/Users/apodusenko/Zotero/storage/FGV2BGYS/Mandt et
              al. - 2018 - Stochastic Gradient Descent as Approximate
              Bayesia.pdf:application/pdf;arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/8BUK6PCF/1704.html:text/html}
}

@article{dhaka_robust_2020,
  title    = {Robust, {Accurate} {Stochastic} {Optimization} for {Variational} {
              Inference}},
  url      = {http://arxiv.org/abs/2009.00666},
  abstract = {We consider the problem of fitting variational posterior
              approximations using stochastic optimization methods. The
              performance of these approximations depends on (1) how well the
              variational family matches the true posterior distribution,(2) the
              choice of divergence, and (3) the optimization of the variational
              objective. We show that even in the best-case scenario when the
              exact posterior belongs to the assumed variational family, common
              stochastic optimization methods lead to poor variational
              approximations if the problem dimension is moderately large. We
              also demonstrate that these methods are not robust across diverse
              model types. Motivated by these findings, we develop a more robust
              and accurate stochastic optimization framework by viewing the
              underlying optimization algorithm as producing a Markov chain. Our
              approach is theoretically motivated and includes a diagnostic for
              convergence and a novel stopping rule, both of which are robust to
              noisy evaluations of the objective function. We show empirically
              that the proposed framework works well on a diverse set of models:
              it can automatically detect stochastic optimization failure or
              inaccurate variational approximation},
  urldate  = {2021-10-13},
  journal  = {arXiv:2009.00666 [cs, stat]},
  author   = {Dhaka, Akash Kumar and Catalina, Alejandro and Andersen, Michael
              Riis and Magnusson, Måns and Huggins, Jonathan H. and Vehtari, Aki},
  month    = sep,
  year     = {2020},
  note     = {arXiv: 2009.00666},
  keywords = {Statistics - Machine Learning, Statistics - Methodology, Computer
              Science - Machine Learning},
  file     = {arXiv Fulltext PDF:/Users/apodusenko/Zotero/storage/LDLREBPL/Dhaka et
              al. - 2020 - Robust, Accurate Stochastic Optimization for
              Varia.pdf:application/pdf;arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/TCGVB8CI/2009.html:text/html}
}

@book{gelman_bayesian_2015,
  address   = {New York},
  edition   = {3},
  title     = {Bayesian {Data} {Analysis}},
  isbn      = {978-0-429-11307-9},
  abstract  = {Winner of the 2016 De Groot Prize from the International Society
               for Bayesian AnalysisNow in its third edition, this classic book is
               widely considered the leading text on Bayesian methods, lauded for
               its accessible, practical approach to analyzing data and solving
               research problems. Bayesian Data Analysis, Third Edition continues
               to take an applied},
  publisher = {Chapman and Hall/CRC},
  author    = {Gelman, Andrew and Carlin, John B. and Stern, Hal S. and Dunson,
               David B. and Vehtari, Aki and Rubin, Donald B.},
  month     = jul,
  year      = {2015},
  doi       = {10.1201/b16018}
}

@article{kucukelbir_automatic_2015-1,
  title    = {Automatic {Variational} {Inference} in {Stan}},
  url      = {http://arxiv.org/abs/1506.03431},
  abstract = {Variational inference is a scalable technique for approximate
              Bayesian inference. Deriving variational inference algorithms
              requires tedious model-specific calculations; this makes it
              difficult to automate. We propose an automatic variational
              inference algorithm, automatic differentiation variational
              inference (ADVI). The user only provides a Bayesian model and a
              dataset; nothing else. We make no conjugacy assumptions and support
              a broad class of models. The algorithm automatically determines an
              appropriate variational family and optimizes the variational
              objective. We implement ADVI in Stan (code available now), a
              probabilistic programming framework. We compare ADVI to MCMC
              sampling across hierarchical generalized linear models,
              nonconjugate matrix factorization, and a mixture model. We train
              the mixture model on a quarter million images. With ADVI we can use
              variational inference on any model we write in Stan.},
  urldate  = {2021-10-13},
  journal  = {arXiv:1506.03431 [stat]},
  author   = {Kucukelbir, Alp and Ranganath, Rajesh and Gelman, Andrew and Blei,
              David M.},
  month    = jun,
  year     = {2015},
  note     = {arXiv: 1506.03431},
  keywords = {Statistics - Machine Learning},
  file     = {arXiv Fulltext
              PDF:/Users/apodusenko/Zotero/storage/9EWGAF9A/Kucukelbir et al. - 2015
              - Automatic Variational Inference in Stan.pdf:application/pdf;arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/AG33Y6U4/1506.html:text/html}
}

@article{kingma_adam_2017,
  title      = {Adam: {A} {Method} for {Stochastic} {Optimization}},
  shorttitle = {Adam},
  url        = {http://arxiv.org/abs/1412.6980},
  abstract   = {We introduce Adam, an algorithm for first-order gradient-based
                optimization of stochastic objective functions, based on adaptive
                estimates of lower-order moments. The method is straightforward to
                implement, is computationally efficient, has little memory
                requirements, is invariant to diagonal rescaling of the gradients,
                and is well suited for problems that are large in terms of data
                and/or parameters. The method is also appropriate for
                non-stationary objectives and problems with very noisy and/or
                sparse gradients. The hyper-parameters have intuitive
                interpretations and typically require little tuning. Some
                connections to related algorithms, on which Adam was inspired, are
                discussed. We also analyze the theoretical convergence properties
                of the algorithm and provide a regret bound on the convergence rate
                that is comparable to the best known results under the online
                convex optimization framework. Empirical results demonstrate that
                Adam works well in practice and compares favorably to other
                stochastic optimization methods. Finally, we discuss AdaMax, a
                variant of Adam based on the infinity norm.},
  urldate    = {2021-10-13},
  journal    = {arXiv:1412.6980 [cs]},
  author     = {Kingma, Diederik P. and Ba, Jimmy},
  month      = jan,
  year       = {2017},
  note       = {arXiv: 1412.6980},
  keywords   = {Computer Science - Machine Learning},
  file       = {arXiv Fulltext PDF:/Users/apodusenko/Zotero/storage/HY7F375C/Kingma
                and Ba - 2017 - Adam A Method for Stochastic
                Optimization.pdf:application/pdf;arXiv.org
                Snapshot:/Users/apodusenko/Zotero/storage/XXURE73R/1412.html:text/html}
}

@article{lin_handling_2020,
  title    = {Handling the {Positive}-{Definite} {Constraint} in the {Bayesian} {
              Learning} {Rule}},
  url      = {http://arxiv.org/abs/2002.10060},
  abstract = {The Bayesian learning rule is a natural-gradient variational
              inference method, which not only contains many existing learning
              algorithms as special cases but also enables the design of new
              algorithms. Unfortunately, when variational parameters lie in an
              open constraint set, the rule may not satisfy the constraint and
              requires line-searches which could slow down the algorithm. In this
              work, we address this issue for positive-definite constraints by
              proposing an improved rule that naturally handles the constraints.
              Our modification is obtained by using Riemannian gradient methods,
              and is valid when the approximation attains a {\textbackslash}emph
              \{block-coordinate natural parameterization\} (e.g., Gaussian
              distributions and their mixtures). We propose a principled way to
              derive Riemannian gradients and retractions from scratch. Our
              method outperforms existing methods without any significant
              increase in computation. Our work makes it easier to apply the rule
              in the presence of positive-definite constraints in parameter
              spaces.},
  urldate  = {2021-09-17},
  journal  = {arXiv:2002.10060 [cs, stat]},
  author   = {Lin, Wu and Schmidt, Mark and Khan, Mohammad Emtiyaz},
  month    = oct,
  year     = {2020},
  note     = {arXiv: 2002.10060},
  keywords = {Statistics - Machine Learning, Computer Science - Machine Learning
              },
  file     = {arXiv Fulltext PDF:/Users/apodusenko/Zotero/storage/TTSILZIV/Lin et
              al. - 2020 - Handling the Positive-Definite Constraint in the
              B.pdf:application/pdf;arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/DDB62IJK/2002.html:text/html}
}

@article{gelman_bayesian_nodate,
  title    = {Bayesian {Data} {Analysis} {Third} edition (with errors ﬁxed as of 6
              {April} 2021)},
  language = {en},
  author   = {Gelman, Andrew and Carlin, John B and Stern, Hal S and Dunson, David
              B and Vehtari, Aki and Rubin, Donald B},
  pages    = {677},
  file     = {Gelman et al. - Bayesian Data Analysis Third edition (with errors
              .pdf:/Users/apodusenko/Zotero/storage/CPNZHEKL/Gelman et al. - Bayesian
              Data Analysis Third edition (with errors .pdf:application/pdf}
}

@inproceedings{ranganath_adaptive_2013,
  title     = {An {Adaptive} {Learning} {Rate} for {Stochastic} {Variational} {
               Inference}},
  url       = {https://proceedings.mlr.press/v28/ranganath13.html},
  abstract  = {Stochastic variational inference finds good posterior
               approximations of probabilistic models with very large data sets.
               It optimizes the variational objective with stochastic optimization
               , following noisy estimates of the natural gradient. Operationally,
               stochastic inference iteratively subsamples from the data, analyzes
               the subsample, and updates parameters with a decreasing learning
               rate. However, the algorithm is sensitive to that rate, which
               usually requires hand-tuning to each application. We solve this
               problem by developing an adaptive learning rate for stochastic
               inference. Our method requires no tuning and is easily implemented
               with computations already made in the algorithm. We demonstrate our
               approach with latent Dirichlet allocation applied to three large
               text corpora. Inference with the adaptive learning rate converges
               faster and to a better approximation than the best settings of
               hand-tuned rates.},
  language  = {en},
  urldate   = {2021-10-13},
  booktitle = {Proceedings of the 30th {International} {Conference} on {Machine}
               {Learning}},
  publisher = {PMLR},
  author    = {Ranganath, Rajesh and Wang, Chong and David, Blei and Xing, Eric},
  month     = may,
  year      = {2013},
  note      = {ISSN: 1938-7228},
  pages     = {298--306},
  file      = {Full Text PDF:/Users/apodusenko/Zotero/storage/Q3ULZI7Y/Ranganath et
               al. - 2013 - An Adaptive Learning Rate for Stochastic
               Variation.pdf:application/pdf}
}

@article{zhang_new_2009,
  title    = {A {New} and {Efficient} {Estimation} {Method} for the {Generalized} {
              Pareto} {Distribution}},
  volume   = {51},
  issn     = {0040-1706},
  url      = {https://doi.org/10.1198/tech.2009.08017},
  doi      = {10.1198/tech.2009.08017},
  abstract = {The generalized Pareto distribution (GPD) is widely used to model
              extreme values, for example, exceedences over thresholds, in
              modeling floods. Existing methods for estimating parameters have
              theoretical or computational defects. An efficient new estimator is
              proposed, which is computationally easy, free from the problems
              observed in traditional approaches, and performs well compared with
              existing estimators. A numerical example involving heights of waves
              is used to illustrate the various methods and tests of fit are
              performed to compare them.},
  number   = {3},
  urldate  = {2021-10-13},
  journal  = {Technometrics},
  author   = {Zhang, Jin and Stephens, Michael A.},
  month    = aug,
  year     = {2009},
  note     = {Publisher: Taylor \& Francis \_eprint:
              https://doi.org/10.1198/tech.2009.08017},
  keywords = {Maximum likelihood estimation, Efficiency, Method of moment
              estimation, Probability-weighted moment estimation},
  pages    = {316--325},
  file     = {
              Snapshot:/Users/apodusenko/Zotero/storage/KCBHKQTS/tech.2009.html:text/html
              }
}

@article{parr_generalised_2019,
  title   = {Generalised free energy and active inference},
  volume  = {113},
  number  = {5},
  journal = {Biological cybernetics},
  author  = {Parr, Thomas and Friston, Karl J.},
  year    = {2019},
  note    = {Publisher: Springer},
  pages   = {495--513},
  file    = {Parr and Friston - 2019 - Generalised free energy and active
             inference.html:/Users/apodusenko/Zotero/storage/DKR63RPT/Parr and
             Friston - 2019 - Generalised free energy and active
             inference.html:text/html}
}

@article{hohwy_conscious_2021,
  title   = {Conscious self-evidencing},
  journal = {Review of Philosophy and Psychology},
  author  = {Hohwy, Jakob},
  year    = {2021},
  note    = {Publisher: Springer},
  pages   = {1--20},
  file    = {Hohwy - 2021 - Conscious
             self-evidencing.pdf:/Users/apodusenko/Zotero/storage/FYZG42AE/Hohwy -
             2021 - Conscious
             self-evidencing.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/23RSV3JF/s13164-021-00578-x.html:text/html
             }
}

@incollection{saad_bayesian_1999,
  edition   = {1},
  title     = {A {Bayesian} {Approach} to {On}-line {Learning}},
  isbn      = {978-0-521-65263-6 978-0-521-11791-3 978-0-511-56992-0},
  url       = {
               https://www.cambridge.org/core/product/identifier/CBO9780511569920A023/type/book_part
               },
  abstract  = {Online learning is discussed from the viewpoint of Bayesian
               statistical inference. By replacing the true posterior distribution
               with a simpler parametric distribution, one can deﬁne an online
               algorithm by a repetition of two steps: An update of the
               approximate posterior, when a new example arrives, and an optimal
               projection into the parametric family. Choosing this family to be
               Gaussian, we show that the algorithm achieves asymptotic eﬃciency.
               An application to learning in single layer neural networks is
               given.},
  language  = {en},
  urldate   = {2022-01-17},
  booktitle = {On-{Line} {Learning} in {Neural} {Networks}},
  publisher = {Cambridge University Press},
  author    = {Opper, Manfred},
  editor    = {Saad, David},
  month     = jan,
  year      = {1999},
  doi       = {10.1017/CBO9780511569920.017},
  pages     = {363--378},
  file      = {Opper - 1999 - A Bayesian Approach to On-line
               Learning.pdf:/Users/apodusenko/Zotero/storage/AWXX9NC4/Opper - 1999 - A
               Bayesian Approach to On-line Learning.pdf:application/pdf}
}

@book{shalev-shwartz_understanding_2014,
  address    = {Cambridge},
  title      = {Understanding {Machine} {Learning}: {From} {Theory} to {Algorithms}},
  isbn       = {978-1-107-29801-9},
  shorttitle = {Understanding {Machine} {Learning}},
  url        = {http://ebooks.cambridge.org/ref/id/CBO9781107298019},
  language   = {en},
  urldate    = {2022-01-17},
  publisher  = {Cambridge University Press},
  author     = {Shalev-Shwartz, Shai and Ben-David, Shai},
  year       = {2014},
  doi        = {10.1017/CBO9781107298019},
  file       = {Shalev-Shwartz and Ben-David - 2014 - Understanding Machine Learning
                From Theory to
                Alg.pdf:/Users/apodusenko/Zotero/storage/A6RJ9CVN/Shalev-Shwartz and
                Ben-David - 2014 - Understanding Machine Learning From Theory to
                Alg.pdf:application/pdf}
}

@misc{noauthor_notitle_nodate,
  url     = {https://www.cs.huji.ac.il/w~
             shais/UnderstandingMachineLearning/copy.html},
  urldate = {2022-01-17},
  file    = {https\://www.cs.huji.ac.il/w~
             shais/UnderstandingMachineLearning/copy.html:/Users/apodusenko/Zotero/storage/THVDE8YZ/copy.html:text/html
             }
}

@book{shalev-shwartz_understanding_2014-1,
  address    = {Cambridge},
  title      = {Understanding {Machine} {Learning}: {From} {Theory} to {Algorithms}},
  isbn       = {978-1-107-05713-5},
  shorttitle = {Understanding {Machine} {Learning}},
  url        = {
                https://www.cambridge.org/core/books/understanding-machine-learning/3059695661405D25673058E43C8BE2A6
                },
  abstract   = {Machine learning is one of the fastest growing areas of computer
                science, with far-reaching applications. The aim of this textbook
                is to introduce machine learning, and the algorithmic paradigms it
                offers, in a principled way. The book provides a theoretical
                account of the fundamentals underlying machine learning and the
                mathematical derivations that transform these principles into
                practical algorithms. Following a presentation of the basics, the
                book covers a wide array of central topics unaddressed by previous
                textbooks. These include a discussion of the computational
                complexity of learning and the concepts of convexity and stability;
                important algorithmic paradigms including stochastic gradient
                descent, neural networks, and structured output learning; and
                emerging theoretical concepts such as the PAC-Bayes approach and
                compression-based bounds. Designed for advanced undergraduates or
                beginning graduates, the text makes the fundamentals and algorithms
                of machine learning accessible to students and non-expert readers
                in statistics, computer science, mathematics and engineering.},
  urldate    = {2022-01-17},
  publisher  = {Cambridge University Press},
  author     = {Shalev-Shwartz, Shai and Ben-David, Shai},
  year       = {2014},
  doi        = {10.1017/CBO9781107298019},
  file       = {
                Snapshot:/Users/apodusenko/Zotero/storage/XAUHU3KX/3059695661405D25673058E43C8BE2A6.html:text/html
                }
}

@misc{noauthor_pdf_nodate,
  title   = {[{PDF}] {A} {Bayesian} {Approach} to {Online} {Learning} {\textbar} {
             Semantic} {Scholar}},
  url     = {
             https://www.semanticscholar.org/paper/A-Bayesian-Approach-to-Online-Learning-Opper/199e1fe8cf3646f6352532dec35017d0a48d357b
             },
  urldate = {2022-01-17},
  file    = {[PDF] A Bayesian Approach to Online Learning | Semantic
             Scholar:/Users/apodusenko/Zotero/storage/3Z2Y8DN2/199e1fe8cf3646f6352532dec35017d0a48d357b.html:text/html
             }
}

@article{pezzulo_evolution_2022,
  title    = {The evolution of brain architectures for predictive coding and active
              inference},
  volume   = {377},
  url      = {https://royalsocietypublishing.org/doi/full/10.1098/rstb.2020.0531},
  doi      = {10.1098/rstb.2020.0531},
  abstract = {This article considers the evolution of brain architectures for
              predictive processing. We argue that brain mechanisms for
              predictive perception and action are not late evolutionary
              additions of advanced creatures like us. Rather, they emerged
              gradually from simpler predictive loops (e.g. autonomic and motor
              reflexes) that were a legacy from our earlier evolutionary
              ancestors—and were key to solving their fundamental problems of
              adaptive regulation. We characterize simpler-to-more-complex brains
              formally, in terms of generative models that include predictive
              loops of increasing hierarchical breadth and depth. These may start
              from a simple homeostatic motif and be elaborated during evolution
              in four main ways: these include the multimodal expansion of
              predictive control into an allostatic loop; its duplication to form
              multiple sensorimotor loops that expand an animal's behavioural
              repertoire; and the gradual endowment of generative models with
              hierarchical depth (to deal with aspects of the world that unfold
              at different spatial scales) and temporal depth (to select plans in
              a future-oriented manner). In turn, these elaborations underwrite
              the solution to biological regulation problems faced by
              increasingly sophisticated animals. Our proposal aligns
              neuroscientific theorising—about predictive processing—with
              evolutionary and comparative data on brain architectures in
              different animal species. This article is part of the theme issue
              ‘Systems neuroscience through the lens of evolutionary theory’.},
  number   = {1844},
  urldate  = {2022-01-05},
  journal  = {Philosophical Transactions of the Royal Society B: Biological
              Sciences},
  author   = {Pezzulo, Giovanni and Parr, Thomas and Friston, Karl},
  month    = feb,
  year     = {2022},
  note     = {Publisher: Royal Society},
  keywords = {natural selection, active inference, predictive processing, brain
              architecture, brain evolution, model selection},
  pages    = {20200531},
  file     = {Pezzulo et al. - 2022 - The evolution of brain architectures for
              predictiv.pdf:/Users/apodusenko/Zotero/storage/P6PWZE48/Pezzulo et al.
              - 2022 - The evolution of brain architectures for
              predictiv.pdf:application/pdf}
}

@article{bagaev_reactive_2021,
  title    = {Reactive {Message} {Passing} for {Scalable} {Bayesian} {Inference}},
  url      = {http://arxiv.org/abs/2112.13251},
  abstract = {We introduce Reactive Message Passing (RMP) as a framework for
              executing schedule-free, robust and scalable message passing-based
              inference in a factor graph representation of a probabilistic
              model. RMP is based on the reactive programming style that only
              describes how nodes in a factor graph react to changes in connected
              nodes. The absence of a fixed message passing schedule improves
              robustness, scalability and execution time of the inference
              procedure. We also present ReactiveMP.jl, which is a Julia package
              for realizing RMP through minimization of a constrained Bethe free
              energy. By user-defined specification of local form and
              factorization constraints on the variational posterior distribution
              , ReactiveMP.jl executes hybrid message passing algorithms
              including belief propagation, variational message passing,
              expectation propagation, and expectation maximisation update rules.
              Experimental results demonstrate the improved performance of
              ReactiveMP-based RMP in comparison to other Julia packages for
              Bayesian inference across a range of probabilistic models. In
              particular, we show that the RMP framework is able to run Bayesian
              inference for large-scale probabilistic state space models with
              hundreds of thousands of random variables on a standard laptop
              computer.},
  urldate  = {2022-01-04},
  journal  = {arXiv:2112.13251 [cs]},
  author   = {Bagaev, Dmitry and de Vries, Bert},
  month    = dec,
  year     = {2021},
  note     = {arXiv: 2112.13251},
  keywords = {Computer Science - Artificial Intelligence, Computer Science -
              Machine Learning},
  file     = {arXiv Fulltext PDF:/Users/apodusenko/Zotero/storage/UKKDFCW5/Bagaev
              and de Vries - 2021 - Reactive Message Passing for Scalable Bayesian
              Inf.pdf:application/pdf;arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/V3SNIXX9/2112.html:text/html}
}

@article{podusenko_aida_2021,
  title      = {{AIDA}: {An} {Active} {Inference}-based {Design} {Agent} for {Audio}
                {Processing} {Algorithms}},
  shorttitle = {{AIDA}},
  url        = {http://arxiv.org/abs/2112.13366},
  abstract   = {In this paper we present AIDA, which is an active inference-based
                agent that iteratively designs a personalized audio processing
                algorithm through situated interactions with a human client. The
                target application of AIDA is to propose on-the-spot the most
                interesting alternative values for the tuning parameters of a
                hearing aid (HA) algorithm, whenever a HA client is not satisfied
                with their HA performance. AIDA interprets searching for the "most
                interesting alternative" as an issue of optimal (acoustic)
                context-aware Bayesian trial design. In computational terms, AIDA
                is realized as an active inference-based agent with an Expected
                Free Energy criterion for trial design. This type of architecture
                is inspired by neuro-economic models on efficient (Bayesian) trial
                design in brains and implies that AIDA comprises generative
                probabilistic models for acoustic signals and user responses. We
                propose a novel generative model for acoustic signals as a sum of
                time-varying auto-regressive filters and a user response model
                based on a Gaussian Process Classifier. The full AIDA agent has
                been implemented in a factor graph for the generative model and all
                tasks (parameter learning, acoustic context classification, trial
                design, etc.) are realized by variational message passing on the
                factor graph. All verification and validation experiments and
                demonstrations are freely accessible at our GitHub repository.},
  urldate    = {2022-01-04},
  journal    = {arXiv:2112.13366 [cs, eess, stat]},
  author     = {Podusenko, Albert and van Erp, Bart and Koudahl, Magnus and de Vries
                , Bert},
  month      = dec,
  year       = {2021},
  note       = {arXiv: 2112.13366},
  keywords   = {Computer Science - Sound, Statistics - Machine Learning,
                Electrical Engineering and Systems Science - Audio and Speech
                Processing, Computer Science - Machine Learning},
  file       = {arXiv Fulltext PDF:/Users/apodusenko/Zotero/storage/37WCIF5W/Podusenko
                et al. - 2021 - AIDA An Active Inference-based Design Agent for
                A.pdf:application/pdf;arXiv.org
                Snapshot:/Users/apodusenko/Zotero/storage/KAHLFMQ2/2112.html:text/html}
}

@article{taylor_variational_2021,
  title    = {Variational message passing ({VMP}) applied to {LDA}},
  url      = {http://arxiv.org/abs/2111.01480},
  abstract = {Variational Bayes (VB) applied to latent Dirichlet allocation
              (LDA) is the original inference mechanism for LDA. Many variants of
              VB for LDA, as well as for VB in general, have been developed since
              LDA's inception in 2013, but standard VB is still widely applied to
              LDA. Variational message passing (VMP) is the message passing
              equivalent of VB and is a useful tool for constructing a
              variational inference solution for a large variety of conjugate
              exponential graphical models (there is also a non conjugate variant
              available for other models). In this article we present the VMP
              equations for LDA and also provide a brief discussion of the
              equations. We hope that this will assist others when deriving
              variational inference solutions to other similar graphical models.},
  urldate  = {2022-01-03},
  journal  = {arXiv:2111.01480 [cs, stat]},
  author   = {Taylor, Rebecca M. C. and Preez, Johan A. du},
  month    = nov,
  year     = {2021},
  note     = {arXiv: 2111.01480},
  keywords = {Statistics - Machine Learning, G.3, Computer Science - Machine
              Learning},
  file     = {arXiv Fulltext PDF:/Users/apodusenko/Zotero/storage/8YGM4VKH/Taylor
              and Preez - 2021 - Variational message passing (VMP) applied to
              LDA.pdf:application/pdf;arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/NNM6J5WD/2111.html:text/html}
}

@inproceedings{huszar_gp_2011,
  address   = {Sierra Nevada, Spain},
  title     = {A {GP} classiﬁcation approach to preference learning},
  abstract  = {In this abstract I present the problem of learning from pairwise
               preference judgements as a special case of binary classification. I
               discuss why kernel classifiers using traditional kernels based on
               the distance between items cannot be used to address the problem
               effectively: the preference prediction problem has inherent
               symmetry properties that these kernels cannot model. I will review
               a hierarchical Bayesian model for preference learning that by
               construction respects these symmetry properties and show its
               equivalence to probit Gaussian process (GP) classification.
               Motivated by this model I define the preference judgement kernel
               and show that it is capable of modelling the symmetry of preference
               judgement. The resulting reproducing kernel can be used in
               conjunction with any standard kernel-based method for
               classification, notably state-of-the-art large-scale classifiers,
               or active learning methods. Thus, the main contribution of this
               work is that a wide range of advanced methods developed for
               kernelbased classification can now be applied to preference
               learning practically without changing anything but the kernel. The
               present work also highlights how hierarchical generative models
               involving Gaussian process priors can be used to construct
               meaningful combinations of kernels for non-standardlearning
               problems. Using the same principles it is possible to construct
               similarly meaningful kernels for more convoluted problems such as
               multi-user preference learning or multi-task classification.},
  language  = {en},
  booktitle = {{NIPS} {Workshop} on {Choice} {Models} and {Preference} {Learning
               }},
  author    = {Huszar, Ferenc},
  year      = {2011},
  pages     = {4},
  file      = {Huszar - A GP classiﬁcation approach to preference
               learning.pdf:/Users/apodusenko/Zotero/storage/9IVF5QLQ/Huszar - A GP
               classiﬁcation approach to preference learning.pdf:application/pdf}
}

@misc{lin_juliastatsdistributionsjl_2019,
  title  = {{JuliaStats}/{Distributions}.jl: a {Julia} package for probability
            distributions and associated functions},
  url    = {https://doi.org/10.5281/zenodo.2647458},
  author = {Lin, Dahua and White, John Myles and Byrne, Simon and Bates, Douglas
            and Noack, Andreas and Pearson, John and Arslan, Alex and Squire,
            Kevin and Anthoff, David and Papamarkou, Theodore and Besançon,
            Mathieu and Drugowitsch, Jan and Schauer, Moritz and contributors,
            other},
  month  = jul,
  year   = {2019},
  doi    = {10.5281/zenodo.2647458}
}

@article{besancon_distributionsjl_2021,
  title    = {Distributions.jl: {Definition} and {Modeling} of {Probability} {
              Distributions} in the {JuliaStats} {Ecosystem}},
  volume   = {98},
  issn     = {1548-7660},
  url      = {https://www.jstatsoft.org/v098/i16},
  doi      = {10.18637/jss.v098.i16},
  number   = {16},
  journal  = {Journal of Statistical Software},
  author   = {Besançon, Mathieu and Papamarkou, Theodore and Anthoff, David and
              Arslan, Alex and Byrne, Simon and Lin, Dahua and Pearson, John},
  year     = {2021},
  keywords = {modeling, inference, sampling, Julia, distributions, interface,
              KDE, mixture, probabilistic programming},
  pages    = {1--30}
}

@article{friston_sophisticated_2021,
  title    = {Sophisticated {Inference}},
  volume   = {33},
  issn     = {0899-7667},
  url      = {https://doi.org/10.1162/neco_a_01351},
  doi      = {10.1162/neco_a_01351},
  abstract = {Active inference offers a first principle account of sentient
              behavior, from which special and important cases—for example,
              reinforcement learning, active learning, Bayes optimal inference,
              Bayes optimal design—can be derived. Active inference finesses the
              exploitation-exploration dilemma in relation to prior preferences
              by placing information gain on the same footing as reward or value.
              In brief, active inference replaces value functions with
              functionals of (Bayesian) beliefs, in the form of an expected
              (variational) free energy. In this letter, we consider a
              sophisticated kind of active inference using a recursive form of
              expected free energy. Sophistication describes the degree to which
              an agent has beliefs about beliefs. We consider agents with beliefs
              about the counterfactual consequences of action for states of
              affairs and beliefs about those latent states. In other words, we
              move from simply considering beliefs about “what would happen if I
              did that” to “what I would believe about what would happen if I did
              that.” The recursive form of the free energy functional effectively
              implements a deep tree search over actions and outcomes in the
              future. Crucially, this search is over sequences of belief states
              as opposed to states per se. We illustrate the competence of this
              scheme using numerical simulations of deep decision problems.},
  number   = {3},
  urldate  = {2021-12-22},
  journal  = {Neural Computation},
  author   = {Friston, Karl and Da Costa, Lancelot and Hafner, Danijar and Hesp,
              Casper and Parr, Thomas},
  month    = mar,
  year     = {2021},
  pages    = {713--763},
  file     = {Full Text:/Users/apodusenko/Zotero/storage/2NHQ7CM3/Friston et al. -
              2021 - Sophisticated
              Inference.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/APVDBICC/Sophisticated-Inference.html:text/html
              }
}

@article{xie_time-frequency_2012,
  title    = {Time-{Frequency} {Approach} to {Underdetermined} {Blind} {Source} {
              Separation}},
  volume   = {23},
  issn     = {2162-2388},
  doi      = {10.1109/TNNLS.2011.2177475},
  abstract = {This paper presents a new time-frequency (TF) underdetermined
              blind source separation approach based on Wigner-Ville distribution
              (WVD) and Khatri-Rao product to separate N non-stationary sources
              from M(M {\textless}; N) mixtures. First, an improved method is
              proposed for estimating the mixing matrix, where the negative value
              of the auto WVD of the sources is fully considered. Then after
              extracting all the auto-term TF points, the auto WVD value of the
              sources at every auto-term TF point can be found out exactly with
              the proposed approach no matter how many active sources there are
              as long as N ≤ 2M-1. Further discussion about the extraction of
              auto-term TF points is made and finally the numerical simulation
              results are presented to show the superiority of the proposed
              algorithm by comparing it with the existing ones.},
  number   = {2},
  journal  = {IEEE Transactions on Neural Networks and Learning Systems},
  author   = {Xie, S. and Yang, L. and Yang, J. and Zhou, G. and Xiang, Y.},
  month    = feb,
  year     = {2012},
  keywords = {Estimation, Equations, Mathematical model, Vectors, matrix algebra
              , Time frequency analysis, time-frequency analysis, blind source
              separation, mixing matrix, auto-term TF point extraction, Blind
              source separation, Eigenvalues and eigenfunctions, Khatri-Rao
              product, nonstationary sources, numerical simulation,
              time-frequency approach, underdetermined blind source separation,
              underdetermined blind source separation approach, Wigner
              distribution, Wigner-Ville distribution},
  pages    = {306--316},
  file     = {IEEE Xplore Abstract
              Record:/Users/apodusenko/Zotero/storage/3DA76AA2/6125248.html:text/html;IEEE
              Xplore Full Text PDF:/Users/apodusenko/Zotero/storage/SFZJ5SN3/Xie et
              al. - 2012 - Time-Frequency Approach to Underdetermined Blind
              S.pdf:application/pdf}
}

@article{hsiao_identification_2008,
  title    = {Identification of {Time}-{Varying} {Autoregressive} {Systems} {Using}
              {Maximum} a {Posteriori} {Estimation}},
  volume   = {56},
  issn     = {1941-0476},
  doi      = {10.1109/TSP.2008.919393},
  abstract = {Time-varying systems and nonstationary signals arise naturally in
              many engineering applications, such as speech, biomedical, and
              seismic signal processing. Thus, identification of the time-varying
              parameters is of crucial importance in the analysis and synthesis
              of these systems. The present time-varying system identification
              techniques require either demanding computation power to draw a
              large amount of samples (Monte Carlo-based methods) or a wise
              selection of basis functions (basis expansion methods). In this
              paper, the identification of time-varying autoregressive systems is
              investigated. It is formulated as a Bayesian inference problem with
              constraints on the conditional and prior probabilities of the
              time-varying parameters. These constraints can be set without
              further knowledge about the physical system. In addition, only a
              few hyper parameters need tuning for better performance. Based on
              these probabilistic constraints, an iterative algorithm is proposed
              to evaluate the maximum a posteriori estimates of the parameters.
              The proposed method is computationally efficient since random
              sampling is no longer required. Simulation results show that it is
              able to estimate the time-varying parameters reasonably well and a
              balance between the bias and variance of the estimation is achieved
              by adjusting the hyperparameters. Moreover, simulation results
              indicate that the proposed method outperforms the particle filter
              in terms of estimation errors and computational efficiency.},
  number   = {8},
  journal  = {IEEE Transactions on Signal Processing},
  author   = {Hsiao, Tesheng},
  month    = aug,
  year     = {2008},
  keywords = {Time varying systems, Speech processing, Power engineering and
              energy, Signal processing, Computational modeling, time-varying
              autoregressive model, Speech synthesis, Signal synthesis, System
              identification, Biomedical engineering, Biomedical signal
              processing, Maximum a posteriori estimation, time-varying system
              identification},
  pages    = {3497--3509},
  file     = {IEEE Xplore Abstract
              Record:/Users/apodusenko/Zotero/storage/P2986MP2/4567655.html:text/html;IEEE
              Xplore Full Text PDF:/Users/apodusenko/Zotero/storage/RREW3GDP/Hsiao -
              2008 - Identification of Time-Varying Autoregressive
              Syst.pdf:application/pdf}
}

@inproceedings{rennie_dynamic_2006,
  address   = {Toulouse, France},
  title     = {Dynamic noise adaptation},
  volume    = {1},
  doi       = {10.1109/ICASSP.2006.1660241},
  urldate   = {2015-12-29},
  booktitle = {2006 {IEEE} {International} {Conference} on {Acoustics} {Speech}
               and {Signal} {Processing} {Proceedings}},
  publisher = {IEEE},
  author    = {Rennie, Steven and Kristjansson, Trausti and Olsen, Peder and
               Gopinath, Ramesh},
  year      = {2006},
  pages     = {1--4},
  file      = {Rennie et al. - 2006 - Dynamic noise
               adaptation.pdf:/Users/apodusenko/Zotero/storage/DEHHZK33/Rennie et al.
               - 2006 - Dynamic noise
               adaptation.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/Z5JU27VC/abs_all.html:text/html
               }
}

@techreport{minka_divergence_2005,
  title       = {Divergence {Measures} and {Message} {Passing}},
  abstract    = {This paper presents a unifying view of messagepassing algorithms,
                 as methods to approximate a complex Bayesian network by a simpler
                 network with minimum information divergence. In this view, the
                 difference between mean-field methods and belief propagation is not
                 the amount of structure they model, but only the measure of loss
                 they minimize (‘exclusive ’ versus ‘inclusive’ Kullback-Leibler
                 divergence). In each case, message-passing arises by minimizing a
                 localized version of the divergence, local to each factor. By
                 examining these divergence measures, we can intuit the types of
                 solution they prefer (symmetry-breaking, for example) and their
                 suitability for different tasks. Furthermore, by considering a
                 wider variety of divergence measures (such as alpha-divergences),
                 we can achieve different complexity and performance goals. 1},
  institution = {Microsoft Research},
  author      = {Minka, Thomas},
  year        = {2005},
  file        = {Citeseer -
                 Snapshot:/Users/apodusenko/Zotero/storage/WAM6JF29/summary.html:text/html;Minka
                 - 2005 - Divergence Measures and Message
                 Passing.pdf:/Users/apodusenko/Zotero/storage/FMIGZ88M/Minka - 2005 -
                 Divergence Measures and Message Passing.pdf:application/pdf}
}

@inproceedings{cox_parametric_2017,
  address   = {Long Beach, USA},
  title     = {A parametric approach to {Bayesian} optimization with pairwise
               comparisons},
  abstract  = {Optimizing a (preference) function through a small number of
               pairwise comparisons is challenging since pairwise comparisons
               provide limited information about the underlying function. In
               practice, preference functions often have a single peak, and this
               property could be exploited to speed up the optimization process.
               In this paper we describe a Bayesian optimization method aimed at
               achieving this.},
  language  = {en},
  booktitle = {{NIPS} {Workshop} on {Bayesian} {Optimization} ({BayesOpt} 2017)},
  author    = {Cox, Marco and de Vries, Bert},
  month     = dec,
  year      = {2017},
  pages     = {1--5},
  file      = {Cox and de Vries - A parametric approach to Bayesian optimization
               wit.pdf:/Users/apodusenko/Zotero/storage/YRX7M2FK/Cox and de Vries - A
               parametric approach to Bayesian optimization wit.pdf:application/pdf}
}

@article{sajid_active_2021,
  title      = {Active {Inference}: {Demystified} and {Compared}},
  volume     = {33},
  issn       = {0899-7667},
  shorttitle = {Active {Inference}},
  url        = {https://doi.org/10.1162/neco_a_01357},
  doi        = {10.1162/neco_a_01357},
  abstract   = {Active inference is a first principle account of how autonomous
                agents operate in dynamic, nonstationary environments. This problem
                is also considered in reinforcement learning, but limited work
                exists on comparing the two approaches on the same discrete-state
                environments. In this letter, we provide (1) an accessible overview
                of the discrete-state formulation of active inference, highlighting
                natural behaviors in active inference that are generally engineered
                in reinforcement learning, and (2) an explicit discrete-state
                comparison between active inference and reinforcement learning on
                an OpenAI gym baseline. We begin by providing a condensed overview
                of the active inference literature, in particular viewing the
                various natural behaviors of active inference agents through the
                lens of reinforcement learning. We show that by operating in a pure
                belief-based setting, active inference agents can carry out
                epistemic exploration—and account for uncertainty about their
                environment—in a Bayes-optimal fashion. Furthermore, we show that
                the reliance on an explicit reward signal in reinforcement learning
                is removed in active inference, where reward can simply be treated
                as another observation we have a preference over; even in the total
                absence of rewards, agent behaviors are learned through preference
                learning. We make these properties explicit by showing two
                scenarios in which active inference agents can infer behaviors in
                reward-free environments compared to both Q-learning and Bayesian
                model-based reinforcement learning agents and by placing zero prior
                preferences over rewards and learning the prior preferences over
                the observations corresponding to reward. We conclude by noting
                that this formalism can be applied to more complex settings (e.g.,
                robotic arm movement, Atari games) if appropriate generative models
                can be formulated. In short, we aim to demystify the behavior of
                active inference agents by presenting an accessible discrete
                state-space and time formulation and demonstrate these behaviors in
                a OpenAI gym environment, alongside reinforcement learning agents.},
  number     = {3},
  urldate    = {2021-12-22},
  journal    = {Neural Computation},
  author     = {Sajid, Noor and Ball, Philip J. and Parr, Thomas and Friston, Karl
                J.},
  month      = mar,
  year       = {2021},
  pages      = {674--712},
  file       = {Full Text:/Users/apodusenko/Zotero/storage/3BUSJC8E/Sajid et al. -
                2021 - Active Inference Demystified and
                Compared.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/VXIGAKY7/Active-Inference-Demystified-and-Compared.html:text/html
                }
}

@inproceedings{chu_preference_2005,
  address   = {New York, NY, USA},
  series    = {{ICML} '05},
  title     = {Preference learning with {Gaussian} processes},
  isbn      = {978-1-59593-180-1},
  url       = {https://doi.org/10.1145/1102351.1102369},
  doi       = {10.1145/1102351.1102369},
  abstract  = {In this paper, we propose a probabilistic kernel approach to
               preference learning based on Gaussian processes. A new likelihood
               function is proposed to capture the preference relations in the
               Bayesian framework. The generalized formulation is also applicable
               to tackle many multiclass problems. The overall approach has the
               advantages of Bayesian methods for model selection and
               probabilistic prediction. Experimental results compared against the
               constraint classification approach on several benchmark datasets
               verify the usefulness of this algorithm.},
  urldate   = {2021-12-22},
  booktitle = {Proceedings of the 22nd international conference on {Machine}
               learning},
  publisher = {Association for Computing Machinery},
  author    = {Chu, Wei and Ghahramani, Zoubin},
  month     = aug,
  year      = {2005},
  pages     = {137--144},
  file      = {Full Text PDF:/Users/apodusenko/Zotero/storage/6KGAIWIT/Chu and
               Ghahramani - 2005 - Preference learning with Gaussian
               processes.pdf:application/pdf}
}

@inproceedings{kakusho_hierarchical_1982,
  address   = {Paris, France},
  title     = {Hierarchical {AR} model for time varying speech signals},
  volume    = {7},
  doi       = {10.1109/ICASSP.1982.1171643},
  abstract  = {The auto-regressive(AR) model is adopted to the trajectories of
               speech feature parameters such as linear predictors and formant
               frequencies. The procedure is hierarchical in its structure and is
               expected to be suitable for the analysis of time varying speech or
               non-stationary parts of speech. The method is formulated in matrix
               form and a feature transition matrix is introduced to express the
               temporal variation of feature parameter vectors. Analysis examples
               for CV syllables are shown and the method is confirmed by
               successful reconstruction of the trajectories of feature parameters
               based on the analysis results. The method is divided into two
               stages of LP analysis and the problems are to choose the preferable
               feature parameters for the second stage analysis and to find the
               appropriate values for the analysis parameters such as the window
               length, shift interval for the first stage analysis, the prediction
               order and the window length for the second stage analysis.},
  booktitle = {{ICASSP} '82. {IEEE} {International} {Conference} on {Acoustics},
               {Speech}, and {Signal} {Processing}},
  author    = {Kakusho, O. and Yanagida, M.},
  month     = may,
  year      = {1982},
  keywords  = {Kalman filters, Speech analysis, Signal analysis, Time series
               analysis, Frequency, Speech coding, Trajectory, Predictive models,
               Linear approximation, Prediction methods},
  pages     = {1295--1298},
  file      = {IEEE Xplore Abstract
               Record:/Users/apodusenko/Zotero/storage/LALKEURJ/1171643.html:text/html;IEEE
               Xplore Full Text PDF:/Users/apodusenko/Zotero/storage/35LML84B/Kakusho
               and Yanagida - 1982 - Hierarchical AR model for time varying speech
               sign.pdf:application/pdf}
}

@article{k_mogensen_optim_2018,
  title      = {Optim: {A} mathematical optimization package for {Julia}},
  volume     = {3},
  issn       = {2475-9066},
  shorttitle = {Optim},
  url        = {http://joss.theoj.org/papers/10.21105/joss.00615},
  doi        = {10.21105/joss.00615},
  abstract   = {Optim provides a range of optimization capabilities written in the
                Julia programming language (Bezanson et al. 2017). Our aim is to
                enable researchers, users, and other Julia packages to solve
                optimization problems without writing such algorithms themselves.
                The package supports optimization on manifolds, functions of
                complex numbers, and input types such as arbitrary precision
                vectors and matrices. We have implemented routines for derivative
                free, first-order, and second-order optimization methods. The user
                can provide derivatives themselves, or request that they are
                calculated using automatic differentiation or finite difference
                methods. The main focus of the package has currently been on
                unconstrained optimization, however, box-constrained optimization
                is supported, and a more comprehensive support for constraints is
                underway. Similar to Optim, the C library NLopt (Johnson 2008)
                contains a collection of nonlinear optimization routines. In Python
                , scipy.optimize supports many of the same algorithms as Optim does
                , and Pymanopt (Townsend, Niklas, and Weichwald 2016) is a toolbox
                for manifold optimization. Within the Julia community, the packages
                BlackBoxOptim.jl and Optimize.jl provide optimization capabilities
                focusing on derivative-free and large-scale smooth problems
                respectively. The packages Convex.jl and JuMP.jl (Dunning, Huchette
                , and Lubin 2017) define modelling languages for which users can
                formulate optimization problems. In contrast to the previously
                mentioned optimization codes, Convex and JuMP work as abstraction
                layers between the user and solvers from a other packages.},
  language   = {en},
  number     = {24},
  urldate    = {2021-09-03},
  journal    = {Journal of Open Source Software},
  author     = {K Mogensen, Patrick and N Riseth, Asbjørn},
  month      = apr,
  year       = {2018},
  pages      = {615},
  file       = {K Mogensen and N Riseth - 2018 - Optim A mathematical optimization
                package for Jul.pdf:/Users/apodusenko/Zotero/storage/T9RBG4L9/K
                Mogensen and N Riseth - 2018 - Optim A mathematical optimization
                package for Jul.pdf:application/pdf}
}

@article{sarkka_temporal_2020,
  title    = {Temporal {Parallelization} of {Bayesian} {Smoothers}},
  url      = {http://arxiv.org/abs/1905.13002},
  abstract = {This paper presents algorithms for temporal parallelization of
              Bayesian smoothers. We define the elements and the operators to
              pose these problems as the solutions to all-prefix-sums operations
              for which efficient parallel scan-algorithms are available. We
              present the temporal parallelization of the general Bayesian
              filtering and smoothing equations and specialize them to
              linear/Gaussian models. The advantage of the proposed algorithms is
              that they reduce the linear complexity of standard smoothing
              algorithms with respect to time to logarithmic.},
  urldate  = {2021-12-13},
  journal  = {arXiv:1905.13002 [cs, math, stat]},
  author   = {Särkkä, Simo and García-Fernández, Ángel F.},
  month    = feb,
  year     = {2020},
  note     = {arXiv: 1905.13002},
  keywords = {Statistics - Computation, Computer Science - Distributed, Parallel
              , and Cluster Computing, Mathematics - Dynamical Systems},
  file     = {arXiv Fulltext PDF:/Users/apodusenko/Zotero/storage/4LSJZNT8/Särkkä
              and García-Fernández - 2020 - Temporal Parallelization of Bayesian
              Smoothers.pdf:application/pdf;arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/8HKN2QBC/1905.html:text/html}
}

@misc{van_den_hof_how_2017,
  title     = {How to write the introductory chapter of a research report (thesis)},
  publisher = {personal document},
  author    = {van den Hof, Paul},
  year      = {2017},
  file      = {van den Hof - 2017 - How to write the introductory chapter of a
               researc.pdf:/Users/apodusenko/Zotero/storage/KWZKD5NZ/van den Hof -
               2017 - How to write the introductory chapter of a
               researc.pdf:application/pdf}
}

@article{so_modulation-domain_2011,
  title    = {Modulation-domain {Kalman} filtering for single-channel speech
              enhancement},
  volume   = {53},
  issn     = {0167-6393},
  url      = {http://www.sciencedirect.com/science/article/pii/S0167639311000197},
  doi      = {10.1016/j.specom.2011.02.001},
  abstract = {In this paper, we investigate the modulation-domain Kalman filter
              (MDKF) and compare its performance with other time-domain and
              acoustic-domain speech enhancement methods. In contrast to
              previously reported modulation domain-enhancement methods based on
              fixed bandpass filtering, the MDKF is an adaptive and linear MMSE
              estimator that uses models of the temporal changes of the magnitude
              spectrum for both speech and noise. Also, because the Kalman filter
              is a joint magnitude and phase spectrum estimator, under
              non-stationarity assumptions, it is highly suited for
              modulation-domain processing, as phase information has been shown
              to play an important role in the modulation domain. We have found
              that the Kalman filter is better suited for processing in the
              modulation-domain, rather than in the time-domain, since the low
              order linear predictor is sufficient at modelling the dynamics of
              slow changes in the modulation domain, while being insufficient at
              modelling the long-term correlation speech information in the time
              domain. As a result, the MDKF method produces enhanced speech that
              has very minimal distortion and residual noise, in the ideal case.
              The results from objective experiments and blind subjective
              listening tests using the NOIZEUS corpus show that the MDKF (with
              clean speech parameters) outperforms all the acoustic and
              time-domain enhancement methods that were evaluated, including the
              time-domain Kalman filter with clean speech parameters. A practical
              MDKF that uses the MMSE-STSA method to enhance noisy speech in the
              acoustic domain prior to LPC analysis was also evaluated and showed
              promising results.},
  number   = {6},
  urldate  = {2015-04-28},
  journal  = {Speech Communication},
  author   = {So, Stephen and Paliwal, Kuldip K.},
  month    = jul,
  year     = {2011},
  keywords = {Kalman filters, Speech enhancement, Noise measurement, speech
              intelligibility, SE Kalman, adaptive filters, Kalman filtering,
              speech enhancement, minimum mean-square error (MMSE) estimation,
              signal denoising, Noise reduction, dereverberation, Reverberation,
              Modulation domain, Kalman filter update step models,
              modulation-domain Kalman filtering, adaptive algorithm, blind joint
              denoising, direct-to-reverberant energy ratio, DRR, interframe
              speech dynamics, log-magnitude spectrum, modulation, monaural blind
              speech denoising, monaural blind speech dereverberation, monaural
              speech enhancement algorithm, noisy reverberant speech, posterior
              distribution estimation, reverberant speech to noise ratios,
              reverberation, reverberation time T60 parameters, speech
              intelligibility quality, speech log-magnitude spectrum estimation,
              Time-frequency analysis, time-frequency log-magnitude spectra},
  pages    = {818--829},
  file     = {So and Paliwal - 2011 - Modulation-domain Kalman filtering for
              single-chan.pdf:/Users/apodusenko/Zotero/storage/69FTBV5E/So and
              Paliwal - 2011 - Modulation-domain Kalman filtering for
              single-chan.pdf:application/pdf}
}

@article{bezanson_julia:_2017,
  title      = {Julia: {A} {Fresh} {Approach} to {Numerical} {Computing}},
  volume     = {59},
  issn       = {0036-1445},
  shorttitle = {Julia},
  url        = {https://epubs.siam.org/doi/abs/10.1137/141000671},
  doi        = {10.1137/141000671},
  abstract   = {Bridging cultures that have often been distant, Julia combines
                expertise from the diverse fields of computer science and
                computational science to create a new approach to numerical
                computing. Julia is designed to be easy and fast and questions
                notions generally held to be “laws of nature" by practitioners of
                numerical computing: {\textbackslash}beginlist {\textbackslash}item
                High-level dynamic programs have to be slow. {\textbackslash}item
                One must prototype in one language and then rewrite in another
                language for speed or deployment. {\textbackslash}item There are
                parts of a system appropriate for the programmer, and other parts
                that are best left untouched as they have been built by the
                experts. {\textbackslash}endlist We introduce the Julia programming
                language and its design---a dance between specialization and
                abstraction. Specialization allows for custom treatment. Multiple
                dispatch, a technique from computer science, picks the right
                algorithm for the right circumstance. Abstraction, which is what
                good computation is really about, recognizes what remains the same
                after differences are stripped away. Abstractions in mathematics
                are captured as code through another technique from computer
                science, generic programming. Julia shows that one can achieve
                machine performance without sacrificing human convenience.},
  number     = {1},
  urldate    = {2018-04-10},
  journal    = {SIAM Review},
  author     = {Bezanson, J. and Edelman, A. and Karpinski, S. and Shah, V.},
  month      = jan,
  year       = {2017},
  pages      = {65--98},
  file       = {Bezanson et al. - 2017 - Julia A Fresh Approach to Numerical
                Computing.pdf:/Users/apodusenko/Zotero/storage/GS7LQE5E/Bezanson et al.
                - 2017 - Julia A Fresh Approach to Numerical
                Computing.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/EI9VN2TA/141000671.html:text/html
                }
}

@book{rabiner_fundamentals_1993,
  address   = {Englewood Cliffs, N.J},
  series    = {Prentice {Hall} signal processing series},
  title     = {Fundamentals of speech recognition},
  isbn      = {978-0-13-015157-5},
  publisher = {PTR Prentice Hall},
  author    = {Rabiner, Lawrence R. and Juang, B. H.},
  year      = {1993},
  keywords  = {Automatic speech recognition, Speech processing systems}
}

@article{haussmann_sampling-free_2019,
  title    = {Sampling-{Free} {Variational} {Inference} of {Bayesian} {Neural} {
              Networks} by {Variance} {Backpropagation}},
  url      = {http://arxiv.org/abs/1805.07654},
  abstract = {We propose a new Bayesian Neural Net formulation that affords
              variational inference for which the evidence lower bound is
              analytically tractable subject to a tight approximation. We achieve
              this tractability by (i) decomposing ReLU nonlinearities into the
              product of an identity and a Heaviside step function, (ii)
              introducing a separate path that decomposes the neural net
              expectation from its variance. We demonstrate formally that
              introducing separate latent binary variables to the activations
              allows representing the neural network likelihood as a chain of
              linear operations. Performing variational inference on this
              construction enables a sampling-free computation of the evidence
              lower bound which is a more effective approximation than the widely
              applied Monte Carlo sampling and CLT related techniques. We
              evaluate the model on a range of regression and classification
              tasks against BNN inference alternatives, showing competitive or
              improved performance over the current state-of-the-art.},
  urldate  = {2021-06-29},
  journal  = {arXiv:1805.07654 [cs, stat]},
  author   = {Haussmann, Manuel and Hamprecht, Fred A. and Kandemir, Melih},
  month    = jun,
  year     = {2019},
  note     = {arXiv: 1805.07654},
  keywords = {Statistics - Machine Learning, Computer Science - Machine Learning
              },
  file     = {arXiv Fulltext PDF:/Users/apodusenko/Zotero/storage/7CXUHY8H/Haussmann
              et al. - 2019 - Sampling-Free Variational Inference of Bayesian
              Ne.pdf:application/pdf;arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/ECD766QP/1805.html:text/html}
}

@article{zhang_matrix-variate_2014,
  title    = {Matrix-{Variate} {Dirichlet} {Process} {Priors} with {Applications}},
  volume   = {9},
  url      = {https://doi.org/10.1214/13-BA853},
  doi      = {10.1214/13-BA853},
  number   = {2},
  journal  = {Bayesian Analysis},
  author   = {Zhang, Zhihua and Wang, Dakan and Dai, Guang and Jordan, Michael I.},
  year     = {2014},
  note     = {Publisher: International Society for Bayesian Analysis},
  keywords = {Dirichlet processes, latent factor regression, matrix-variate
              distributions, nonparametric dependent modeling, nonparametric
              discriminative analysis},
  pages    = {259 -- 286}
}

@book{manning_foundations_1999,
  address   = {Cambridge, Massachusetts},
  title     = {Foundations of {Statistical} {Natural} {Language} {Processing}},
  url       = {http://nlp.stanford.edu/fsnlp/},
  publisher = {The MIT Press},
  author    = {Manning, Christopher D. and Schütze, Hinrich},
  year      = {1999},
  keywords  = {lecture nlp}
}

@book{jelinek_statistical_1998,
  address   = {Cambridge, MA, USA},
  title     = {Statistical {Methods} for {Speech} {Recognition}},
  isbn      = {0-262-10066-5},
  publisher = {MIT Press},
  author    = {Jelinek, Frederick},
  year      = {1998}
}

@article{kalman_new_1960,
  title   = {A {New} {Approach} to {Linear} {Filtering} and {Prediction} {Problems
             }},
  volume  = {82},
  number  = {Series D},
  journal = {Transactions of the ASME–Journal of Basic Engineering},
  author  = {Kalman, Rudolph Emil},
  year    = {1960},
  pages   = {35--45}
}

@article{busch_pushnet_2020,
  title      = {{PushNet}: {Efficient} and {Adaptive} {Neural} {Message} {Passing}},
  shorttitle = {{PushNet}},
  url        = {http://arxiv.org/abs/2003.02228},
  doi        = {10.3233/FAIA200199},
  abstract   = {Message passing neural networks have recently evolved into a
                state-of-the-art approach to representation learning on graphs.
                Existing methods perform synchronous message passing along all
                edges in multiple subsequent rounds and consequently suffer from
                various shortcomings: Propagation schemes are inflexible since they
                are restricted to \$k\$-hop neighborhoods and insensitive to actual
                demands of information propagation. Further, long-range
                dependencies cannot be modeled adequately and learned
                representations are based on correlations of fixed locality. These
                issues prevent existing methods from reaching their full potential
                in terms of prediction performance. Instead, we consider a novel
                asynchronous message passing approach where information is pushed
                only along the most relevant edges until convergence. Our proposed
                algorithm can equivalently be formulated as a single synchronous
                message passing iteration using a suitable neighborhood function,
                thus sharing the advantages of existing methods while addressing
                their central issues. The resulting neural network utilizes a
                node-adaptive receptive field derived from meaningful sparse node
                neighborhoods. In addition, by learning and combining node
                representations over differently sized neighborhoods, our model is
                able to capture correlations on multiple scales. We further propose
                variants of our base model with different inductive bias. Empirical
                results are provided for semi-supervised node classification on
                five real-world datasets following a rigorous evaluation protocol.
                We find that our models outperform competitors on all datasets in
                terms of accuracy with statistical significance. In some cases, our
                models additionally provide faster runtime.},
  urldate    = {2021-11-29},
  journal    = {arXiv:2003.02228 [cs, stat]},
  author     = {Busch, Julian and Pi, Jiaxing and Seidl, Thomas},
  month      = dec,
  year       = {2020},
  note       = {arXiv: 2003.02228},
  keywords   = {Statistics - Machine Learning, Computer Science - Machine Learning
                },
  file       = {arXiv Fulltext PDF:/Users/apodusenko/Zotero/storage/FU73MIV3/Busch et
                al. - 2020 - PushNet Efficient and Adaptive Neural Message
                Pas.pdf:application/pdf;arXiv.org
                Snapshot:/Users/apodusenko/Zotero/storage/Q6LGEWGZ/2003.html:text/html}
}

@incollection{paszke_pytorch_2019,
  title     = {{PyTorch}: {An} {Imperative} {Style}, {High}-{Performance} {Deep} {
               Learning} {Library}},
  url       = {
               http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf
               },
  booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 32},
  publisher = {Curran Associates, Inc.},
  author    = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and
               Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin,
               Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban
               and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison,
               Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner,
               Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
  editor    = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and Alché-Buc, F.
               d' and Fox, E. and Garnett, R.},
  year      = {2019},
  pages     = {8024--8035}
}

@misc{martin_abadi_tensorflow_2015,
  title  = {{TensorFlow}: {Large}-{Scale} {Machine} {Learning} on {Heterogeneous}
            {Systems}},
  url    = {https://www.tensorflow.org/},
  author = {{Martín Abadi} and {Ashish Agarwal} and {Paul Barham} and {Eugene
            Brevdo} and {Zhifeng Chen} and {Craig Citro} and {Greg S. Corrado}
            and {Andy Davis} and {Jeffrey Dean} and {Matthieu Devin} and {Sanjay
            Ghemawat} and {Ian Goodfellow} and {Andrew Harp} and {Geoffrey Irving
            } and {Michael Isard} and Jia, Yangqing and {Rafal Jozefowicz} and {
            Lukasz Kaiser} and {Manjunath Kudlur} and {Josh Levenberg} and {
            Dandelion Mané} and {Rajat Monga} and {Sherry Moore} and {Derek
            Murray} and {Chris Olah} and {Mike Schuster} and {Jonathon Shlens}
            and {Benoit Steiner} and {Ilya Sutskever} and {Kunal Talwar} and {
            Paul Tucker} and {Vincent Vanhoucke} and {Vijay Vasudevan} and {
            Fernanda Viégas} and {Oriol Vinyals} and {Pete Warden} and {Martin
            Wattenberg} and {Martin Wicke} and {Yuan Yu} and {Xiaoqiang Zheng}},
  year   = {2015}
}

@article{holmes_active_2021,
  title    = {Active inference, selective attention, and the cocktail party problem
              },
  volume   = {131},
  issn     = {1873-7528},
  doi      = {10.1016/j.neubiorev.2021.09.038},
  abstract = {In this paper, we introduce a new generative model for an active
              inference account of preparatory and selective attention, in the
              context of a classic 'cocktail party' paradigm. In this setup,
              pairs of words are presented simultaneously to the left and right
              ears and an instructive spatial cue directs attention to the left
              or right. We use this generative model to test competing hypotheses
              about the way that human listeners direct preparatory and selective
              attention. We show that assigning low precision to words at
              attended-relative to unattended-locations can explain why a
              listener reports words from a competing sentence. Under this model,
              temporal changes in sensory precision were not needed to account
              for faster reaction times with longer cue-target intervals, but
              were necessary to explain ramping effects on event-related
              potentials (ERPs)-resembling the contingent negative variation
              (CNV)-during the preparatory interval. These simulations reveal
              that different processes are likely to underlie the improvement in
              reaction times and the ramping of ERPs that are associated with
              spatial cueing.},
  language = {eng},
  journal  = {Neuroscience and Biobehavioral Reviews},
  author   = {Holmes, Emma and Parr, Thomas and Griffiths, Timothy D. and Friston,
              Karl J.},
  month    = oct,
  year     = {2021},
  pmid     = {34687699},
  keywords = {Active inference, Cocktail party listening, Preparatory attention,
              Selective attention, Spatial attention, Temporal attention},
  pages    = {1288--1304},
  file     = {Submitted Version:/Users/apodusenko/Zotero/storage/2LFS4D5G/Holmes et
              al. - 2021 - Active inference, selective attention, and the
              coc.pdf:application/pdf}
}

@article{hines_visqol_2015,
  title      = {{ViSQOL}: an objective speech quality model},
  volume     = {2015},
  issn       = {1687-4722},
  shorttitle = {{ViSQOL}},
  url        = {
                https://asmp-eurasipjournals.springeropen.com/articles/10.1186/s13636-015-0054-9
                },
  doi        = {10.1186/s13636-015-0054-9},
  abstract   = {This paper presents an objective speech quality model, ViSQOL, the
                Virtual Speech Quality Objective Listener. It is a signal-based,
                full-reference, intrusive metric that models human speech quality
                perception using a spectro-temporal measure of similarity between a
                reference and a test speech signal. The metric has been
                particularly designed to be robust for quality issues associated
                with Voice over IP (VoIP) transmission. This paper describes the
                algorithm and compares the quality predictions with the ITU-T
                standard metrics PESQ and POLQA for common problems in VoIP: clock
                drift, associated time warping, and playout delays. The results
                indicate that ViSQOL and POLQA significantly outperform PESQ, with
                ViSQOL competing well with POLQA. An extensive benchmarking against
                PESQ, POLQA, and simpler distance metrics using three speech
                corpora (NOIZEUS and E4 and the ITU-T P.Sup. 23 database) is also
                presented. These experiments benchmark the performance for a wide
                range of quality impairments, including VoIP degradations, a
                variety of background noise types, speech enhancement methods, and
                SNR levels. The results and subsequent analysis show that both
                ViSQOL and POLQA have some performance weaknesses and under-predict
                perceived quality in certain VoIP conditions. Both have a wider
                application and robustness to conditions than PESQ or more trivial
                distance metrics. ViSQOL is shown to offer a useful alternative to
                POLQA in predicting speech quality in VoIP scenarios.},
  language   = {en},
  number     = {1},
  urldate    = {2020-08-09},
  journal    = {EURASIP Journal on Audio, Speech, and Music Processing},
  author     = {Hines, Andrew and Skoglund, Jan and Kokaram, Anil C and Harte, Naomi
                },
  month      = dec,
  year       = {2015},
  pages      = {13},
  file       = {Hines et al. - 2015 - ViSQOL an objective speech quality
                model.pdf:/Users/apodusenko/Zotero/storage/B6AM8AZN/Hines et al. - 2015
                - ViSQOL an objective speech quality model.pdf:application/pdf}
}

@article{chinen_visqol_2020,
  title      = {{ViSQOL} v3: {An} {Open} {Source} {Production} {Ready} {Objective} {
                Speech} and {Audio} {Metric}},
  shorttitle = {{ViSQOL} v3},
  url        = {http://arxiv.org/abs/2004.09584},
  abstract   = {Estimation of perceptual quality in audio and speech is possible
                using a variety of methods. The combined v3 release of ViSQOL and
                ViSQOLAudio (for speech and audio, respectively,) provides
                improvements upon previous versions, in terms of both design and
                usage. As an open source C++ library or binary with permissive
                licensing, ViSQOL can now be deployed beyond the research context
                into production usage. The feedback from internal production teams
                at Google has helped to improve this new release, and serves to
                show cases where it is most applicable, as well as to highlight
                limitations. The new model is benchmarked against real-world data
                for evaluation purposes. The trends and direction of future work is
                discussed.},
  urldate    = {2020-08-03},
  journal    = {arXiv:2004.09584 [cs, eess]},
  author     = {Chinen, Michael and Lim, Felicia S. C. and Skoglund, Jan and Gureev,
                Nikita and O'Gorman, Feargus and Hines, Andrew},
  month      = apr,
  year       = {2020},
  keywords   = {Computer Science - Sound, Electrical Engineering and Systems
                Science - Audio and Speech Processing, Electrical Engineering and
                Systems Science - Signal Processing},
  file       = {arXiv Fulltext PDF:/Users/apodusenko/Zotero/storage/BHU4PNJ5/Chinen et
                al. - 2020 - ViSQOL v3 An Open Source Production Ready
                Objecti.pdf:application/pdf;arXiv.org
                Snapshot:/Users/apodusenko/Zotero/storage/Z2D7GUHY/2004.html:text/html}
}

@article{beerends_perceptual_2013,
  title    = {Perceptual {Objective} {Listening} {Quality} {Assessment} ({POLQA}),
              {The} {Third} {Generation} {ITU}-{T} {Standard} for {End}-to-{End} {
              Speech} {Quality} {Measurement} {Part} {I}—{Temporal} {Alignment}},
  volume   = {61},
  url      = {http://www.aes.org/e-lib/browse.cfm?elib=16829},
  abstract = {In this and the companion paper Part II, the authors present the
              Perceptual Objective Listening Quality Assessment (POLQA), the
              third-generation speech quality measurement algorithm, standardized
              by the International Telecommunication Union in 2011 as
              Recommendation P.863. In contrast to the previous standard (P.862
              Perceptual Evaluation of Speech Quality), a more complex temporal
              alignment was developed allowing for the alignment of a wide
              variety of complex distortions for which P.862 was...},
  language = {English},
  number   = {6},
  urldate  = {2020-08-03},
  journal  = {Journal of the Audio Engineering Society},
  author   = {Beerends, John G. and Schmidmer, Christian and Berger, Jens and
              Obermann, Matthias and Ullmann, Raphael and Pomy, Joachim and Keyhl,
              Michael},
  month    = jul,
  year     = {2013},
  note     = {Publisher: Audio Engineering Society},
  pages    = {366--384},
  file     = {Full Text PDF:/Users/apodusenko/Zotero/storage/BUYK25QP/Beerends et
              al. - 2013 - Perceptual Objective Listening Quality Assessment
              .pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/3DZVZMVA/browse.html:text/html
              }
}

@article{taal_algorithm_2011,
  title    = {An {Algorithm} for {Intelligibility} {Prediction} of {Time}–{
              Frequency} {Weighted} {Noisy} {Speech}},
  volume   = {19},
  issn     = {1558-7924},
  doi      = {10.1109/TASL.2011.2114881},
  abstract = {In the development process of noise-reduction algorithms, an
              objective machine-driven intelligibility measure which shows high
              correlation with speech intelligibility is of great interest.
              Besides reducing time and costs compared to real listening
              experiments, an objective intelligibility measure could also help
              provide answers on how to improve the intelligibility of noisy
              unprocessed speech. In this paper, a short-time objective
              intelligibility measure (STOI) is presented, which shows high
              correlation with the intelligibility of noisy and time-frequency
              weighted noisy speech (e.g., resulting from noise reduction) of
              three different listening experiments. In general, STOI showed
              better correlation with speech intelligibility compared to five
              other reference objective intelligibility models. In contrast to
              other conventional intelligibility models which tend to rely on
              global statistics across entire sentences, STOI is based on shorter
              time segments (386 ms). Experiments indeed show that it is
              beneficial to take segment lengths of this order into account. In
              addition, a free Matlab implementation is provided.},
  number   = {7},
  journal  = {IEEE Transactions on Audio, Speech, and Language Processing},
  author   = {Taal, Cees H. and Hendriks, Richard C. and Heusdens, Richard and
              Jensen, Jesper},
  month    = sep,
  year     = {2011},
  keywords = {Noise measurement, Signal to noise ratio, Speech, Speech
              processing, Correlation, objective measure, speech enhancement,
              Noise reduction, Time frequency analysis, speech intelligibility
              prediction},
  pages    = {2125--2136},
  file     = {IEEE Xplore Abstract
              Record:/Users/apodusenko/Zotero/storage/XAIF92VJ/5713237.html:text/html;IEEE
              Xplore Full Text PDF:/Users/apodusenko/Zotero/storage/ST95PQTF/Taal et
              al. - 2011 - An Algorithm for Intelligibility Prediction of
              Tim.pdf:application/pdf}
}

@article{radfar_nonlinear_2006,
  title    = {Nonlinear minimum mean square error estimator for
              mixture-maximisation approximation},
  volume   = {42},
  issn     = {0013-5194},
  doi      = {10.1049/el:20060510},
  abstract = {In many speech separation, enhancement, and recognition techniques
              , it is necessary to express the log spectrum of a mixture speech
              signal in terms of the log spectra of the underlying speech
              signals. For this purpose, the mixture-maximisation (MIXMAX)
              approximation is commonly used. Presented is a proof for this
              approximation in a statistical framework. It is concluded that this
              approximation is a nonlinear minimum mean square error estimator
              with the assumption of uniform distributions for phase information
              of the underlying speech signals.},
  number   = {12},
  journal  = {Electronics Letters},
  author   = {Radfar, M.H. and Banihashemi, A.H. and Dansereau, R.M. and Sayadiyan
              , A.},
  month    = jun,
  year     = {2006},
  keywords = {optimisation, speech recognition, least mean squares methods,
              speech signals, speech enhancement, speech separation, statistical
              distributions, log spectrum, mean square error estimation,
              mixture-maximisation approximation, phase information, statistical
              framework, uniform distributions},
  pages    = {724--725},
  file     = {IEEE Xplore Abstract
              Record:/Users/apodusenko/Zotero/storage/TB2YGEFS/1642500.html:text/html;IEEE
              Xplore Full Text PDF:/Users/apodusenko/Zotero/storage/FT5S4L74/Radfar
              et al. - 2006 - Nonlinear minimum mean square error estimator for
              .pdf:application/pdf}
}

@inproceedings{hershey_signal_2010,
  address   = {Makuhari, Chiba, Japan},
  title     = {Signal {Interaction} and the {Devil} {Function}},
  abstract  = {It is common in signal processing to model signals in the log
               power spectrum domain. In this domain, when multiple signals are
               present, they combine in a nonlinear way. If the phases of the
               signals are independent, then we can analyze the interaction in
               terms of a probability density we call the “devil function,” after
               its treacherous form. This paper derives an analytical expression
               for the devil function, and discusses its properties with respect
               to model-based signal enhancement. Exact inference in this problem
               requires integrals involving the devil function that are
               intractable. Previous methods have used approximations to derive
               closed-form solutions. However it is unknown how these
               approximations differ from the true interaction function in terms
               of performance. We propose Monte-Carlo methods for approximating
               the required integrals. Tests are conducted on a speech separation
               and recognition problem to compare these methods with past
               approximations.},
  language  = {en},
  booktitle = {Proceedings of the {Interspeech} 2010},
  author    = {Hershey, John R and Olsen, Peder and Rennie, Steven J},
  year      = {2010},
  pages     = {334--337},
  file      = {Hershey et al. - 2010 - Signal Interaction and the Devil
               Function.pdf:/Users/apodusenko/Zotero/storage/L4I488S9/Hershey et al. -
               2010 - Signal Interaction and the Devil Function.pdf:application/pdf}
}

@inproceedings{zalmai_unsupervised_2017,
  address   = {Kos, Greece},
  title     = {Unsupervised feature extraction, signal labeling, and blind signal
               separation in a state space world},
  doi       = {10.23919/EUSIPCO.2017.8081325},
  abstract  = {The paper addresses the problem of joint signal separation and
               estimation in a single-channel discrete-time signal composed of a
               wandering baseline and overlapping repetitions of unknown (or
               known) signal shapes. All signals are represented by a linear state
               space model (LSSM). The baseline model is driven by white Gaussian
               noise, but the other signal models are triggered by sparse inputs.
               Sparsity is achieved by normal priors with unknown variance (NUV)
               from sparse Bayesian learning. All signals and system parameters
               are jointly estimated with an efficient expectation maximization
               (EM) algorithm based on Gaussian message passing, which works both
               for known and unknown signal shapes. The proposed method outputs a
               sparse multi-channel representation of the given signal, which can
               be interpreted as a signal labeling.},
  booktitle = {2017 25th {European} {Signal} {Processing} {Conference} ({EUSIPCO
               })},
  author    = {Zalmai, Nour and Keusch, Raphael and Malmberg, Hampus and Loeliger,
               Hans-Andrea},
  month     = aug,
  year      = {2017},
  keywords  = {Estimation, Message passing, Signal processing algorithms,
               Gaussian noise, Shape, Electrocardiography, Dictionaries},
  pages     = {838--842},
  file      = {IEEE Xplore Abstract
               Record:/Users/apodusenko/Zotero/storage/TYB66GQW/8081325.html:text/html;IEEE
               Xplore Full Text PDF:/Users/apodusenko/Zotero/storage/2J7L2XNM/Zalmai
               et al. - 2017 - Unsupervised feature extraction, signal labeling,
               .pdf:application/pdf}
}

@inproceedings{laufer_bayesian_2021,
  title     = {A {Bayesian} {Hierarchical} {Model} for {Blind} {Audio} {Source} {
               Separation}},
  doi       = {10.23919/Eusipco47968.2020.9287348},
  abstract  = {This paper presents a fully Bayesian hierarchical model for blind
               audio source separation in a noisy environment. Our probabilistic
               approach is based on Gaussian priors for the speech signals, Gamma
               hyperpriors for the speech precisions and a Gamma prior for the
               noise precision. The time-varying acoustic channels are modelled
               with a linear-Gaussian state-space model. The inference is carried
               out using a variational Expectation-Maximization (VEM) algorithm,
               leading to a variant of the multi-speaker multichannel Wiener
               filter (MCWF) to separate and enhance the audio sources, and a
               Kalman smoother to infer the acoustic channels. The VEM speech
               estimator can be decomposed into two stages: A multi-speaker
               linearly constrained minimum variance (LCMV) beamformer followed by
               a variational multi-speaker postfilter. The proposed algorithm is
               evaluated in a static scenario using recorded room impulse
               responses (RIRs) with two reverberation levels, showing superior
               performance compared to competing methods.},
  booktitle = {2020 28th {European} {Signal} {Processing} {Conference} ({EUSIPCO
               })},
  author    = {Laufer, Y. and Gannot, S.},
  month     = jan,
  year      = {2021},
  note      = {ISSN: 2076-1465},
  keywords  = {Inference algorithms, Signal processing algorithms, Bayes methods,
               Noise measurement, Wiener filters, Reverberation, Source separation
               , Audio source separation, Variational EM},
  pages     = {276--280},
  file      = {IEEE Xplore Abstract
               Record:/Users/apodusenko/Zotero/storage/ZA6SQCZK/9287348.html:text/html;IEEE
               Xplore Full Text PDF:/Users/apodusenko/Zotero/storage/CSVB7DVN/Laufer
               and Gannot - 2021 - A Bayesian Hierarchical Model for Blind Audio
               Sour.pdf:application/pdf}
}

@inproceedings{rennie_single-channel_2009,
  address   = {Taipei, Taiwan},
  title     = {Single-channel speech separation and recognition using loopy belief
               propagation},
  doi       = {10.1109/ICASSP.2009.4960466},
  abstract  = {We address the problem of single-channel speech separation and
               recognition using loopy belief propagation in a way that enables
               efficient inference for an arbitrary number of speech sources. The
               graphical model consists of a set of N Markov chains, each of which
               represents a language model or grammar for a given speaker. A
               Gaussian mixture model with shared states is used to model the
               hidden acoustic signal for each grammar state of each source. The
               combination of sources is modeled in the log spectrum domain using
               non-linear interaction functions. Previously, temporal inference in
               such a model has been performed using an N-dimensional Viterbi
               algorithm that scales exponentially with the number of sources. In
               this paper, we describe a loopy message passing algorithm that
               scales linearly with language model size. The algorithm achieves
               human levels of performance, and is an order of magnitude faster
               than competitive systems for two speakers.},
  booktitle = {{IEEE} {International} {Conference} on {Acoustics}, {Speech} and
               {Signal} {Processing}, 2009. {ICASSP} 2009},
  author    = {Rennie, S.J. and Hershey, J.R. and Olsen, P.A.},
  month     = apr,
  year      = {2009},
  keywords  = {Humans, Belief propagation, Graphical models, Inference algorithms
               , Markov Chains, Hidden Markov models, speech recognition, Markov
               processes, loudspeakers, Gaussian processes, SE, hidden Markov
               models, Algonquin, Automatic speech recognition, Computational
               efficiency, Speech recognition, Viterbi algorithm, ASR, belief
               maintenance, factorial hidden Markov models, Gaussian mixture model
               , hidden acoustic signal, Iroquois, log spectrum domain, loopy
               belief propagation, loopy message passing, Loudspeakers, Markov
               chains, Max model, non-linear interaction functions, single-channel
               speech separation, Speech separation, temporal inference},
  pages     = {3845--3848},
  file      = {IEEE Xplore Abstract
               Record:/Users/apodusenko/Zotero/storage/MNGMADNR/login.html:text/html;IEEE
               Xplore Abstract
               Record:/Users/apodusenko/Zotero/storage/FNMFXB2I/4960466.html:text/html;IEEE
               Xplore Full Text PDF:/Users/apodusenko/Zotero/storage/7SEDZW5W/Rennie
               et al. - 2009 - Single-channel speech separation and recognition
               u.pdf:application/pdf;Rennie et al. - 2009 - Single-channel speech
               separation and recognition
               u.pdf:/Users/apodusenko/Zotero/storage/UG2H3VAZ/Rennie et al. - 2009 -
               Single-channel speech separation and recognition u.pdf:application/pdf}
}

@inproceedings{popescu_kalman_1998,
  address   = {Seattle, WA, USA},
  title     = {Kalman filtering of colored noise for speech enhancement},
  volume    = {2},
  doi       = {10.1109/ICASSP.1998.675435},
  abstract  = {A method for applying Kalman filtering to speech signals corrupted
               by colored noise is presented. Both speech and colored noise are
               modeled as autoregressive (AR) processes using speech and silence
               regions determined by an automatic end-point detector. Due to the
               non-stationary nature of the speech signal, non-stationary Kalman
               filter is used. Experiments indicate that non-stationary Kalman
               filtering outperforms the stationary case, the average SNR
               improvement increasing from 0.53 dB to 2.3 dB. Even better results
               are obtained if noise is considered also non-stationary, in
               addition to being colored, achieving an average of 7.14 dB SNR
               improvement.},
  booktitle = {Proceedings of the 1998 {IEEE} {International} {Conference} on {
               Acoustics}, {Speech} and {Signal} {Processing}, {ICASSP} '98},
  author    = {Popescu, D.C. and Zeljkovic, I.},
  month     = may,
  year      = {1998},
  note      = {ISSN: 1520-6149},
  keywords  = {Kalman filters, White noise, Background noise, Speech enhancement,
               Working environment noise, Signal to noise ratio, Automatic speech
               recognition, Filtering, Noise shaping, Colored noise},
  pages     = {997--1000},
  file      = {IEEE Xplore Abstract
               Record:/Users/apodusenko/Zotero/storage/L4NZCRJ5/675435.html:text/html;IEEE
               Xplore Full Text PDF:/Users/apodusenko/Zotero/storage/2GT4VU7Q/Popescu
               and Zeljkovic - 1998 - Kalman filtering of colored noise for speech
               enhan.pdf:application/pdf}
}

@inproceedings{podusenko_online_2020,
  address   = {Los Angeles, CA, USA},
  title     = {Online {Variational} {Message} {Passing} in {Hierarchical} {
               Autoregressive} {Models}},
  doi       = {10.1109/ISIT44484.2020.9174134},
  abstract  = {Hierarchical autoregressive (AR) models can describe many complex
               physical processes. Unfortunately, online adaptation in these
               models under non-stationary conditions remains a challenge. In this
               paper, we track states and parameters in a hierarchical AR filter
               by means of variational message passing (VMP) in a factor graph. We
               derive VMP update rules for an "AR node" that can be re-used at
               various hierarchical levels and supports automated message
               passing-based inference for states and parameters. The proposed
               method is experimentally validated for a 2-level hierarchical AR
               model.},
  booktitle = {2020 {IEEE} {International} {Symposium} on {Information} {Theory}
               ({ISIT})},
  author    = {Podusenko, Albert and Kouw, Wouter M. and de Vries, Bert},
  month     = jun,
  year      = {2020},
  note      = {ISSN: 2157-8117},
  keywords  = {graph theory, expectation-maximisation algorithm, message passing,
               autoregressive processes, factor graph, 2-level hierarchical AR
               model, complex physical processes, hierarchical AR filter,
               hierarchical autoregressive models, hierarchical levels, message
               passing-based inference, nonstationary conditions, online
               adaptation, online variational message passing, VMP update rules},
  pages     = {1337--1342},
  file      = {IEEE Xplore Abstract
               Record:/Users/apodusenko/Zotero/storage/TM3BLRII/9174134.html:text/html;IEEE
               Xplore Full Text
               PDF:/Users/apodusenko/Zotero/storage/Z6S4YI44/Podusenko et al. - 2020 -
               Online Variational Message Passing in
               Hierarchical.pdf:application/pdf;Podusenko et al. - 2020 - Online
               Variational Message Passing in
               Hierarchical.pdf:/Users/apodusenko/Zotero/storage/BCNPMTKK/Podusenko et
               al. - 2020 - Online Variational Message Passing in
               Hierarchical.pdf:application/pdf}
}

@inproceedings{pearl_reverend_1982,
  address    = {Pittsburgh, Pennsylvania},
  series     = {{AAAI}'82},
  title      = {Reverend {Bayes} on {Inference} {Engines}: {A} {Distributed} {
                Hierarchical} {Approach}},
  shorttitle = {Reverend {Bayes} on {Inference} {Engines}},
  url        = {http://www.aaai.org/Papers/AAAI/1982/AAAI82-032.pdf},
  abstract   = {This paper presents generalizations of Bayes likelihood-ratio
                updating rule which facilitate an asynchronous propagation of the
                impacts of new beliefs and/or new evidence in hierarchically
                organized inference structures with multi-hypotheses variables. The
                computational scheme proposed specifies a set of belief parameters,
                communication messages and updating rules which guarantee that the
                diffusion of updated beliefs is accomplished in a single pass and
                complies with the tenets of Bayes calculus.},
  urldate    = {2017-07-28},
  booktitle  = {Proceedings of the {Second} {AAAI} {Conference} on {Artificial} {
                Intelligence}},
  publisher  = {AAAI Press},
  author     = {Pearl, Judea},
  year       = {1982},
  pages      = {133--136}
}

@inproceedings{paliwal_speech_1987,
  address   = {Dallas, TX, USA},
  title     = {A speech enhancement method based on {Kalman} filtering},
  volume    = {12},
  doi       = {10.1109/ICASSP.1987.1169756},
  abstract  = {In this paper, the problem of speech enhancement when only
               corrupted speech signal is available for processing is considered.
               For this, the Kalman filtering method is studied and compared with
               the Wiener filtering method. Its performance is found to be
               significantly better than the Wiener filtering method. A
               delayed-Kalman filtering method is also proposed which improves the
               speech enhancement performance of Kalman filter further.},
  booktitle = {{ICASSP} '87. {IEEE} {International} {Conference} on {Acoustics},
               {Speech}, and {Signal} {Processing}},
  author    = {Paliwal, K. and Basu, A.},
  month     = apr,
  year      = {1987},
  keywords  = {Kalman filters, Equations, White noise, Speech enhancement, Speech
               processing, State estimation, Signal processing, Filtering, Wiener
               filter, Delay},
  pages     = {177--180},
  file      = {IEEE Xplore Abstract
               Record:/Users/apodusenko/Zotero/storage/W9TPAEZ4/1169756.html:text/html;IEEE
               Xplore Full Text PDF:/Users/apodusenko/Zotero/storage/CZ75LFRM/Paliwal
               and Basu - 1987 - A speech enhancement method based on Kalman
               filter.pdf:application/pdf;Paliwal and Basu - 1987 - A speech
               enhancement method based on Kalman
               filter.html:/Users/apodusenko/Zotero/storage/2G8HGL9D/Paliwal and Basu
               - 1987 - A speech enhancement method based on Kalman
               filter.html:text/html;Paliwal and Basu - 1987 - A speech enhancement
               method based on Kalman
               filter.pdf:/Users/apodusenko/Zotero/storage/2H6GB6YY/Paliwal and Basu -
               1987 - A speech enhancement method based on Kalman
               filter.pdf:application/pdf}
}

@article{ozerov_multichannel_2010,
  title    = {Multichannel {Nonnegative} {Matrix} {Factorization} in {Convolutive}
              {Mixtures} for {Audio} {Source} {Separation}},
  volume   = {18},
  issn     = {1558-7924},
  doi      = {10.1109/TASL.2009.2031510},
  abstract = {We consider inference in a general data-driven object-based model
              of multichannel audio data, assumed generated as a possibly
              underdetermined convolutive mixture of source signals. We work in
              the short-time Fourier transform (STFT) domain, where convolution
              is routinely approximated as linear instantaneous mixing in each
              frequency band. Each source STFT is given a model inspired from
              nonnegative matrix factorization (NMF) with the Itakura-Saito
              divergence, which underlies a statistical model of superimposed
              Gaussian components. We address estimation of the mixing and source
              parameters using two methods. The first one consists of maximizing
              the exact joint likelihood of the multichannel data using an
              expectation-maximization (EM) algorithm. The second method consists
              of maximizing the sum of individual likelihoods of all channels
              using a multiplicative update algorithm inspired from NMF
              methodology. Our decomposition algorithms are applied to stereo
              audio source separation in various settings, covering blind and
              supervised separation, music and speech sources, synthetic
              instantaneous and convolutive mixtures, as well as professionally
              produced music recordings. Our EM method produces competitive
              results with respect to state-of-the-art as illustrated on two
              tasks from the international Signal Separation Evaluation Campaign
              (SiSEC 2008).},
  number   = {3},
  journal  = {IEEE Transactions on Audio, Speech, and Language Processing},
  author   = {Ozerov, A. and Fevotte, C.},
  month    = mar,
  year     = {2010},
  keywords = {Fourier transforms, Filters, Additive noise, statistical analysis,
              speech processing, Frequency, Tensile stress, audio signal
              processing, Signal generators, matrix decomposition, Source
              separation, Spectrogram, statistical model,
              expectation-maximization algorithm, Matrix decomposition, Fourier
              transform, blind separation, blind source separation, convolution,
              convolutive mixtures, decomposition algorithm,
              Expectation-maximization (EM) algorithm, general data-driven
              object-based model, Itakura-Saito divergence, linear instantaneous
              mixing, multichannel audio, multichannel audio data, multichannel
              data, multichannel nonnegative matrix factorization, multiplicative
              update algorithm, music sources, NMF methodology, nonnegative
              matrix factorization (NMF), nonnegative tensor factorization (NTF),
              signal separation evaluation campaign, source signals, speech
              sources, stereo audio source separation, superimposed Gaussian
              component, supervised separation, synthetic instantaneous mixtures,
              Telecommunications, underdetermined convolutive blind source
              separation (BSS), underdetermined convolutive mixture},
  pages    = {550--563},
  file     = {IEEE Xplore Abstract
              Record:/Users/apodusenko/Zotero/storage/3NFZ65SE/5229304.html:text/html;IEEE
              Xplore Full Text PDF:/Users/apodusenko/Zotero/storage/X2VC9Y96/Ozerov
              and Fevotte - 2010 - Multichannel Nonnegative Matrix Factorization in
              C.pdf:application/pdf}
}

@article{loeliger_factor_2007,
  title    = {The {Factor} {Graph} {Approach} to {Model}-{Based} {Signal} {
              Processing}},
  volume   = {95},
  issn     = {0018-9219},
  doi      = {10.1109/JPROC.2007.896497},
  abstract = {The message-passing approach to model-based signal processing is
              developed with a focus on Gaussian message passing in linear
              state-space models, which includes recursive least squares, linear
              minimum-mean-squared-error estimation, and Kalman filtering
              algorithms. Tabulated message computation rules for the building
              blocks of linear models allow us to compose a variety of such
              algorithms without additional derivations or computations. Beyond
              the Gaussian case, it is emphasized that the message-passing
              approach encourages us to mix and match different algorithmic
              techniques, which is exemplified by two different approaches -
              steepest descent and expectation maximization - to message passing
              through a multiplier node.},
  number   = {6},
  urldate  = {2014-04-10},
  journal  = {Proceedings of the IEEE},
  author   = {Loeliger, Hans-Andrea and Dauwels, Justin and Hu, Junli and Korl,
              Sascha and Ping, Li and Kschischang, Frank R.},
  month    = jun,
  year     = {2007},
  keywords = {Machine learning algorithms, Estimation, Kalman filters, Message
              passing, Graphical models, factor graphs, graph theory, expectation
              maximization, expectation-maximisation algorithm, graphical models,
              Information technology, message passing, Signal processing
              algorithms, least mean squares methods, filtering theory, Algorithm
              design and analysis, Kalman filtering, Signal processing, signal
              processing, steepest descent, factor graph approach, Gaussian
              message passing, Least squares approximation, Kalman filtering
              algorithms, linear minimum-mean-squared-error estimation, linear
              state-space models, message-passing approach, model-based signal
              processing, recursive least squares, Signal design, tabulated
              message computation},
  pages    = {1295--1322},
  file     = {IEEE Xplore Abstract
              Record:/Users/apodusenko/Zotero/storage/SIYC5SNE/4282128.html:text/html;Loeliger
              et al. - 2007 - The Factor Graph Approach to Model-Based Signal
              Pr.pdf:/Users/apodusenko/Zotero/storage/EQLYHD43/Loeliger et al. - 2007
              - The Factor Graph Approach to Model-Based Signal
              Pr.pdf:application/pdf}
}

@article{loeliger_introduction_2004,
  title    = {An introduction to factor graphs},
  volume   = {21},
  url      = {https://ieeexplore.ieee.org/document/1267047},
  doi      = {10.1109/MSP.2004.1267047},
  abstract = {Graphical models such as factor graphs allow a unified approach to
              a number of key topics in coding and signal processing such as the
              iterative decoding of turbo codes, LDPC codes and similar codes,
              joint decoding, equalization, parameter estimation, hidden-Markov
              models, Kalman filtering, and recursive least squares. Graphical
              models can represent complex real-world systems, and such
              representations help to derive practical detection/estimation
              algorithms in a wide area of applications. Most known signal
              processing techniques -including gradient methods, Kalman filtering
              , and particle methods -can be used as components of such
              algorithms. Other than most of the previous literature, we have
              used Forney-style factor graphs, which support hierarchical
              modeling and are compatible with standard block diagrams.},
  number   = {1},
  urldate  = {2014-04-10},
  journal  = {Signal Processing Magazine, IEEE},
  author   = {Loeliger, Hans-Andrea},
  month    = jan,
  year     = {2004},
  keywords = {factor graphs, sum-product algorithm},
  pages    = {28--41},
  file     = {Loeliger - 2004 - An introduction to factor
              graphs.pdf:/Users/apodusenko/Zotero/storage/8TVTVXXU/Loeliger - 2004 -
              An introduction to factor graphs.pdf:application/pdf}
}

@article{kschischang_factor_2001,
  title   = {Factor graphs and the sum-product algorithm},
  volume  = {47},
  url     = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=910572},
  doi     = {10.1109/18.910572},
  number  = {2},
  urldate = {2014-04-10},
  journal = {IEEE Transactions on information theory},
  author  = {Kschischang, Frank R. and Frey, Brendan J. and Loeliger, H.-A.},
  year    = {2001},
  pages   = {498--519},
  file    = {Kschischang and Loeliger - 2001 - Factor Graphs and the Sum-Product
             Algorithm.pdf:/Users/apodusenko/Zotero/storage/M7C8QTPZ/Kschischang and
             Loeliger - 2001 - Factor Graphs and the Sum-Product
             Algorithm.pdf:application/pdf;Kschischang and Loeliger - 2001 - Factor
             Graphs and the Sum-Product
             Algorithm.pdf:/Users/apodusenko/Zotero/storage/AMMT3DXU/Kschischang and
             Loeliger - 2001 - Factor Graphs and the Sum-Product
             Algorithm.pdf:application/pdf;Kschischang et al. - 2001 - Factor graphs
             and the sum-product
             algorithm.pdf:/Users/apodusenko/Zotero/storage/MPFQNUX4/Kschischang et
             al. - 2001 - Factor graphs and the sum-product
             algorithm.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/V4985ARU/login.html:text/html;Snapshot:/Users/apodusenko/Zotero/storage/K7SDNMGF/login.html:text/html
             }
}

@article{kleibergen_bayesian_1995,
  title    = {Bayesian {Analysis} of {ARMA} models using {Noninformative} {Priors}},
  volume   = {1995-116},
  abstract = {Parameters in AutoRegressive Moving Average (ARMA) models are
              locally nonidenti ed, due to the problem of root cancellation.
              Parameters can be constructed which represent this identi cation
              problem. We argue that ARMA parameters should be analyzed
              conditional on these identifying parameters. Priors exploiting this
              feature result in regular posteriors, while priors which neglect it
              result in posteriori favor of nonidenti ed parameter values. By
              considering the implicit AR representation of an ARMA model a prior
              with the desired proporties is obtained. The implicit AR
              representation also allows to construct easily implemented
              algorithms to analyze ARMA parameters. As a byproduct, posteriors
              odds ratios can be computed to compare (nonnested) parsimonious
              ARMA models. The procedures are applied to two datasets, the
              (extended) Nelson-Plosser data and monthly observations of US
              3-month and 10 year interest rates. For approximately 50\% of the
              series in these two datasets an ARMA model is favored above an AR
              model.},
  language = {en},
  journal  = {CentER Discussion Paper},
  author   = {Kleibergen, Frank and Hoek, Henk},
  year     = {1995},
  pages    = {24},
  file     = {Kleibergen and Hoek - Bayesian Analysis of ARMA models using
              Noninformat.pdf:/Users/apodusenko/Zotero/storage/JPGDFRQ9/Kleibergen
              and Hoek - Bayesian Analysis of ARMA models using
              Noninformat.pdf:application/pdf}
}

@article{kates_multichannel_2005,
  title    = {Multichannel {Dynamic}-{Range} {Compression} {Using} {Digital} {
              Frequency} {Warping}},
  volume   = {18},
  url      = {http://asp.eurasipjournals.com/content/pdf/1687-6180-2005-483486.pdf},
  doi      = {10.1155/ASP.2005.3003},
  abstract = {A multichannel dynamic-range compressor system using digital
              frequency warping is described. A frequency-warped filter is
              realized by replacing the filter unit delays with all-pass filters.
              The appropriate design of the frequency warping gives a nonuniform
              frequency representation very close to the auditory Bark scale. The
              warped compressor is shown to have substantially reduced group
              delay in comparison with a conventional design having comparable
              frequency resolution. The warped compressor, however, has more
              delay at low than at high frequencies, which can lead to
              perceptible changes in the signal. The detection threshold for the
              compressor group delay was determined as a function of the number
              of all-pass filter sections in cascade needed for a detectible
              change in signal quality. The test signals included clicks, vowels,
              and speech, and results are presented for both normal-hearing and
              hearing-impaired subjects. Thresholds for clicks are lower than
              thresholds for vowels, and hearing-impaired subjects have higher
              thresholds than normal-hearing listeners. A frequency-warped
              compressor using a cascade of 31 all-pass filter sections offers a
              combination of low overall delay, good frequency resolution, and
              imperceptible frequency-dependent delay effects for most listening
              conditions.},
  journal  = {EURASIP Journal on Applied Signal Processing},
  author   = {Kates, James and Arehart, Kathryn},
  year     = {2005},
  keywords = {hearing aids, delay perception, dynamic-range compression,
              frequency warping, Multiple Dynamic Range Compression, Warp},
  pages    = {3003--3014},
  file     = {Kates and Arehart - 2005 - Multichannel Dynamic-Range Compression
              Using Digit.pdf:/Users/apodusenko/Zotero/storage/PILA44T3/Kates and
              Arehart - 2005 - Multichannel Dynamic-Range Compression Using
              Digit.pdf:application/pdf;Springer Full Text
              PDF:/Users/apodusenko/Zotero/storage/EIJVLGTE/Kates and Arehart - 2005
              - Multichannel Dynamic-Range Compression Using
              Digit.pdf:application/pdf}
}

@article{kappen_optimal_2012,
  title    = {Optimal control as a graphical model inference problem},
  volume   = {87},
  doi      = {10.1007/s10994-012-5278-7},
  abstract = {We reformulate a class of non-linear stochastic optimal control
              problems introduced by Todorov (in Advances in Neural Information
              Processing Systems, vol. 19, pp. 1369–1376, 2007) as a
              Kullback-Leibler (KL) minimization problem. As a result, the
              optimal control computation reduces to an inference computation and
              approximate inference methods can be applied to efficiently compute
              approximate optimal controls. We show how this KL control theory
              contains the path integral control method as a special case. We
              provide an example of a block stacking task and a multi-agent
              cooperative game where we demonstrate how approximate inference can
              be successfully applied to instances that are too complex for exact
              computation. We discuss the relation of the KL control approach to
              other inference approaches to control.},
  number   = {2},
  journal  = {Machine learning},
  author   = {Kappen, Hilbert J. and Gómez, Vicenç and Opper, Manfred},
  year     = {2012},
  pages    = {159--182},
  file     = {Full
              Text:/Users/apodusenko/Zotero/storage/ZYWLQXUH/s10994-012-5278-7.html:text/html
              }
}

@article{gibson_filtering_1991,
  title    = {Filtering of colored noise for speech enhancement and coding},
  volume   = {39},
  issn     = {1941-0476},
  doi      = {10.1109/78.91144},
  abstract = {Scalar and vector Kalman filters are implemented for filtering
              speech contaminated by additive white noise or colored noise, and
              an iterative signal and parameter estimator which can be used for
              both noise types is presented. Particular emphasis is placed on the
              removal of colored noise, such as helicopter noise, by using
              state-of-the-art colored-noise-assumption Kalman filters. The
              results indicate that the colored noise Kalman filters provide a
              significant gain in signal-to-noise ratio (SNR), a visible
              improvement in the sound spectrogram, and an audible improvement in
              output speech quality, none of which are available with
              white-noise-assumption Kalman and Wiener filters. When the filter
              is used as a prefilter for linear predictive coding, the coded
              output speech quality and intelligibility are enhanced in
              comparison to direct coding of the noisy speech.{\textless}{
              \textgreater}},
  number   = {8},
  journal  = {IEEE Transactions on Signal Processing},
  author   = {Gibson, J.D. and Koo, B. and Gray, S.D.},
  month    = aug,
  year     = {1991},
  keywords = {Parameter estimation, Acoustic noise, Speech enhancement, Signal
              to noise ratio, Filtering, Speech coding, Wiener filter, Additive
              white noise, Colored noise, Helicopters},
  pages    = {1732--1742},
  file     = {IEEE Xplore Abstract
              Record:/Users/apodusenko/Zotero/storage/8IFL7MVD/91144.html:text/html;IEEE
              Xplore Full Text PDF:/Users/apodusenko/Zotero/storage/AJEBEXT7/Gibson
              et al. - 1991 - Filtering of colored noise for speech enhancement
              .pdf:application/pdf}
}

@article{friston_active_2015,
  title   = {Active inference and epistemic value},
  volume  = {6},
  doi     = {10.1080/17588928.2015.1020053},
  number  = {4},
  journal = {Cognitive Neuroscience},
  author  = {Friston, Karl and Rigoli, Francesco and Ognibene, Dimitri and Mathys
             , Christoph and Fitzgerald, Thomas and Pezzulo, Giovanni},
  month   = mar,
  year    = {2015},
  pages   = {187--214},
  file    = {Friston et al. - 2015 - Active inference and epistemic
             value.pdf:/Users/apodusenko/Zotero/storage/EAKPI7XM/Friston et al. -
             2015 - Active inference and epistemic
             value.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/F8ECBVA3/17588928.2015.html:text/html
             }
}

@inproceedings{dauwels_variational_2007,
  address   = {Nice, France},
  title     = {On {Variational} {Message} {Passing} on {Factor} {Graphs}},
  url       = {http://ieeexplore.ieee.org/abstract/document/4557602},
  doi       = {10.1109/ISIT.2007.4557602},
  abstract  = {In this paper, it is shown how (naive and structured) variational
               algorithms may be derived from a factor graph by mechanically
               applying generic message computation rules; in this way, one can
               bypass error-prone variational calculus. In prior work by Bishop et
               al., Xing et al., and Geiger, directed and undirected graphical
               models have been used for this purpose. The factor graph notation
               amounts to simpler generic variational message computation rules;
               by means of factor graphs, variational methods can
               straightforwardly be compared to and combined with various other
               message-passing inference algorithms, e.g., Kalman filters and
               smoothers, iterated conditional modes, expectation maximization
               (EM), gradient methods, and particle filters. Some of those
               combinations have been explored in the literature, others seem to
               be new. Generic message computation rules for such combinations are
               formulated.},
  booktitle = {{IEEE} {International} {Symposium} on {Information} {Theory}},
  author    = {Dauwels, Justin},
  month     = jun,
  year      = {2007},
  keywords  = {Kalman filters, Message passing, information theory, Graphical
               models, factor graphs, graph theory, Inference algorithms,
               inference mechanisms, expectation maximization,
               expectation-maximisation algorithm, message passing, Particle
               filters, History, Random variables, variational techniques, State
               estimation, particle filtering (numerical methods), gradient
               methods, State-space methods, variational message passing, Calculus
               , error-prone variational calculus, generic variational message
               computation rules, Gradient methods, iterated conditional modes,
               message-passing inference algorithms, particle filters},
  pages     = {2546--2550},
  file      = {Dauwels - 2007 - (long article) On Variational Message Passing on
               Factor Graphs.pdf:/Users/apodusenko/Zotero/storage/QGTK43RA/Dauwels -
               2007 - (long article) On Variational Message Passing on Factor
               Graphs.pdf:application/pdf;IEEE Xplore Abstract
               Record:/Users/apodusenko/Zotero/storage/JJ36RIZM/4557602.html:text/html
               }
}

@inproceedings{frey_algonquin_2001,
  address   = {Aalborg, Denmark},
  title     = {{ALGONQUIN}: {Iterating} {Laplace}'s {Method} to {Remove} {Multiple}
               {Types} of {Acoustic} {Distortion} for {Robust} {Speech} {Recognition}
               },
  abstract  = {One approach to robust speech recognition is to use a simple
               speech model to remove the distortion, before applying the speech
               recognizer. Previous attempts at this approach have relied on
               unimodal or point estimates of the noise for each utterance. In
               challenging acoustic environments, e.g., an airport, the spectrum
               of the noise changes rapidly during an utterance, making a point
               estimate a poor representation. We show how an iterative form of
               Laplace’s method can be used to estimate the clean speech, using a
               time-varying probability model of the log-spectra of the clean
               speech, noise and channel distortion. We use this method, called
               ALGONQUIN, to denoise speech features and then feed these features
               into a large vocabulary speech recognizer whose WER on the clean
               Wall Street Journal data is 4.9\%. When 10 dB of noise consisting
               of an airplane engine shutting down is added to the data, the
               recognizer obtains a WER of 28.8\%. ALGONQUIN reduces the WER to
               12.6\%, well below the WER of 25.0\% obtained by our spectral
               subtraction algorithm, and close to the WER of 9.7\% obtained by
               the slow procedure of retraining the recognizer on training data
               corrupted by the exact same noise. In fact, if ALGONQUIN is used to
               denoise the noisy training data before the recognizer is retrained,
               the WER is improved to 8.5\%. For 10 dB of additive uniform white
               noise, our spectral subtraction algorithm reduces the WER from 55.1
               \% to 33.8\%. ALGONQUIN reduces the WER to 14.2\%. The recognizer
               trained on noisy data obtains a WER of 14\%, whereas the recognizer
               trained on noisy data denoised by ALGONQUIN obtains a WER of 9.9\%.
               },
  language  = {en},
  booktitle = {Proceedings of the {Eurospeech} {Conference}},
  author    = {Frey, Brendan J and Deng, Li and Acero, Alex and Kristjansson,
               Trausti},
  month     = sep,
  year      = {2001},
  pages     = {901--904},
  file      = {Frey et al. - 2001 - ALGONQUIN Iterating Laplace's Method to Remove
               Mu.pdf:/Users/apodusenko/Zotero/storage/MYQT488R/Frey et al. - 2001 -
               ALGONQUIN Iterating Laplace's Method to Remove Mu.pdf:application/pdf}
}

@article{alamdari_personalization_2020,
  title    = {Personalization of {Hearing} {Aid} {Compression} by {Human}-in-the-{
              Loop} {Deep} {Reinforcement} {Learning}},
  volume   = {8},
  issn     = {2169-3536},
  doi      = {10.1109/ACCESS.2020.3035728},
  abstract = {Existing prescriptive compression strategies used in hearing aid
              fitting are designed based on gain averages from a group of users
              which may not be necessarily optimal for a specific user. Nearly
              half of hearing aid users prefer settings that differ from the
              commonly prescribed settings. This paper presents a
              human-in-the-loop deep reinforcement learning approach that
              personalizes hearing aid compression to achieve improved hearing
              perception. The developed approach is designed to learn a specific
              user's hearing preferences in order to optimize compression based
              on the user's feedbacks. Both simulation and subject testing
              results are reported. These results demonstrate the
              proof-of-concept of achieving personalized compression via
              human-in-the-loop deep reinforcement learning.},
  journal  = {IEEE Access},
  author   = {Alamdari, Nasim and Lobarinas, Edward and Kehtarnavaz, Nasser},
  year     = {2020},
  keywords = {Auditory system, Hearing aids, Testing, Reinforcement learning,
              deep reinforcement learning, Fitting, hearing aid compression,
              human-in-the-loop personalization, Personalized audio compression,
              personalized hearing aid},
  pages    = {203503--203515},
  file     = {IEEE Xplore Abstract
              Record:/Users/apodusenko/Zotero/storage/VJF287RC/9247199.html:text/html;IEEE
              Xplore Full Text PDF:/Users/apodusenko/Zotero/storage/6AL9D34A/Alamdari
              et al. - 2020 - Personalization of Hearing Aid Compression by
              Huma.pdf:application/pdf}
}

@article{reddy_individualized_2017,
  title    = {An {Individualized} {Super}-{Gaussian} {Single} {Microphone} {Speech}
              {Enhancement} for {Hearing} {Aid} {Users} {With} {Smartphone} as an {
              Assistive} {Device}},
  volume   = {24},
  issn     = {1558-2361},
  doi      = {10.1109/LSP.2017.2750979},
  abstract = {In this letter, we derive a new super-Gaussian joint maximum a
              posteriori (SGJMAP) based single microphone speech enhancement gain
              function. The developed speech enhancement method is implemented on
              a smartphone, and this arrangement functions as an assistive device
              to hearing aids. We introduce a tradeoff parameter in the derived
              gain function that allows the smartphone user to customize their
              listening preference, by controlling the amount of noise
              suppression and speech distortion in real-time based on their level
              of hearing comfort perceived in noisy real-world acoustic
              environment. Objective quality and intelligibility measures show
              the effectiveness of the proposed method in comparison to benchmark
              techniques considered in this letter. Subjective results reflect
              the usefulness of the developed speech enhancement application in
              real-world noisy conditions at signal to noise ratio levels of -5,
              0, and 5 dB.},
  number   = {11},
  journal  = {IEEE Signal Processing Letters},
  author   = {Reddy, C. Karadagur Ananda and Shankar, N. and Bhat, G. Shreedhar
              and Charan, R. and Panahi, I.},
  month    = nov,
  year     = {2017},
  keywords = {Auditory system, hearing aids, noise suppression, Speech
              enhancement, Noise measurement, Speech, Real-time systems, smart
              phones, maximum likelihood estimation, Microphones, speech
              enhancement, Distortion, speech distortion, signal denoising,
              assistive device, Customizable, derived gain function, hearing aid
              (HA), hearing aid users, hearing comfort level, individualized
              super-Gaussian single-microphone speech enhancement,
              intelligibility measure, noisy real-world acoustic environment,
              objective quality, real-world noisy conditions, SGJMAP,
              signal-to-noise ratio level, single-microphone speech enhancement
              gain function, smartphone, smartphone user, speech enhancement (SE)
              , super-Gaussian, super-Gaussian joint maximum a posteriori},
  pages    = {1601--1605},
  file     = {IEEE Xplore Abstract
              Record:/Users/apodusenko/Zotero/storage/8XUIMYQK/8031044.html:text/html;IEEE
              Xplore Full Text PDF:/Users/apodusenko/Zotero/storage/ZUN9V6BG/Reddy et
              al. - 2017 - An Individualized Super-Gaussian Single
              Microphone.pdf:application/pdf}
}

@book{rasmussen_gaussian_2006,
  title     = {Gaussian {Processes} for {Machine} {Learning}},
  url       = {http://mitpress.mit.edu/books/chapters/026218253Xchap2.pdf},
  publisher = {MIT Press},
  author    = {Rasmussen, Carl Edward and Williams, Christopher K. I},
  year      = {2006},
  keywords  = {machine learning, Gaussian processes},
  file      = {Rasmussen and Williams - 2006 - Gaussian Processes for Machine
               Learning.pdf:/Users/apodusenko/Zotero/storage/VNXZQDMP/Rasmussen and
               Williams - 2006 - Gaussian Processes for Machine
               Learning.pdf:application/pdf}
}

@article{rudoy_time-varying_2011,
  title      = {Time-{Varying} {Autoregressions} in {Speech}: {Detection} {Theory}
                and {Applications}},
  volume     = {19},
  issn       = {1558-7924},
  shorttitle = {Time-{Varying} {Autoregressions} in {Speech}},
  doi        = {10.1109/TASL.2010.2073704},
  abstract   = {This paper develops a general detection theory for speech analysis
                based on time-varying autoregressive models, which themselves
                generalize the classical linear predictive speech analysis
                framework. This theory leads to a computationally efficient
                decision-theoretic procedure that may be applied to detect the
                presence of vocal tract variation in speech waveform data. A
                corresponding generalized likelihood ratio test is derived and
                studied both empirically for short data records, using formant-like
                synthetic examples, and asymptotically, leading to constant false
                alarm rate hypothesis tests for changes in vocal tract
                configuration. Two in-depth case studies then serve to illustrate
                the practical efficacy of this procedure across different time
                scales of speech dynamics: first, the detection of formant changes
                on the scale of tens of milliseconds of data, and second, the
                identification of glottal opening and closing instants on time
                scales below ten milliseconds.},
  number     = {4},
  journal    = {IEEE Transactions on Audio, Speech, and Language Processing},
  author     = {Rudoy, Daniel and Quatieri, Thomas F. and Wolfe, Patrick J.},
  month      = may,
  year       = {2011},
  keywords   = {Statistics - Applications, autoregressive processes, Speech,
                Speech analysis, Maximum likelihood estimation, speech processing,
                Trajectory, Computational modeling, Predictive models, time-varying
                systems, linear prediction, TV, decision theory, decision-theoretic
                procedure, detection theory, false alarm rate, generalized
                likelihood ratio test, Glottal airflow, glottal opening, likelihood
                ratio test, linear predictive speech analysis, maximum likelihood
                detection, nonstationary time series, prediction theory, speech
                dynamics, speech waveform data, time-varying autoregressions, vocal
                tract variation},
  pages      = {977--989},
  file       = {arXiv Fulltext PDF:/Users/apodusenko/Zotero/storage/7WP9SB94/Rudoy et
                al. - 2011 - Time-Varying Autoregressions in Speech Detection
                .pdf:application/pdf;arXiv.org
                Snapshot:/Users/apodusenko/Zotero/storage/A8YR5NGT/0911.html:text/html;IEEE
                Xplore Abstract
                Record:/Users/apodusenko/Zotero/storage/EIQW83XD/5570952.html:text/html;IEEE
                Xplore Abstract
                Record:/Users/apodusenko/Zotero/storage/Q4W4IRGG/5570952.html:text/html;IEEE
                Xplore Full Text PDF:/Users/apodusenko/Zotero/storage/J9WLT3IG/Rudoy et
                al. - 2011 - Time-Varying Autoregressions in Speech Detection
                .pdf:application/pdf;IEEE Xplore Full Text
                PDF:/Users/apodusenko/Zotero/storage/YZEALFYR/Rudoy et al. - 2011 -
                Time-Varying Autoregressions in Speech Detection .pdf:application/pdf}
}

@article{knuth_bayesian_2014,
  title    = {Bayesian {Evidence} and {Model} {Selection}},
  url      = {http://arxiv.org/abs/1411.3013},
  abstract = {In this paper we review the concept of the Bayesian evidence and
              its application to model selection. The theory is presented along
              with a discussion of analytic, approximate and numerical
              techniques. Application to several practical examples within the
              context of signal processing are discussed.},
  urldate  = {2014-12-01},
  journal  = {arXiv:1411.3013 [astro-ph, stat]},
  author   = {Knuth, Kevin H. and Habeck, Michael and Malakar, Nabin K. and Mubeen
              , Asim M. and Placek, Ben},
  month    = nov,
  year     = {2014},
  note     = {arXiv: 1411.3013},
  keywords = {Statistics - Machine Learning, Statistics - Computation,
              Astrophysics - Instrumentation and Methods for Astrophysics,
              Bayesian evidence, Bayesian signal processing, Model testing,
              Nested sampling, Odds ratio, Statistics - Applications, Statistics
              - Methodology},
  file     = {arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/86VERHRT/1411.html:text/html;arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/X5XKQVT5/1411.html:text/html;Knuth
              et al. - 2014 - Bayesian Evidence and Model
              Selection.pdf:/Users/apodusenko/Zotero/storage/NZS76NQZ/Knuth et al. -
              2014 - Bayesian Evidence and Model Selection.pdf:application/pdf;Knuth
              et al. - 2014 - Bayesian Evidence and Model
              Selection.pdf:/Users/apodusenko/Zotero/storage/DDWXS484/Knuth et al. -
              2014 - Bayesian Evidence and Model Selection.pdf:application/pdf;SCOPUS
              Snapshot:/Users/apodusenko/Zotero/storage/WKB454LS/display.html:text/html;Submitted
              Version:/Users/apodusenko/Zotero/storage/ZYAWBD28/Knuth et al. - 2015 -
              Bayesian evidence and model selection.pdf:application/pdf}
}

@techreport{smith_step-by-step_2021,
  title       = {A {Step}-by-{Step} {Tutorial} on {Active} {Inference} and its {
                 Application} to {Empirical} {Data}},
  url         = {https://psyarxiv.com/b4jm6/},
  abstract    = {The active inference framework, and in particular its recent
                 formulation as a partially observable Markov decision process
                 (POMDP), has gained increasing popularity in recent years as a
                 useful approach for modelling neurocognitive processes. This
                 framework is highly general and flexible in its ability to be
                 customized to model any cognitive process, as well as simulate
                 predicted neuronal responses based on its accompanying neural
                 process theory. It also affords both simulation experiments for
                 proof of principle and behavioral modelling for empirical studies.
                 However, there are limited resources that explain how to build and
                 run these models in practice, which limits their widespread use.
                 Most introductions assume a technical background in programming,
                 mathematics, and machine learning. In this paper we offer a
                 step-by-step tutorial on how to build POMDPs, run simulations using
                 standard MATLAB routines, and fit these models to empirical data.
                 We assume a minimal background in programming and mathematics,
                 thoroughly explain all equations, and provide exemplar scripts that
                 can be customized for both theoretical and empirical studies. Our
                 goal is to provide the reader with the requisite background
                 knowledge and practical tools to apply active inference to their
                 own research. We also provide optional technical sections and
                 several appendices, which offer the interested reader additional
                 technical details. This tutorial should provide the reader with all
                 the tools necessary to use these models and to follow emerging
                 advances in active inference research.},
  urldate     = {2021-01-04},
  institution = {PsyArXiv},
  author      = {Smith, Ryan and Friston, Karl and Whyte, Christopher},
  month       = jan,
  year        = {2021},
  doi         = {10.31234/osf.io/b4jm6},
  keywords    = {Cognitive Psychology, Active Inference, Neuroscience,
                 Computational modeling, Cognitive Neuroscience, Computational
                 Neuroscience, Discrete state space models, Partially observable
                 Markov decision processes (POMDPs), Social and Behavioral Sciences,
                 Tutorial},
  file        = {Full Text PDF:/Users/apodusenko/Zotero/storage/LRGJX2RZ/Smith et al. -
                 2021 - A Step-by-Step Tutorial on Active Inference and
                 it.pdf:application/pdf;Smith et al. - 2021 - A Step-by-Step Tutorial on
                 Active Inference and
                 it.pdf:/Users/apodusenko/Zotero/storage/C9RET4H6/Smith et al. - 2021 -
                 A Step-by-Step Tutorial on Active Inference and it.pdf:application/pdf}
}

@article{zhang_co-optimizating_nodate,
  title    = {Co-{Optimizating} {Multi}-{Agent} {Placement} with {Task} {Assignment
              } and {Scheduling}},
  abstract = {To enable large-scale multi-agent coordination under temporal and
              spatial constraints, we formulate it as a multi-level optimization
              problem and develop a multi-abstraction search approach for
              cooptimizing agent placement with task assignment and scheduling.
              This approach begins with a highly abstract agent placement problem
              and the rapid computation of an initial solution, which is then
              improved upon using a hill climbing algorithm for a less abstract
              problem; ﬁnally, the solution is ﬁnetuned within the original
              problem space. Empirical results demonstrate that this
              multi-abstraction approach signiﬁcantly outperforms a conventional
              hill climbing algorithm and an approximate mixedinteger linear
              programming approach.},
  language = {en},
  author   = {Zhang, Chongjie and Shah, Julie A},
  pages    = {7},
  file     = {Zhang and Shah - Co-Optimizating Multi-Agent Placement with Task
              As.pdf:/Users/apodusenko/Zotero/storage/2DQZGPU7/Zhang and Shah -
              Co-Optimizating Multi-Agent Placement with Task
              As.pdf:application/pdf;Zhang and Shah - Co-Optimizating Multi-Agent
              Placement with Task
              As.pdf:/Users/apodusenko/Zotero/storage/W7SXJUJE/Zhang and Shah -
              Co-Optimizating Multi-Agent Placement with Task As.pdf:application/pdf}
}

@article{zwicker_subdivision_1961,
  title   = {Subdivision of the {Audible} {Frequency} {Range} into {Critical} {
             Bands} ({Frequenzgruppen})},
  volume  = {33},
  issn    = {0001-4966},
  url     = {https://asa.scitation.org/doi/abs/10.1121/1.1908630},
  doi     = {10.1121/1.1908630},
  number  = {2},
  urldate = {2020-02-21},
  journal = {The Journal of the Acoustical Society of America},
  author  = {Zwicker, E.},
  month   = feb,
  year    = {1961},
  pages   = {248--248},
  file    = {
             Snapshot:/Users/apodusenko/Zotero/storage/WJSXFREC/1.html:text/html;Zwicker
             - 1961 - Subdivision of the Audible Frequency Range into
             Cr.pdf:/Users/apodusenko/Zotero/storage/HQD77USC/Zwicker - 1961 -
             Subdivision of the Audible Frequency Range into Cr.pdf:application/pdf}
}

@inproceedings{bruderer_deconvolution_2015,
  address   = {Hong Kong, China},
  title     = {Deconvolution of weakly-sparse signals and dynamical-system
               identification by {Gaussian} message passing},
  doi       = {10.1109/ISIT.2015.7282470},
  abstract  = {We use ideas from sparse Bayesian learning for estimating the
               (weakly) sparse input signal of a linear state space model.
               Variational representations of the sparsifying prior lead to
               algorithms that essentially amount to Gaussian message passing. The
               approach is extended to the case where the state space model is not
               known and must be estimated. Experimental results with a real-world
               application substantiate the applicability of the proposed method.},
  booktitle = {2015 {IEEE} {International} {Symposium} on {Information} {Theory}
               ({ISIT})},
  author    = {Bruderer, L. and Malmberg, H. and Loeliger, H.},
  month     = jun,
  year      = {2015},
  note      = {ISSN: 2157-8117},
  keywords  = {Estimation, Message passing, message passing, Signal processing
               algorithms, Bayes methods, learning (artificial intelligence),
               signal representation, state-space methods, Computational modeling,
               Gaussian message passing, deconvolution, Dictionaries,
               Deconvolution, dynamical system identification, linear state space
               model, sparse Bayesian learning, variational representation,
               weakly-sparse signal deconvolution},
  pages     = {326--330},
  file      = {IEEE Xplore Abstract
               Record:/Users/apodusenko/Zotero/storage/VMQ4ZYZ3/7282470.html:text/html;IEEE
               Xplore Full Text PDF:/Users/apodusenko/Zotero/storage/X4JFE62I/Bruderer
               et al. - 2015 - Deconvolution of weakly-sparse signals and
               dynamic.pdf:application/pdf}
}

@inproceedings{loeliger_sparsity_2016,
  address   = {La Jolla, CA, USA},
  title     = {On sparsity by {NUV}-{EM}, {Gaussian} message passing, and {Kalman}
               smoothing},
  doi       = {10.1109/ITA.2016.7888168},
  abstract  = {Normal priors with unknown variance (NUV) have long been known to
               promote sparsity and to blend well with parameter learning by
               expectation maximization (EM). In this paper, we advocate this
               approach for linear state space models for applications such as the
               estimation of impulsive signals, the detection of localized events,
               smoothing with occasional jumps in the state space, and the
               detection and removal of outliers. The actual computations boil
               down to multivariate-Gaussian message passing algorithms that are
               closely related to Kalman smoothing. We give improved tables of
               Gaussian-message computations from which such algorithms are easily
               synthesized, and we point out two preferred such algorithms.},
  booktitle = {2016 {Information} {Theory} and {Applications} {Workshop} ({ITA})
               },
  author    = {Loeliger, H. and Bruderer, L. and Malmberg, H. and Wadehn, F. and
               Zalmai, N.},
  month     = jan,
  year      = {2016},
  keywords  = {Kalman filters, Message passing, smoothing methods, expectation
               maximization, optimisation, Gaussian processes, linear state space
               models, Maximum likelihood estimation, Covariance matrices,
               Smoothing methods, Kalman smoothing, Computational modeling,
               Gaussian message passing, EM, normal priors with unknown variance,
               NUV, parameter learning},
  pages     = {1--10},
  file      = {IEEE Xplore Abstract
               Record:/Users/apodusenko/Zotero/storage/HUKB3IHV/7888168.html:text/html;IEEE
               Xplore Full Text PDF:/Users/apodusenko/Zotero/storage/NQTKPSSE/Loeliger
               et al. - 2016 - On sparsity by NUV-EM, Gaussian message passing,
               a.pdf:application/pdf}
}

@inproceedings{kouw_variational_2021,
  author    = {Kouw, Wouter M. and Podusenko, Albert and Koudahl, MagnusT. and Schoukens, Maarten},
  booktitle = {2022 American Control Conference (ACC)},
  title     = {Variational message passing for online polynomial NARMAX identification},
  year      = {2022},
  volume    = {},
  number    = {},
  pages     = {2755-2760},
  doi       = {10.23919/ACC53348.2022.9867898}
}

@article{granger_time_1976,
  title    = {Time {Series} {Modelling} and {Interpretation}},
  volume   = {139},
  issn     = {0035-9238},
  url      = {https://www.jstor.org/stable/2345178},
  doi      = {10.2307/2345178},
  abstract = {By considering the model generating the sum of two or more series,
              it is shown that the mixed ARMA model is the one most likely to
              occur. As most economic series are both aggregates and are measured
              with error it follows that such mixed models will often be found in
              practice. If such a model is found, the possibility of resolving
              the series into simple components is considered both theoretically
              and for simulated data.},
  number   = {2},
  urldate  = {2021-10-14},
  journal  = {Journal of the Royal Statistical Society. Series A (General)},
  author   = {Granger, C. W. J. and Morris, M. J.},
  year     = {1976},
  note     = {Publisher: [Royal Statistical Society, Wiley]},
  pages    = {246--257}
}

@article{van_erp_bayesian_2021,
  title     = {A {Bayesian} {Modeling} {Approach} to {Situated} {Design} of {
               Personalized} {Soundscaping} {Algorithms}},
  volume    = {11},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  url       = {https://www.mdpi.com/2076-3417/11/20/9535},
  doi       = {10.3390/app11209535},
  abstract  = {Effective noise reduction and speech enhancement algorithms have
               great potential to enhance lives of hearing aid users by restoring
               speech intelligibility. An open problem in today’s commercial
               hearing aids is how to take into account users’ preferences,
               indicating which acoustic sources should be suppressed or enhanced,
               since they are not only user-specific but also depend on many
               situational factors. In this paper, we develop a fully
               probabilistic approach to “situated soundscaping”, which aims at
               enabling users to make on-the-spot (“situated”) decisions about the
               enhancement or suppression of individual acoustic sources. The
               approach rests on a compact generative probabilistic model for
               acoustic signals. In this framework, all signal processing tasks
               (source modeling, source separation and soundscaping) are framed as
               automatable probabilistic inference tasks. These tasks can be
               efficiently executed using message passing-based inference on
               factor graphs. Since all signal processing tasks are automatable,
               the approach supports fast future model design cycles in an effort
               to reach commercializable performance levels. The presented results
               show promising performance in terms of SNR, PESQ and STOI
               improvements in a situated setting.},
  language  = {en},
  number    = {20},
  urldate   = {2021-10-14},
  journal   = {Applied Sciences},
  author    = {van Erp, Bart and Podusenko, Albert and Ignatenko, Tanya and de
               Vries, Bert},
  month     = oct,
  year      = {2021},
  note      = {Number: 20 Publisher: Multidisciplinary Digital Publishing Institute},
  keywords  = {factor graphs, noise reduction, speech enhancement, variational
               message passing, bayesian machine learning, situated soundscaping},
  pages     = {9535},
  file      = {Full Text PDF:/Users/apodusenko/Zotero/storage/XFK4U69L/van Erp et al.
               - 2021 - A Bayesian Modeling Approach to Situated Design
               of.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/SNHLCEJR/9535.html:text/html
               }
}

@inproceedings{mesot_bayesian_2007,
  title     = {A {Bayesian} {Alternative} to {Gain} {Adaptation} in {Autoregressive}
               {Hidden} {Markov} {Models}},
  volume    = {2},
  doi       = {10.1109/ICASSP.2007.366266},
  abstract  = {Models dealing directly with the raw acoustic speech signal are an
               alternative to conventional feature-based HMMs. A popular way to
               model the raw speech signal is by means of an autoregressive (AR)
               process. Being too simple to cope with the nonlinearity of the
               speech signal, the AR process is generally embedded into a more
               elaborate model, such as the switching autoregressive HMM
               (SAR-HMM). A fundamental issue faced by models based on AR
               processes is that they are very sensitive to variations in the
               amplitude of the signal. One way to overcome this limitation is to
               use gain adaptation to adjust the amplitude by maximising the
               likelihood of the observed signal. However, adjusting model
               parameters by maximising test likelihoods is fundamentally outside
               the framework of standard statistical approaches to machine
               learning, since this may lead to overfitting when the models are
               sufficiently flexible. We propose a statistically principled
               alternative based on an exact Bayesian procedure in which priors
               are explicitly defined on the parameters of the AR process.
               Explicitly, we present the Bayesian SAR-HMM and compare the
               performance of this model against the standard gain-adapted SAR-HMM
               on a single digit recognition task, showing the effectiveness of
               the approach and suggesting thereby a principled and
               straightforward solution to the issue of gain adaptation.},
  booktitle = {2007 {IEEE} {International} {Conference} on {Acoustics}, {Speech}
               and {Signal} {Processing} - {ICASSP} '07},
  author    = {Mesot, Bertrand and Barber, David},
  month     = apr,
  year      = {2007},
  note      = {ISSN: 2379-190X},
  keywords  = {Machine learning, Bayesian methods, Hidden Markov models,
               Performance gain, Testing, Speech processing, Signal processing,
               Autoregressive processes, Technological innovation, Speech
               recognition, Bayes procedures, Gain control},
  pages     = {II--437--II--440},
  file      = {IEEE Xplore Abstract
               Record:/Users/apodusenko/Zotero/storage/UQPNQEA3/4217439.html:text/html;IEEE
               Xplore Full Text PDF:/Users/apodusenko/Zotero/storage/3N3UDENW/Mesot
               and Barber - 2007 - A Bayesian Alternative to Gain Adaptation in
               Autor.pdf:application/pdf}
}

@article{ruli_improved_2016,
  title    = {Improved {Laplace} {Approximation} for {Marginal} {Likelihoods}},
  volume   = {10},
  issn     = {1935-7524},
  url      = {http://arxiv.org/abs/1502.06440},
  doi      = {10.1214/16-EJS1218},
  abstract = {Statistical applications often involve the calculation of
              intractable multidimensional integrals. The Laplace formula is
              widely used to approximate such integrals. However, in
              high-dimensional or small sample size problems, the shape of the
              integrand function may be far from that of the Gaussian density,
              and thus the standard Laplace approximation can be inaccurate. We
              propose an improved Laplace approximation that reduces the
              asymptotic error of the standard Laplace formula by one order of
              magnitude, thus leading to third-order accuracy. We also show, by
              means of practical examples of various complexity, that the
              proposed method is extremely accurate, even in high dimensions,
              improving over the standard Laplace formula. Such examples also
              demonstrate that the accuracy of the proposed method is comparable
              with that of other existing methods, which are computationally more
              demanding. An R implementation of the improved Laplace
              approximation is also provided through the R package iLaplace
              available on CRAN.},
  number   = {2},
  urldate  = {2021-10-11},
  journal  = {Electronic Journal of Statistics},
  author   = {Ruli, Erlis and Sartori, Nicola and Ventura, Laura},
  month    = jan,
  year     = {2016},
  note     = {arXiv: 1502.06440},
  keywords = {Statistics - Computation, Statistics - Methodology},
  file     = {arXiv Fulltext PDF:/Users/apodusenko/Zotero/storage/WE6VHEC8/Ruli et
              al. - 2016 - Improved Laplace Approximation for Marginal
              Likeli.pdf:application/pdf;arXiv.org
              Snapshot:/Users/apodusenko/Zotero/storage/L598E7IG/1502.html:text/html}
}

@article{boyd_finding_2013,
  title      = {Finding the {Zeros} of a {Univariate} {Equation}: {Proxy} {
                Rootfinders}, {Chebyshev} {Interpolation}, and the {Companion} {Matrix
                }},
  volume     = {55},
  issn       = {0036-1445, 1095-7200},
  shorttitle = {Finding the {Zeros} of a {Univariate} {Equation}},
  url        = {http://epubs.siam.org/doi/10.1137/110838297},
  doi        = {10.1137/110838297},
  abstract   = {When a function f (x) is holomorphic on an interval x ∈ [a, b],
                its roots on the interval can be computed by the following
                three-step procedure. First, approximate f (x) on [a, b] by a
                polynomial fN (x) using adaptive Chebyshev interpolation. Second,
                form the Chebyshev–Frobenius companion matrix whose elements are
                trivial functions of the Chebyshev coeﬃcients of the interpolant fN
                (x). Third, compute all the eigenvalues of the companion matrix.
                The eigenvalues λ which lie on the real interval λ ∈ [a, b] are
                very accurate approximations to the zeros of f (x) on the target
                interval. (To minimize cost, the adaptive phase can automatically
                subdivide the interval, applying the Chebyshev rootﬁnder separately
                on each subinterval, to keep N bounded or to solve rare “dynamic
                range” complications.) We also discuss generalizations to compute
                roots on an inﬁnite interval, zeros of functions singular on the
                interval [a, b], and slightly complex roots. The underlying ideas
                are undergraduate-friendly, but link the disparate ﬁelds of
                algebraic geometry, linear algebra, and approximation theory.},
  language   = {en},
  number     = {2},
  urldate    = {2021-10-11},
  journal    = {SIAM Review},
  author     = {Boyd, John P.},
  month      = jan,
  year       = {2013},
  pages      = {375--396},
  file       = {Boyd - 2013 - Finding the Zeros of a Univariate Equation Proxy
                .pdf:/Users/apodusenko/Zotero/storage/Z8DWJZ9I/Boyd - 2013 - Finding
                the Zeros of a Univariate Equation Proxy .pdf:application/pdf}
}

@article{sebah_newtons_nodate,
  title    = {Newton’s method and high order iterations},
  abstract = {An introduction to the famous method introduced by Newton to solve
              non linear equations. It includes a description of higher order
              methods (cubic and more).},
  language = {en},
  author   = {Sebah, Pascal and Gourdon, Xavier},
  pages    = {10},
  file     = {Sebah and Gourdon - Newton’s method and high order
              iterations.pdf:/Users/apodusenko/Zotero/storage/HZU6GT3N/Sebah and
              Gourdon - Newton’s method and high order iterations.pdf:application/pdf
              }
}

@inproceedings{wadehn_state_2019,
  address   = {A Coruna, Spain},
  title     = {State {Space} {Models} with {Dynamical} and {Sparse} {Variances}, and
               {Inference} by {EM} {Message} {Passing}},
  isbn      = {978-90-827970-3-9},
  url       = {https://ieeexplore.ieee.org/document/8902815/},
  doi       = {10.23919/EUSIPCO.2019.8902815},
  abstract  = {Sparse Bayesian learning (SBL) is a probabilistic approach to
               estimation problems based on representing sparsitypromoting priors
               by Normals with Unknown Variances. This representation blends well
               with linear Gaussian state space models (SSMs). However, in
               classical SBL the unknown variances are a priori independent, which
               is not suited for modeling group sparse signals, or signals whose
               variances have structure. To model signals with, e.g.,
               exponentially decaying or piecewiseconstant (in particular
               block-sparse) variances, we propose SSMs with dynamical and sparse
               variances (SSM-DSV). These are twolayer SSMs, where the bottom
               layer models physical signals, and the top layer models dynamical
               variances that are subject to abrupt changes. Inference and
               learning in these hierarchical models is performed with a message
               passing version of the expectation maximization (EM) algorithm,
               which is a special instance of the more general class of
               variational message passing algorithms. We validated the proposed
               model and estimation algorithm with two applications, using both
               simulated and real data. First, we implemented a block-outlier
               insensitive Kalman smoother by modeling the disturbance process
               with a SSM-DSV. Second, we used SSM-DSV to model the oculomotor
               system and employed EM-message passing for estimating neural
               controller signals from eye position data.},
  language  = {en},
  urldate   = {2021-10-08},
  booktitle = {2019 27th {European} {Signal} {Processing} {Conference} ({EUSIPCO
               })},
  publisher = {IEEE},
  author    = {Wadehn, Federico and Weber, Thilo and Loeliger, Hans-Andrea},
  month     = sep,
  year      = {2019},
  pages     = {1--5},
  file      = {Wadehn et al. - 2019 - State Space Models with Dynamical and Sparse
               Varia.pdf:/Users/apodusenko/Zotero/storage/KQC4HMQB/Wadehn et al. -
               2019 - State Space Models with Dynamical and Sparse
               Varia.pdf:application/pdf}
}

@article{hoffmann_linear_2017-1,
  series   = {20th {IFAC} {World} {Congress}},
  title    = {Linear {Optimal} {Control} on {Factor} {Graphs} — {A} {Message} {
              Passing} {Perspective} —},
  volume   = {50},
  issn     = {2405-8963},
  url      = {https://www.sciencedirect.com/science/article/pii/S2405896317313800},
  doi      = {10.1016/j.ifacol.2017.08.914},
  abstract = {Factor graphs form a class of probabilistic graphical models
              representing the factorization of probability density functions as
              bipartite graphs. They can be used to exploit the conditional
              independence structure of the underlying model to efficiently solve
              inference problems by message passing. The present paper advocates
              the use of factor graphs in control and highlights similarities to,
              e. g., signal processing and communications where this class of
              models is widely used. By applying the factor graph framework to a
              probabilistic interpretation of optimal control, several classical
              results are recovered. The dynamic programming approach to linear
              quadratic Gaussian control is described as a message passing
              algorithm on factor graph on which possible extensions are
              exemplified. A factor graph-based iterative learning control scheme
              is outlined and an expectation maximization-based estimation of
              normal unknown variance priors is adapted for the derivation of
              sparse control signals, highlighting the benefits of using a
              unified framework across disciplines by mixing and matching
              corresponding graphical algorithms.},
  language = {en},
  number   = {1},
  urldate  = {2021-04-22},
  journal  = {IFAC-PapersOnLine},
  author   = {Hoffmann, Christian and Rostalski, Philipp},
  month    = jul,
  year     = {2017},
  keywords = {Message passing, Kalman filtering, Factor graphs, Expectation
              maximization, Iterative learning control, LQG control, Normal
              unknown variance prior, Probabilistic models},
  pages    = {6314--6319},
  file     = {Hoffmann and Rostalski - 2017 - Linear Optimal Control on Factor
              Graphs — A
              Messag.pdf:/Users/apodusenko/Zotero/storage/ZCDAZVJN/Hoffmann and
              Rostalski - 2017 - Linear Optimal Control on Factor Graphs — A
              Messag.pdf:application/pdf;ScienceDirect
              Snapshot:/Users/apodusenko/Zotero/storage/XABMTEU8/S2405896317313800.html:text/html
              }
}

@article{conant_every_1970,
  title    = {Every good regulator of a system must be a model of that system},
  abstract = {The design of a complex regulator often includes the making of a
              model of the system to be regulated. The making of such a model has
              hitherto been regarded as optional, as merely one of many possible
              ways. m this paper a theorem is presented which shows, under very
              broad conditions, that any regulator that is maximally both
              successful and simple must be isomorphic with the system being
              regulated. (The exact assumptions are given.) Making a model is
              thus necessary. The theorem has the interesting corollary that the
              living brain, so far as it is to be successful and efficient as a
              regulator for survival, must proceed, in learning, by the formation
              of a model (or models) of its environment. 1.},
  journal  = {Intl. J. Systems Science},
  author   = {Conant, Roger C. and Ashby, W. Ross},
  year     = {1970},
  pages    = {89--97},
  file     = {Citeseer -
              Snapshot:/Users/apodusenko/Zotero/storage/X2RRVEBQ/summary.html:text/html;Conant
              and Ashby - 1970 - Every good regulator of a system must be a model
              o.pdf:/Users/apodusenko/Zotero/storage/U564PLUL/Conant and Ashby - 1970
              - Every good regulator of a system must be a model
              o.pdf:application/pdf}
}

@article{tabak_family_2013,
  title    = {A {Family} of {Nonparametric} {Density} {Estimation} {Algorithms}},
  volume   = {66},
  issn     = {00103640},
  doi      = {10.1002/cpa.21423},
  abstract = {A new methodology for density estimation is proposed. The
              methodology, which builds on the one developed in [17], normalizes
              the data points through the composition of simple maps. The
              parameters of each map are determined through the maximization of a
              local quadratic approximation to the log-likelihood. Various
              candidates for the elementary maps of each step are proposed;
              criteria for choosing one includes robustness, computational
              simplicity and good behavior in high-dimensional settings. A good
              choice is that of localized radial expansions, which depend on a
              single parameter: all the complexity of arbitrary, possibly
              convoluted probability densities can be built through the
              composition of such simple maps. c 2000 Wiley Periodicals, Inc.},
  language = {en},
  number   = {2},
  journal  = {Communications on Pure and Applied Mathematics},
  author   = {Tabak, E. G. and Turner, Cristina V.},
  month    = feb,
  year     = {2013},
  pages    = {145--164},
  file     = {Tabak and Turner - 2013 - A Family of Nonparametric Density Estimation
              Algor.pdf:/Users/apodusenko/Zotero/storage/TI2JCJHC/Tabak and Turner -
              2013 - A Family of Nonparametric Density Estimation
              Algor.pdf:application/pdf}
}

@article{tabak_density_2010,
  title    = {Density estimation by dual ascent of the log-likelihood},
  volume   = {8},
  issn     = {1539-6746, 1945-0796},
  abstract = {A methodology is developed to assign, from an observed sample, a
              joint-probability distribution to a set of continuous variables.
              The algorithm proposed performs this assignment by mapping the
              original variables onto a jointly-Gaussian set. The map is built
              iteratively, ascending the log-likelihood of the observations,
              through a series of steps that move the marginal distributions
              along a random set of orthogonal directions towards normality.},
  number   = {1},
  journal  = {Communications in Mathematical Sciences},
  author   = {Tabak, Esteban G. and Vanden-Eijnden, Eric},
  month    = mar,
  year     = {2010},
  note     = {Publisher: International Press of Boston},
  keywords = {machine learning, 34A50, 60H35, 65C30, 65L20, Density estimation,
              maximum likelihood},
  pages    = {217--233},
  file     = {
              Snapshot:/Users/apodusenko/Zotero/storage/5ZXKLU4G/1266935020.html:text/html;Tabak
              and Vanden-Eijnden - 2010 - Density estimation by dual ascent of the
              log-likel.pdf:/Users/apodusenko/Zotero/storage/8WKFSXHT/Tabak and
              Vanden-Eijnden - 2010 - Density estimation by dual ascent of the
              log-likel.pdf:application/pdf}
}

@article{dinh_nice_2015,
  title      = {{NICE}: {Non}-linear {Independent} {Components} {Estimation}},
  shorttitle = {{NICE}},
  abstract   = {We propose a deep learning framework for modeling complex
                high-dimensional densities called Non-linear Independent Component
                Estimation (NICE). It is based on the idea that a good
                representation is one in which the data has a distribution that is
                easy to model. For this purpose, a non-linear deterministic
                transformation of the data is learned that maps it to a latent
                space so as to make the transformed data conform to a factorized
                distribution, i.e., resulting in independent latent variables. We
                parametrize this transformation so that computing the Jacobian
                determinant and inverse transform is trivial, yet we maintain the
                ability to learn complex non-linear transformations, via a
                composition of simple building blocks, each based on a deep neural
                network. The training criterion is simply the exact log-likelihood,
                which is tractable. Unbiased ancestral sampling is also easy. We
                show that this approach yields good generative models on four image
                datasets and can be used for inpainting.},
  journal    = {arXiv:1410.8516 [cs]},
  author     = {Dinh, Laurent and Krueger, David and Bengio, Yoshua},
  month      = apr,
  year       = {2015},
  note       = {arXiv: 1410.8516},
  keywords   = {Computer Science - Machine Learning},
  file       = {arXiv.org
                Snapshot:/Users/apodusenko/Zotero/storage/9ZUGJEPA/1410.html:text/html;Dinh
                et al. - 2015 - NICE Non-linear Independent Components
                Estimation.pdf:/Users/apodusenko/Zotero/storage/3KMCNUIF/Dinh et al. -
                2015 - NICE Non-linear Independent Components
                Estimation.pdf:application/pdf}
}

@article{yedidia_bethe_2001,
  title    = {Bethe free energy, {Kikuchi} approximations, and belief propagation
              algorithms},
  volume   = {13},
  abstract = {This is an updated and expanded version of TR2000-26, but it is
              still in draft form. Belief propagation (BP) was only supposed to
              work for tree-like networks but works surprisingly well in many
              applications involving networks with loops, including turbo codes.
              However, there has been little understanding of the algorithm or
              the nature of the solutions it ﬁnds for general graphs.},
  language = {en},
  journal  = {Advances in neural information processing systems},
  author   = {Yedidia, Jonathan S and Freeman, William T and Weiss, Yair},
  year     = {2001},
  pages    = {24},
  file     = {Yedidia et al. - Bethe free energy, Kikuchi approximations, and
              bel.pdf:/Users/apodusenko/Zotero/storage/TZAKQHTZ/Yedidia et al. -
              Bethe free energy, Kikuchi approximations, and bel.pdf:application/pdf}
}

@book{amari_information_2016,
  series    = {Applied {Mathematical} {Sciences}},
  title     = {Information {Geometry} and {Its} {Applications}},
  isbn      = {978-4-431-55977-1},
  url       = {https://www.springer.com/gp/book/9784431559771},
  abstract  = {This is the first comprehensive book on information geometry,
               written by the founder of the field. It begins with an elementary
               introduction to dualistic geometry and proceeds to a wide range of
               applications, covering information science, engineering, and
               neuroscience. It consists of four parts, which on the whole can be
               read independently. A manifold with a divergence function is first
               introduced, leading directly to dualistic structure, the heart of
               information geometry. This part (Part I) can be apprehended without
               any knowledge of differential geometry. An intuitive explanation of
               modern differential geometry then follows in Part II, although the
               book is for the most part understandable without modern
               differential geometry. Information geometry of statistical
               inference, including time series analysis and semiparametric
               estimation (the Neyman–Scott problem), is demonstrated concisely in
               Part III. Applications addressed in Part IV include hot current
               topics in machine learning, signal processing, optimization, and
               neural networks. The book is interdisciplinary, connecting
               mathematics, information sciences, physics, and neurosciences,
               inviting readers to a new world of information and geometry. This
               book is highly recommended to graduate students and researchers who
               seek new mathematical methods and tools useful in their own fields.
               },
  language  = {en},
  urldate   = {2021-09-09},
  publisher = {Springer Japan},
  author    = {Amari, Shun-ichi},
  year      = {2016},
  doi       = {10.1007/978-4-431-55978-8},
  file      = {
               Snapshot:/Users/apodusenko/Zotero/storage/NDKM7QHQ/9784431559771.html:text/html
               }
}

@article{friston_active_2021,
  title    = {Active listening},
  volume   = {399},
  issn     = {03785955},
  url      = {https://linkinghub.elsevier.com/retrieve/pii/S0378595519303491},
  doi      = {10.1016/j.heares.2020.107998},
  abstract = {This paper introduces active listening, as a uniﬁed framework for
              synthesising and recognising speech. The notion of active listening
              inherits from active inference, which considers perception and
              action under one universal imperative: to maximise the evidence for
              our (generative) models of the world. First, we describe a
              generative model of spoken words that simulates (i) how discrete
              lexical, prosodic, and speaker attributes give rise to continuous
              acoustic signals; and conversely (ii) how continuous acoustic
              signals are recognised as words. The ‘active’ aspect involves
              (covertly) segmenting spoken sentences and borrows ideas from
              active vision. It casts speech segmentation as the selection of
              internal actions, corresponding to the placement of word
              boundaries. Practically, word boundaries are selected that maximise
              the evidence for an internal model of how individual words are
              generated. We establish face validity by simulating speech
              recognition and showing how the inferred content of a sentence
              depends on prior beliefs and background noise. Finally, we consider
              predictive validity by associating neuronal or physiological
              responses, such as the mismatch negativity and P300, with belief
              updating under active listening, which is greatest in the absence
              of accurate prior beliefs about what will be heard next.},
  language = {en},
  number   = {Stimulus-specific adaptation, MMN and predicting coding},
  urldate  = {2021-09-01},
  journal  = {Hearing Research},
  author   = {Friston, Karl J. and Sajid, Noor and Quiroga-Martinez, David Ricardo
              and Parr, Thomas and Price, Cathy J. and Holmes, Emma},
  month    = jan,
  year     = {2021},
  pages    = {107998},
  file     = {Friston et al. - 2021 - Active
              listening.pdf:/Users/apodusenko/Zotero/storage/N3ILHLS6/Friston et al.
              - 2021 - Active listening.pdf:application/pdf}
}

@inproceedings{van_erp_variational_2021,
  title     = {Variational {Log}-{Power} {Spectral} {Tracking} for {Acoustic} {
               Signals}},
  doi       = {10.1109/SSP49050.2021.9513757},
  abstract  = {This paper proposes a generative hierarchical probabilistic model
               for acoustic signals where both the frequency decomposition and
               log-power spectrum appear as latent variables. In order to
               facilitate efficient inference, we represent the model in a factor
               graph that includes a probabilistic Fourier transform and a
               Gaussian scale model as modules. We derive novel ways of performing
               variational message passing-based inference in the Gaussian scale
               model. As a result, in this model a probabilistic representation of
               the log-power spectrum of an acoustic signal can be effectively
               inferred online. The proposed model may find applications as a
               front end wherever probabilistic log-power spectral features of a
               signal are needed. We validate the model and message passing-based
               inference methods by tracking the log-power spectrum of a speech
               signal.},
  booktitle = {2021 {IEEE} {Statistical} {Signal} {Processing} {Workshop} ({SSP}
               )},
  author    = {van Erp, Bart and Şenöz, İsmail and de Vries, Bert},
  month     = jul,
  year      = {2021},
  note      = {ISSN: 2693-3551},
  keywords  = {Inference algorithms, Fourier transforms, Signal processing
               algorithms, Acoustics, Conferences, Probabilistic logic, Signal
               processing},
  pages     = {311--315},
  file      = {IEEE Xplore Abstract
               Record:/Users/apodusenko/Zotero/storage/UF4RVXLB/9513757.html:text/html
               }
}

@misc{noauthor_wenjun_nodate,
  title    = {Wenjun {Huang} - 2021 - {Collaborative} {Bayesian} {Optimization} {
              Framework} with {Pairwise} {Comparison}},
  url      = {https://www.overleaf.com/project/601982489202bf27c07af0d2},
  abstract = {An online LaTeX editor that's easy to use. No installation,
              real-time collaboration, version control, hundreds of LaTeX
              templates, and more.},
  language = {en},
  urldate  = {2021-08-03},
  file     = {
              Snapshot:/Users/apodusenko/Zotero/storage/B4BM246I/601982489202bf27c07af0d2.html:text/html
              }
}

@article{millidge_understanding_2021,
  title   = {Understanding the origin of information-seeking exploration in
             probabilistic objectives for control},
  journal = {arXiv preprint arXiv:2103.06859},
  author  = {Millidge, Beren and Tschantz, Alexander and Seth, Anil and Buckley,
             Christopher},
  year    = {2021},
  file    = {Full Text:/Users/apodusenko/Zotero/storage/8H35WX6J/Millidge et al. -
             2021 - Understanding the origin of information-seeking
             ex.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/N7WN3K35/2103.html:text/html
             }
}

@article{sajid_bayesian_2021,
  title   = {Bayesian brains and the {Renyi} divergence},
  journal = {arXiv preprint arXiv:2107.05438},
  author  = {Sajid, Noor and Faccio, Francesco and Da Costa, Lancelot and Parr,
             Thomas and Schmidhuber, Jürgen and Friston, Karl},
  year    = {2021},
  file    = {Full Text:/Users/apodusenko/Zotero/storage/MM6BBVQ4/Sajid et al. -
             2021 - Bayesian brains and the R\$backslash\$'enyi
             diverge.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/BQWR97PU/2107.html:text/html
             }
}

@phdthesis{van_de_laar_automated_2019,
  address = {Eindhoven, The Netherlands},
  title   = {Automated {Design} of {Bayesian} {Signal} {Processing} {Algorithms}},
  school  = {Eindhoven University of Technology},
  author  = {van de Laar, Thijs},
  year    = {2019},
  file    = {Full Text PDF:/Users/apodusenko/Zotero/storage/WSH3TZGA/Laar - 2019 -
             Automated design of Bayesian signal processing
             alg.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/6CFLE7LZ/automated-design-of-bayesian-signal-processing-algorithms.html:text/html
             }
}

@misc{loeliger_factor_2007-1,
  title  = {Factor {Graphs} and {Message} {Passing} {Algorithms} -- {Part} 1: {
            Introduction}},
  url    = {http://www.crm.sns.it/media/course/1524/Loeliger_A.pdf},
  author = {Loeliger, Hans-Andrea},
  year   = {2007},
  note   = {http://www.crm.sns.it/media/course/1524/Loeliger\_A.pdf, last accessed
            on 3-4-2019},
  file   = {Loeliger - 2007 - Factor Graphs and Message Passing Algorithms --
            Pa.pdf:/Users/apodusenko/Zotero/storage/3K3HQIXN/Loeliger - 2007 -
            Factor Graphs and Message Passing Algorithms -- Pa.pdf:application/pdf}
}

@article{harma_time-varying_2000,
  title    = {Time-varying autoregressive modeling of audio and speech signals},
  volume   = {2015},
  abstract = {Conventional linear predictive techniques for modeling of speech
              and audio signals are based on an assumption that a signal is
              stationary within each analysis frame. However, natural signals are
              often continuously time-varying, i.e., non-stationary. Therefore
              this assumption might not be well jus-tified. In this paper, we
              study a time-varying autoregressive (TVAR) modeling technique in
              which this restriction is re-laxed. A frequency-warped formulation
              of the Subba Rao-Liporace TVAR algorithm is introduced in the
              article. The applicability of the presented methodology to various
              speech and audio signal processing tasks is illustrated and
              discussed. It is also shown that the TVAR scheme yields an
              efficient parametrization for time-varying sounds.},
  journal  = {European Signal Processing Conference},
  author   = {Härmä, Aki and Juntunen, Marko and Kaipio, Jari},
  month    = jan,
  year     = {2000}
}

@article{baldi_bayesian_2020,
  title    = {Bayesian {Causality}},
  volume   = {74},
  issn     = {0003-1305},
  url      = {https://doi.org/10.1080/00031305.2019.1647876},
  doi      = {10.1080/00031305.2019.1647876},
  abstract = {Although no universally accepted definition of causality exists,
              in practice one is often faced with the question of statistically
              assessing causal relationships in different settings. We present a
              uniform general approach to causality problems derived from the
              axiomatic foundations of the Bayesian statistical framework. In
              this approach, causality statements are viewed as hypotheses, or
              models, about the world and the fundamental object to be computed
              is the posterior distribution of the causal hypotheses, given the
              data and the background knowledge. Computation of the posterior,
              illustrated here in simple examples, may involve complex
              probabilistic modeling but this is no different than in any other
              Bayesian modeling situation. The main advantage of the approach is
              its connection to the axiomatic foundations of the Bayesian
              framework, and the general uniformity with which it can be applied
              to a variety of causality settings, ranging from specific to
              general cases, or from causes of effects to effects of causes.},
  number   = {3},
  urldate  = {2021-07-19},
  journal  = {The American Statistician},
  author   = {Baldi, Pierre and Shahbaba, Babak},
  month    = jul,
  year     = {2020},
  pmid     = {33041343},
  note     = {Publisher: Taylor \& Francis \_eprint:
              https://doi.org/10.1080/00031305.2019.1647876},
  keywords = {Bayesian statistics, Causal inference, Foundations, Hypothesis
              testing},
  pages    = {249--257},
  file     = {Full Text PDF:/Users/apodusenko/Zotero/storage/R2FSAP6H/Baldi and
              Shahbaba - 2020 - Bayesian Causality.pdf:application/pdf}
}

@article{caticha_entropy_2021,
  title     = {Entropy, {Information}, and the {Updating} of {Probabilities}},
  volume    = {23},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  url       = {https://www.mdpi.com/1099-4300/23/7/895},
  doi       = {10.3390/e23070895},
  abstract  = {This paper is a review of a particular approach to the method of
               maximum entropy as a general framework for inference. The
               discussion emphasizes pragmatic elements in the derivation. An
               epistemic notion of information is defined in terms of its relation
               to the Bayesian beliefs of ideally rational agents. The method of
               updating from a prior to posterior probability distribution is
               designed through an eliminative induction process. The logarithmic
               relative entropy is singled out as a unique tool for updating (a)
               that is of universal applicability, (b) that recognizes the value
               of prior information, and (c) that recognizes the privileged role
               played by the notion of independence in science. The resulting
               framework—the ME method—can handle arbitrary priors and arbitrary
               constraints. It includes the MaxEnt and Bayes’ rules as special
               cases and, therefore, unifies entropic and Bayesian methods into a
               single general inference scheme. The ME method goes beyond the mere
               selection of a single posterior, and also addresses the question of
               how much less probable other distributions might be, which provides
               a direct bridge to the theories of fluctuations and large
               deviations.},
  language  = {en},
  number    = {7},
  urldate   = {2021-07-19},
  journal   = {Entropy},
  author    = {Caticha, Ariel},
  month     = jul,
  year      = {2021},
  note      = {Number: 7 Publisher: Multidisciplinary Digital Publishing Institute},
  keywords  = {Bayesian inference, maximum entropy, updating probabilities},
  pages     = {895},
  file      = {Full Text PDF:/Users/apodusenko/Zotero/storage/NHUM46HP/Caticha - 2021
               - Entropy, Information, and the Updating of
               Probabil.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/Q754YF98/htm.html:text/html
               }
}

@inproceedings{tschantz_scaling_2020,
  title     = {Scaling active inference},
  booktitle = {2020 {International} {Joint} {Conference} on {Neural} {Networks}
               ({IJCNN})},
  publisher = {IEEE},
  author    = {Tschantz, Alexander and Baltieri, Manuel and Seth, Anil K. and
               Buckley, Christopher L.},
  year      = {2020},
  pages     = {1--8},
  file      = {
               Snapshot:/Users/apodusenko/Zotero/storage/CM3SC8AQ/9207382.html:text/html;Tschantz
               et al. - 2020 - Scaling active
               inference.pdf:/Users/apodusenko/Zotero/storage/DPX5FCXD/Tschantz et al.
               - 2020 - Scaling active inference.pdf:application/pdf}
}

@article{millidge_predictive_2020,
  title   = {Predictive coding approximates backprop along arbitrary computation
             graphs},
  journal = {arXiv preprint arXiv:2006.04182},
  author  = {Millidge, Beren and Tschantz, Alexander and Buckley, Christopher L.},
  year    = {2020},
  file    = {Millidge et al. - 2020 - Predictive coding approximates backprop along
             arbi.pdf:/Users/apodusenko/Zotero/storage/FVR4JDSW/Millidge et al. -
             2020 - Predictive coding approximates backprop along
             arbi.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/B7C7KKFN/2006.html:text/html
             }
}

@article{parr_markov_2020,
  title   = {Markov blankets, information geometry and stochastic thermodynamics},
  volume  = {378},
  number  = {2164},
  journal = {Philosophical Transactions of the Royal Society A},
  author  = {Parr, Thomas and Da Costa, Lancelot and Friston, Karl},
  year    = {2020},
  note    = {Publisher: The Royal Society Publishing},
  pages   = {20190159},
  file    = {Parr et al. - 2020 - Markov blankets, information geometry and
             stochast.pdf:/Users/apodusenko/Zotero/storage/AMFWTMN6/Parr et al. -
             2020 - Markov blankets, information geometry and
             stochast.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/63P9YYRD/rsta.2019.html:text/html
             }
}

@article{friston_free_2019,
  title   = {A free energy principle for a particular physics},
  journal = {arXiv preprint arXiv:1906.10184},
  author  = {Friston, Karl},
  year    = {2019},
  file    = {Friston - 2019 - A free energy principle for a particular
             physics.pdf:/Users/apodusenko/Zotero/storage/9V55TASB/Friston - 2019 -
             A free energy principle for a particular
             physics.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/WEW7GPG9/1906.html:text/html
             }
}

@article{podusenko_message_2021-1,
  title     = {Message {Passing}-{Based} {Inference} for {Time}-{Varying} {
               Autoregressive} {Models}},
  volume    = {23},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  url       = {https://www.mdpi.com/1099-4300/23/6/683},
  doi       = {10.3390/e23060683},
  abstract  = {Time-varying autoregressive (TVAR) models are widely used for
               modeling of non-stationary signals. Unfortunately, online joint
               adaptation of both states and parameters in these models remains a
               challenge. In this paper, we represent the TVAR model by a factor
               graph and solve the inference problem by automated message
               passing-based inference for states and parameters. We derive
               structured variational update rules for a composite “AR node” with
               probabilistic observations that can be used as a plug-in module in
               hierarchical models, for example, to model the time-varying
               behavior of the hyper-parameters of a time-varying AR model. Our
               method includes tracking of variational free energy (FE) as a
               Bayesian measure of TVAR model performance. The proposed methods
               are verified on a synthetic data set and validated on real-world
               data from temperature modeling and speech enhancement tasks.},
  language  = {en},
  number    = {6},
  urldate   = {2021-07-07},
  journal   = {Entropy},
  author    = {Podusenko, Albert and Kouw, Wouter M. and de Vries, Bert},
  month     = jun,
  year      = {2021},
  note      = {Number: 6 Publisher: Multidisciplinary Digital Publishing Institute},
  keywords  = {Bayesian inference, free energy, factor graph, model selection,
               hybrid message passing, non-stationary systems, probabilistic
               graphical models},
  pages     = {683},
  file      = {Podusenko et al. - 2021 - Message Passing-Based Inference for
               Time-Varying A.html:/Users/apodusenko/Zotero/storage/DSEAPYBR/Podusenko
               et al. - 2021 - Message Passing-Based Inference for Time-Varying
               A.html:text/html;Podusenko et al. - 2021 - Message Passing-Based
               Inference for Time-Varying
               A.pdf:/Users/apodusenko/Zotero/storage/R74TNLN2/Podusenko et al. - 2021
               - Message Passing-Based Inference for Time-Varying
               A.pdf:application/pdf}
}

@article{karbalayghareh_optimal_2018,
  title    = {Optimal {Bayesian} {Transfer} {Learning}},
  volume   = {66},
  issn     = {1941-0476},
  doi      = {10.1109/TSP.2018.2839583},
  abstract = {Transfer learning has recently attracted significant research
              attention, as it simultaneously learns from different source
              domains, which have plenty of labeled data, and transfers the
              relevant knowledge to the target domain with limited labeled data
              to improve the prediction performance. We propose a Bayesian
              transfer learning framework, in the homogeneous transfer learning
              scenario, where the source and target domains are related through
              the joint prior density of the model parameters. The modeling of
              joint prior densities enables better understanding of the
              “transferability” between domains. We define a joint Wishart
              distribution for the precision matrices of the Gaussian
              feature-label distributions in the source and target domains to act
              like a bridge that transfers the useful information of the source
              domain to help classification in the target domain by improving the
              target posteriors. Using several theorems in multivariate
              statistics, the posteriors and posterior predictive densities are
              derived in closed forms with hypergeometric functions of matrix
              argument, leading to our novel closed-form and fast Optimal
              Bayesian Transfer Learning (OBTL) classifier. Experimental results
              on both synthetic and real-world benchmark data confirm the superb
              performance of the OBTL compared to the other state-of-the-art
              transfer learning and domain adaptation methods.},
  number   = {14},
  journal  = {IEEE Transactions on Signal Processing},
  author   = {Karbalayghareh, Alireza and Qian, Xiaoning and Dougherty, Edward R.},
  month    = jul,
  year     = {2018},
  note     = {Conference Name: IEEE Transactions on Signal Processing},
  keywords = {Diseases, Bayes methods, Adaptation models, Data models,
              Covariance matrices, Benchmark testing, domain adaptation, optimal
              Bayesian classifier, optimal Bayesian transfer learning, Transfer
              learning, Two dimensional displays},
  pages    = {3724--3739},
  file     = {IEEE Xplore Abstract
              Record:/Users/apodusenko/Zotero/storage/E9DU9M9Z/8362683.html:text/html;IEEE
              Xplore Full Text
              PDF:/Users/apodusenko/Zotero/storage/4ENWUS3T/Karbalayghareh et al. -
              2018 - Optimal Bayesian Transfer Learning.pdf:application/pdf}
}

@inproceedings{chakrabarty_exploring_2015,
  title     = {Exploring the role of temporal dynamics in acoustic scene
               classification},
  doi       = {10.1109/WASPAA.2015.7336898},
  abstract  = {Identification of acoustic scenes often relies on finding the most
               informative features that best characterize the physical nature of
               sound sources in the scene. In this paper, we propose a framework
               that provides a detailed local analysis of spectro-temporal
               modulations augmented with generative modeling that map both the
               average modulation statistics of the scene using Gaussian Mixture
               Modeling (GMM) as well temporal trajectories of these modulations
               using Hidden Markov Modeling (HMM). Our analysis shows that a
               hybrid system of these two representations can capture the
               non-trivial commonalities within a sound class and differences
               between sound classes. The proposed hybrid system outperforms
               current systems in the literature by about 30 \% and surpasses the
               performance of the individual GMM and HMM systems suggesting that
               these representations provide complimentary information about
               acoustic scenes.},
  booktitle = {2015 {IEEE} {Workshop} on {Applications} of {Signal} {Processing}
               to {Audio} and {Acoustics} ({WASPAA})},
  author    = {Chakrabarty, Debmalya and Elhilali, Mounya},
  month     = oct,
  year      = {2015},
  keywords  = {Hidden Markov models, Tensile stress, Trajectory, Modulation,
               Time-frequency analysis, Auditory scenes, Gussian Mixture Models,
               Hidden Markov Models, Mel frequency cepstral coefficient,
               Specto-temporal modulations, Temporal trajectories},
  pages     = {1--5},
  file      = {IEEE Xplore Abstract
               Record:/Users/apodusenko/Zotero/storage/QIJS3FFY/7336898.html:text/html;IEEE
               Xplore Full Text
               PDF:/Users/apodusenko/Zotero/storage/PPJJHPDF/Chakrabarty and Elhilali
               - 2015 - Exploring the role of temporal dynamics in
               acousti.pdf:application/pdf}
}

@incollection{levchuk_chapter_2019,
  title      = {Chapter 4 - {Active} {Inference} in {Multiagent} {Systems}: {Context}
                -{Driven} {Collaboration} and {Decentralized} {Purpose}-{Driven} {Team
                } {Adaptation}},
  isbn       = {978-0-12-817636-8},
  shorttitle = {Chapter 4 - {Active} {Inference} in {Multiagent} {Systems}},
  url        = {http://www.sciencedirect.com/science/article/pii/B9780128176368000041},
  abstract   = {The Internet of things (IoT), from heart monitoring implants to
                home-heating control systems, is becoming integral to our daily
                lives. We expect the technologies that comprise IoT to become
                smarter; to autonomously reason, act, and communicate with other
                entities in the environment; and to achieve shared goals. To
                realize the full potential of these systems, we must understand the
                mechanisms that allow multiple intelligent entities to effectively
                operate, collaborate, and learn in changing and uncertain
                environments. The future IoT devices must not only maintain enough
                intelligence to perceive and act locally, but also possess
                team-level collaboration and adaptation processes. We posit that
                such processes embody energy-minimizing mechanisms found in all
                biological and physical systems, and operate over the objectives
                and constraints that can be defined and analyzed locally by
                individual devices without the need for global centralized control.
                In this chapter, we represent multiple IoT devices as a team of
                intelligent agents, and postulate that multiagent systems achieve
                adaptive behaviors by minimizing a team’s free energy, which
                decomposes into distributed iterative perception (inference) and
                control (action) processes. First, we discuss instantiation of this
                mechanism for a joint distributed decision-making problem. Next, we
                present experimental evidence that energy-based teams outperform
                utility-based teams. Finally, we discuss different learning
                processes that support team-level adaptation.},
  language   = {en},
  urldate    = {2020-05-26},
  booktitle  = {Artificial {Intelligence} for the {Internet} of {Everything}},
  publisher  = {Academic Press},
  author     = {Levchuk, Georgiy and Pattipati, Krishna and Serfaty, Daniel and
                Fouse, Adam and McCormack, Robert},
  editor     = {Lawless, William and Mittu, Ranjeev and Sofge, Donald and Moskowitz,
                Ira S. and Russell, Stephen},
  month      = jan,
  year       = {2019},
  doi        = {10.1016/B978-0-12-817636-8.00004-1},
  keywords   = {Free energy, Constraints, Humanmachine teams, Internet of Things
                (IoT), Multiple intelligent entities, Organizational structure,
                Perceptual control, Team adaptation, Variational inference},
  pages      = {67--85},
  file       = {Levchuk et al. - 2019 - Chapter 4 - Active Inference in Multiagent
                Systems.pdf:/Users/apodusenko/Zotero/storage/3ETUFI5W/Levchuk et al. -
                2019 - Chapter 4 - Active Inference in Multiagent
                Systems.pdf:application/pdf;ScienceDirect
                Snapshot:/Users/apodusenko/Zotero/storage/4QJVYUGY/B9780128176368000041.html:text/html
                }
}

@article{champion_realising_2021,
  title      = {Realising {Active} {Inference} in {Variational} {Message} {Passing}:
                the {Outcome}-blind {Certainty} {Seeker}},
  shorttitle = {Realising {Active} {Inference} in {Variational} {Message} {
                Passing}},
  url        = {http://arxiv.org/abs/2104.11798},
  abstract   = {Active inference is a state-of-the-art framework in neuroscience
                that offers a unified theory of brain function. It is also proposed
                as a framework for planning in AI. Unfortunately, the complex
                mathematics required to create new models -- can impede application
                of active inference in neuroscience and AI research. This paper
                addresses this problem by providing a complete mathematical
                treatment of the active inference framework -- in discrete time and
                state spaces -- and the derivation of the update equations for any
                new model. We leverage the theoretical connection between active
                inference and variational message passing as describe by John Winn
                and Christopher M. Bishop in 2005. Since, variational message
                passing is a well-defined methodology for deriving Bayesian belief
                update equations, this paper opens the door to advanced generative
                models for active inference. We show that using a fully factorized
                variational distribution simplifies the expected free energy --
                that furnishes priors over policies -- so that agents seek
                unambiguous states. Finally, we consider future extensions that
                support deep tree searches for sequential policy optimisation --
                based upon structure learning and belief propagation.},
  urldate    = {2021-05-01},
  journal    = {arXiv:2104.11798 [cs]},
  author     = {Champion, Théophile and Grześ, Marek and Bowman, Howard},
  month      = apr,
  year       = {2021},
  note       = {arXiv: 2104.11798},
  keywords   = {Computer Science - Machine Learning},
  file       = {arXiv.org
                Snapshot:/Users/apodusenko/Zotero/storage/AUIXARRE/2104.html:text/html;Champion
                et al. - 2021 - Realising Active Inference in Variational Message
                .pdf:/Users/apodusenko/Zotero/storage/J3SHAMHQ/Champion et al. - 2021 -
                Realising Active Inference in Variational Message .pdf:application/pdf}
}

@article{catal_learning_2020,
  title    = {Learning {Generative} {State} {Space} {Models} for {Active} {
              Inference}},
  volume   = {14},
  issn     = {1662-5188},
  url      = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7701292/},
  doi      = {10.3389/fncom.2020.574372},
  abstract = {In this paper we investigate the active inference framework as a
              means to enable autonomous behavior in artificial agents. Active
              inference is a theoretical framework underpinning the way organisms
              act and observe in the real world. In active inference, agents act
              in order to minimize their so called free energy, or prediction
              error. Besides being biologically plausible, active inference has
              been shown to solve hard exploration problems in various simulated
              environments. However, these simulations typically require
              handcrafting a generative model for the agent. Therefore we propose
              to use recent advances in deep artificial neural networks to learn
              generative state space models from scratch, using only
              observation-action sequences. This way we are able to scale active
              inference to new and challenging problem domains, whilst still
              building on the theoretical backing of the free energy principle.
              We validate our approach on the mountain car problem to illustrate
              that our learnt models can indeed trade-off instrumental value and
              ambiguity. Furthermore, we show that generative models can also be
              learnt using high-dimensional pixel observations, both in the
              OpenAI Gym car racing environment and a real-world robotic
              navigation task. Finally we show that active inference based
              policies are an order of magnitude more sample efficient than Deep
              Q Networks on RL tasks.},
  urldate  = {2020-12-16},
  journal  = {Frontiers in Computational Neuroscience},
  author   = {Çatal, Ozan and Wauthier, Samuel and De Boom, Cedric and Verbelen,
              Tim and Dhoedt, Bart},
  month    = nov,
  year     = {2020},
  pmid     = {33304260},
  pmcid    = {PMC7701292},
  file     = {Çatal et al. - 2020 - Learning Generative State Space Models for
              Active .pdf:/Users/apodusenko/Zotero/storage/NHFX2TQI/Çatal et al. -
              2020 - Learning Generative State Space Models for Active
              .pdf:application/pdf}
}

@inproceedings{senoz_online_2020,
  title     = {Online {Message} {Passing}-based {Inference} in the {Hierarchical} {
               Gaussian} {Filter}},
  doi       = {10.1109/ISIT44484.2020.9173980},
  abstract  = {We address the problem of online state and parameter estimation in
               the Hierarchical Gaussian Filter (HGF), which is a multi-layer
               dynamic model with non-conjugate couplings between upper-layer
               hidden states and parameters of a lower layer. These
               non-conjugacies necessitate the approximation of marginalization
               and expectation integrals, while the online inference constraint
               renders batch learning and Monte Carlo sampling unsuitable. Here we
               formulate the problem as a message passing task on a factor graph
               and propose an online variational message passing-based state and
               parameter tracking algorithm, which uses Gaussian quadrature to
               deal with non-conjugacies. We present improved message update rules
               for all non-conjugate couplings, thus allowing a plug-in inference
               method for alternative models with equivalent non-conjugate layer
               couplings. The method is validated on a recorded time series of
               Bitcoin prices.},
  booktitle = {2020 {IEEE} {International} {Symposium} on {Information} {Theory}
               ({ISIT})},
  author    = {Şenöz, Ismail and de Vries, Bert},
  month     = jun,
  year      = {2020},
  note      = {ISSN: 2157-8117},
  keywords  = {factor graphs, graph theory, inference mechanisms,
               expectation-maximisation algorithm, message passing, learning
               (artificial intelligence), Gaussian processes, parameter estimation
               , Monte Carlo methods, variational message passing, online learning
               , Hierarchical Gaussian Filter, batch learning, dynamic modeling,
               expectation integrals, Gaussian quadrature, hierarchical Gaussian
               filter, marginalization, message passing task, message update rules
               , Monte Carlo sampling, multilayer dynamic model, nonconjugacies,
               nonconjugate couplings, nonconjugate layer couplings, online
               inference constraint, online message passing-based inference,
               online state, online variational message passing-based state,
               plug-in inference method, upper-layer hidden states},
  pages     = {2676--2681},
  file      = {IEEE Xplore Abstract
               Record:/Users/apodusenko/Zotero/storage/G6NAPIEX/9173980.html:text/html;IEEE
               Xplore Full Text PDF:/Users/apodusenko/Zotero/storage/A8UIAALQ/Şenöz
               and de Vries - 2020 - Online Message Passing-based Inference in the
               Hier.pdf:application/pdf}
}

@article{van_de_laar_chance-constrained_2021,
  title   = {Chance-{Constrained} {Active} {Inference}},
  journal = {arXiv preprint arXiv:2102.08792},
  author  = {van de Laar, Thijs and Şenöz, Ismail and Özçelikkale, Ayça and
             Wymeersch, Henk},
  year    = {2021},
  file    = {Full Text:/Users/apodusenko/Zotero/storage/8D6A9T5R/van de Laar et al.
             - 2021 - Chance-Constrained Active
             Inference.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/MYRUP9NS/2102.html:text/html
             }
}

@article{contreras-reyes_bayesian_2018,
  title      = {Bayesian modeling of individual growth variability using
                back-calculation: {Application} to pink cusk-eel ({Genypterus}
                blacodes) off {Chile}},
  volume     = {385},
  shorttitle = {Bayesian modeling of individual growth variability using
                back-calculation},
  url        = {https://ideas.repec.org/a/eee/ecomod/v385y2018icp145-153.html},
  abstract   = {The von Bertalanffy growth function (VBGF) with random effects has
                been widely used to estimate growth parameters incorporating
                individual variability of length-at-age. Trajectories of individual
                growth can be inferred using either mark-recapture or
                back-calculation of length-at-age from growth marks in hard body
                parts such as otoliths. Modern statistical methods evaluate
                individual variation usually from mark-recapture data, and the
                parameters describing this function are estimated using empirical
                Bayes methods assuming Gaussian error. In this paper, we combine
                recent studies in non-Gaussian distributions and a Bayesian
                approach to model growth variability using back-calculated data in
                harvested fish populations. We presumed that errors in the VBGF can
                be assumed as a Student-t distribution, given the abundance of
                individuals with extreme length values. The proposed method was
                applied and compared to the standard methods using back-calculated
                length-at-age data for pink cusk-eel (Genypterus blacodes) off
                Chile. Considering several information criteria, and comparing
                males and females, we have found that males grow significantly
                faster than females, and that length-at-age for both sexes exhibits
                extreme length observations. Comparisons indicated that a Student-t
                model with mixed effects describes best back-calculated data
                regarding pink cusk-eel. This framework merged the strengths of
                different approaches to estimate growth parameters in harvested
                fish populations, considering modeling of individual variability of
                length-at-age, Bayesian inference, and distribution of errors from
                the Student-t model.},
  language   = {en},
  number     = {C},
  urldate    = {2021-06-07},
  journal    = {Ecological Modelling},
  author     = {Contreras-Reyes, Javier E. and López Quintero, Freddy O. and Wiff,
                Rodrigo},
  year       = {2018},
  note       = {Publisher: Elsevier},
  keywords   = {Chile, Individual variability, Pink cusk-eel, Student-t
                distribution, Von Bertalanffy model},
  pages      = {145--153},
  file       = {
                Snapshot:/Users/apodusenko/Zotero/storage/BLPTZ4UG/v385y2018icp145-153.html:text/html
                }
}

@article{diethe_bayesian_nodate,
  title    = {Bayesian {Active} {Transfer} {Learning} in {Smart} {Homes}},
  abstract = {There are at least two major challenges for machine learning in
              the smart-home setting. Firstly, the deployment context will be
              very different to the the context in which learning occurs, due to
              both individual differences in typical activity patterns and
              different house and sensor layouts. Secondly, accurate labelling of
              training data is an extremely time-consuming process, and the
              resulting labels are potentially noisy and error-prone. The
              resulting framework is therefore a combination of active and
              transfer learning. We argue that hierarchical Bayesian methods are
              particularly well suited to problems of this nature, and give a
              possible formulation of such a model.},
  language = {en},
  author   = {Diethe, Tom and Twomey, Niall and Flach, Peter},
  pages    = {6},
  file     = {Diethe et al. - Bayesian Active Transfer Learning in Smart
              Homes.pdf:/Users/apodusenko/Zotero/storage/WZKIJP38/Diethe et al. -
              Bayesian Active Transfer Learning in Smart Homes.pdf:application/pdf}
}

@article{moala_bayesian_2013,
  title    = {Bayesian {Inference} for {Two}-{Parameter} {Gamma} {Distribution} {
              Assuming} {Diﬀerent} {Noninformative} {Priors}},
  volume   = {36},
  abstract = {In this paper distinct prior distributions are derived in a
              Bayesian inference of the two-parameters Gamma distribution.
              Noniformative priors, such as Jeﬀreys, reference, MDIP, Tibshirani
              and an innovative prior based on the copula approach are
              investigated. We show that the maximal data information prior
              provides in an improper posterior density and that the diﬀerent
              choices of the parameter of interest lead to diﬀerent reference
              priors in this case. Based on the simulated data sets, the Bayesian
              estimates and credible intervals for the unknown parameters are
              computed and the performance of the prior distributions are
              evaluated. The Bayesian analysis is conducted using the Markov
              Chain Monte Carlo (MCMC) methods to generate samples from the
              posterior distributions under the above priors.},
  language = {en},
  number   = {2},
  journal  = {Revista Colombiana de Estadística},
  author   = {Moala, Fernando Antonio and Ramos, Pedro Luiz and Achcar, Jorge
              Alberto},
  year     = {2013},
  pages    = {319--336},
  file     = {Moala et al. - 2013 - Bayesian Inference for Two-Parameter Gamma
              Distrib.pdf:/Users/apodusenko/Zotero/storage/RKCFVYA3/Moala et al. -
              2013 - Bayesian Inference for Two-Parameter Gamma
              Distrib.pdf:application/pdf}
}

@article{merkle_logarithmic_1996,
  title    = {Logarithmic {Convexity} and {Inequalities} for the {Gamma} {Function}
              },
  volume   = {203},
  issn     = {0022247X},
  url      = {https://linkinghub.elsevier.com/retrieve/pii/S0022247X96903856},
  doi      = {10.1006/jmaa.1996.0385},
  language = {en},
  number   = {2},
  urldate  = {2021-05-24},
  journal  = {Journal of Mathematical Analysis and Applications},
  author   = {Merkle, Milan},
  month    = oct,
  year     = {1996},
  pages    = {369--380},
  file     = {Merkle - 1996 - Logarithmic Convexity and Inequalities for the
              Gam.pdf:/Users/apodusenko/Zotero/storage/PUFSB27S/Merkle - 1996 -
              Logarithmic Convexity and Inequalities for the Gam.pdf:application/pdf}
}

@article{da_costa_relationship_2020,
  title      = {The relationship between dynamic programming and active inference:
                the discrete, finite-horizon case},
  shorttitle = {The relationship between dynamic programming and active
                inference},
  url        = {http://arxiv.org/abs/2009.08111},
  abstract   = {Active inference is a normative framework for generating behaviour
                based upon the free energy principle, a theory of
                self-organisation. This framework has been successfully used to
                solve reinforcement learning and stochastic control problems, yet,
                the formal relation between active inference and reward
                maximisation has not been fully explicated. In this paper, we
                consider the relation between active inference and dynamic
                programming under the Bellman equation, which underlies many
                approaches to reinforcement learning and control. We show that, on
                partially observable Markov decision processes, dynamic programming
                is a limiting case of active inference. In active inference, agents
                select actions to minimise expected free energy. In the absence of
                ambiguity about states, this reduces to matching expected states
                with a target distribution encoding the agent’s preferences. When
                target states correspond to rewarding states, this maximises
                expected reward, as in reinforcement learning. When states are
                ambiguous, active inference agents will choose actions that
                simultaneously minimise ambiguity. This allows active inference
                agents to supplement their reward maximising (or exploitative)
                behaviour with novelty-seeking (or exploratory) behaviour. This
                clariﬁes the connection between active inference and reinforcement
                learning, and how both frameworks may beneﬁt from each other.},
  language   = {en},
  urldate    = {2021-05-20},
  journal    = {arXiv:2009.08111 [cs, math, q-bio]},
  author     = {Da Costa, Lancelot and Sajid, Noor and Parr, Thomas and Friston,
                Karl and Smith, Ryan},
  month      = sep,
  year       = {2020},
  note       = {arXiv: 2009.08111},
  keywords   = {Computer Science - Artificial Intelligence, Quantitative Biology -
                Neurons and Cognition, Mathematics - Optimization and Control},
  file       = {Da Costa et al. - 2020 - The relationship between dynamic programming
                and a.pdf:/Users/apodusenko/Zotero/storage/DZ9UABVF/Da Costa et al. -
                2020 - The relationship between dynamic programming and
                a.pdf:application/pdf}
}

@article{moon_estimation_2020,
  title     = {Estimation of {Autoregressive} {Parameters} from {Noisy} {
               Observations} {Using} {Iterated} {Covariance} {Updates}},
  volume    = {22},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  url       = {https://www.mdpi.com/1099-4300/22/5/572},
  doi       = {10.3390/e22050572},
  abstract  = {Estimating the parameters of the autoregressive (AR) random
               process is a problem that has been well-studied. In many
               applications, only noisy measurements of AR process are available.
               The effect of the additive noise is that the system can be modeled
               as an AR model with colored noise, even when the measurement noise
               is white, where the correlation matrix depends on the AR
               parameters. Because of the correlation, it is expedient to compute
               using multiple stacked observations. Performing a weighted
               least-squares estimation of the AR parameters using an inverse
               covariance weighting can provide significantly better parameter
               estimates, with improvement increasing with the stack depth. The
               estimation algorithm is essentially a vector RLS adaptive filter,
               with time-varying covariance matrix. Different ways of estimating
               the unknown covariance are presented, as well as a method to
               estimate the variances of the AR and observation noise. The
               notation is extended to vector autoregressive (VAR) processes.
               Simulation results demonstrate performance improvements in
               coefficient error and in spectrum estimation.},
  language  = {en},
  number    = {5},
  urldate   = {2021-05-20},
  journal   = {Entropy},
  author    = {Moon, Todd K. and Gunther, Jacob H.},
  month     = may,
  year      = {2020},
  note      = {Number: 5 Publisher: Multidisciplinary Digital Publishing Institute},
  keywords  = {autoregressive model estimation, RLS algorithm, spectrum
               estimation, vector AR model},
  pages     = {572},
  file      = {Full Text PDF:/Users/apodusenko/Zotero/storage/W5F6GTW3/Moon and
               Gunther - 2020 - Estimation of Autoregressive Parameters from
               Noisy.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/VCYPHHC4/htm.html:text/html
               }
}

@article{ramachandran_information-theoretic_nodate,
  title    = {Information-theoretic {Transfer} {Learning} framework for {Bayesian}
              {Optimisation}},
  abstract = {Transfer learning in Bayesian optimisation is a popular way to
              alleviate “cold start” issue. However, most of the existing
              transfer learning algorithms use overall function space similarity,
              not a more aligned similarity measure for Bayesian optimisation
              based on the location of the optima. That makes these algorithms
              fragile to noisy perturbations, and even simple scaling of function
              values. In this paper, we propose a robust transfer learning based
              approach that transfer knowledge of the optima using a consistent
              probabilistic framework. From the ﬁnite samples for both source and
              target, a distribution on the optima is computed and then
              divergence between these distributions are used to compute
              similarities. Based on the similarities a mixture distribution is
              constructed, which is then used to build a new
              information-theoretic acquisition function in a manner similar to
              Predictive Entropy Search (PES). The proposed approach also offers
              desirable “no bias” transfer learning in the limit. Experiments on
              both synthetic functions and a set of hyperparameter tuning tests
              clearly demonstrate the effectiveness of our approach compared to
              the existing transfer learning methods.},
  language = {en},
  author   = {Ramachandran, Anil and Gupta, Sunil and Rana, Santu and Venkatesh,
              Svetha},
  pages    = {15},
  file     = {Ramachandran et al. - Information-theoretic Transfer Learning
              framework .pdf:/Users/apodusenko/Zotero/storage/A9CAD5VI/Ramachandran
              et al. - Information-theoretic Transfer Learning framework
              .pdf:application/pdf}
}

@article{bogacz_tutorial_2017,
  series   = {Model-based {Cognitive} {Neuroscience}},
  title    = {A tutorial on the free-energy framework for modelling perception and
              learning},
  volume   = {76},
  issn     = {0022-2496},
  url      = {https://www.sciencedirect.com/science/article/pii/S0022249615000759},
  doi      = {10.1016/j.jmp.2015.11.003},
  abstract = {This paper provides an easy to follow tutorial on the free-energy
              framework for modelling perception developed by Friston, which
              extends the predictive coding model of Rao and Ballard. These
              models assume that the sensory cortex infers the most likely values
              of attributes or features of sensory stimuli from the noisy inputs
              encoding the stimuli. Remarkably, these models describe how this
              inference could be implemented in a network of very simple
              computational elements, suggesting that this inference could be
              performed by biological networks of neurons. Furthermore, learning
              about the parameters describing the features and their uncertainty
              is implemented in these models by simple rules of synaptic
              plasticity based on Hebbian learning. This tutorial introduces the
              free-energy framework using very simple examples, and provides
              step-by-step derivations of the model. It also discusses in more
              detail how the model could be implemented in biological neural
              circuits. In particular, it presents an extended version of the
              model in which the neurons only sum their inputs, and synaptic
              plasticity only depends on activity of pre-synaptic and
              post-synaptic neurons.},
  language = {en},
  urldate  = {2021-05-19},
  journal  = {Journal of Mathematical Psychology},
  author   = {Bogacz, Rafal},
  month    = feb,
  year     = {2017},
  pages    = {198--211},
  file     = {Bogacz - 2017 - A tutorial on the free-energy framework for
              modell.pdf:/Users/apodusenko/Zotero/storage/RM9P59QH/Bogacz - 2017 - A
              tutorial on the free-energy framework for
              modell.pdf:application/pdf;ScienceDirect
              Snapshot:/Users/apodusenko/Zotero/storage/MU9FXVBJ/S0022249615000759.html:text/html
              }
}

@article{kaarnioja_smolyak_nodate,
  title    = {Smolyak {Quadrature}},
  language = {en},
  author   = {Kaarnioja, Vesa},
  pages    = {77},
  file     = {Kaarnioja - Smolyak
              Quadrature.pdf:/Users/apodusenko/Zotero/storage/672YQ2JE/Kaarnioja -
              Smolyak Quadrature.pdf:application/pdf}
}

@inproceedings{van_erp_hybrid_2022,
  title     = {Hybrid {Inference} with {Invertible} {Neural} {Networks} in {Factor}
               {Graphs}},
  abstract  = {This paper bridges the gap in the literature between neural
               networks and probabilistic graphical models. invertible neural
               networks are incorporated in factor graphs and inference in this
               model is described by linearization of the network. Consequently,
               hybrid probabilistic inference in the model is realized through
               message passing with local constraints on the Bethe free energy. We
               provide the local Bethe free energy for the invertible neural
               network node, which allows for evaluation of the performance of the
               entire probabilistic model. Experimental results show effective
               hybrid inference in a neural network-based probabilistic model for
               a binary classification task, paving the way towards a novel class
               of machine learning models.},
  booktitle = {2022 30th {European} {Signal} {Processing} {Conference} ({EUSIPCO
               })},
  author    = {van Erp, Bart and de Vries, Bert},
  year      = {2022},
  note      = {in press},
  file      = {van Erp and de Vries - 2022 - Hybrid Inference with Invertible Neural
               Networks i.pdf:/Users/apodusenko/Zotero/storage/9UHTPBPV/van Erp and de
               Vries - 2022 - Hybrid Inference with Invertible Neural Networks
               i.pdf:application/pdf}
}

@inproceedings{korl_ar_2004,
  address    = {Montreal, Que., Canada},
  title      = {{AR} model parameter estimation: from factor graphs to algorithms},
  volume     = {5},
  isbn       = {978-0-7803-8484-2},
  shorttitle = {{AR} model parameter estimation},
  url        = {http://ieeexplore.ieee.org/document/1327159/},
  doi        = {10.1109/ICASSP.2004.1327159},
  abstract   = {The classic problem of estimating the parameters of an
                auto-regressive (AR) model is considered from a graphicalmodel
                viewpoint. A number of practical parameter estimation
                algorithms—some of them well known, others apparently new—are
                derived as “summary propagation” in a factor graph. In particular,
                we demonstrate joint estimation of AR coefﬁcients, innovation
                variance, and noise variance.},
  language   = {en},
  urldate    = {2022-04-25},
  booktitle  = {2004 {IEEE} {International} {Conference} on {Acoustics}, {Speech}
                , and {Signal} {Processing}},
  publisher  = {IEEE},
  author     = {Korl, S. and Loeliger, H.A. and Lindgren, A.G.},
  year       = {2004},
  pages      = {V--509--12},
  file       = {Korl et al. - 2004 - AR model parameter estimation from factor graphs
                .pdf:/Users/apodusenko/Zotero/storage/LX6SBCHN/Korl et al. - 2004 - AR
                model parameter estimation from factor graphs .pdf:application/pdf}
}

@article{drost_factor-graph_2007,
  title    = {Factor-{Graph} {Algorithms} for {Equalization}},
  volume   = {55},
  issn     = {1053-587X},
  url      = {http://ieeexplore.ieee.org/document/4156381/},
  doi      = {10.1109/TSP.2007.893200},
  abstract = {In this paper, we use the factor-graph framework to describe the
              statistical relationships that arise in the equalization of data
              transmitted over an intersymbol interference channel, and use it to
              develop several new algorithms for linear and decision feedback
              approaches. Speciﬁcally, we examine both unconstrained and
              constrained linear equalization and decision feedback equalization
              of a sequence of nonidentically distributed symbols that are
              transmitted over a linear, possibly time-varying, ﬁnite-length
              channel and then corrupted by additive white noise. Factor graphs
              are used to derive algorithms for each of these equalization tasks,
              including fast implementations. One important application of these
              algorithms is linear turbo equalization, which requires a linear
              equalizer that can process observations of nonidentically
              distributed transmitted symbols. We show how the output of these
              factor-graph-based algorithms can be used in an efﬁcient
              implementation of a linear turbo equalizer.},
  language = {en},
  number   = {5},
  urldate  = {2022-04-25},
  journal  = {IEEE Transactions on Signal Processing},
  author   = {Drost, Robert J. and Singer, Andrew C.},
  month    = may,
  year     = {2007},
  pages    = {2052--2065},
  file     = {Drost and Singer - 2007 - Factor-Graph Algorithms for
              Equalization.pdf:/Users/apodusenko/Zotero/storage/BJ4R822R/Drost and
              Singer - 2007 - Factor-Graph Algorithms for
              Equalization.pdf:application/pdf}
}

@misc{noauthor_monte_nodate,
  title   = {Monte {Carlo} theory, methods and examples},
  url     = {https://artowen.su.domains/mc/},
  urldate = {2022-04-11},
  file    = {Monte Carlo theory, methods and
             examples:/Users/apodusenko/Zotero/storage/294BEH2W/mc.html:text/html}
}

@article{isomura_canonical_2022,
  title     = {Canonical neural networks perform active inference},
  volume    = {5},
  copyright = {2022 The Author(s)},
  issn      = {2399-3642},
  url       = {https://www.nature.com/articles/s42003-021-02994-2},
  doi       = {10.1038/s42003-021-02994-2},
  abstract  = {This work considers a class of canonical neural networks
               comprising rate coding models, wherein neural activity and
               plasticity minimise a common cost function—and plasticity is
               modulated with a certain delay. We show that such neural networks
               implicitly perform active inference and learning to minimise the
               risk associated with future outcomes. Mathematical analyses
               demonstrate that this biological optimisation can be cast as
               maximisation of model evidence, or equivalently minimisation of
               variational free energy, under the well-known form of a partially
               observed Markov decision process model. This equivalence indicates
               that the delayed modulation of Hebbian plasticity—accompanied with
               adaptation of firing thresholds—is a sufficient neuronal substrate
               to attain Bayes optimal inference and control. We corroborated this
               proposition using numerical analyses of maze tasks. This theory
               offers a universal characterisation of canonical neural networks in
               terms of Bayesian belief updating and provides insight into the
               neuronal mechanisms underlying planning and adaptive behavioural
               control.},
  language  = {en},
  number    = {1},
  urldate   = {2022-04-07},
  journal   = {Communications Biology},
  author    = {Isomura, Takuya and Shimazaki, Hideaki and Friston, Karl J.},
  month     = jan,
  year      = {2022},
  note      = {Number: 1 Publisher: Nature Publishing Group},
  keywords  = {Machine learning, Dynamical systems, Learning algorithms},
  pages     = {1--15},
  file      = {Isomura et al. - 2022 - Canonical neural networks perform active
               inference.pdf:/Users/apodusenko/Zotero/storage/GEZUBMHT/Isomura et al.
               - 2022 - Canonical neural networks perform active
               inference.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/XTRTLWUI/s42003-021-02994-2.html:text/html
               }
}

@article{van_de_laar_active_2022,
  title    = {Active {Inference} and {Epistemic} {Value} in {Graphical} {Models}},
  volume   = {9},
  issn     = {2296-9144},
  url      = {https://www.frontiersin.org/article/10.3389/frobt.2022.794464},
  abstract = {The Free Energy Principle (FEP) postulates that biological agents
              perceive and interact with their environment in order to minimize a
              Variational Free Energy (VFE) with respect to a generative model of
              their environment. The inference of a policy (future control
              sequence) according to the FEP is known as Active Inference (AIF).
              The AIF literature describes multiple VFE objectives for policy
              planning that lead to epistemic (information-seeking) behavior.
              However, most objectives have limited modeling flexibility. This
              paper approaches epistemic behavior from a constrained Bethe Free
              Energy (CBFE) perspective. Crucially, variational optimization of
              the CBFE can be expressed in terms of message passing on free-form
              generative models. The key intuition behind the CBFE is that we
              impose a point-mass constraint on predicted outcomes, which
              explicitly encodes the assumption that the agent will make
              observations in the future. We interpret the CBFE objective in
              terms of its constituent behavioral drives. We then illustrate
              resulting behavior of the CBFE by planning and interacting with a
              simulated T-maze environment. Simulations for the T-maze task
              illustrate how the CBFE agent exhibits an epistemic drive, and
              actively plans ahead to account for the impact of predicted
              outcomes. Compared to an EFE agent, the CBFE agent incurs expected
              reward in significantly more environmental scenarios. We conclude
              that CBFE optimization by message passing suggests a general
              mechanism for epistemic-aware AIF in free-form generative models.},
  urldate  = {2022-04-06},
  journal  = {Frontiers in Robotics and AI},
  author   = {van de Laar, Thijs and Koudahl, Magnus and van Erp, Bart and de
              Vries, Bert},
  year     = {2022},
  file     = {Full Text PDF:/Users/apodusenko/Zotero/storage/Q9I82H28/van de Laar et
              al. - 2022 - Active Inference and Epistemic Value in Graphical
              .pdf:application/pdf}
}

@misc{bagaev_dmitry_reactivempjl_2021,
  title      = {{ReactiveMP}.jl: a {Julia} package for automatic {Bayesian} inference
                on a factor graph with reactive message passing.},
  copyright  = {MIT License, Open Access},
  shorttitle = {{ReactiveMP}.jl},
  url        = {https://zenodo.org/record/6365000},
  abstract   = {ReactiveMP v1.3.2 Diff since v1.3.1 {\textless}strong{\textgreater
                }Closed issues:{\textless}/strong{\textgreater} {\textless}code{
                \textgreater}+{\textless}/code{\textgreater} node should use {
                \textless}code{\textgreater}convolve{\textless}/code{\textgreater}
                from Distributions.jl (\#72) Use safe domains for point mass
                constraints optimisations (\#83) {\textless}strong{\textgreater}
                Merged pull requests:{\textless}/strong{\textgreater} feat(): Add
                boundaries specification to point mass form constraint (\#85)
                (@bvdmitri) Add mean-field rules for AR node (\#86) (@albertpod)
                fix(): fix AbstractFloat constructors for tiny and huge (\#87)
                (@bvdmitri) Fix Flow tutorial and demo in the documentation (2prev
                PR) (\#88) (@bvdmitri) fix(): fix equality node cache computation
                bug (\#89) (@bvdmitri)},
  urldate    = {2022-04-05},
  publisher  = {Zenodo},
  author     = {Bagaev, Dmitry},
  month      = jan,
  year       = {2021},
  doi        = {10.5281/ZENODO.6365000},
  keywords   = {Factor Graphs, Message Passing, Bayesian Inference, Reactive
                Programming, Variational Inference}
}

@article{akbayrak_extended_2021,
  title    = {Extended {Variational} {Message} {Passing} for {Automated} {
              Approximate} {Bayesian} {Inference}},
  volume   = {23},
  issn     = {1099-4300},
  url      = {https://www.mdpi.com/1099-4300/23/7/815},
  doi      = {10.3390/e23070815},
  abstract = {Variational Message Passing (VMP) provides an automatable and
              efficient algorithmic framework for approximating Bayesian
              inference in factorized probabilistic models that consist of
              conjugate exponential family distributions. The automation of
              Bayesian inference tasks is very important since many data
              processing problems can be formulated as inference tasks on a
              generative probabilistic model. However, accurate generative models
              may also contain deterministic and possibly nonlinear variable
              mappings and non-conjugate factor pairs that complicate the
              automatic execution of the VMP algorithm. In this paper, we show
              that executing VMP in complex models relies on the ability to
              compute the expectations of the statistics of hidden variables. We
              extend the applicability of VMP by approximating the required
              expectation quantities in appropriate cases by importance sampling
              and Laplace approximation. As a result, the proposed Extended VMP
              (EVMP) approach supports automated efficient inference for a very
              wide range of probabilistic model specifications. We implemented
              EVMP in the Julia language in the probabilistic programming package
              ForneyLab.jl and show by a number of examples that EVMP renders an
              almost universal inference engine for factorized probabilistic
              models.},
  language = {en},
  number   = {7},
  urldate  = {2022-04-04},
  journal  = {Entropy},
  author   = {Akbayrak, Semih and Bocharov, Ivan and de Vries, Bert},
  month    = jun,
  year     = {2021},
  pages    = {815},
  file     = {Full Text:/Users/apodusenko/Zotero/storage/UDXGJRGE/Akbayrak et al. -
              2021 - Extended Variational Message Passing for
              Automated.pdf:application/pdf}
}

@inproceedings{akbayrak_reparameterization_2019,
  address   = {A Coruna, Spain},
  title     = {Reparameterization {Gradient} {Message} {Passing}},
  isbn      = {978-90-827970-3-9},
  url       = {https://ieeexplore.ieee.org/document/8902930/},
  doi       = {10.23919/EUSIPCO.2019.8902930},
  urldate   = {2022-04-04},
  booktitle = {2019 27th {European} {Signal} {Processing} {Conference} ({EUSIPCO
               })},
  publisher = {IEEE},
  author    = {Akbayrak, Semih and Vries, Bert de},
  month     = sep,
  year      = {2019},
  pages     = {1--5},
  file      = {Akbayrak and Vries - 2019 - Reparameterization Gradient Message
               Passing.pdf:/Users/apodusenko/Zotero/storage/7ZW2F36W/Akbayrak and
               Vries - 2019 - Reparameterization Gradient Message
               Passing.pdf:application/pdf}
}

@article{bagaev_reactivempjl_2022,
  title      = {{ReactiveMP}.jl: {A} {Julia} {Package} for {Reactive} {Message} {
                Passing}-based {Bayesian} {Inference}},
  volume     = {1},
  issn       = {2642-4029},
  shorttitle = {{ReactiveMP}.jl},
  url        = {https://proceedings.juliacon.org/papers/10.21105/jcon.00091},
  doi        = {10.21105/jcon.00091},
  number     = {1},
  urldate    = {2022-03-25},
  journal    = {JuliaCon Proceedings},
  author     = {Bagaev, Dmitry and De Vries, Bert},
  month      = jan,
  year       = {2022},
  pages      = {91},
  file       = {Full Text:/Users/apodusenko/Zotero/storage/9G4CDZTA/2022 -
                ReactiveMP.jl A Julia Package for Reactive Messag.pdf:application/pdf}
}

@article{shimazaki_principles_2019,
  title      = {The principles of adaptation in organisms and machines {I}: machine
                learning, information theory, and thermodynamics},
  shorttitle = {The principles of adaptation in organisms and machines {I}},
  url        = {http://arxiv.org/abs/1902.11233},
  abstract   = {How do organisms recognize their environment by acquiring
                knowledge about the world, and what actions do they take based on
                this knowledge? This article examines hypotheses about organisms'
                adaptation to the environment from machine learning,
                information-theoretic, and thermodynamic perspectives. We start
                with constructing a hierarchical model of the world as an internal
                model in the brain, and review standard machine learning methods to
                infer causes by approximately learning the model under the maximum
                likelihood principle. This in turn provides an overview of the free
                energy principle for an organism, a hypothesis to explain
                perception and action from the principle of least surprise.
                Treating this statistical learning as communication between the
                world and brain, learning is interpreted as a process to maximize
                information about the world. We investigate how the classical
                theories of perception such as the infomax principle relates to
                learning the hierarchical model. We then present an approach to the
                recognition and learning based on thermodynamics, showing that
                adaptation by causal learning results in the second law of
                thermodynamics whereas inference dynamics that fuses observation
                with prior knowledge forms a thermodynamic process. These provide a
                unified view on the adaptation of organisms to the environment.},
  urldate    = {2020-04-11},
  journal    = {arXiv:1902.11233 [q-bio, stat]},
  author     = {Shimazaki, Hideaki},
  month      = feb,
  year       = {2019},
  note       = {arXiv: 1902.11233},
  keywords   = {Statistics - Machine Learning, Quantitative Biology - Neurons and
                Cognition},
  file       = {arXiv.org
                Snapshot:/Users/apodusenko/Zotero/storage/E35W62NT/1902.html:text/html;Shimazaki
                - 2019 - The principles of adaptation in organisms and
                mach.pdf:/Users/apodusenko/Zotero/storage/CW6P2N5C/Shimazaki - 2019 -
                The principles of adaptation in organisms and mach.pdf:application/pdf}
}

@article{shimazaki_principles_2020,
  title      = {The principles of adaptation in organisms and machines {II}:
                thermodynamics of the {Bayesian} brain},
  shorttitle = {The principles of adaptation in organisms and machines {II}},
  journal    = {arXiv preprint arXiv:2006.13158},
  author     = {Shimazaki, Hideaki},
  year       = {2020},
  file       = {Shimazaki - 2020 - The principles of adaptation in organisms and
                mach.pdf:/Users/apodusenko/Zotero/storage/BZWWGMC4/Shimazaki - 2020 -
                The principles of adaptation in organisms and mach.pdf:application/pdf}
}

@inproceedings{van_erp_online_2022,
  title     = {Online {Single}-{Microphone} {Source} {Separation} using {Non}-{
               Linear} {Autoregressive} {Models}},
  author    = {van Erp, Bart and de Vries, Bert},
  month     = oct,
  year      = {2022},
  booktitle = {2022 {International} {Conference} on {Probabilistic} {Graphical}
               {Models} ({PGM})},
  note      = {accepted},
  file      = {van Erp and de Vries - 2022 - Online Single-Microphone Source
               Separation using N.pdf:/Users/apodusenko/Zotero/storage/V5IMEIHL/van
               Erp and de Vries - 2022 - Online Single-Microphone Source Separation
               using N.pdf:application/pdf}
}

@techreport{chang_fast_2020,
  title       = {Fast {Variational} {Learning} in {State}-{Space} {Gaussian} {Process}
                 {Models}},
  url         = {http://arxiv.org/abs/2007.04731},
  abstract    = {Gaussian process (GP) regression with 1D inputs can often be
                 performed in linear time via a stochastic differential equation
                 formulation. However, for non-Gaussian likelihoods, this requires
                 application of approximate inference methods which can make the
                 implementation difficult, e.g., expectation propagation can be
                 numerically unstable and variational inference can be
                 computationally inefficient. In this paper, we propose a new method
                 that removes such difficulties. Building upon an existing method
                 called conjugate-computation variational inference, our approach
                 enables linear-time inference via Kalman recursions while avoiding
                 numerical instabilities and convergence issues. We provide an
                 efficient JAX implementation which exploits just-in-time
                 compilation and allows for fast automatic differentiation through
                 large for-loops. Overall, our approach leads to fast and stable
                 variational inference in state-space GP models that can be scaled
                 to time series with millions of data points.},
  number      = {arXiv:2007.04731},
  urldate     = {2022-06-02},
  institution = {arXiv},
  author      = {Chang, Paul E. and Wilkinson, William J. and Khan, Mohammad Emtiyaz
                 and Solin, Arno},
  month       = jul,
  year        = {2020},
  note        = {arXiv:2007.04731 [cs, stat] type: article},
  keywords    = {Statistics - Machine Learning, Computer Science - Machine Learning
                 },
  file        = {arXiv Fulltext PDF:/Users/apodusenko/Zotero/storage/9MF5M9FM/Chang et
                 al. - 2020 - Fast Variational Learning in State-Space Gaussian
                 .pdf:application/pdf;arXiv.org
                 Snapshot:/Users/apodusenko/Zotero/storage/ALUS3AH6/2007.html:text/html}
}

@article{van_de_laar_simulating_2019-1,
  title    = {Simulating {Active} {Inference} {Processes} by {Message} {Passing}},
  volume   = {6},
  issn     = {2296-9144},
  url      = {https://www.frontiersin.org/articles/10.3389/frobt.2019.00020/full},
  doi      = {10.3389/frobt.2019.00020},
  abstract = {The free energy principle (FEP) offers a variational
              calculus-based description for how biological agents persevere
              through interactions with their environment. Active inference (AI)
              is a corollary of the FEP, which states that biological agents act
              to fulfill prior beliefs about preferred future observations
              (target priors). Purposeful behavior then results from variational
              free energy minimization with respect to a generative model of the
              environment with included target priors. However, manual
              derivations for free energy minimizing algorithms on custom dynamic
              models can become tedious and error-prone. While probabilistic
              programming (PP) techniques enable automatic derivation of
              inference algorithms on free-form models, full automation of AI
              requires specialized tools for inference on dynamic models,
              together with the description of an experimental protocol that
              governs the interaction between the agent and its simulated
              environment. The contributions of the present paper are two-fold.
              Firstly, we illustrate how AI can be automated with the use of
              ForneyLab, a recent PP toolbox that specializes in variational
              inference on flexibly definable dynamic models. More specifically,
              we describe AI agents in a dynamic environment as probabilistic
              state space models (SSM) and perform inference for perception and
              control in these agents by message passing on a factor graph
              representation of the SSM. Secondly, we propose a formal
              experimental protocol for simulated AI. We exemplify how this
              protocol leads to goal-directed behavior for flexibly definable AI
              agents in two classical RL examples, namely the Bayesian thermostat
              and the mountain car parking problems.},
  language = {English},
  urldate  = {2019-05-09},
  journal  = {Frontiers in Robotics and AI},
  author   = {van de Laar, Thijs W. and de Vries, Bert},
  year     = {2019},
  keywords = {message passing, state-space models, active inference,
              Forney-style factor graphs, free-energy principle},
  file     = {van de Laar and de Vries - 2019 - Simulating Active Inference
              Processes by Message
              P.pdf:/Users/apodusenko/Zotero/storage/L5PWHWNF/van de Laar and de
              Vries - 2019 - Simulating Active Inference Processes by Message
              P.pdf:application/pdf}
}

@techreport{champion_branching_2021,
  title       = {Branching {Time} {Active} {Inference} with {Bayesian} {Filtering}},
  url         = {http://arxiv.org/abs/2112.07406},
  abstract    = {Branching Time Active Inference (Champion et al., 2021b,a) is a
                 framework proposing to look at planning as a form of Bayesian model
                 expansion. Its root can be found in Active Inference (Friston et
                 al., 2016; Da Costa et al., 2020; Champion et al., 2021c), a
                 neuroscientific framework widely used for brain modelling, as well
                 as in Monte Carlo Tree Search (Browne et al., 2012), a method
                 broadly applied in the Reinforcement Learning literature. Up to now
                 , the inference of the latent variables was carried out by taking
                 advantage of the flexibility offered by Variational Message Passing
                 (Winn and Bishop, 2005), an iterative process that can be
                 understood as sending messages along the edges of a factor graph
                 (Forney, 2001). In this paper, we harness the efficiency of an
                 alternative method for inference called Bayesian Filtering (Fox et
                 al., 2003), which does not require the iteration of the update
                 equations until convergence of the Variational Free Energy. Instead
                 , this scheme alternates between two phases: integration of
                 evidence and prediction of future states. Both of those phases can
                 be performed efficiently and this provides a seventy times speed up
                 over the state-of-the-art.},
  number      = {arXiv:2112.07406},
  urldate     = {2022-06-03},
  institution = {arXiv},
  author      = {Champion, Théophile and Grześ, Marek and Bowman, Howard},
  month       = dec,
  year        = {2021},
  doi         = {10.48550/arXiv.2112.07406},
  note        = {arXiv:2112.07406 [cs] type: article},
  keywords    = {Computer Science - Machine Learning},
  file        = {arXiv.org
                 Snapshot:/Users/apodusenko/Zotero/storage/DEPIDUGQ/2112.html:text/html;Champion
                 et al. - 2021 - Branching Time Active Inference with Bayesian
                 Filt.pdf:/Users/apodusenko/Zotero/storage/YGFN74VC/Champion et al. -
                 2021 - Branching Time Active Inference with Bayesian
                 Filt.pdf:application/pdf}
}

@article{friston_sophisticated_2021-1,
  title    = {Sophisticated {Inference}},
  volume   = {33},
  issn     = {0899-7667},
  url      = {https://doi.org/10.1162/neco_a_01351},
  doi      = {10.1162/neco_a_01351},
  abstract = {Active inference offers a first principle account of sentient
              behavior, from which special and important cases—for example,
              reinforcement learning, active learning, Bayes optimal inference,
              Bayes optimal design—can be derived. Active inference finesses the
              exploitation-exploration dilemma in relation to prior preferences
              by placing information gain on the same footing as reward or value.
              In brief, active inference replaces value functions with
              functionals of (Bayesian) beliefs, in the form of an expected
              (variational) free energy. In this letter, we consider a
              sophisticated kind of active inference using a recursive form of
              expected free energy. Sophistication describes the degree to which
              an agent has beliefs about beliefs. We consider agents with beliefs
              about the counterfactual consequences of action for states of
              affairs and beliefs about those latent states. In other words, we
              move from simply considering beliefs about “what would happen if I
              did that” to “what I would believe about what would happen if I did
              that.” The recursive form of the free energy functional effectively
              implements a deep tree search over actions and outcomes in the
              future. Crucially, this search is over sequences of belief states
              as opposed to states per se. We illustrate the competence of this
              scheme using numerical simulations of deep decision problems.},
  number   = {3},
  urldate  = {2022-02-14},
  journal  = {Neural Computation},
  author   = {Friston, Karl and Da Costa, Lancelot and Hafner, Danijar and Hesp,
              Casper and Parr, Thomas},
  month    = mar,
  year     = {2021},
  pages    = {713--763},
  file     = {Friston et al. - 2021 - Sophisticated
              Inference.pdf:/Users/apodusenko/Zotero/storage/EN57AYTX/Friston et al.
              - 2021 - Sophisticated
              Inference.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/5MGJRLQY/Sophisticated-Inference.html:text/html
              }
}

@article{palmieri_unifying_2022,
  title    = {A {Unifying} {View} of {Estimation} and {Control} {Using} {Belief} {
              Propagation} {With} {Application} to {Path} {Planning}},
  volume   = {10},
  issn     = {2169-3536},
  doi      = {10.1109/ACCESS.2022.3148127},
  abstract = {The use of estimation techniques on stochastic models to solve
              control problems is an emerging paradigm that falls under the
              rubric of Active Inference (AI) and Control as Inference (CAI). In
              this work, we use probability propagation on factor graphs to show
              that various algorithms proposed in the literature can be seen as
              specific composition rules in a factor graph. We show how this
              unified approach, presented both in probability space and in log of
              the probability space, provides a very general framework that
              includes the Sum-product, the Max-product, Dynamic programming and
              mixed Reward/Entropy criteria-based algorithms. The framework also
              expands algorithmic design options that lead to new smoother or
              sharper policy distributions. We propose original recursions such
              as: a generalized Sum/Max-product algorithm, a Smooth Dynamic
              programming algorithm and a modified versions of the Reward/Entropy
              algorithm. The discussion is carried over with reference to a path
              planning problem where the recursions that arise from various cost
              functions, although they may appear similar in scope, bear
              noticeable differences. We provide a comprehensive table of
              composition rules and a comparison through simulations, first on a
              synthetic small grid with a single goal with obstacles, and then on
              a grid extrapolated from a real-world scene with multiple goals and
              a semantic map.},
  journal  = {IEEE Access},
  author   = {Palmieri, Francesco A. N. and Pattipati, Krishna R. and Gennaro,
              Giovanni Di and Fioretti, Giovanni and Verolla, Francesco and
              Buonanno, Amedeo},
  year     = {2022},
  note     = {Conference Name: IEEE Access},
  keywords = {Estimation, Belief propagation, Bayes methods, reinforcement
              learning, Probabilistic logic, Heuristic algorithms, dynamic
              programming, Dynamic programming, Markov decision process, path
              planning, Path planning},
  pages    = {15193--15216},
  file     = {IEEE Xplore Abstract
              Record:/Users/apodusenko/Zotero/storage/YRRZT5DY/9698186.html:text/html;Palmieri
              et al. - 2022 - A Unifying View of Estimation and Control Using
              Be.pdf:/Users/apodusenko/Zotero/storage/VFTAEXPM/Palmieri et al. - 2022
              - A Unifying View of Estimation and Control Using
              Be.pdf:application/pdf}
}

@techreport{da_costa_relationship_2020-1,
  title       = {The relationship between dynamic programming and active inference:
                 the discrete, finite-horizon case},
  shorttitle  = {The relationship between dynamic programming and active
                 inference},
  url         = {http://arxiv.org/abs/2009.08111},
  abstract    = {Active inference is a normative framework for generating behaviour
                 based upon the free energy principle, a theory of
                 self-organisation. This framework has been successfully used to
                 solve reinforcement learning and stochastic control problems, yet,
                 the formal relation between active inference and reward
                 maximisation has not been fully explicated. In this paper, we
                 consider the relation between active inference and dynamic
                 programming under the Bellman equation, which underlies many
                 approaches to reinforcement learning and control. We show that, on
                 partially observable Markov decision processes, dynamic programming
                 is a limiting case of active inference. In active inference, agents
                 select actions to minimise expected free energy. In the absence of
                 ambiguity about states, this reduces to matching expected states
                 with a target distribution encoding the agent's preferences. When
                 target states correspond to rewarding states, this maximises
                 expected reward, as in reinforcement learning. When states are
                 ambiguous, active inference agents will choose actions that
                 simultaneously minimise ambiguity. This allows active inference
                 agents to supplement their reward maximising (or exploitative)
                 behaviour with novelty-seeking (or exploratory) behaviour. This
                 clarifies the connection between active inference and reinforcement
                 learning, and how both frameworks may benefit from each other.},
  number      = {arXiv:2009.08111},
  urldate     = {2022-06-06},
  institution = {arXiv},
  author      = {Da Costa, Lancelot and Sajid, Noor and Parr, Thomas and Friston,
                 Karl and Smith, Ryan},
  month       = sep,
  year        = {2020},
  doi         = {10.48550/arXiv.2009.08111},
  note        = {arXiv:2009.08111 [cs, math, q-bio] type: article},
  keywords    = {Computer Science - Artificial Intelligence, Quantitative Biology -
                 Neurons and Cognition, Mathematics - Optimization and Control},
  file        = {arXiv.org
                 Snapshot:/Users/apodusenko/Zotero/storage/TTSL9CDG/2009.html:text/html;Da
                 Costa et al. - 2020 - The relationship between dynamic programming and
                 a.pdf:/Users/apodusenko/Zotero/storage/6Z3D7XBD/Da Costa et al. - 2020
                 - The relationship between dynamic programming and
                 a.pdf:application/pdf}
}

@article{levine_reinforcement_2018-1,
  title      = {Reinforcement {Learning} and {Control} as {Probabilistic} {Inference}
                : {Tutorial} and {Review}},
  shorttitle = {Reinforcement {Learning} and {Control} as {Probabilistic} {
                Inference}},
  url        = {http://arxiv.org/abs/1805.00909},
  abstract   = {The framework of reinforcement learning or optimal control
                provides a mathematical formalization of intelligent decision
                making that is powerful and broadly applicable. While the general
                form of the reinforcement learning problem enables effective
                reasoning about uncertainty, the connection between reinforcement
                learning and inference in probabilistic models is not immediately
                obvious. However, such a connection has considerable value when it
                comes to algorithm design: formalizing a problem as probabilistic
                inference in principle allows us to bring to bear a wide array of
                approximate inference tools, extend the model in flexible and
                powerful ways, and reason about compositionality and partial
                observability. In this article, we will discuss how a
                generalization of the reinforcement learning or optimal control
                problem, which is sometimes termed maximum entropy reinforcement
                learning, is equivalent to exact probabilistic inference in the
                case of deterministic dynamics, and variational inference in the
                case of stochastic dynamics. We will present a detailed derivation
                of this framework, overview prior work that has drawn on this and
                related ideas to propose new reinforcement learning and control
                algorithms, and describe perspectives on future research.},
  urldate    = {2018-05-03},
  journal    = {arXiv:1805.00909 [cs, stat]},
  author     = {Levine, Sergey},
  month      = may,
  year       = {2018},
  note       = {arXiv: 1805.00909},
  keywords   = {Computer Science - Learning, Statistics - Machine Learning,
                Computer Science - Artificial Intelligence, Computer Science -
                Robotics, Computer Science - Machine Learning},
  file       = {arXiv.org
                Snapshot:/Users/apodusenko/Zotero/storage/AKNJ3WN6/1805.html:text/html;arXiv.org
                Snapshot:/Users/apodusenko/Zotero/storage/GIBT3JIE/1805.html:text/html;Levine
                - 2018 - Reinforcement Learning and Control as
                Probabilisti.pdf:/Users/apodusenko/Zotero/storage/3FDZSI4S/Levine -
                2018 - Reinforcement Learning and Control as
                Probabilisti.pdf:application/pdf}
}

@article{mirza_introducing_2019,
  title     = {Introducing a {Bayesian} model of selective attention based on active
               inference},
  volume    = {9},
  copyright = {2019 The Author(s)},
  issn      = {2045-2322},
  url       = {https://www.nature.com/articles/s41598-019-50138-8},
  doi       = {10.1038/s41598-019-50138-8},
  abstract  = {Information gathering comprises actions whose (sensory)
               consequences resolve uncertainty (i.e., are salient). In other
               words, actions that solicit salient information cause the greatest
               shift in beliefs (i.e., information gain) about the causes of our
               sensations. However, not all information is relevant to the task at
               hand: this is especially the case in complex, naturalistic scenes.
               This paper introduces a formal model of selective attention based
               on active inference and contextual epistemic foraging. We consider
               a visual search task with a special emphasis on goal-directed and
               task-relevant exploration. In this scheme, attention modulates the
               expected fidelity (precision) of the mapping between observations
               and hidden states in a state-dependent or context-sensitive manner.
               This ensures task-irrelevant observations have little expected
               information gain, and so the agent – driven to reduce expected
               surprise (i.e., uncertainty) – does not actively seek them out.
               Instead, it selectively samples task-relevant observations, which
               inform (task-relevant) hidden states. We further show, through
               simulations, that the atypical exploratory behaviours in conditions
               such as autism and anxiety may be due to a failure to appropriately
               modulate sensory precision in a context-specific way.},
  language  = {en},
  number    = {1},
  urldate   = {2019-09-30},
  journal   = {Scientific Reports},
  author    = {Mirza, M. Berk and Adams, Rick A. and Friston, Karl and Parr, Thomas
               },
  month     = sep,
  year      = {2019},
  pages     = {1--22},
  file      = {Mirza et al. - 2019 - Introducing a Bayesian model of selective
               attentio.pdf:/Users/apodusenko/Zotero/storage/TU895ZH4/Mirza et al. -
               2019 - Introducing a Bayesian model of selective
               attentio.pdf:application/pdf}
}

@article{moon_em_1996,
  author  = {Moon, T.K.},
  year    = {1996},
  month   = {12},
  pages   = {47 - 60},
  title   = {The expectation-maximization algorithm},
  volume  = {13},
  journal = {Signal Processing Magazine, IEEE},
  doi     = {10.1109/79.543975}
}

@phdthesis{senoz_thesis,
  title     = {Message Passing Algorithms for Hierarchical Dynamical Models},
  author    = {Ismail Șenoz},
  note      = {Proefschrift.},
  year      = {2022},
  month     = {jun},
  day       = {24},
  language  = {English},
  isbn      = {978-90-386-5532-1},
  publisher = {Eindhoven University of Technology},
  school    = {Electrical Engineering}
}

@inproceedings{akbayrak_adaptive_2022,
  author    = {Akbayrak, Semih and Şenöz, İsmail and Vries, Bert de},
  booktitle = {2022 IEEE International Symposium on Information Theory (ISIT)},
  title     = {Adaptive Importance Sampling Message Passing},
  year      = {2022},
  volume    = {},
  number    = {},
  pages     = {1199-1204},
  doi       = {10.1109/ISIT50566.2022.9834628}
}

@article{levien_double_1993,
  title      = {Double pendulum: {An} experiment in chaos},
  volume     = {61},
  issn       = {0002-9505, 1943-2909},
  shorttitle = {Double pendulum},
  url        = {https://pubs.aip.org/aapt/ajp/article/61/11/1038-1044/1054221},
  doi        = {10.1119/1.17335},
  language   = {en},
  number     = {11},
  urldate    = {2023-05-24},
  journal    = {American Journal of Physics},
  author     = {Levien, R. B. and Tan, S. M.},
  month      = nov,
  year       = {1993},
  pages      = {1038--1044}
}

@phdthesis{podusenko_thesis,
  title     = {Message Passing-based Inference in Hierarchical Autoregressive Models
               },
  author    = {Albert Podusenko},
  note      = {Proefschrift.},
  year      = {2022},
  month     = dec,
  day       = {20},
  language  = {English},
  isbn      = {978-90-386-5594-9},
  publisher = {Eindhoven University of Technology},
  school    = {Electrical Engineering}
}

@inproceedings{nguyen_efficient_2022,
  address   = {Rennes, France},
  title     = {Efficient {Model} {Evidence} {Computation} in {Tree}-structured {
               Factor} {Graphs}},
  abstract  = {Model evidence is a fundamental performance measure in Bayesian
               machine learning as it represents how well a model fits an observed
               data set. Since model evidence is often an intractable quantity,
               the literature often resorts to computing instead the Bethe Free
               Energy (BFE), which for cyclefree models is a tractable upper bound
               on the (negative log-) model evidence. In this paper, we propose a
               different and faster evidence computation approach by tracking
               local normalization constants of sum-product messages, termed scale
               factors. We tabulate scale factor update rules for various
               elementary factor nodes and by experimental validation we verify
               the correctness of these update rules for models involving both
               discrete and continuous variables. We show how tracking scale
               factors leads to performance improvements compared to the
               traditional BFE computation approach.},
  language  = {en},
  booktitle = {2022 {IEEE} {Workshop} on {Signal} {Processing} {Systems} ({SiPS}
               )},
  author    = {Nguyen, Hoang M H and van Erp, Bart and Senoz, Ismail and de Vries,
               Bert},
  year      = {2022},
  note      = {in press},
  pages     = {6}
}

@article{Lawrence_kalman_equalization,
  author  = {Lawrence, R. and Kaufman, H.},
  journal = {IEEE Transactions on Communication Technology},
  title   = {The Kalman Filter for the Equalization of a Digital Communications Channel},
  year    = {1971},
  volume  = {19},
  number  = {6},
  pages   = {1137-1141},
  doi     = {10.1109/TCOM.1971.1090786}
}


@inproceedings{pmlr-v134-chen21c,
  title     = {Black-Box Control for Linear Dynamical Systems},
  author    = {Chen, Xinyi and Hazan, Elad},
  booktitle = {Proceedings of Thirty Fourth Conference on Learning Theory},
  pages     = {1114--1143},
  year      = {2021},
  editor    = {Belkin, Mikhail and Kpotufe, Samory},
  volume    = {134},
  series    = {Proceedings of Machine Learning Research},
  month     = {15--19 Aug},
  publisher = {PMLR},
  pdf       = {http://proceedings.mlr.press/v134/chen21c/chen21c.pdf},
  url       = {https://proceedings.mlr.press/v134/chen21c.html},
  abstract  = {We consider the problem of black-box control: the task of controlling an unknown linear time-invariant dynamical system from a single trajectory without a stabilizing controller. Under the assumption that the system is controllable, we give the first {\it efficient} algorithm that is capable of attaining sublinear regret under the setting of online nonstochastic control. This resolves an open problem since the work of Abbasi-Yadkori and Szepesvari(2011) on the stochastic LQR problem, and in a more challenging setting that allows for adversarial perturbations and adversarially chosen changing convex loss functions. We give finite-time regret bounds for our algorithm on the order of $2^{poly(d)} + \tilde{O}(poly(d) T^{2/3})$ for general nonstochastic control, and $2^{poly(d)} + \tilde{O}(poly(d) \sqrt{T})$ for black-box LQR. To complete the picture, we investigate the complexity of the online black-box control problem and give a matching regret lower bound of $2^{\Omega(d)}$, showing that the exponential cost is inevitable. This lower bound holds even in the noiseless setting, and applies to any, randomized or deterministic, black-box control method.}
}

@article{Lahmiri_lds_in_finance,
  author  = {Lahmiri, Salim},
  year    = {2012},
  month   = {10},
  pages   = {2551-2556},
  title   = {Linear and nonlinear dynamic systems in financial time series prediction},
  volume  = {2},
  journal = {Management Science Letters},
  doi     = {10.5267/j.msl.2012.07.009}
}

@article{rauch_maximum_1965,
  title    = {Maximum likelihood estimates of linear dynamic systems},
  volume   = {3},
  issn     = {0001-1452, 1533-385X},
  url      = {https://arc.aiaa.org/doi/10.2514/3.3166},
  doi      = {10.2514/3.3166},
  language = {en},
  number   = {8},
  urldate  = {2023-06-22},
  journal  = {AIAA Journal},
  author   = {Rauch, H. E. and Tung, F. and Striebel, C. T.},
  month    = aug,
  year     = {1965},
  pages    = {1445--1450}
}

@book{cernousko_control_2008,
  address    = {Berlin Heidelberg},
  series     = {Communications and control engineering},
  title      = {Control of nonlinear dynamical systems: methods and applications},
  isbn       = {978-3-540-70782-0},
  shorttitle = {Control of nonlinear dynamical systems},
  language   = {eng},
  publisher  = {Springer},
  author     = {Černous'ko, Feliks L. and Ananievskij, Igor M. and Rešmin, Sergej A. and Černousko, Feliks Leonidovič and Ananievski, Igor M.},
  year       = {2008},
  annote     = {Literaturverz. S. 389 - 393}
}

@article{hsieh_chaos_1991,
  title      = {Chaos and {Nonlinear} {Dynamics}: {Application} to {Financial} {Markets}},
  volume     = {46},
  issn       = {00221082},
  shorttitle = {Chaos and {Nonlinear} {Dynamics}},
  url        = {https://onlinelibrary.wiley.com/doi/10.1111/j.1540-6261.1991.tb04646.x},
  doi        = {10.1111/j.1540-6261.1991.tb04646.x},
  language   = {en},
  number     = {5},
  urldate    = {2023-06-22},
  journal    = {The Journal of Finance},
  author     = {Hsieh, David A.},
  month      = dec,
  year       = {1991},
  pages      = {1839--1877}
}

@article{Contopoulos_astronomy_nlds,
  author  = {Contopoulos, G.},
  year    = {2001},
  month   = {01},
  pages   = {89-114},
  title   = {The Development of Nonlinear Dynamics in Astronomy},
  volume  = {31},
  journal = {Foundations of Physics},
  doi     = {10.1023/A:1004155905361}
}

@article{Janson_bilogy_nlds,
  author  = {Janson, Natalia},
  year    = {2012},
  month   = {03},
  pages   = {},
  title   = {Non-linear dynamics of biological systems},
  volume  = {53},
  journal = {Contemporary Physics},
  doi     = {10.1080/00107514.2011.644441}
}

@article{mukhin_principal_2015,
  title    = {Principal nonlinear dynamical modes of climate variability},
  volume   = {5},
  issn     = {2045-2322},
  url      = {https://www.nature.com/articles/srep15510},
  doi      = {10.1038/srep15510},
  abstract = {Abstract
              We suggest a new nonlinear expansion of space-distributed observational time series. The expansion allows constructing principal nonlinear manifolds holding essential part of observed variability. It yields low-dimensional hidden time series interpreted as internal modes driving observed multivariate dynamics as well as their mapping to a geographic grid. Bayesian optimality is used for selecting relevant structure of nonlinear transformation, including both the number of principal modes and degree of nonlinearity. Furthermore, the optimal characteristic time scale of the reconstructed modes is also found. The technique is applied to monthly sea surface temperature (SST) time series having a duration of 33 years and covering the globe. Three dominant nonlinear modes were extracted from the time series: the first efficiently separates the annual cycle, the second is responsible for ENSO variability and combinations of the second and the third modes explain substantial parts of Pacific and Atlantic dynamics. A relation of the obtained modes to decadal natural climate variability including current hiatus in global warming is exhibited and discussed.},
  language = {en},
  number   = {1},
  urldate  = {2023-06-22},
  journal  = {Scientific Reports},
  author   = {Mukhin, Dmitry and Gavrilov, Andrey and Feigin, Alexander and Loskutov, Evgeny and Kurths, Juergen},
  month    = oct,
  year     = {2015},
  pages    = {15510}
}

@book{roubicek_nonlinear_2013,
  address   = {Basel},
  series    = {International {Series} of {Numerical} {Mathematics}},
  title     = {Nonlinear {Partial} {Differential} {Equations} with {Applications}},
  volume    = {153},
  isbn      = {978-3-0348-0512-4 978-3-0348-0513-1},
  url       = {http://link.springer.com/10.1007/978-3-0348-0513-1},
  language  = {en},
  urldate   = {2023-06-22},
  publisher = {Springer Basel},
  author    = {Roubíček, Tomáš},
  year      = {2013},
  doi       = {10.1007/978-3-0348-0513-1}
}

@book{hasselblatt_handbook_2002,
  address   = {Amsterdam, New York},
  edition   = {1st ed},
  title     = {Handbook of dynamical systems},
  isbn      = {978-0-08-093226-2},
  abstract  = {This handbook is volume II in a series collecting mathematical state-of-the-art surveys in the field of dynamical systems. Much of this field has developed from interactions with other areas of science, and this volume shows how concepts of dynamical systems further the understanding of mathematical issues that arise in applications. Although modeling issues are addressed, the central theme is the mathematically rigorous investigation of the resulting differential equations and their dynamic behavior. However, the authors and editors have made an effort to ensure readability on a non-technical level for mathematicians from other fields and for other scientists and engineers. The eighteen surveys collected here do not aspire to encyclopedic completeness, but present selected paradigms. The surveys are grouped into those emphasizing finite-dimensional methods, numerics, topological methods, and partial differential equations. Application areas include the dynamics of neural networks, fluid flows, nonlinear optics, and many others. While the survey articles can be read independently, they deeply share recurrent themes from dynamical systems. Attractors, bifurcations, center manifolds, dimension reduction, ergodicity, homoclinicity, hyperbolicity, invariant and inertial manifolds, normal forms, recurrence, shift dynamics, stability, to name just a few, are ubiquitous dynamical concepts throughout the articles},
  language  = {eng},
  publisher = {N.H. North Holland : Elsevier},
  author    = {Hasselblatt, Boris and Katok, A. B.},
  year      = {2002},
  note      = {OCLC: 162578012}
}

@article{raichle_appraising_2002,
  title    = {Appraising the brain's energy budget},
  volume   = {99},
  issn     = {0027-8424, 1091-6490},
  url      = {https://pnas.org/doi/full/10.1073/pnas.172399499},
  doi      = {10.1073/pnas.172399499},
  language = {en},
  number   = {16},
  urldate  = {2023-06-23},
  journal  = {Proceedings of the National Academy of Sciences},
  author   = {Raichle, Marcus E. and Gusnard, Debra A.},
  month    = aug,
  year     = {2002},
  pages    = {10237--10239}
}

@article{kovac_20_2010,
  title    = {The 20 {W} sleep‐walkers},
  volume   = {11},
  issn     = {1469-221X, 1469-3178},
  url      = {https://www.embopress.org/doi/10.1038/embor.2009.266},
  doi      = {10.1038/embor.2009.266},
  language = {en},
  number   = {1},
  urldate  = {2023-06-23},
  journal  = {EMBO reports},
  author   = {Kováč, Ladislav},
  month    = jan,
  year     = {2010},
  pages    = {2--2}
}

@article{herculano-houzel_human_2009,
  title      = {The human brain in numbers: a linearly scaled-up primate brain},
  volume     = {3},
  issn       = {16625161},
  shorttitle = {The human brain in numbers},
  url        = {http://journal.frontiersin.org/article/10.3389/neuro.09.031.2009/abstract},
  doi        = {10.3389/neuro.09.031.2009},
  urldate    = {2023-06-23},
  journal    = {Frontiers in Human Neuroscience},
  author     = {Herculano-Houzel, Suzana},
  year       = {2009}
}

@article{barrett_optimal_2016,
  title    = {Optimal compensation for neuron loss},
  volume   = {5},
  issn     = {2050-084X},
  url      = {https://elifesciences.org/articles/12454},
  doi      = {10.7554/eLife.12454},
  abstract = {The brain has an impressive ability to withstand neural damage. Diseases that kill neurons can go unnoticed for years, and incomplete brain lesions or silencing of neurons often fail to produce any behavioral effect. How does the brain compensate for such damage, and what are the limits of this compensation? We propose that neural circuits instantly compensate for neuron loss, thereby preserving their function as much as possible. We show that this compensation can explain changes in tuning curves induced by neuron silencing across a variety of systems, including the primary visual cortex. We find that compensatory mechanisms can be implemented through the dynamics of networks with a tight balance of excitation and inhibition, without requiring synaptic plasticity. The limits of this compensatory mechanism are reached when excitation and inhibition become unbalanced, thereby demarcating a recovery boundary, where signal representation fails and where diseases may become symptomatic.},
  language = {en},
  urldate  = {2023-06-23},
  journal  = {eLife},
  author   = {Barrett, David Gt and Denève, Sophie and Machens, Christian K},
  month    = dec,
  year     = {2016},
  pages    = {e12454}
}

@article{henriksen_parallel_2012,
  title    = {Parallel {Implementation} of {Particle} {MCMC} {Methods} on a {GPU}*},
  volume   = {45},
  issn     = {1474-6670},
  url      = {https://www.sciencedirect.com/science/article/pii/S1474667015381076},
  doi      = {https://doi.org/10.3182/20120711-3-BE-2027.00296},
  abstract = {This paper examines the problem of estimating the parameters describing system models of quite general nonlinear and multi-variable form. The approach is a computational one in which quantities that are intractable to evaluate exactly are approximated by sample averages from randomized algorithms. The main contribution is to illustrate the viability and utility of this approach by examining how high computational loads can be simply managed using commodity hardware. The proposed algorithms and solution architectures are profiled on concrete examples.},
  number   = {16},
  journal  = {IFAC Proceedings Volumes},
  author   = {Henriksen, Soren and Wills, Adrian and Schön, Thomas B. and Ninness, Brett},
  year     = {2012},
  keywords = {Graphics Processing Unit, Markov Chain Monte Carlo, Nonlinear dynamical systems, nonlinear estimation, parallel computation, Particle filter},
  pages    = {1143--1148},
  annote   = {16th IFAC Symposium on System Identification}
}

@article{terenin_gpu-accelerated_2019,
  title      = {{GPU}-accelerated {Gibbs} sampling: a case study of the {Horseshoe} {Probit} model},
  volume     = {29},
  issn       = {0960-3174, 1573-1375},
  shorttitle = {{GPU}-accelerated {Gibbs} sampling},
  url        = {http://link.springer.com/10.1007/s11222-018-9809-3},
  doi        = {10.1007/s11222-018-9809-3},
  language   = {en},
  number     = {2},
  urldate    = {2023-06-28},
  journal    = {Statistics and Computing},
  author     = {Terenin, Alexander and Dong, Shawfeng and Draper, David},
  month      = mar,
  year       = {2019},
  pages      = {301--310}
}

@inproceedings{radosavljevic_optimized_2005,
  address   = {Pacific Grove, California},
  title     = {Optimized {Message} {Passing} {Schedules} for {LDPC} {Decoding}},
  isbn      = {978-1-4244-0131-4},
  url       = {http://ieeexplore.ieee.org/document/1599818/},
  doi       = {10.1109/ACSSC.2005.1599818},
  urldate   = {2023-03-30},
  booktitle = {Conference {Record} of the {Thirty}-{Ninth} {Asilomar} {
               Conference} {onSignals}, {Systems} and {Computers}, 2005.},
  publisher = {IEEE},
  author    = {Radosavljevic, P. and de Baynast, A. and Cavallaro, J.R.},
  year      = {2005},
  pages     = {591--595}
}

@inproceedings{sharon_efficient_2004,
  address   = {Tel-Aviv, Israel},
  title     = {An efficient message-passing schedule for {LDPC} decoding},
  isbn      = {978-0-7803-8427-9},
  url       = {http://ieeexplore.ieee.org/document/1361130/},
  doi       = {10.1109/EEEI.2004.1361130},
  urldate   = {2023-03-30},
  booktitle = {2004 23rd {IEEE} {Convention} of {Electrical} and {Electronics} {
               Engineers} in {Israel}},
  publisher = {IEEE},
  author    = {Sharon, E. and Litsyn, S. and Goldberger, J.},
  year      = {2004},
  pages     = {223--226}
}

@misc{van_de_meent_introduction_2021,
  title     = {An {Introduction} to {Probabilistic} {Programming}},
  url       = {http://arxiv.org/abs/1809.10756},
  abstract  = {This book is a graduate-level introduction to probabilistic programming. It not only provides a thorough background for anyone wishing to use a probabilistic programming system, but also introduces the techniques needed to design and build these systems. It is aimed at people who have an undergraduate-level understanding of either or, ideally, both probabilistic machine learning and programming languages. We start with a discussion of model-based reasoning and explain why conditioning is a foundational computation central to the fields of probabilistic machine learning and artificial intelligence. We then introduce a first-order probabilistic programming language (PPL) whose programs correspond to graphical models with a known, finite, set of random variables. In the context of this PPL we introduce fundamental inference algorithms and describe how they can be implemented. We then turn to higher-order probabilistic programming languages. Programs in such languages can define models with dynamic computation graphs, which may not instantiate the same set of random variables in each execution. Inference requires methods that generate samples by repeatedly evaluating the program. Foundational algorithms for this kind of language are discussed in the context of an interface between program executions and an inference controller. Finally we consider the intersection of probabilistic and differentiable programming. We begin with a discussion of automatic differentiation, and how it can be used to implement efficient inference methods based on Hamiltonian Monte Carlo. We then discuss gradient-based maximum likelihood estimation in programs that are parameterized using neural networks, how to amortize inference using by learning neural approximations to the program posterior, and how language features impact the design of deep probabilistic programming systems.},
  urldate   = {2023-06-28},
  publisher = {arXiv},
  author    = {van de Meent, Jan-Willem and Paige, Brooks and Yang, Hongseok and Wood, Frank},
  month     = oct,
  year      = {2021},
  note      = {arXiv:1809.10756 [cs, stat]},
  keywords  = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Programming Languages, Statistics - Machine Learning},
  annote    = {Comment: Under review at Foundations and Trends in Machine Learning}
}

@incollection{appice_message_2015,
  address   = {Cham},
  title     = {Message {Scheduling} {Methods} for {Belief} {Propagation}},
  volume    = {9285},
  isbn      = {978-3-319-23524-0 978-3-319-23525-7},
  url       = {http://link.springer.com/10.1007/978-3-319-23525-7_18},
  language  = {en},
  urldate   = {2023-03-30},
  booktitle = {Machine {Learning} and {Knowledge} {Discovery} in {Databases}},
  publisher = {Springer International Publishing},
  author    = {Knoll, Christian and Rath, Michael and Tschiatschek, Sebastian and
               Pernkopf, Franz},
  editor    = {Appice, Annalisa and Rodrigues, Pedro Pereira and Santos Costa,
               Vítor and Gama, João and Jorge, Alípio and Soares, Carlos},
  year      = {2015},
  doi       = {10.1007/978-3-319-23525-7_18},
  note      = {Series Title: Lecture Notes in Computer Science},
  pages     = {295--310}
}

@article{bainomugisha_reactive_survey_2013,
  author     = {Bainomugisha, Engineer and Carreton, Andoni Lombide and Cutsem, Tom
                van and Mostinckx, Stijn and Meuter, Wolfgang de},
  title      = {A Survey on Reactive Programming},
  year       = {2013},
  issue_date = {August 2013},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = {45},
  number     = {4},
  issn       = {0360-0300},
  url        = {https://doi.org/10.1145/2501654.2501666},
  doi        = {10.1145/2501654.2501666},
  abstract   = {Reactive programming has recently gained popularity as a paradigm
                that is well-suited for developing event-driven and interactive
                applications. It facilitates the development of such applications
                by providing abstractions to express time-varying values and
                automatically managing dependencies between such values. A number
                of approaches have been recently proposed embedded in various
                languages such as Haskell, Scheme, JavaScript, Java, .NET, etc.
                This survey describes and provides a taxonomy of existing reactive
                programming approaches along six axes: representation of
                time-varying values, evaluation model, lifting operations,
                multidirectionality, glitch avoidance, and support for
                distribution. From this taxonomy, we observe that there are still
                open challenges in the field of reactive programming. For instance,
                multidirectionality is supported only by a small number of
                languages, which do not automatically track dependencies between
                time-varying values. Similarly, glitch avoidance, which is subtle
                in reactive programs, cannot be ensured in distributed reactive
                programs using the current techniques.},
  journal    = {ACM Comput. Surv.},
  month      = {aug},
  articleno  = {52},
  numpages   = {34},
  keywords   = {event-driven applications, reactive systems, dataflow programming,
                Reactive programming, interactive applications, functional reactive
                programming}
}

@article{zhang_unifying_2021,
  author  = {Zhang, Dan and Song, Xiaohang and Wang, Wenjin and Fettweis, Gerhard
             and Gao, Xiqi},
  journal = {IEEE Transactions on Wireless Communications},
  title   = {Unifying Message Passing Algorithms Under the Framework of
             Constrained {Bethe} Free Energy Minimization},
  year    = {2021},
  volume  = {20},
  number  = {7},
  pages   = {4144-4158}
}

@article{sims_modelling_2021,
  title      = {Modelling ourselves: what the free energy principle reveals about our implicit notions of representation},
  volume     = {199},
  issn       = {0039-7857, 1573-0964},
  shorttitle = {Modelling ourselves},
  url        = {https://link.springer.com/10.1007/s11229-021-03140-5},
  doi        = {10.1007/s11229-021-03140-5},
  abstract   = {Abstract
                
                Predictive processing theories are increasingly popular in philosophy of mind; such process theories often gain support from the Free Energy Principle (FEP)—a normative principle for adaptive self-organized systems. Yet there is a current and much discussed debate about conflicting philosophical interpretations of FEP, e.g., representational versus non-representational. Here we argue that these different interpretations depend on implicit assumptions about what qualifies (or fails to qualify) as representational. We deploy the Free Energy Principle (FEP) instrumentally to distinguish four main notions of representation, which focus on organizational, structural, content-related and functional aspects, respectively. The various ways that these different aspects matter in arriving at representational or non-representational interpretations of the Free Energy Principle are discussed. We also discuss how the Free Energy Principle may be seen as a unified view where terms that traditionally belong to different ontologies—e.g., notions of model and expectation versus notions of autopoiesis and synchronization—can be harmonized. However, rather than attempting to settle the representationalist versus non-representationalist debate and reveal something about what representations are
                simpliciter
                , this paper demonstrates how the Free Energy Principle may be used to reveal something about those partaking in the debate; namely, what
                our
                hidden assumptions about what representations are—assumptions that act as sometimes antithetical starting points in this persistent philosophical debate.},
  language   = {en},
  number     = {3-4},
  urldate    = {2023-07-11},
  journal    = {Synthese},
  author     = {Sims, Matt and Pezzulo, Giovanni},
  month      = dec,
  year       = {2021},
  pages      = {7801--7833}
}